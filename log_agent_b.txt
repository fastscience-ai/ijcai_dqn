(10,)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 10)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               5632      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 5)                 2565      
_________________________________________________________________
activation_3 (Activation)    (None, 5)                 0         
=================================================================
Total params: 270,853
Trainable params: 270,853
Non-trainable params: 0
_________________________________________________________________
None
load model
Training for 500000 steps ...
[F[K     39/500000: episode: 1, duration: 0.562s, episode steps: 39, steps per second: 69, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.498 [0.470, 0.510], loss: 0.880180, mean_absolute_error: 82.667548, mean_q: 104.039556
[F[K    106/500000: episode: 2, duration: 0.322s, episode steps: 67, steps per second: 208, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.284 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 13.879645, mean_absolute_error: 77.133308, mean_q: 97.586159
[F[K    146/500000: episode: 3, duration: 0.208s, episode steps: 40, steps per second: 193, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.625 [0.000, 3.000], mean observation: 0.517 [0.470, 0.630], loss: 19.687922, mean_absolute_error: 74.115128, mean_q: 94.512253
[F[K    198/500000: episode: 4, duration: 0.291s, episode steps: 52, steps per second: 179, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.365 [0.000, 4.000], mean observation: 0.497 [0.360, 0.590], loss: 22.968493, mean_absolute_error: 72.733223, mean_q: 92.759552
[F[K    258/500000: episode: 5, duration: 0.335s, episode steps: 60, steps per second: 179, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.683 [0.000, 4.000], mean observation: 0.509 [0.460, 0.620], loss: 22.424034, mean_absolute_error: 70.933311, mean_q: 90.589096
[F[K    331/500000: episode: 6, duration: 0.404s, episode steps: 73, steps per second: 180, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.548 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 17.801552, mean_absolute_error: 70.952011, mean_q: 90.811714
[F[K    384/500000: episode: 7, duration: 0.352s, episode steps: 53, steps per second: 150, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.113 [0.000, 4.000], mean observation: 0.518 [0.490, 0.590], loss: 16.835354, mean_absolute_error: 69.947899, mean_q: 89.650360
[F[K    443/500000: episode: 8, duration: 0.342s, episode steps: 59, steps per second: 173, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.407 [0.000, 4.000], mean observation: 0.506 [0.460, 0.610], loss: 25.652506, mean_absolute_error: 70.299965, mean_q: 90.050026
[F[K    558/500000: episode: 9, duration: 0.696s, episode steps: 115, steps per second: 165, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.904 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 18.886782, mean_absolute_error: 71.445923, mean_q: 91.824440
[F[K    614/500000: episode: 10, duration: 0.358s, episode steps: 56, steps per second: 157, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.339 [0.000, 4.000], mean observation: 0.476 [0.350, 0.530], loss: 19.104254, mean_absolute_error: 70.787621, mean_q: 91.110245
[F[K    650/500000: episode: 11, duration: 0.258s, episode steps: 36, steps per second: 140, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.917 [0.000, 4.000], mean observation: 0.499 [0.390, 0.580], loss: 16.133413, mean_absolute_error: 69.405670, mean_q: 89.538742
[F[K    705/500000: episode: 12, duration: 0.380s, episode steps: 55, steps per second: 145, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.492 [0.370, 0.590], loss: 17.175287, mean_absolute_error: 69.458130, mean_q: 89.527885
[F[K    764/500000: episode: 13, duration: 0.382s, episode steps: 59, steps per second: 155, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.763 [0.000, 4.000], mean observation: 0.510 [0.490, 0.550], loss: 18.289421, mean_absolute_error: 68.370811, mean_q: 88.106438
[F[K    811/500000: episode: 14, duration: 0.338s, episode steps: 47, steps per second: 139, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.319 [0.000, 4.000], mean observation: 0.506 [0.430, 0.630], loss: 16.321476, mean_absolute_error: 69.228569, mean_q: 89.306267
[F[K    873/500000: episode: 15, duration: 0.490s, episode steps: 62, steps per second: 127, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 0.505 [0.440, 0.610], loss: 21.186390, mean_absolute_error: 68.040779, mean_q: 87.802742
[F[K    924/500000: episode: 16, duration: 0.433s, episode steps: 51, steps per second: 118, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.804 [0.000, 4.000], mean observation: 0.517 [0.500, 0.590], loss: 18.083292, mean_absolute_error: 68.694740, mean_q: 88.739487
[F[K    994/500000: episode: 17, duration: 0.551s, episode steps: 70, steps per second: 127, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.214 [0.000, 4.000], mean observation: 0.500 [0.400, 0.580], loss: 17.236725, mean_absolute_error: 69.384926, mean_q: 89.445671
[F[K   1039/500000: episode: 18, duration: 0.336s, episode steps: 45, steps per second: 134, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.244 [0.000, 4.000], mean observation: 0.510 [0.470, 0.630], loss: 22.778107, mean_absolute_error: 68.358856, mean_q: 88.181862
[F[K   1087/500000: episode: 19, duration: 0.359s, episode steps: 48, steps per second: 134, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.504 [0.440, 0.580], loss: 27.564392, mean_absolute_error: 67.993362, mean_q: 87.612160
[F[K   1142/500000: episode: 20, duration: 0.474s, episode steps: 55, steps per second: 116, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.382 [0.000, 4.000], mean observation: 0.493 [0.390, 0.540], loss: 26.385689, mean_absolute_error: 68.449318, mean_q: 88.251793
[F[K   1173/500000: episode: 21, duration: 0.242s, episode steps: 31, steps per second: 128, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.032 [0.000, 3.000], mean observation: 0.525 [0.470, 0.640], loss: 16.765648, mean_absolute_error: 68.485413, mean_q: 88.582108
[F[K   1228/500000: episode: 22, duration: 0.466s, episode steps: 55, steps per second: 118, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.618 [0.000, 4.000], mean observation: 0.492 [0.370, 0.560], loss: 24.643671, mean_absolute_error: 67.610962, mean_q: 87.344002
[F[K   1293/500000: episode: 23, duration: 0.558s, episode steps: 65, steps per second: 116, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.354 [0.000, 4.000], mean observation: 0.514 [0.490, 0.570], loss: 20.569139, mean_absolute_error: 67.116234, mean_q: 86.862595
[F[K   1328/500000: episode: 24, duration: 0.301s, episode steps: 35, steps per second: 116, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.543 [0.000, 3.000], mean observation: 0.515 [0.470, 0.630], loss: 22.282202, mean_absolute_error: 69.045525, mean_q: 88.968140
[F[K   1376/500000: episode: 25, duration: 0.341s, episode steps: 48, steps per second: 141, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.708 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 19.193592, mean_absolute_error: 67.658203, mean_q: 87.373253
[F[K   1415/500000: episode: 26, duration: 0.328s, episode steps: 39, steps per second: 119, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.795 [0.000, 4.000], mean observation: 0.499 [0.430, 0.530], loss: 26.343632, mean_absolute_error: 67.018974, mean_q: 86.343956
[F[K   1497/500000: episode: 27, duration: 0.609s, episode steps: 82, steps per second: 135, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.519 [0.470, 0.630], loss: 20.510832, mean_absolute_error: 67.208099, mean_q: 86.685211
[F[K   1558/500000: episode: 28, duration: 0.503s, episode steps: 61, steps per second: 121, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.820 [0.000, 4.000], mean observation: 0.478 [0.350, 0.520], loss: 20.377731, mean_absolute_error: 66.433907, mean_q: 85.806015
[F[K   1608/500000: episode: 29, duration: 0.431s, episode steps: 50, steps per second: 116, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.300 [0.000, 4.000], mean observation: 0.508 [0.490, 0.540], loss: 22.971098, mean_absolute_error: 66.516258, mean_q: 85.669083
[F[K   1661/500000: episode: 30, duration: 0.513s, episode steps: 53, steps per second: 103, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.499 [0.420, 0.570], loss: 19.090237, mean_absolute_error: 66.528770, mean_q: 85.780380
[F[K   1694/500000: episode: 31, duration: 0.296s, episode steps: 33, steps per second: 111, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.526 [0.470, 0.660], loss: 17.293459, mean_absolute_error: 65.876732, mean_q: 85.043549
[F[K   1728/500000: episode: 32, duration: 0.289s, episode steps: 34, steps per second: 118, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.526 [0.470, 0.660], loss: 21.272814, mean_absolute_error: 65.055290, mean_q: 84.001404
[F[K   1766/500000: episode: 33, duration: 0.348s, episode steps: 38, steps per second: 109, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.421 [0.000, 4.000], mean observation: 0.487 [0.350, 0.530], loss: 20.877584, mean_absolute_error: 63.863056, mean_q: 82.543144
[F[K   1817/500000: episode: 34, duration: 0.428s, episode steps: 51, steps per second: 119, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.255 [0.000, 4.000], mean observation: 0.482 [0.370, 0.530], loss: 18.053028, mean_absolute_error: 64.370667, mean_q: 83.158867
[F[K   1867/500000: episode: 35, duration: 0.477s, episode steps: 50, steps per second: 105, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.080 [0.000, 4.000], mean observation: 0.511 [0.490, 0.540], loss: 20.567163, mean_absolute_error: 63.110260, mean_q: 81.619217
[F[K   1943/500000: episode: 36, duration: 0.708s, episode steps: 76, steps per second: 107, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.158 [0.000, 4.000], mean observation: 0.492 [0.440, 0.530], loss: 22.172764, mean_absolute_error: 63.808884, mean_q: 82.270958
[F[K   2002/500000: episode: 37, duration: 0.470s, episode steps: 59, steps per second: 126, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.610 [0.000, 4.000], mean observation: 0.513 [0.490, 0.580], loss: 18.534985, mean_absolute_error: 63.209049, mean_q: 81.643982
[F[K   2081/500000: episode: 38, duration: 0.664s, episode steps: 79, steps per second: 119, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.772 [0.000, 4.000], mean observation: 0.515 [0.470, 0.590], loss: 19.313038, mean_absolute_error: 63.943935, mean_q: 82.367027
[F[K   2134/500000: episode: 39, duration: 0.532s, episode steps: 53, steps per second: 100, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.283 [0.000, 4.000], mean observation: 0.495 [0.430, 0.530], loss: 19.557255, mean_absolute_error: 64.551079, mean_q: 83.153641
[F[K   2185/500000: episode: 40, duration: 0.502s, episode steps: 51, steps per second: 102, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.039 [0.000, 4.000], mean observation: 0.474 [0.360, 0.510], loss: 20.934614, mean_absolute_error: 63.004238, mean_q: 81.083946
[F[K   2236/500000: episode: 41, duration: 0.496s, episode steps: 51, steps per second: 103, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.353 [0.000, 4.000], mean observation: 0.502 [0.450, 0.580], loss: 20.413771, mean_absolute_error: 63.914658, mean_q: 82.139168
[F[K   2289/500000: episode: 42, duration: 0.520s, episode steps: 53, steps per second: 102, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.508 [0.430, 0.620], loss: 18.256140, mean_absolute_error: 62.427929, mean_q: 80.393822
[F[K   2338/500000: episode: 43, duration: 0.481s, episode steps: 49, steps per second: 102, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.980 [0.000, 3.000], mean observation: 0.498 [0.360, 0.640], loss: 18.111843, mean_absolute_error: 61.671169, mean_q: 79.499809
[F[K   2400/500000: episode: 44, duration: 0.607s, episode steps: 62, steps per second: 102, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.371 [0.000, 4.000], mean observation: 0.496 [0.360, 0.590], loss: 20.787905, mean_absolute_error: 61.600193, mean_q: 79.228798
[F[K   2471/500000: episode: 45, duration: 0.746s, episode steps: 71, steps per second: 95, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.535 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 17.302610, mean_absolute_error: 61.296745, mean_q: 78.971634
[F[K   2529/500000: episode: 46, duration: 0.519s, episode steps: 58, steps per second: 112, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.276 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 20.024008, mean_absolute_error: 61.665024, mean_q: 79.238411
[F[K   2586/500000: episode: 47, duration: 0.554s, episode steps: 57, steps per second: 103, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.737 [0.000, 4.000], mean observation: 0.520 [0.480, 0.610], loss: 22.808535, mean_absolute_error: 62.260616, mean_q: 79.890152
[F[K   2642/500000: episode: 48, duration: 0.617s, episode steps: 56, steps per second: 91, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.511 [0.460, 0.650], loss: 16.522493, mean_absolute_error: 60.816223, mean_q: 78.165657
[F[K   2696/500000: episode: 49, duration: 0.561s, episode steps: 54, steps per second: 96, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.426 [0.000, 4.000], mean observation: 0.510 [0.490, 0.570], loss: 18.632393, mean_absolute_error: 60.218513, mean_q: 77.512886
[F[K   2771/500000: episode: 50, duration: 0.800s, episode steps: 75, steps per second: 94, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.187 [0.000, 4.000], mean observation: 0.501 [0.440, 0.570], loss: 17.380856, mean_absolute_error: 60.592903, mean_q: 77.932106
[F[K   2837/500000: episode: 51, duration: 0.667s, episode steps: 66, steps per second: 99, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.455 [0.000, 4.000], mean observation: 0.498 [0.470, 0.520], loss: 20.543816, mean_absolute_error: 60.256042, mean_q: 77.362961
[F[K   2899/500000: episode: 52, duration: 0.598s, episode steps: 62, steps per second: 104, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.806 [0.000, 4.000], mean observation: 0.501 [0.440, 0.560], loss: 16.572632, mean_absolute_error: 60.548504, mean_q: 77.762627
[F[K   2952/500000: episode: 53, duration: 0.555s, episode steps: 53, steps per second: 96, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.830 [0.000, 4.000], mean observation: 0.487 [0.420, 0.530], loss: 17.098270, mean_absolute_error: 60.066666, mean_q: 77.110466
[F[K   3006/500000: episode: 54, duration: 0.572s, episode steps: 54, steps per second: 94, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.870 [0.000, 4.000], mean observation: 0.513 [0.500, 0.590], loss: 16.774277, mean_absolute_error: 61.310596, mean_q: 78.728554
[F[K   3047/500000: episode: 55, duration: 0.388s, episode steps: 41, steps per second: 106, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.488 [0.420, 0.530], loss: 15.824840, mean_absolute_error: 60.663567, mean_q: 77.845352
[F[K   3100/500000: episode: 56, duration: 0.525s, episode steps: 53, steps per second: 101, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.321 [0.000, 4.000], mean observation: 0.477 [0.370, 0.530], loss: 19.554564, mean_absolute_error: 59.431068, mean_q: 76.165756
[F[K   3142/500000: episode: 57, duration: 0.445s, episode steps: 42, steps per second: 94, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.905 [0.000, 4.000], mean observation: 0.494 [0.360, 0.560], loss: 18.297024, mean_absolute_error: 59.917915, mean_q: 76.660286
[F[K   3181/500000: episode: 58, duration: 0.452s, episode steps: 39, steps per second: 86, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.795 [0.000, 4.000], mean observation: 0.513 [0.470, 0.640], loss: 19.482347, mean_absolute_error: 58.924988, mean_q: 75.554794
[F[K   3243/500000: episode: 59, duration: 0.670s, episode steps: 62, steps per second: 93, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.048 [0.000, 4.000], mean observation: 0.509 [0.480, 0.590], loss: 17.828171, mean_absolute_error: 58.577408, mean_q: 75.256775
[F[K   3327/500000: episode: 60, duration: 0.912s, episode steps: 84, steps per second: 92, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 18.155529, mean_absolute_error: 58.355858, mean_q: 74.747231
[F[K   3387/500000: episode: 61, duration: 0.669s, episode steps: 60, steps per second: 90, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.817 [0.000, 4.000], mean observation: 0.504 [0.450, 0.590], loss: 18.174109, mean_absolute_error: 58.671459, mean_q: 75.236595
[F[K   3438/500000: episode: 62, duration: 0.605s, episode steps: 51, steps per second: 84, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.294 [0.000, 4.000], mean observation: 0.506 [0.460, 0.560], loss: 17.938276, mean_absolute_error: 57.942101, mean_q: 74.471443
[F[K   3495/500000: episode: 63, duration: 0.675s, episode steps: 57, steps per second: 84, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.511 [0.470, 0.580], loss: 15.914855, mean_absolute_error: 57.489498, mean_q: 73.784363
[F[K   3548/500000: episode: 64, duration: 0.637s, episode steps: 53, steps per second: 83, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.925 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 18.601761, mean_absolute_error: 58.011135, mean_q: 74.278755
[F[K   3586/500000: episode: 65, duration: 0.407s, episode steps: 38, steps per second: 93, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: 0.515 [0.470, 0.640], loss: 18.686935, mean_absolute_error: 56.841816, mean_q: 72.857491
[F[K   3637/500000: episode: 66, duration: 0.522s, episode steps: 51, steps per second: 98, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.784 [0.000, 4.000], mean observation: 0.519 [0.470, 0.620], loss: 17.627434, mean_absolute_error: 56.099293, mean_q: 71.972549
[F[K   3684/500000: episode: 67, duration: 0.539s, episode steps: 47, steps per second: 87, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.489 [0.000, 4.000], mean observation: 0.498 [0.360, 0.620], loss: 17.606218, mean_absolute_error: 56.769249, mean_q: 72.912010
[F[K   3753/500000: episode: 68, duration: 0.729s, episode steps: 69, steps per second: 95, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.391 [0.000, 4.000], mean observation: 0.498 [0.440, 0.530], loss: 17.855026, mean_absolute_error: 56.252140, mean_q: 72.187859
[F[K   3810/500000: episode: 69, duration: 0.708s, episode steps: 57, steps per second: 81, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.491 [0.000, 4.000], mean observation: 0.498 [0.460, 0.530], loss: 17.815554, mean_absolute_error: 55.148472, mean_q: 70.861671
[F[K   3871/500000: episode: 70, duration: 0.684s, episode steps: 61, steps per second: 89, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.494 [0.450, 0.520], loss: 15.964568, mean_absolute_error: 55.890232, mean_q: 71.825706
[F[K   3919/500000: episode: 71, duration: 0.613s, episode steps: 48, steps per second: 78, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.042 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 16.789753, mean_absolute_error: 55.838207, mean_q: 71.532509
[F[K   3965/500000: episode: 72, duration: 0.498s, episode steps: 46, steps per second: 92, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.522 [0.000, 3.000], mean observation: 0.519 [0.470, 0.620], loss: 16.141363, mean_absolute_error: 55.935818, mean_q: 71.796638
[F[K   3999/500000: episode: 73, duration: 0.364s, episode steps: 34, steps per second: 93, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.088 [0.000, 3.000], mean observation: 0.518 [0.470, 0.640], loss: 16.710768, mean_absolute_error: 55.066235, mean_q: 70.647011
[F[K   4049/500000: episode: 74, duration: 0.575s, episode steps: 50, steps per second: 87, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.540 [0.000, 4.000], mean observation: 0.500 [0.460, 0.530], loss: 20.350786, mean_absolute_error: 55.114456, mean_q: 70.585663
[F[K   4096/500000: episode: 75, duration: 0.517s, episode steps: 47, steps per second: 91, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.170 [0.000, 4.000], mean observation: 0.483 [0.390, 0.520], loss: 16.298014, mean_absolute_error: 54.425549, mean_q: 69.726013
[F[K   4152/500000: episode: 76, duration: 0.666s, episode steps: 56, steps per second: 84, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.536 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 18.612728, mean_absolute_error: 54.433922, mean_q: 69.765678
[F[K   4200/500000: episode: 77, duration: 0.581s, episode steps: 48, steps per second: 83, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.979 [0.000, 4.000], mean observation: 0.508 [0.470, 0.550], loss: 16.192327, mean_absolute_error: 55.570316, mean_q: 71.241646
[F[K   4262/500000: episode: 78, duration: 0.795s, episode steps: 62, steps per second: 78, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.468 [0.000, 4.000], mean observation: 0.496 [0.410, 0.560], loss: 18.384441, mean_absolute_error: 53.402592, mean_q: 68.543800
[F[K   4328/500000: episode: 79, duration: 0.804s, episode steps: 66, steps per second: 82, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.030 [0.000, 4.000], mean observation: 0.500 [0.400, 0.600], loss: 19.420685, mean_absolute_error: 54.744450, mean_q: 70.129105
[F[K   4386/500000: episode: 80, duration: 0.605s, episode steps: 58, steps per second: 96, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.586 [0.000, 4.000], mean observation: 0.511 [0.470, 0.640], loss: 17.723961, mean_absolute_error: 52.696232, mean_q: 67.501846
[F[K   4441/500000: episode: 81, duration: 0.617s, episode steps: 55, steps per second: 89, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.782 [0.000, 4.000], mean observation: 0.503 [0.410, 0.630], loss: 17.275028, mean_absolute_error: 52.348324, mean_q: 67.103676
[F[K   4509/500000: episode: 82, duration: 0.855s, episode steps: 68, steps per second: 80, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.838 [0.000, 4.000], mean observation: 0.496 [0.390, 0.530], loss: 18.463980, mean_absolute_error: 51.609161, mean_q: 66.338028
[F[K   4568/500000: episode: 83, duration: 0.692s, episode steps: 59, steps per second: 85, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.678 [0.000, 4.000], mean observation: 0.507 [0.470, 0.580], loss: 17.585018, mean_absolute_error: 51.979198, mean_q: 66.524818
[F[K   4620/500000: episode: 84, duration: 0.634s, episode steps: 52, steps per second: 82, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 0.497 [0.450, 0.530], loss: 17.206089, mean_absolute_error: 51.557369, mean_q: 66.182434
[F[K   4680/500000: episode: 85, duration: 0.685s, episode steps: 60, steps per second: 88, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.267 [0.000, 4.000], mean observation: 0.494 [0.370, 0.560], loss: 17.173212, mean_absolute_error: 50.930138, mean_q: 65.332245
[F[K   4749/500000: episode: 86, duration: 0.851s, episode steps: 69, steps per second: 81, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.159 [0.000, 4.000], mean observation: 0.495 [0.460, 0.520], loss: 15.734617, mean_absolute_error: 51.529934, mean_q: 66.050591
[F[K   4799/500000: episode: 87, duration: 0.686s, episode steps: 50, steps per second: 73, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.080 [0.000, 4.000], mean observation: 0.513 [0.470, 0.600], loss: 17.538555, mean_absolute_error: 51.376080, mean_q: 65.867455
[F[K   4843/500000: episode: 88, duration: 0.538s, episode steps: 44, steps per second: 82, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.273 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 16.692116, mean_absolute_error: 50.978283, mean_q: 65.527260
[F[K   4912/500000: episode: 89, duration: 0.837s, episode steps: 69, steps per second: 82, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.507 [0.000, 4.000], mean observation: 0.494 [0.450, 0.510], loss: 19.189392, mean_absolute_error: 52.254749, mean_q: 66.936653
[F[K   4971/500000: episode: 90, duration: 0.810s, episode steps: 59, steps per second: 73, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.864 [0.000, 4.000], mean observation: 0.499 [0.410, 0.590], loss: 18.473808, mean_absolute_error: 50.540283, mean_q: 64.864197
[F[K   5032/500000: episode: 91, duration: 0.754s, episode steps: 61, steps per second: 81, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.836 [0.000, 4.000], mean observation: 0.486 [0.390, 0.520], loss: 17.997988, mean_absolute_error: 49.748478, mean_q: 63.977673
[F[K   5068/500000: episode: 92, duration: 0.475s, episode steps: 36, steps per second: 76, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.111 [0.000, 3.000], mean observation: 0.523 [0.470, 0.650], loss: 17.729296, mean_absolute_error: 49.393639, mean_q: 63.451828
[F[K   5139/500000: episode: 93, duration: 0.867s, episode steps: 71, steps per second: 82, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.493 [0.000, 4.000], mean observation: 0.502 [0.460, 0.550], loss: 18.129135, mean_absolute_error: 49.795673, mean_q: 63.850800
[F[K   5200/500000: episode: 94, duration: 0.771s, episode steps: 61, steps per second: 79, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.082 [0.000, 4.000], mean observation: 0.479 [0.370, 0.520], loss: 17.366663, mean_absolute_error: 49.735954, mean_q: 63.511211
[F[K   5238/500000: episode: 95, duration: 0.493s, episode steps: 38, steps per second: 77, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.519 [0.470, 0.630], loss: 17.007683, mean_absolute_error: 50.110645, mean_q: 64.393570
[F[K   5297/500000: episode: 96, duration: 0.794s, episode steps: 59, steps per second: 74, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.441 [0.000, 4.000], mean observation: 0.498 [0.430, 0.560], loss: 19.339018, mean_absolute_error: 50.153770, mean_q: 63.975872
[F[K   5360/500000: episode: 97, duration: 0.760s, episode steps: 63, steps per second: 83, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.746 [0.000, 4.000], mean observation: 0.523 [0.470, 0.620], loss: 18.370356, mean_absolute_error: 48.212639, mean_q: 61.649700
[F[K   5407/500000: episode: 98, duration: 0.568s, episode steps: 47, steps per second: 83, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.021 [0.000, 4.000], mean observation: 0.516 [0.470, 0.600], loss: 19.310734, mean_absolute_error: 47.648125, mean_q: 61.037552
[F[K   5481/500000: episode: 99, duration: 0.971s, episode steps: 74, steps per second: 76, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.595 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 14.887790, mean_absolute_error: 48.503429, mean_q: 62.392235
[F[K   5540/500000: episode: 100, duration: 0.746s, episode steps: 59, steps per second: 79, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.254 [0.000, 4.000], mean observation: 0.497 [0.470, 0.520], loss: 17.541403, mean_absolute_error: 47.956970, mean_q: 61.562508
[F[K   5596/500000: episode: 101, duration: 0.706s, episode steps: 56, steps per second: 79, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.089 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 17.460556, mean_absolute_error: 47.818245, mean_q: 61.282558
[F[K   5625/500000: episode: 102, duration: 0.386s, episode steps: 29, steps per second: 75, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.379 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 16.851357, mean_absolute_error: 47.591797, mean_q: 60.982719
[F[K   5761/500000: episode: 103, duration: 1.645s, episode steps: 136, steps per second: 83, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.257 [0.000, 4.000], mean observation: 0.491 [0.330, 0.560], loss: 15.999167, mean_absolute_error: 47.543663, mean_q: 61.082153
[F[K   5819/500000: episode: 104, duration: 0.741s, episode steps: 58, steps per second: 78, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.724 [0.000, 4.000], mean observation: 0.495 [0.420, 0.530], loss: 15.983768, mean_absolute_error: 47.596813, mean_q: 61.019085
[F[K   5916/500000: episode: 105, duration: 1.237s, episode steps: 97, steps per second: 78, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.979 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 15.556629, mean_absolute_error: 47.145679, mean_q: 60.706306
[F[K   5968/500000: episode: 106, duration: 0.701s, episode steps: 52, steps per second: 74, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.231 [0.000, 4.000], mean observation: 0.514 [0.470, 0.620], loss: 17.177650, mean_absolute_error: 46.893856, mean_q: 60.169617
[F[K   6039/500000: episode: 107, duration: 0.966s, episode steps: 71, steps per second: 73, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.380 [0.000, 4.000], mean observation: 0.501 [0.420, 0.570], loss: 13.920614, mean_absolute_error: 47.507843, mean_q: 61.245804
[F[K   6101/500000: episode: 108, duration: 0.875s, episode steps: 62, steps per second: 71, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.919 [0.000, 4.000], mean observation: 0.492 [0.430, 0.520], loss: 16.846048, mean_absolute_error: 46.728252, mean_q: 59.969116
[F[K   6155/500000: episode: 109, duration: 0.792s, episode steps: 54, steps per second: 68, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.611 [0.000, 4.000], mean observation: 0.500 [0.400, 0.630], loss: 17.921068, mean_absolute_error: 46.537067, mean_q: 59.766556
[F[K   6216/500000: episode: 110, duration: 0.877s, episode steps: 61, steps per second: 70, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.246 [0.000, 4.000], mean observation: 0.504 [0.450, 0.600], loss: 14.490175, mean_absolute_error: 46.608723, mean_q: 59.924973
[F[K   6273/500000: episode: 111, duration: 0.798s, episode steps: 57, steps per second: 71, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.211 [0.000, 4.000], mean observation: 0.490 [0.420, 0.520], loss: 15.756515, mean_absolute_error: 47.217968, mean_q: 60.467010
[F[K   6320/500000: episode: 112, duration: 0.691s, episode steps: 47, steps per second: 68, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.191 [0.000, 4.000], mean observation: 0.475 [0.360, 0.530], loss: 15.078183, mean_absolute_error: 46.765514, mean_q: 60.131130
[F[K   6371/500000: episode: 113, duration: 0.765s, episode steps: 51, steps per second: 67, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.509 [0.460, 0.630], loss: 16.414736, mean_absolute_error: 45.348045, mean_q: 58.490719
[F[K   6420/500000: episode: 114, duration: 0.680s, episode steps: 49, steps per second: 72, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.286 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 16.159193, mean_absolute_error: 45.885242, mean_q: 59.107841
[F[K   6496/500000: episode: 115, duration: 1.143s, episode steps: 76, steps per second: 67, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.645 [0.000, 4.000], mean observation: 0.513 [0.470, 0.610], loss: 13.104489, mean_absolute_error: 46.156464, mean_q: 59.355564
[F[K   6543/500000: episode: 116, duration: 0.632s, episode steps: 47, steps per second: 74, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.515 [0.470, 0.640], loss: 16.720766, mean_absolute_error: 45.821629, mean_q: 58.800247
[F[K   6599/500000: episode: 117, duration: 0.795s, episode steps: 56, steps per second: 70, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.661 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 15.032510, mean_absolute_error: 45.634487, mean_q: 58.902813
[F[K   6658/500000: episode: 118, duration: 0.809s, episode steps: 59, steps per second: 73, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.509 [0.470, 0.570], loss: 16.025370, mean_absolute_error: 45.482559, mean_q: 58.687859
[F[K   6705/500000: episode: 119, duration: 0.693s, episode steps: 47, steps per second: 68, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.660 [0.000, 3.000], mean observation: 0.498 [0.370, 0.620], loss: 16.849648, mean_absolute_error: 44.277153, mean_q: 56.875973
[F[K   6743/500000: episode: 120, duration: 0.547s, episode steps: 38, steps per second: 70, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.499 [0.360, 0.640], loss: 12.672407, mean_absolute_error: 45.228832, mean_q: 58.215565
[F[K   6804/500000: episode: 121, duration: 0.875s, episode steps: 61, steps per second: 70, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.213 [0.000, 4.000], mean observation: 0.489 [0.420, 0.520], loss: 16.143688, mean_absolute_error: 44.310440, mean_q: 56.945381
[F[K   6846/500000: episode: 122, duration: 0.689s, episode steps: 42, steps per second: 61, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.619 [0.000, 4.000], mean observation: 0.512 [0.470, 0.650], loss: 14.472287, mean_absolute_error: 44.609497, mean_q: 57.393024
[F[K   6909/500000: episode: 123, duration: 0.927s, episode steps: 63, steps per second: 68, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.651 [0.000, 4.000], mean observation: 0.512 [0.470, 0.620], loss: 15.698507, mean_absolute_error: 43.845039, mean_q: 56.242218
[F[K   6946/500000: episode: 124, duration: 0.637s, episode steps: 37, steps per second: 58, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.243 [0.000, 4.000], mean observation: 0.520 [0.470, 0.630], loss: 15.895395, mean_absolute_error: 44.005009, mean_q: 56.397449
[F[K   7001/500000: episode: 125, duration: 0.812s, episode steps: 55, steps per second: 68, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.545 [0.000, 4.000], mean observation: 0.498 [0.390, 0.610], loss: 14.734184, mean_absolute_error: 43.323215, mean_q: 55.875237
[F[K   7097/500000: episode: 126, duration: 1.428s, episode steps: 96, steps per second: 67, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.510 [0.000, 4.000], mean observation: 0.487 [0.380, 0.510], loss: 14.889531, mean_absolute_error: 43.278393, mean_q: 55.783497
[F[K   7147/500000: episode: 127, duration: 0.796s, episode steps: 50, steps per second: 63, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.505 [0.400, 0.650], loss: 11.796588, mean_absolute_error: 42.061073, mean_q: 54.356293
[F[K   7203/500000: episode: 128, duration: 0.914s, episode steps: 56, steps per second: 61, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.179 [0.000, 4.000], mean observation: 0.503 [0.410, 0.630], loss: 14.019052, mean_absolute_error: 45.163967, mean_q: 58.062199
[F[K   7259/500000: episode: 129, duration: 0.949s, episode steps: 56, steps per second: 59, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.286 [0.000, 4.000], mean observation: 0.503 [0.410, 0.630], loss: 15.817263, mean_absolute_error: 44.257587, mean_q: 56.863247
[F[K   7324/500000: episode: 130, duration: 0.995s, episode steps: 65, steps per second: 65, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.708 [0.000, 4.000], mean observation: 0.498 [0.470, 0.520], loss: 13.243170, mean_absolute_error: 42.795040, mean_q: 55.180386
[F[K   7394/500000: episode: 131, duration: 1.198s, episode steps: 70, steps per second: 58, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.496 [0.470, 0.520], loss: 14.220540, mean_absolute_error: 42.609558, mean_q: 54.975800
[F[K   7448/500000: episode: 132, duration: 0.852s, episode steps: 54, steps per second: 63, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.259 [0.000, 4.000], mean observation: 0.505 [0.470, 0.570], loss: 12.715763, mean_absolute_error: 42.393658, mean_q: 54.710285
[F[K   7520/500000: episode: 133, duration: 1.252s, episode steps: 72, steps per second: 58, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.778 [0.000, 4.000], mean observation: 0.508 [0.470, 0.560], loss: 14.485814, mean_absolute_error: 42.236393, mean_q: 54.559937
[F[K   7578/500000: episode: 134, duration: 0.825s, episode steps: 58, steps per second: 70, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.493 [0.470, 0.520], loss: 14.713013, mean_absolute_error: 41.967583, mean_q: 54.138561
[F[K   7618/500000: episode: 135, duration: 0.622s, episode steps: 40, steps per second: 64, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.725 [0.000, 4.000], mean observation: 0.507 [0.450, 0.630], loss: 16.351955, mean_absolute_error: 41.581627, mean_q: 53.459923
[F[K   7657/500000: episode: 136, duration: 0.622s, episode steps: 39, steps per second: 63, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.500 [0.350, 0.640], loss: 13.353435, mean_absolute_error: 42.325745, mean_q: 54.550056
[F[K   7695/500000: episode: 137, duration: 0.621s, episode steps: 38, steps per second: 61, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: 0.516 [0.470, 0.640], loss: 14.589746, mean_absolute_error: 42.151196, mean_q: 54.491833
[F[K   7754/500000: episode: 138, duration: 0.898s, episode steps: 59, steps per second: 66, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.495 [0.430, 0.520], loss: 12.834871, mean_absolute_error: 42.062634, mean_q: 54.230473
[F[K   7807/500000: episode: 139, duration: 0.863s, episode steps: 53, steps per second: 61, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.497 [0.400, 0.540], loss: 12.124468, mean_absolute_error: 41.147320, mean_q: 53.117462
[F[K   7857/500000: episode: 140, duration: 0.752s, episode steps: 50, steps per second: 66, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.511 [0.470, 0.570], loss: 13.989605, mean_absolute_error: 42.148781, mean_q: 54.355991
[F[K   7912/500000: episode: 141, duration: 0.618s, episode steps: 55, steps per second: 89, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.345 [0.000, 4.000], mean observation: 0.493 [0.390, 0.550], loss: 15.751687, mean_absolute_error: 41.363499, mean_q: 53.289715
[F[K   7985/500000: episode: 142, duration: 0.990s, episode steps: 73, steps per second: 74, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.397 [0.000, 4.000], mean observation: 0.481 [0.380, 0.520], loss: 13.234107, mean_absolute_error: 41.125637, mean_q: 53.277306
[F[K   8041/500000: episode: 143, duration: 0.736s, episode steps: 56, steps per second: 76, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.982 [0.000, 4.000], mean observation: 0.485 [0.420, 0.520], loss: 13.261067, mean_absolute_error: 40.337166, mean_q: 52.043194
[F[K   8187/500000: episode: 144, duration: 1.885s, episode steps: 146, steps per second: 77, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.479 [0.000, 4.000], mean observation: 0.485 [0.380, 0.530], loss: 13.549891, mean_absolute_error: 40.780521, mean_q: 52.670311
[F[K   8235/500000: episode: 145, duration: 0.603s, episode steps: 48, steps per second: 80, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.487 [0.390, 0.520], loss: 12.361000, mean_absolute_error: 40.979416, mean_q: 52.866318
[F[K   8298/500000: episode: 146, duration: 0.800s, episode steps: 63, steps per second: 79, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.476 [0.000, 4.000], mean observation: 0.486 [0.420, 0.520], loss: 11.518714, mean_absolute_error: 41.330818, mean_q: 53.419693
[F[K   8341/500000: episode: 147, duration: 0.527s, episode steps: 43, steps per second: 82, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.372 [0.000, 4.000], mean observation: 0.490 [0.350, 0.530], loss: 13.502322, mean_absolute_error: 41.384083, mean_q: 53.515987
[F[K   8394/500000: episode: 148, duration: 0.718s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.943 [0.000, 4.000], mean observation: 0.495 [0.400, 0.550], loss: 11.789667, mean_absolute_error: 41.668629, mean_q: 54.132671
[F[K   8435/500000: episode: 149, duration: 0.565s, episode steps: 41, steps per second: 73, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.509 [0.470, 0.640], loss: 13.261186, mean_absolute_error: 40.431702, mean_q: 52.482845
[F[K   8491/500000: episode: 150, duration: 0.787s, episode steps: 56, steps per second: 71, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.496 [0.440, 0.530], loss: 13.029406, mean_absolute_error: 40.790871, mean_q: 52.671669
[F[K   8549/500000: episode: 151, duration: 0.769s, episode steps: 58, steps per second: 75, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.552 [0.000, 4.000], mean observation: 0.494 [0.450, 0.520], loss: 14.655252, mean_absolute_error: 40.194126, mean_q: 52.134205
[F[K   8618/500000: episode: 152, duration: 0.817s, episode steps: 69, steps per second: 84, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.623 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 13.125241, mean_absolute_error: 40.430542, mean_q: 52.359287
[F[K   8678/500000: episode: 153, duration: 0.826s, episode steps: 60, steps per second: 73, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.514 [0.470, 0.590], loss: 13.514112, mean_absolute_error: 40.287640, mean_q: 52.017727
[F[K   8719/500000: episode: 154, duration: 0.645s, episode steps: 41, steps per second: 64, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.049 [0.000, 4.000], mean observation: 0.499 [0.450, 0.540], loss: 13.596568, mean_absolute_error: 40.982784, mean_q: 52.807224
[F[K   8772/500000: episode: 155, duration: 0.696s, episode steps: 53, steps per second: 76, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.475 [0.370, 0.530], loss: 12.306231, mean_absolute_error: 40.077141, mean_q: 51.897663
[F[K   8811/500000: episode: 156, duration: 0.491s, episode steps: 39, steps per second: 79, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.410 [0.000, 3.000], mean observation: 0.523 [0.470, 0.630], loss: 12.299728, mean_absolute_error: 40.928349, mean_q: 52.718029
[F[K   8853/500000: episode: 157, duration: 0.594s, episode steps: 42, steps per second: 71, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.810 [0.000, 4.000], mean observation: 0.492 [0.410, 0.540], loss: 13.228086, mean_absolute_error: 40.221310, mean_q: 52.238705
[F[K   8918/500000: episode: 158, duration: 0.905s, episode steps: 65, steps per second: 72, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.677 [0.000, 4.000], mean observation: 0.504 [0.420, 0.620], loss: 13.734672, mean_absolute_error: 39.894382, mean_q: 51.698147
[F[K   8992/500000: episode: 159, duration: 1.150s, episode steps: 74, steps per second: 64, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.297 [0.000, 4.000], mean observation: 0.491 [0.440, 0.530], loss: 12.997526, mean_absolute_error: 39.556408, mean_q: 51.196003
[F[K   9051/500000: episode: 160, duration: 0.915s, episode steps: 59, steps per second: 64, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.407 [0.000, 4.000], mean observation: 0.509 [0.470, 0.590], loss: 13.299514, mean_absolute_error: 39.419888, mean_q: 51.049774
[F[K   9096/500000: episode: 161, duration: 0.669s, episode steps: 45, steps per second: 67, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.489 [0.000, 4.000], mean observation: 0.490 [0.360, 0.550], loss: 10.478493, mean_absolute_error: 41.117779, mean_q: 53.238113
[F[K   9155/500000: episode: 162, duration: 0.813s, episode steps: 59, steps per second: 73, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.847 [0.000, 4.000], mean observation: 0.480 [0.380, 0.530], loss: 12.260923, mean_absolute_error: 40.368732, mean_q: 52.213249
[F[K   9215/500000: episode: 163, duration: 0.943s, episode steps: 60, steps per second: 64, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.300 [0.000, 4.000], mean observation: 0.478 [0.360, 0.520], loss: 12.488225, mean_absolute_error: 40.533169, mean_q: 52.396221
[F[K   9273/500000: episode: 164, duration: 0.762s, episode steps: 58, steps per second: 76, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.914 [0.000, 4.000], mean observation: 0.487 [0.370, 0.520], loss: 13.983169, mean_absolute_error: 39.021603, mean_q: 50.618843
[F[K   9330/500000: episode: 165, duration: 0.785s, episode steps: 57, steps per second: 73, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 0.509 [0.470, 0.590], loss: 12.870174, mean_absolute_error: 40.644943, mean_q: 52.580490
[F[K   9375/500000: episode: 166, duration: 0.628s, episode steps: 45, steps per second: 72, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.133 [0.000, 4.000], mean observation: 0.481 [0.350, 0.510], loss: 11.889250, mean_absolute_error: 40.354958, mean_q: 52.039265
[F[K   9415/500000: episode: 167, duration: 0.612s, episode steps: 40, steps per second: 65, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.775 [0.000, 4.000], mean observation: 0.496 [0.450, 0.530], loss: 10.270973, mean_absolute_error: 39.115959, mean_q: 50.534225
[F[K   9471/500000: episode: 168, duration: 0.785s, episode steps: 56, steps per second: 71, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.714 [0.000, 4.000], mean observation: 0.499 [0.390, 0.600], loss: 12.992637, mean_absolute_error: 39.355679, mean_q: 50.915653
[F[K   9520/500000: episode: 169, duration: 0.596s, episode steps: 49, steps per second: 82, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.898 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 12.063504, mean_absolute_error: 38.588505, mean_q: 50.076241
[F[K   9589/500000: episode: 170, duration: 1.052s, episode steps: 69, steps per second: 66, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.739 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 13.208867, mean_absolute_error: 38.978588, mean_q: 50.506401
[F[K   9645/500000: episode: 171, duration: 0.789s, episode steps: 56, steps per second: 71, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.375 [0.000, 4.000], mean observation: 0.504 [0.470, 0.550], loss: 12.057679, mean_absolute_error: 38.546135, mean_q: 49.973423
[F[K   9694/500000: episode: 172, duration: 0.681s, episode steps: 49, steps per second: 72, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.509 [0.470, 0.550], loss: 11.046105, mean_absolute_error: 39.964874, mean_q: 51.723698
[F[K   9776/500000: episode: 173, duration: 1.129s, episode steps: 82, steps per second: 73, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.537 [0.000, 4.000], mean observation: 0.485 [0.380, 0.530], loss: 13.220515, mean_absolute_error: 40.024124, mean_q: 51.687275
[F[K   9844/500000: episode: 174, duration: 0.861s, episode steps: 68, steps per second: 79, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.618 [0.000, 4.000], mean observation: 0.497 [0.400, 0.550], loss: 11.773136, mean_absolute_error: 38.844269, mean_q: 50.328083
[F[K   9882/500000: episode: 175, duration: 0.537s, episode steps: 38, steps per second: 71, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.105 [0.000, 4.000], mean observation: 0.479 [0.350, 0.510], loss: 13.468489, mean_absolute_error: 39.326561, mean_q: 50.918522
[F[K  10007/500000: episode: 176, duration: 1.934s, episode steps: 125, steps per second: 65, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.368 [0.000, 4.000], mean observation: 0.494 [0.360, 0.570], loss: 12.285847, mean_absolute_error: 39.350853, mean_q: 51.005180
[F[K  10086/500000: episode: 177, duration: 1.202s, episode steps: 79, steps per second: 66, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.557 [0.000, 4.000], mean observation: 0.501 [0.420, 0.590], loss: 11.693971, mean_absolute_error: 38.915833, mean_q: 50.476837
[F[K  10123/500000: episode: 178, duration: 0.559s, episode steps: 37, steps per second: 66, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.432 [0.000, 2.000], mean observation: 0.525 [0.470, 0.630], loss: 13.873514, mean_absolute_error: 38.163048, mean_q: 49.516159
[F[K  10169/500000: episode: 179, duration: 0.659s, episode steps: 46, steps per second: 70, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.326 [0.000, 4.000], mean observation: 0.490 [0.400, 0.530], loss: 10.204721, mean_absolute_error: 38.707863, mean_q: 50.399792
[F[K  10256/500000: episode: 180, duration: 1.317s, episode steps: 87, steps per second: 66, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.805 [0.000, 4.000], mean observation: 0.508 [0.460, 0.570], loss: 13.440697, mean_absolute_error: 38.225155, mean_q: 49.509483
[F[K  10286/500000: episode: 181, duration: 0.492s, episode steps: 30, steps per second: 61, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: 0.524 [0.470, 0.650], loss: 12.553838, mean_absolute_error: 37.430370, mean_q: 48.471882
[F[K  10338/500000: episode: 182, duration: 0.728s, episode steps: 52, steps per second: 71, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.731 [0.000, 4.000], mean observation: 0.485 [0.370, 0.530], loss: 11.391648, mean_absolute_error: 38.515396, mean_q: 49.719963
[F[K  10394/500000: episode: 183, duration: 0.804s, episode steps: 56, steps per second: 70, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.643 [0.000, 4.000], mean observation: 0.505 [0.420, 0.600], loss: 10.447072, mean_absolute_error: 37.257915, mean_q: 48.298401
[F[K  10441/500000: episode: 184, duration: 0.607s, episode steps: 47, steps per second: 77, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.319 [0.000, 4.000], mean observation: 0.511 [0.470, 0.560], loss: 12.021621, mean_absolute_error: 37.630371, mean_q: 48.793430
[F[K  10482/500000: episode: 185, duration: 0.604s, episode steps: 41, steps per second: 68, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.509 [0.450, 0.650], loss: 11.210214, mean_absolute_error: 38.486374, mean_q: 49.910828
[F[K  10516/500000: episode: 186, duration: 0.451s, episode steps: 34, steps per second: 75, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.506 [0.420, 0.650], loss: 11.307600, mean_absolute_error: 37.954208, mean_q: 49.174126
[F[K  10576/500000: episode: 187, duration: 0.729s, episode steps: 60, steps per second: 82, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.317 [0.000, 4.000], mean observation: 0.506 [0.450, 0.580], loss: 11.891093, mean_absolute_error: 37.509655, mean_q: 48.809021
[F[K  10629/500000: episode: 188, duration: 0.743s, episode steps: 53, steps per second: 71, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.906 [0.000, 4.000], mean observation: 0.498 [0.370, 0.610], loss: 12.207212, mean_absolute_error: 38.174026, mean_q: 49.355045
[F[K  10679/500000: episode: 189, duration: 0.700s, episode steps: 50, steps per second: 71, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.940 [0.000, 4.000], mean observation: 0.507 [0.440, 0.620], loss: 10.928162, mean_absolute_error: 37.470428, mean_q: 48.536251
[F[K  10748/500000: episode: 190, duration: 0.999s, episode steps: 69, steps per second: 69, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.841 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 10.878492, mean_absolute_error: 37.617886, mean_q: 48.816154
[F[K  10818/500000: episode: 191, duration: 0.936s, episode steps: 70, steps per second: 75, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 10.951713, mean_absolute_error: 37.868801, mean_q: 49.240997
[F[K  10873/500000: episode: 192, duration: 0.720s, episode steps: 55, steps per second: 76, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.073 [0.000, 4.000], mean observation: 0.514 [0.470, 0.600], loss: 10.316989, mean_absolute_error: 38.641731, mean_q: 50.039173
[F[K  10932/500000: episode: 193, duration: 0.932s, episode steps: 59, steps per second: 63, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.441 [0.000, 3.000], mean observation: 0.519 [0.470, 0.640], loss: 12.793347, mean_absolute_error: 36.832336, mean_q: 47.760899
[F[K  10999/500000: episode: 194, duration: 0.954s, episode steps: 67, steps per second: 70, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.567 [0.000, 4.000], mean observation: 0.486 [0.420, 0.520], loss: 11.112088, mean_absolute_error: 37.362961, mean_q: 48.567669
[F[K  11043/500000: episode: 195, duration: 0.739s, episode steps: 44, steps per second: 60, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.614 [0.000, 4.000], mean observation: 0.498 [0.380, 0.600], loss: 13.241703, mean_absolute_error: 36.941452, mean_q: 47.928078
[F[K  11098/500000: episode: 196, duration: 0.921s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.382 [0.000, 4.000], mean observation: 0.492 [0.420, 0.530], loss: 11.940557, mean_absolute_error: 37.544472, mean_q: 48.718128
[F[K  11158/500000: episode: 197, duration: 1.006s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.133 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 12.607465, mean_absolute_error: 37.378517, mean_q: 48.402817
[F[K  11216/500000: episode: 198, duration: 0.881s, episode steps: 58, steps per second: 66, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.507 [0.450, 0.590], loss: 11.419456, mean_absolute_error: 37.274750, mean_q: 48.220901
[F[K  11279/500000: episode: 199, duration: 0.956s, episode steps: 63, steps per second: 66, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.460 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 11.203860, mean_absolute_error: 36.151924, mean_q: 46.762875
[F[K  11337/500000: episode: 200, duration: 0.904s, episode steps: 58, steps per second: 64, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.086 [0.000, 4.000], mean observation: 0.510 [0.490, 0.540], loss: 10.253359, mean_absolute_error: 37.999626, mean_q: 49.193794
[F[K  11385/500000: episode: 201, duration: 0.747s, episode steps: 48, steps per second: 64, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.490 [0.420, 0.530], loss: 11.262580, mean_absolute_error: 35.931839, mean_q: 46.895916
[F[K  11424/500000: episode: 202, duration: 0.559s, episode steps: 39, steps per second: 70, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.128 [0.000, 4.000], mean observation: 0.500 [0.420, 0.540], loss: 10.965998, mean_absolute_error: 37.327198, mean_q: 48.435085
[F[K  11484/500000: episode: 203, duration: 0.957s, episode steps: 60, steps per second: 63, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.950 [0.000, 4.000], mean observation: 0.486 [0.430, 0.520], loss: 13.034211, mean_absolute_error: 36.864159, mean_q: 47.852501
[F[K  11557/500000: episode: 204, duration: 1.103s, episode steps: 73, steps per second: 66, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.495 [0.390, 0.550], loss: 10.614777, mean_absolute_error: 36.993946, mean_q: 47.878456
[F[K  11608/500000: episode: 205, duration: 0.608s, episode steps: 51, steps per second: 84, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.196 [0.000, 4.000], mean observation: 0.501 [0.470, 0.540], loss: 12.816982, mean_absolute_error: 36.748405, mean_q: 47.718521
[F[K  11652/500000: episode: 206, duration: 0.695s, episode steps: 44, steps per second: 63, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.455 [0.000, 4.000], mean observation: 0.515 [0.470, 0.630], loss: 10.080972, mean_absolute_error: 35.988937, mean_q: 46.585484
[F[K  11705/500000: episode: 207, duration: 0.737s, episode steps: 53, steps per second: 72, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.494 [0.380, 0.530], loss: 10.858550, mean_absolute_error: 36.392578, mean_q: 47.244694
[F[K  11741/500000: episode: 208, duration: 0.542s, episode steps: 36, steps per second: 66, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.389 [0.000, 2.000], mean observation: 0.528 [0.470, 0.660], loss: 10.056065, mean_absolute_error: 37.850956, mean_q: 49.060593
[F[K  11806/500000: episode: 209, duration: 1.061s, episode steps: 65, steps per second: 61, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.846 [0.000, 4.000], mean observation: 0.520 [0.460, 0.650], loss: 10.748035, mean_absolute_error: 36.742508, mean_q: 47.767002
[F[K  11859/500000: episode: 210, duration: 0.896s, episode steps: 53, steps per second: 59, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.501 [0.420, 0.600], loss: 11.675161, mean_absolute_error: 36.700626, mean_q: 47.593719
[F[K  11900/500000: episode: 211, duration: 0.670s, episode steps: 41, steps per second: 61, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.049 [0.000, 4.000], mean observation: 0.498 [0.360, 0.610], loss: 11.245312, mean_absolute_error: 36.181316, mean_q: 46.668053
[F[K  11963/500000: episode: 212, duration: 0.997s, episode steps: 63, steps per second: 63, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.698 [0.000, 4.000], mean observation: 0.503 [0.470, 0.550], loss: 11.239332, mean_absolute_error: 37.155033, mean_q: 48.107811
[F[K  12016/500000: episode: 213, duration: 0.768s, episode steps: 53, steps per second: 69, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.698 [0.000, 4.000], mean observation: 0.483 [0.360, 0.530], loss: 9.306911, mean_absolute_error: 35.538864, mean_q: 46.305725
[F[K  12081/500000: episode: 214, duration: 0.981s, episode steps: 65, steps per second: 66, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.662 [0.000, 4.000], mean observation: 0.501 [0.420, 0.600], loss: 11.068356, mean_absolute_error: 36.886089, mean_q: 47.861233
[F[K  12144/500000: episode: 215, duration: 0.913s, episode steps: 63, steps per second: 69, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.762 [0.000, 4.000], mean observation: 0.498 [0.400, 0.580], loss: 9.662146, mean_absolute_error: 37.144718, mean_q: 48.223171
[F[K  12199/500000: episode: 216, duration: 0.813s, episode steps: 55, steps per second: 68, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.327 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 12.236105, mean_absolute_error: 35.766762, mean_q: 46.635254
[F[K  12254/500000: episode: 217, duration: 0.831s, episode steps: 55, steps per second: 66, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.513 [0.470, 0.600], loss: 9.083213, mean_absolute_error: 36.848923, mean_q: 47.985756
[F[K  12307/500000: episode: 218, duration: 0.830s, episode steps: 53, steps per second: 64, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.698 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 11.575805, mean_absolute_error: 36.240265, mean_q: 47.001961
[F[K  12358/500000: episode: 219, duration: 0.861s, episode steps: 51, steps per second: 59, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.020 [0.000, 4.000], mean observation: 0.484 [0.390, 0.520], loss: 11.481876, mean_absolute_error: 36.273331, mean_q: 47.000927
[F[K  12430/500000: episode: 220, duration: 1.130s, episode steps: 72, steps per second: 64, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.736 [0.000, 4.000], mean observation: 0.487 [0.420, 0.520], loss: 8.980906, mean_absolute_error: 36.480373, mean_q: 47.423332
[F[K  12466/500000: episode: 221, duration: 0.584s, episode steps: 36, steps per second: 62, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.518 [0.470, 0.630], loss: 11.505902, mean_absolute_error: 35.642311, mean_q: 46.387516
[F[K  12517/500000: episode: 222, duration: 0.879s, episode steps: 51, steps per second: 58, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.039 [0.000, 4.000], mean observation: 0.499 [0.390, 0.610], loss: 11.425856, mean_absolute_error: 35.054710, mean_q: 45.663563
[F[K  12550/500000: episode: 223, duration: 0.494s, episode steps: 33, steps per second: 67, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.513 [0.470, 0.650], loss: 13.510664, mean_absolute_error: 35.229912, mean_q: 45.989288
[F[K  12604/500000: episode: 224, duration: 0.867s, episode steps: 54, steps per second: 62, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.593 [0.000, 4.000], mean observation: 0.500 [0.380, 0.630], loss: 12.811883, mean_absolute_error: 35.851711, mean_q: 46.677914
[F[K  12655/500000: episode: 225, duration: 0.779s, episode steps: 51, steps per second: 66, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.843 [0.000, 4.000], mean observation: 0.481 [0.350, 0.530], loss: 10.345163, mean_absolute_error: 35.036976, mean_q: 45.634621
[F[K  12733/500000: episode: 226, duration: 0.994s, episode steps: 78, steps per second: 78, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.705 [0.000, 4.000], mean observation: 0.499 [0.420, 0.560], loss: 11.155803, mean_absolute_error: 35.491394, mean_q: 46.282314
[F[K  12795/500000: episode: 227, duration: 0.964s, episode steps: 62, steps per second: 64, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.758 [0.000, 4.000], mean observation: 0.503 [0.460, 0.570], loss: 11.289577, mean_absolute_error: 35.441383, mean_q: 46.155247
[F[K  12850/500000: episode: 228, duration: 0.828s, episode steps: 55, steps per second: 66, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.527 [0.000, 4.000], mean observation: 0.491 [0.400, 0.520], loss: 9.182220, mean_absolute_error: 35.618538, mean_q: 46.644863
[F[K  12900/500000: episode: 229, duration: 0.699s, episode steps: 50, steps per second: 72, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.480 [0.000, 4.000], mean observation: 0.505 [0.460, 0.600], loss: 10.813962, mean_absolute_error: 35.143036, mean_q: 45.777435
[F[K  12971/500000: episode: 230, duration: 1.114s, episode steps: 71, steps per second: 64, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.493 [0.000, 4.000], mean observation: 0.502 [0.470, 0.550], loss: 10.282213, mean_absolute_error: 35.203983, mean_q: 46.037907
[F[K  13035/500000: episode: 231, duration: 1.021s, episode steps: 64, steps per second: 63, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.438 [0.000, 4.000], mean observation: 0.512 [0.470, 0.610], loss: 11.076163, mean_absolute_error: 35.258457, mean_q: 45.817158
[F[K  13077/500000: episode: 232, duration: 0.519s, episode steps: 42, steps per second: 81, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.548 [0.000, 4.000], mean observation: 0.475 [0.360, 0.520], loss: 11.396719, mean_absolute_error: 35.867195, mean_q: 46.529076
[F[K  13137/500000: episode: 233, duration: 0.829s, episode steps: 60, steps per second: 72, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.283 [0.000, 4.000], mean observation: 0.496 [0.420, 0.540], loss: 9.974337, mean_absolute_error: 34.597481, mean_q: 45.206837
[F[K  13184/500000: episode: 234, duration: 0.588s, episode steps: 47, steps per second: 80, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.596 [0.000, 4.000], mean observation: 0.495 [0.350, 0.590], loss: 9.568475, mean_absolute_error: 35.222198, mean_q: 46.059959
[F[K  13242/500000: episode: 235, duration: 0.800s, episode steps: 58, steps per second: 73, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.414 [0.000, 4.000], mean observation: 0.487 [0.370, 0.530], loss: 8.964012, mean_absolute_error: 34.966488, mean_q: 45.713013
[F[K  13299/500000: episode: 236, duration: 0.925s, episode steps: 57, steps per second: 62, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 0.499 [0.390, 0.560], loss: 10.087695, mean_absolute_error: 35.517342, mean_q: 46.289734
[F[K  13352/500000: episode: 237, duration: 0.784s, episode steps: 53, steps per second: 68, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.132 [0.000, 4.000], mean observation: 0.496 [0.410, 0.560], loss: 9.997502, mean_absolute_error: 35.495644, mean_q: 46.167076
[F[K  13415/500000: episode: 238, duration: 1.023s, episode steps: 63, steps per second: 62, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.206 [0.000, 4.000], mean observation: 0.497 [0.400, 0.580], loss: 10.212468, mean_absolute_error: 35.250446, mean_q: 45.894516
[F[K  13489/500000: episode: 239, duration: 1.082s, episode steps: 74, steps per second: 68, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.027 [0.000, 4.000], mean observation: 0.502 [0.460, 0.530], loss: 9.612743, mean_absolute_error: 34.862610, mean_q: 45.568516
[F[K  13555/500000: episode: 240, duration: 1.040s, episode steps: 66, steps per second: 63, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.409 [0.000, 4.000], mean observation: 0.499 [0.460, 0.540], loss: 8.637516, mean_absolute_error: 35.013119, mean_q: 45.667446
[F[K  13622/500000: episode: 241, duration: 0.997s, episode steps: 67, steps per second: 67, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.090 [0.000, 4.000], mean observation: 0.501 [0.400, 0.610], loss: 9.660373, mean_absolute_error: 34.330582, mean_q: 44.812008
[F[K  13687/500000: episode: 242, duration: 0.929s, episode steps: 65, steps per second: 70, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.503 [0.420, 0.620], loss: 10.149417, mean_absolute_error: 34.443768, mean_q: 45.033592
[F[K  13742/500000: episode: 243, duration: 0.876s, episode steps: 55, steps per second: 63, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.945 [0.000, 4.000], mean observation: 0.501 [0.420, 0.600], loss: 9.399447, mean_absolute_error: 34.950813, mean_q: 45.507305
[F[K  13796/500000: episode: 244, duration: 0.867s, episode steps: 54, steps per second: 62, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.130 [0.000, 4.000], mean observation: 0.508 [0.470, 0.570], loss: 10.277910, mean_absolute_error: 34.218449, mean_q: 44.787434
[F[K  13859/500000: episode: 245, duration: 0.978s, episode steps: 63, steps per second: 64, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.444 [0.000, 4.000], mean observation: 0.505 [0.460, 0.590], loss: 10.180509, mean_absolute_error: 34.904922, mean_q: 45.582535
[F[K  13927/500000: episode: 246, duration: 0.829s, episode steps: 68, steps per second: 82, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.059 [0.000, 4.000], mean observation: 0.485 [0.380, 0.520], loss: 10.053265, mean_absolute_error: 35.055962, mean_q: 45.633350
[F[K  13977/500000: episode: 247, duration: 0.785s, episode steps: 50, steps per second: 64, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 10.406168, mean_absolute_error: 34.594753, mean_q: 44.887131
[F[K  14033/500000: episode: 248, duration: 0.770s, episode steps: 56, steps per second: 73, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.482 [0.000, 4.000], mean observation: 0.472 [0.350, 0.520], loss: 9.500493, mean_absolute_error: 35.111271, mean_q: 45.681793
[F[K  14101/500000: episode: 249, duration: 0.995s, episode steps: 68, steps per second: 68, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.353 [0.000, 4.000], mean observation: 0.482 [0.370, 0.520], loss: 8.913397, mean_absolute_error: 35.068695, mean_q: 45.751045
[F[K  14155/500000: episode: 250, duration: 0.753s, episode steps: 54, steps per second: 72, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.796 [0.000, 4.000], mean observation: 0.519 [0.470, 0.650], loss: 8.795125, mean_absolute_error: 34.871563, mean_q: 45.345264
[F[K  14232/500000: episode: 251, duration: 1.024s, episode steps: 77, steps per second: 75, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.623 [0.000, 4.000], mean observation: 0.495 [0.460, 0.530], loss: 9.247806, mean_absolute_error: 34.963291, mean_q: 45.485577
[F[K  14291/500000: episode: 252, duration: 0.896s, episode steps: 59, steps per second: 66, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.695 [0.000, 4.000], mean observation: 0.523 [0.480, 0.640], loss: 9.190537, mean_absolute_error: 34.319016, mean_q: 44.912632
[F[K  14370/500000: episode: 253, duration: 0.997s, episode steps: 79, steps per second: 79, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.582 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 9.569471, mean_absolute_error: 34.223381, mean_q: 44.609585
[F[K  14437/500000: episode: 254, duration: 0.941s, episode steps: 67, steps per second: 71, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.582 [0.000, 4.000], mean observation: 0.488 [0.400, 0.520], loss: 10.252965, mean_absolute_error: 34.507126, mean_q: 44.984299
[F[K  14515/500000: episode: 255, duration: 1.064s, episode steps: 78, steps per second: 73, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.494 [0.380, 0.530], loss: 7.904813, mean_absolute_error: 34.513901, mean_q: 45.108284
[F[K  14564/500000: episode: 256, duration: 0.747s, episode steps: 49, steps per second: 66, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.499 [0.470, 0.520], loss: 9.287471, mean_absolute_error: 34.882172, mean_q: 45.531124
[F[K  14611/500000: episode: 257, duration: 0.669s, episode steps: 47, steps per second: 70, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.723 [0.000, 4.000], mean observation: 0.502 [0.360, 0.640], loss: 9.132040, mean_absolute_error: 35.063450, mean_q: 45.940990
[F[K  14717/500000: episode: 258, duration: 1.392s, episode steps: 106, steps per second: 76, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.934 [0.000, 4.000], mean observation: 0.488 [0.430, 0.530], loss: 8.516742, mean_absolute_error: 34.320805, mean_q: 44.823006
[F[K  14780/500000: episode: 259, duration: 0.888s, episode steps: 63, steps per second: 71, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.889 [0.000, 4.000], mean observation: 0.508 [0.450, 0.620], loss: 8.335510, mean_absolute_error: 34.763988, mean_q: 45.383881
[F[K  14843/500000: episode: 260, duration: 0.980s, episode steps: 63, steps per second: 64, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.016 [0.000, 4.000], mean observation: 0.489 [0.430, 0.530], loss: 9.577058, mean_absolute_error: 34.674744, mean_q: 45.231171
[F[K  14903/500000: episode: 261, duration: 0.901s, episode steps: 60, steps per second: 67, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.520 [0.480, 0.610], loss: 9.348718, mean_absolute_error: 34.569225, mean_q: 44.951099
[F[K  14991/500000: episode: 262, duration: 1.299s, episode steps: 88, steps per second: 68, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.170 [0.000, 4.000], mean observation: 0.490 [0.430, 0.520], loss: 9.274580, mean_absolute_error: 33.986790, mean_q: 44.374279
[F[K  15047/500000: episode: 263, duration: 0.908s, episode steps: 56, steps per second: 62, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.554 [0.000, 4.000], mean observation: 0.502 [0.410, 0.620], loss: 9.873003, mean_absolute_error: 34.071663, mean_q: 44.527058
[F[K  15117/500000: episode: 264, duration: 1.011s, episode steps: 70, steps per second: 69, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.686 [0.000, 4.000], mean observation: 0.478 [0.360, 0.510], loss: 8.788398, mean_absolute_error: 34.050369, mean_q: 44.522026
[F[K  15157/500000: episode: 265, duration: 0.620s, episode steps: 40, steps per second: 64, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.515 [0.470, 0.630], loss: 8.326979, mean_absolute_error: 34.653233, mean_q: 45.280006
[F[K  15212/500000: episode: 266, duration: 0.939s, episode steps: 55, steps per second: 59, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.545 [0.000, 4.000], mean observation: 0.511 [0.460, 0.650], loss: 9.749373, mean_absolute_error: 34.226933, mean_q: 44.730350
[F[K  15267/500000: episode: 267, duration: 0.864s, episode steps: 55, steps per second: 64, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.514 [0.470, 0.620], loss: 8.657722, mean_absolute_error: 34.210941, mean_q: 44.900906
[F[K  15337/500000: episode: 268, duration: 1.032s, episode steps: 70, steps per second: 68, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.257 [0.000, 4.000], mean observation: 0.493 [0.440, 0.520], loss: 9.161408, mean_absolute_error: 34.048523, mean_q: 44.455101
[F[K  15405/500000: episode: 269, duration: 1.049s, episode steps: 68, steps per second: 65, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.206 [0.000, 4.000], mean observation: 0.500 [0.440, 0.570], loss: 9.074858, mean_absolute_error: 34.764591, mean_q: 45.398468
[F[K  15463/500000: episode: 270, duration: 0.800s, episode steps: 58, steps per second: 73, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.513 [0.470, 0.640], loss: 7.874037, mean_absolute_error: 33.540737, mean_q: 44.047741
[F[K  15527/500000: episode: 271, duration: 0.941s, episode steps: 64, steps per second: 68, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.750 [0.000, 4.000], mean observation: 0.505 [0.440, 0.600], loss: 9.430977, mean_absolute_error: 34.226601, mean_q: 44.688660
[F[K  15588/500000: episode: 272, duration: 0.980s, episode steps: 61, steps per second: 62, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.738 [0.000, 4.000], mean observation: 0.480 [0.390, 0.530], loss: 8.057299, mean_absolute_error: 34.937092, mean_q: 45.513466
[F[K  15643/500000: episode: 273, duration: 0.845s, episode steps: 55, steps per second: 65, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 0.484 [0.400, 0.520], loss: 9.286709, mean_absolute_error: 33.512203, mean_q: 43.849422
[F[K  15699/500000: episode: 274, duration: 0.856s, episode steps: 56, steps per second: 65, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.893 [0.000, 4.000], mean observation: 0.511 [0.470, 0.640], loss: 7.419430, mean_absolute_error: 33.662502, mean_q: 44.013401
[F[K  15763/500000: episode: 275, duration: 1.077s, episode steps: 64, steps per second: 59, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.498 [0.400, 0.560], loss: 8.650535, mean_absolute_error: 34.675789, mean_q: 45.064751
[F[K  15843/500000: episode: 276, duration: 1.373s, episode steps: 80, steps per second: 58, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.538 [0.000, 4.000], mean observation: 0.494 [0.400, 0.530], loss: 8.416812, mean_absolute_error: 34.028893, mean_q: 44.443474
[F[K  15897/500000: episode: 277, duration: 0.807s, episode steps: 54, steps per second: 67, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.489 [0.410, 0.530], loss: 7.820249, mean_absolute_error: 33.598713, mean_q: 43.800114
[F[K  15944/500000: episode: 278, duration: 0.706s, episode steps: 47, steps per second: 67, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.809 [0.000, 4.000], mean observation: 0.488 [0.390, 0.530], loss: 8.162127, mean_absolute_error: 33.831150, mean_q: 44.165123
[F[K  16014/500000: episode: 279, duration: 0.998s, episode steps: 70, steps per second: 70, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.484 [0.360, 0.520], loss: 7.313638, mean_absolute_error: 33.334057, mean_q: 43.581772
[F[K  16079/500000: episode: 280, duration: 0.836s, episode steps: 65, steps per second: 78, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.969 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 8.732512, mean_absolute_error: 34.272778, mean_q: 44.687614
[F[K  16138/500000: episode: 281, duration: 0.786s, episode steps: 59, steps per second: 75, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.576 [0.000, 4.000], mean observation: 0.497 [0.420, 0.530], loss: 8.984245, mean_absolute_error: 33.815926, mean_q: 44.044220
[F[K  16188/500000: episode: 282, duration: 0.715s, episode steps: 50, steps per second: 70, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.480 [0.360, 0.520], loss: 7.994260, mean_absolute_error: 34.188507, mean_q: 44.565639
[F[K  16244/500000: episode: 283, duration: 0.736s, episode steps: 56, steps per second: 76, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 8.127403, mean_absolute_error: 34.543591, mean_q: 45.106800
[F[K  16320/500000: episode: 284, duration: 1.005s, episode steps: 76, steps per second: 76, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.737 [0.000, 3.000], mean observation: 0.508 [0.460, 0.610], loss: 9.114458, mean_absolute_error: 34.500015, mean_q: 44.951714
[F[K  16378/500000: episode: 285, duration: 0.736s, episode steps: 58, steps per second: 79, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.526 [0.470, 0.650], loss: 9.168099, mean_absolute_error: 33.890511, mean_q: 44.279404
[F[K  16412/500000: episode: 286, duration: 0.461s, episode steps: 34, steps per second: 74, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.588 [0.000, 3.000], mean observation: 0.527 [0.470, 0.650], loss: 7.597571, mean_absolute_error: 33.843792, mean_q: 44.428444
[F[K  16472/500000: episode: 287, duration: 0.759s, episode steps: 60, steps per second: 79, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.717 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 7.979717, mean_absolute_error: 33.335339, mean_q: 43.758190
[F[K  16531/500000: episode: 288, duration: 0.693s, episode steps: 59, steps per second: 85, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.119 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 9.861001, mean_absolute_error: 33.404968, mean_q: 43.760582
[F[K  16599/500000: episode: 289, duration: 0.903s, episode steps: 68, steps per second: 75, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.750 [0.000, 4.000], mean observation: 0.493 [0.440, 0.520], loss: 8.887856, mean_absolute_error: 33.412151, mean_q: 43.733727
[F[K  16652/500000: episode: 290, duration: 0.644s, episode steps: 53, steps per second: 82, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.486 [0.360, 0.520], loss: 8.143521, mean_absolute_error: 33.801823, mean_q: 44.112709
[F[K  16698/500000: episode: 291, duration: 0.647s, episode steps: 46, steps per second: 71, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.348 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 7.446133, mean_absolute_error: 34.575779, mean_q: 45.127956
[F[K  16752/500000: episode: 292, duration: 0.733s, episode steps: 54, steps per second: 74, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.926 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 8.396613, mean_absolute_error: 34.038017, mean_q: 44.230793
[F[K  16794/500000: episode: 293, duration: 0.682s, episode steps: 42, steps per second: 62, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.493 [0.350, 0.560], loss: 9.096431, mean_absolute_error: 33.278069, mean_q: 43.430046
[F[K  16833/500000: episode: 294, duration: 0.576s, episode steps: 39, steps per second: 68, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.795 [0.000, 4.000], mean observation: 0.502 [0.370, 0.650], loss: 6.870315, mean_absolute_error: 33.617493, mean_q: 43.829144
[F[K  16888/500000: episode: 295, duration: 0.791s, episode steps: 55, steps per second: 70, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.127 [0.000, 4.000], mean observation: 0.501 [0.370, 0.640], loss: 8.579800, mean_absolute_error: 33.179909, mean_q: 43.389149
[F[K  16936/500000: episode: 296, duration: 0.621s, episode steps: 48, steps per second: 77, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.896 [0.000, 4.000], mean observation: 0.493 [0.400, 0.540], loss: 8.197739, mean_absolute_error: 33.903831, mean_q: 44.177460
[F[K  16987/500000: episode: 297, duration: 0.596s, episode steps: 51, steps per second: 86, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.176 [0.000, 4.000], mean observation: 0.492 [0.360, 0.560], loss: 9.299664, mean_absolute_error: 33.677677, mean_q: 43.950848
[F[K  17043/500000: episode: 298, duration: 0.903s, episode steps: 56, steps per second: 62, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.089 [0.000, 4.000], mean observation: 0.511 [0.470, 0.620], loss: 9.270929, mean_absolute_error: 33.698795, mean_q: 43.943035
[F[K  17117/500000: episode: 299, duration: 1.095s, episode steps: 74, steps per second: 68, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.649 [0.000, 4.000], mean observation: 0.504 [0.470, 0.550], loss: 9.936528, mean_absolute_error: 33.364857, mean_q: 43.493935
[F[K  17151/500000: episode: 300, duration: 0.490s, episode steps: 34, steps per second: 69, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.882 [0.000, 2.000], mean observation: 0.518 [0.470, 0.650], loss: 7.793704, mean_absolute_error: 32.764698, mean_q: 42.769836
[F[K  17203/500000: episode: 301, duration: 0.785s, episode steps: 52, steps per second: 66, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.135 [0.000, 4.000], mean observation: 0.521 [0.470, 0.630], loss: 9.464767, mean_absolute_error: 33.757439, mean_q: 44.005882
[F[K  17258/500000: episode: 302, duration: 0.860s, episode steps: 55, steps per second: 64, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.491 [0.000, 4.000], mean observation: 0.507 [0.470, 0.550], loss: 8.556260, mean_absolute_error: 33.501335, mean_q: 43.818867
[F[K  17321/500000: episode: 303, duration: 0.777s, episode steps: 63, steps per second: 81, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.495 [0.440, 0.520], loss: 7.492735, mean_absolute_error: 33.582104, mean_q: 43.850769
[F[K  17363/500000: episode: 304, duration: 0.614s, episode steps: 42, steps per second: 68, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.514 [0.470, 0.640], loss: 8.931476, mean_absolute_error: 32.764893, mean_q: 42.820770
[F[K  17402/500000: episode: 305, duration: 0.553s, episode steps: 39, steps per second: 71, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.744 [0.000, 4.000], mean observation: 0.495 [0.370, 0.590], loss: 8.162155, mean_absolute_error: 33.191818, mean_q: 43.434441
[F[K  17465/500000: episode: 306, duration: 0.886s, episode steps: 63, steps per second: 71, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.501 [0.470, 0.560], loss: 8.252432, mean_absolute_error: 33.448353, mean_q: 43.600040
[F[K  17521/500000: episode: 307, duration: 0.806s, episode steps: 56, steps per second: 69, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.589 [0.000, 4.000], mean observation: 0.500 [0.370, 0.620], loss: 7.088480, mean_absolute_error: 33.430779, mean_q: 43.617031
[F[K  17572/500000: episode: 308, duration: 0.752s, episode steps: 51, steps per second: 68, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.392 [0.000, 4.000], mean observation: 0.507 [0.470, 0.600], loss: 10.312772, mean_absolute_error: 33.709156, mean_q: 43.821720
[F[K  17655/500000: episode: 309, duration: 1.095s, episode steps: 83, steps per second: 76, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.735 [0.000, 4.000], mean observation: 0.495 [0.370, 0.580], loss: 8.962375, mean_absolute_error: 32.532440, mean_q: 42.437828
[F[K  17692/500000: episode: 310, duration: 0.455s, episode steps: 37, steps per second: 81, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.243 [0.000, 4.000], mean observation: 0.488 [0.350, 0.540], loss: 9.679993, mean_absolute_error: 32.983940, mean_q: 42.869652
[F[K  17732/500000: episode: 311, duration: 0.582s, episode steps: 40, steps per second: 69, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.492 [0.350, 0.590], loss: 7.594831, mean_absolute_error: 33.369225, mean_q: 43.452362
[F[K  17791/500000: episode: 312, duration: 0.862s, episode steps: 59, steps per second: 68, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.898 [0.000, 4.000], mean observation: 0.492 [0.400, 0.540], loss: 9.227447, mean_absolute_error: 33.444504, mean_q: 43.561420
[F[K  17856/500000: episode: 313, duration: 1.013s, episode steps: 65, steps per second: 64, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.738 [0.000, 4.000], mean observation: 0.491 [0.450, 0.520], loss: 8.224632, mean_absolute_error: 33.704002, mean_q: 43.859844
[F[K  17914/500000: episode: 314, duration: 0.885s, episode steps: 58, steps per second: 66, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 0.501 [0.390, 0.620], loss: 8.195056, mean_absolute_error: 32.422966, mean_q: 42.437695
[F[K  17972/500000: episode: 315, duration: 0.879s, episode steps: 58, steps per second: 66, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.345 [0.000, 4.000], mean observation: 0.493 [0.450, 0.520], loss: 7.819860, mean_absolute_error: 33.349609, mean_q: 43.527943
[F[K  18027/500000: episode: 316, duration: 0.757s, episode steps: 55, steps per second: 73, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.964 [0.000, 4.000], mean observation: 0.514 [0.470, 0.610], loss: 8.511481, mean_absolute_error: 32.916969, mean_q: 42.836254
[F[K  18098/500000: episode: 317, duration: 0.852s, episode steps: 71, steps per second: 83, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 8.506406, mean_absolute_error: 32.430470, mean_q: 42.357349
[F[K  18151/500000: episode: 318, duration: 0.761s, episode steps: 53, steps per second: 70, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.660 [0.000, 4.000], mean observation: 0.513 [0.470, 0.570], loss: 8.341235, mean_absolute_error: 32.917137, mean_q: 42.898636
[F[K  18192/500000: episode: 319, duration: 0.565s, episode steps: 41, steps per second: 73, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.146 [0.000, 4.000], mean observation: 0.494 [0.430, 0.530], loss: 8.425631, mean_absolute_error: 33.379570, mean_q: 43.463982
[F[K  18253/500000: episode: 320, duration: 0.766s, episode steps: 61, steps per second: 80, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.344 [0.000, 4.000], mean observation: 0.492 [0.460, 0.520], loss: 7.930449, mean_absolute_error: 33.124092, mean_q: 43.159645
[F[K  18304/500000: episode: 321, duration: 0.591s, episode steps: 51, steps per second: 86, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.196 [0.000, 4.000], mean observation: 0.512 [0.470, 0.610], loss: 8.690609, mean_absolute_error: 32.824131, mean_q: 42.898811
[F[K  18354/500000: episode: 322, duration: 0.627s, episode steps: 50, steps per second: 80, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.360 [0.000, 4.000], mean observation: 0.484 [0.370, 0.520], loss: 8.022096, mean_absolute_error: 33.468361, mean_q: 43.527348
[F[K  18405/500000: episode: 323, duration: 0.590s, episode steps: 51, steps per second: 86, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.588 [0.000, 4.000], mean observation: 0.502 [0.460, 0.570], loss: 7.577029, mean_absolute_error: 32.697369, mean_q: 42.674843
[F[K  18438/500000: episode: 324, duration: 0.419s, episode steps: 33, steps per second: 79, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.512 [0.470, 0.650], loss: 7.840643, mean_absolute_error: 33.623466, mean_q: 43.683224
[F[K  18514/500000: episode: 325, duration: 0.911s, episode steps: 76, steps per second: 83, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.684 [0.000, 4.000], mean observation: 0.503 [0.460, 0.550], loss: 6.850118, mean_absolute_error: 32.600037, mean_q: 42.609901
[F[K  18596/500000: episode: 326, duration: 0.924s, episode steps: 82, steps per second: 89, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.488 [0.000, 4.000], mean observation: 0.493 [0.410, 0.530], loss: 7.780370, mean_absolute_error: 33.015102, mean_q: 42.981247
[F[K  18660/500000: episode: 327, duration: 0.865s, episode steps: 64, steps per second: 74, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.062 [0.000, 4.000], mean observation: 0.487 [0.400, 0.520], loss: 7.670101, mean_absolute_error: 32.991081, mean_q: 42.973316
[F[K  18714/500000: episode: 328, duration: 0.714s, episode steps: 54, steps per second: 76, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.093 [0.000, 4.000], mean observation: 0.502 [0.400, 0.620], loss: 6.936402, mean_absolute_error: 32.577549, mean_q: 42.677803
[F[K  18773/500000: episode: 329, duration: 0.678s, episode steps: 59, steps per second: 87, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.169 [0.000, 4.000], mean observation: 0.508 [0.470, 0.560], loss: 7.695522, mean_absolute_error: 32.646049, mean_q: 42.650082
[F[K  18853/500000: episode: 330, duration: 0.945s, episode steps: 80, steps per second: 85, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.775 [0.000, 4.000], mean observation: 0.505 [0.470, 0.550], loss: 7.429149, mean_absolute_error: 33.371899, mean_q: 43.491608
[F[K  18916/500000: episode: 331, duration: 0.810s, episode steps: 63, steps per second: 78, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.683 [0.000, 4.000], mean observation: 0.497 [0.410, 0.570], loss: 8.485154, mean_absolute_error: 32.651276, mean_q: 42.544708
[F[K  19007/500000: episode: 332, duration: 1.188s, episode steps: 91, steps per second: 77, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.099 [0.000, 4.000], mean observation: 0.506 [0.470, 0.550], loss: 8.001847, mean_absolute_error: 33.301186, mean_q: 43.348755
[F[K  19066/500000: episode: 333, duration: 0.703s, episode steps: 59, steps per second: 84, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.729 [0.000, 4.000], mean observation: 0.487 [0.370, 0.520], loss: 6.849077, mean_absolute_error: 32.534702, mean_q: 42.210777
[F[K  19122/500000: episode: 334, duration: 0.820s, episode steps: 56, steps per second: 68, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.696 [0.000, 4.000], mean observation: 0.502 [0.450, 0.570], loss: 6.665561, mean_absolute_error: 32.601707, mean_q: 42.495586
[F[K  19186/500000: episode: 335, duration: 0.903s, episode steps: 64, steps per second: 71, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.656 [0.000, 4.000], mean observation: 0.490 [0.430, 0.530], loss: 8.243982, mean_absolute_error: 32.530865, mean_q: 42.422550
[F[K  19238/500000: episode: 336, duration: 0.706s, episode steps: 52, steps per second: 74, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.904 [0.000, 4.000], mean observation: 0.500 [0.440, 0.550], loss: 7.395421, mean_absolute_error: 32.557449, mean_q: 42.561157
[F[K  19305/500000: episode: 337, duration: 1.004s, episode steps: 67, steps per second: 67, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.866 [0.000, 4.000], mean observation: 0.500 [0.360, 0.630], loss: 7.853545, mean_absolute_error: 32.905933, mean_q: 42.862858
[F[K  19357/500000: episode: 338, duration: 0.811s, episode steps: 52, steps per second: 64, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.635 [0.000, 3.000], mean observation: 0.511 [0.470, 0.630], loss: 6.920724, mean_absolute_error: 33.084824, mean_q: 43.085606
[F[K  19412/500000: episode: 339, duration: 0.799s, episode steps: 55, steps per second: 69, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.964 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 8.019837, mean_absolute_error: 32.004208, mean_q: 41.740288
[F[K  19469/500000: episode: 340, duration: 0.792s, episode steps: 57, steps per second: 72, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.421 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 6.056973, mean_absolute_error: 33.220398, mean_q: 43.486748
[F[K  19584/500000: episode: 341, duration: 1.666s, episode steps: 115, steps per second: 69, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.139 [0.000, 4.000], mean observation: 0.474 [0.340, 0.540], loss: 7.710809, mean_absolute_error: 32.997658, mean_q: 43.010864
[F[K  19702/500000: episode: 342, duration: 1.482s, episode steps: 118, steps per second: 80, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.627 [0.000, 4.000], mean observation: 0.489 [0.390, 0.530], loss: 8.341877, mean_absolute_error: 32.838058, mean_q: 42.650272
[F[K  19760/500000: episode: 343, duration: 0.848s, episode steps: 58, steps per second: 68, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.086 [0.000, 4.000], mean observation: 0.509 [0.500, 0.540], loss: 7.062817, mean_absolute_error: 32.862095, mean_q: 42.784000
[F[K  19833/500000: episode: 344, duration: 1.009s, episode steps: 73, steps per second: 72, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.658 [0.000, 4.000], mean observation: 0.497 [0.450, 0.530], loss: 7.404184, mean_absolute_error: 33.147789, mean_q: 43.171646
[F[K  19903/500000: episode: 345, duration: 0.961s, episode steps: 70, steps per second: 73, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.614 [0.000, 4.000], mean observation: 0.508 [0.470, 0.580], loss: 7.750734, mean_absolute_error: 33.162727, mean_q: 43.040161
[F[K  19943/500000: episode: 346, duration: 0.548s, episode steps: 40, steps per second: 73, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.575 [0.000, 4.000], mean observation: 0.494 [0.350, 0.580], loss: 6.691285, mean_absolute_error: 32.872059, mean_q: 42.691772
[F[K  20008/500000: episode: 347, duration: 0.844s, episode steps: 65, steps per second: 77, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.031 [0.000, 4.000], mean observation: 0.490 [0.360, 0.530], loss: 7.792972, mean_absolute_error: 32.770535, mean_q: 42.687531
[F[K  20073/500000: episode: 348, duration: 0.975s, episode steps: 65, steps per second: 67, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.031 [0.000, 4.000], mean observation: 0.512 [0.470, 0.560], loss: 6.558708, mean_absolute_error: 32.079334, mean_q: 41.638325
[F[K  20141/500000: episode: 349, duration: 0.928s, episode steps: 68, steps per second: 73, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.492 [0.370, 0.550], loss: 7.391212, mean_absolute_error: 32.959976, mean_q: 42.936630
[F[K  20228/500000: episode: 350, duration: 1.231s, episode steps: 87, steps per second: 71, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.425 [0.000, 4.000], mean observation: 0.502 [0.410, 0.600], loss: 6.535480, mean_absolute_error: 32.581120, mean_q: 42.407879
[F[K  20278/500000: episode: 351, duration: 0.752s, episode steps: 50, steps per second: 67, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.498 [0.370, 0.610], loss: 6.457025, mean_absolute_error: 32.750458, mean_q: 42.611046
[F[K  20352/500000: episode: 352, duration: 1.133s, episode steps: 74, steps per second: 65, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.507 [0.420, 0.650], loss: 7.431698, mean_absolute_error: 33.676933, mean_q: 43.628624
[F[K  20421/500000: episode: 353, duration: 1.011s, episode steps: 69, steps per second: 68, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.449 [0.000, 4.000], mean observation: 0.507 [0.470, 0.600], loss: 7.290900, mean_absolute_error: 32.449390, mean_q: 42.215519
[F[K  20472/500000: episode: 354, duration: 0.796s, episode steps: 51, steps per second: 64, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.294 [0.000, 4.000], mean observation: 0.487 [0.360, 0.540], loss: 8.410888, mean_absolute_error: 32.723431, mean_q: 42.577606
[F[K  20534/500000: episode: 355, duration: 0.804s, episode steps: 62, steps per second: 77, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 0.488 [0.400, 0.530], loss: 8.332941, mean_absolute_error: 32.871090, mean_q: 42.572170
[F[K  20585/500000: episode: 356, duration: 0.773s, episode steps: 51, steps per second: 66, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.522 [0.470, 0.630], loss: 6.346745, mean_absolute_error: 32.910519, mean_q: 42.673599
[F[K  20669/500000: episode: 357, duration: 1.113s, episode steps: 84, steps per second: 75, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.762 [0.000, 4.000], mean observation: 0.489 [0.400, 0.530], loss: 7.913562, mean_absolute_error: 31.888832, mean_q: 41.459358
[F[K  20729/500000: episode: 358, duration: 1.000s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.600 [0.000, 4.000], mean observation: 0.516 [0.470, 0.640], loss: 7.857033, mean_absolute_error: 32.284473, mean_q: 41.888206
[F[K  20779/500000: episode: 359, duration: 0.757s, episode steps: 50, steps per second: 66, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.680 [0.000, 4.000], mean observation: 0.478 [0.360, 0.520], loss: 6.629811, mean_absolute_error: 31.992176, mean_q: 41.584351
[F[K  20840/500000: episode: 360, duration: 0.864s, episode steps: 61, steps per second: 71, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.607 [0.000, 4.000], mean observation: 0.490 [0.380, 0.520], loss: 8.418349, mean_absolute_error: 32.742229, mean_q: 42.356182
[F[K  20925/500000: episode: 361, duration: 1.288s, episode steps: 85, steps per second: 66, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.965 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 7.193557, mean_absolute_error: 32.461060, mean_q: 42.127480
[F[K  20989/500000: episode: 362, duration: 0.895s, episode steps: 64, steps per second: 72, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.359 [0.000, 4.000], mean observation: 0.492 [0.360, 0.550], loss: 7.129316, mean_absolute_error: 32.874519, mean_q: 42.642365
[F[K  21036/500000: episode: 363, duration: 0.710s, episode steps: 47, steps per second: 66, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.404 [0.000, 4.000], mean observation: 0.491 [0.350, 0.560], loss: 7.847013, mean_absolute_error: 32.120846, mean_q: 41.754002
[F[K  21094/500000: episode: 364, duration: 0.788s, episode steps: 58, steps per second: 74, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.499 [0.370, 0.620], loss: 7.035175, mean_absolute_error: 33.225483, mean_q: 43.173592
[F[K  21156/500000: episode: 365, duration: 0.960s, episode steps: 62, steps per second: 65, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.403 [0.000, 4.000], mean observation: 0.503 [0.470, 0.530], loss: 7.327726, mean_absolute_error: 32.845669, mean_q: 42.660717
[F[K  21193/500000: episode: 366, duration: 0.580s, episode steps: 37, steps per second: 64, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.520 [0.470, 0.650], loss: 7.712557, mean_absolute_error: 32.365101, mean_q: 42.113968
[F[K  21291/500000: episode: 367, duration: 1.494s, episode steps: 98, steps per second: 66, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.502 [0.440, 0.580], loss: 6.936472, mean_absolute_error: 32.462902, mean_q: 42.182518
[F[K  21335/500000: episode: 368, duration: 0.768s, episode steps: 44, steps per second: 57, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.455 [0.000, 4.000], mean observation: 0.494 [0.350, 0.610], loss: 6.603716, mean_absolute_error: 32.966915, mean_q: 42.771225
[F[K  21389/500000: episode: 369, duration: 0.891s, episode steps: 54, steps per second: 61, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.796 [0.000, 4.000], mean observation: 0.499 [0.470, 0.520], loss: 8.197611, mean_absolute_error: 32.848312, mean_q: 42.625118
[F[K  21455/500000: episode: 370, duration: 1.079s, episode steps: 66, steps per second: 61, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.909 [0.000, 4.000], mean observation: 0.504 [0.390, 0.620], loss: 8.115799, mean_absolute_error: 32.931404, mean_q: 42.676670
[F[K  21532/500000: episode: 371, duration: 1.131s, episode steps: 77, steps per second: 68, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.477 [0.370, 0.520], loss: 7.797956, mean_absolute_error: 32.570736, mean_q: 42.171234
[F[K  21578/500000: episode: 372, duration: 0.783s, episode steps: 46, steps per second: 59, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.848 [0.000, 4.000], mean observation: 0.477 [0.360, 0.520], loss: 7.402584, mean_absolute_error: 32.820351, mean_q: 42.450199
[F[K  21652/500000: episode: 373, duration: 1.113s, episode steps: 74, steps per second: 66, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.919 [0.000, 4.000], mean observation: 0.482 [0.370, 0.530], loss: 7.579093, mean_absolute_error: 32.704933, mean_q: 42.547688
[F[K  21721/500000: episode: 374, duration: 1.134s, episode steps: 69, steps per second: 61, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.174 [0.000, 4.000], mean observation: 0.509 [0.440, 0.650], loss: 8.415935, mean_absolute_error: 32.170269, mean_q: 41.784798
[F[K  21781/500000: episode: 375, duration: 0.911s, episode steps: 60, steps per second: 66, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.519 [0.470, 0.600], loss: 7.485354, mean_absolute_error: 32.633030, mean_q: 42.378983
[F[K  21860/500000: episode: 376, duration: 1.125s, episode steps: 79, steps per second: 70, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.595 [0.000, 4.000], mean observation: 0.502 [0.440, 0.560], loss: 7.632569, mean_absolute_error: 32.740314, mean_q: 42.560631
[F[K  21920/500000: episode: 377, duration: 0.876s, episode steps: 60, steps per second: 68, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.497 [0.470, 0.520], loss: 7.241534, mean_absolute_error: 32.068432, mean_q: 41.658611
[F[K  21996/500000: episode: 378, duration: 1.200s, episode steps: 76, steps per second: 63, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.671 [0.000, 4.000], mean observation: 0.485 [0.410, 0.520], loss: 6.659503, mean_absolute_error: 32.730316, mean_q: 42.383862
[F[K  22081/500000: episode: 379, duration: 1.313s, episode steps: 85, steps per second: 65, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.035 [0.000, 4.000], mean observation: 0.500 [0.450, 0.530], loss: 7.369949, mean_absolute_error: 32.485870, mean_q: 42.199093
[F[K  22158/500000: episode: 380, duration: 1.282s, episode steps: 77, steps per second: 60, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.675 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 7.049919, mean_absolute_error: 32.144444, mean_q: 41.745193
[F[K  22241/500000: episode: 381, duration: 1.208s, episode steps: 83, steps per second: 69, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.373 [0.000, 4.000], mean observation: 0.486 [0.400, 0.510], loss: 7.621226, mean_absolute_error: 33.021652, mean_q: 42.731007
[F[K  22307/500000: episode: 382, duration: 1.036s, episode steps: 66, steps per second: 64, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.909 [0.000, 4.000], mean observation: 0.488 [0.370, 0.520], loss: 7.481387, mean_absolute_error: 33.006237, mean_q: 42.679199
[F[K  22356/500000: episode: 383, duration: 0.690s, episode steps: 49, steps per second: 71, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.673 [0.000, 4.000], mean observation: 0.498 [0.360, 0.610], loss: 7.156259, mean_absolute_error: 32.450592, mean_q: 42.078926
[F[K  22429/500000: episode: 384, duration: 1.177s, episode steps: 73, steps per second: 62, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.475 [0.360, 0.520], loss: 6.576167, mean_absolute_error: 32.728981, mean_q: 42.485386
[F[K  22465/500000: episode: 385, duration: 0.534s, episode steps: 36, steps per second: 67, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.361 [0.000, 3.000], mean observation: 0.522 [0.470, 0.630], loss: 7.799895, mean_absolute_error: 31.879337, mean_q: 41.519318
[F[K  22538/500000: episode: 386, duration: 1.132s, episode steps: 73, steps per second: 65, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.274 [0.000, 4.000], mean observation: 0.508 [0.470, 0.560], loss: 6.655629, mean_absolute_error: 32.873615, mean_q: 42.635715
[F[K  22608/500000: episode: 387, duration: 1.049s, episode steps: 70, steps per second: 67, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.586 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 6.383691, mean_absolute_error: 33.118839, mean_q: 43.172344
[F[K  22663/500000: episode: 388, duration: 0.942s, episode steps: 55, steps per second: 58, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.506 [0.460, 0.630], loss: 6.539505, mean_absolute_error: 32.531193, mean_q: 42.253174
[F[K  22700/500000: episode: 389, duration: 0.616s, episode steps: 37, steps per second: 60, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.521 [0.470, 0.640], loss: 6.364184, mean_absolute_error: 33.671471, mean_q: 43.560780
[F[K  22770/500000: episode: 390, duration: 1.074s, episode steps: 70, steps per second: 65, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.686 [0.000, 4.000], mean observation: 0.502 [0.400, 0.630], loss: 6.847263, mean_absolute_error: 32.794903, mean_q: 42.633213
[F[K  22829/500000: episode: 391, duration: 0.705s, episode steps: 59, steps per second: 84, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.514 [0.470, 0.650], loss: 7.907372, mean_absolute_error: 33.110043, mean_q: 42.908508
[F[K  22912/500000: episode: 392, duration: 1.166s, episode steps: 83, steps per second: 71, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.386 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 7.659021, mean_absolute_error: 32.561386, mean_q: 42.269482
[F[K  22971/500000: episode: 393, duration: 0.948s, episode steps: 59, steps per second: 62, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.288 [0.000, 4.000], mean observation: 0.498 [0.390, 0.590], loss: 7.801937, mean_absolute_error: 32.701675, mean_q: 42.424614
[F[K  23038/500000: episode: 394, duration: 1.103s, episode steps: 67, steps per second: 61, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.164 [0.000, 4.000], mean observation: 0.518 [0.460, 0.610], loss: 6.928407, mean_absolute_error: 32.036064, mean_q: 41.615860
[F[K  23117/500000: episode: 395, duration: 1.156s, episode steps: 79, steps per second: 68, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.975 [0.000, 4.000], mean observation: 0.488 [0.400, 0.530], loss: 6.880970, mean_absolute_error: 32.952980, mean_q: 42.689095
[F[K  23170/500000: episode: 396, duration: 0.716s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.528 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 7.867242, mean_absolute_error: 32.806366, mean_q: 42.366669
[F[K  23252/500000: episode: 397, duration: 1.195s, episode steps: 82, steps per second: 69, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.878 [0.000, 4.000], mean observation: 0.518 [0.460, 0.590], loss: 7.658136, mean_absolute_error: 32.618324, mean_q: 42.168514
[F[K  23337/500000: episode: 398, duration: 1.196s, episode steps: 85, steps per second: 71, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.529 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 6.974502, mean_absolute_error: 32.083096, mean_q: 41.668732
[F[K  23398/500000: episode: 399, duration: 0.826s, episode steps: 61, steps per second: 74, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.311 [0.000, 4.000], mean observation: 0.503 [0.440, 0.600], loss: 6.586282, mean_absolute_error: 32.226475, mean_q: 41.828743
[F[K  23466/500000: episode: 400, duration: 0.871s, episode steps: 68, steps per second: 78, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [0.000, 4.000], mean observation: 0.513 [0.460, 0.610], loss: 6.900424, mean_absolute_error: 32.337402, mean_q: 41.818855
[F[K  23512/500000: episode: 401, duration: 0.667s, episode steps: 46, steps per second: 69, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.348 [0.000, 4.000], mean observation: 0.496 [0.350, 0.600], loss: 6.179203, mean_absolute_error: 32.633228, mean_q: 42.320518
[F[K  23568/500000: episode: 402, duration: 0.827s, episode steps: 56, steps per second: 68, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.321 [0.000, 4.000], mean observation: 0.498 [0.350, 0.620], loss: 7.219209, mean_absolute_error: 32.143291, mean_q: 41.653381
[F[K  23674/500000: episode: 403, duration: 1.359s, episode steps: 106, steps per second: 78, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.494 [0.400, 0.550], loss: 7.060904, mean_absolute_error: 32.646660, mean_q: 42.337730
[F[K  23737/500000: episode: 404, duration: 0.874s, episode steps: 63, steps per second: 72, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.905 [0.000, 4.000], mean observation: 0.491 [0.430, 0.530], loss: 7.333377, mean_absolute_error: 32.086994, mean_q: 41.440571
[F[K  23795/500000: episode: 405, duration: 0.838s, episode steps: 58, steps per second: 69, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.491 [0.400, 0.530], loss: 7.761229, mean_absolute_error: 32.952675, mean_q: 42.599918
[F[K  23864/500000: episode: 406, duration: 0.934s, episode steps: 69, steps per second: 74, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.508 [0.480, 0.560], loss: 6.798709, mean_absolute_error: 32.740765, mean_q: 42.486084
[F[K  23936/500000: episode: 407, duration: 1.014s, episode steps: 72, steps per second: 71, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.507 [0.450, 0.630], loss: 7.199377, mean_absolute_error: 32.691772, mean_q: 42.193188
[F[K  24008/500000: episode: 408, duration: 0.852s, episode steps: 72, steps per second: 84, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.736 [0.000, 4.000], mean observation: 0.515 [0.460, 0.600], loss: 7.135081, mean_absolute_error: 32.585918, mean_q: 42.139668
[F[K  24075/500000: episode: 409, duration: 0.686s, episode steps: 67, steps per second: 98, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.254 [0.000, 4.000], mean observation: 0.499 [0.380, 0.580], loss: 7.593466, mean_absolute_error: 32.817196, mean_q: 42.538250
[F[K  24135/500000: episode: 410, duration: 0.749s, episode steps: 60, steps per second: 80, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.567 [0.000, 4.000], mean observation: 0.520 [0.470, 0.640], loss: 7.111509, mean_absolute_error: 32.012493, mean_q: 41.338039
[F[K  24231/500000: episode: 411, duration: 1.273s, episode steps: 96, steps per second: 75, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.427 [0.000, 4.000], mean observation: 0.494 [0.380, 0.540], loss: 7.351185, mean_absolute_error: 32.815716, mean_q: 42.399204
[F[K  24303/500000: episode: 412, duration: 0.804s, episode steps: 72, steps per second: 90, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.639 [0.000, 4.000], mean observation: 0.481 [0.350, 0.510], loss: 6.405286, mean_absolute_error: 32.581039, mean_q: 42.317966
[F[K  24364/500000: episode: 413, duration: 0.809s, episode steps: 61, steps per second: 75, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.459 [0.000, 4.000], mean observation: 0.485 [0.350, 0.510], loss: 7.474352, mean_absolute_error: 32.771725, mean_q: 42.456940
[F[K  24444/500000: episode: 414, duration: 1.051s, episode steps: 80, steps per second: 76, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.562 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 8.145134, mean_absolute_error: 31.878925, mean_q: 41.223030
[F[K  24488/500000: episode: 415, duration: 0.565s, episode steps: 44, steps per second: 78, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.511 [0.470, 0.640], loss: 7.063102, mean_absolute_error: 32.488461, mean_q: 42.119553
[F[K  24547/500000: episode: 416, duration: 0.892s, episode steps: 59, steps per second: 66, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.305 [0.000, 4.000], mean observation: 0.495 [0.420, 0.540], loss: 5.500268, mean_absolute_error: 32.761841, mean_q: 42.371426
[F[K  24616/500000: episode: 417, duration: 0.686s, episode steps: 69, steps per second: 101, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.609 [0.000, 4.000], mean observation: 0.504 [0.470, 0.530], loss: 6.398212, mean_absolute_error: 32.672501, mean_q: 42.299976
[F[K  24676/500000: episode: 418, duration: 0.772s, episode steps: 60, steps per second: 78, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.475 [0.360, 0.520], loss: 6.737863, mean_absolute_error: 32.355892, mean_q: 41.953659
[F[K  24736/500000: episode: 419, duration: 0.779s, episode steps: 60, steps per second: 77, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.517 [0.000, 4.000], mean observation: 0.503 [0.400, 0.650], loss: 6.917916, mean_absolute_error: 31.800457, mean_q: 41.435539
[F[K  24800/500000: episode: 420, duration: 0.752s, episode steps: 64, steps per second: 85, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.203 [0.000, 4.000], mean observation: 0.499 [0.460, 0.560], loss: 7.123480, mean_absolute_error: 31.947994, mean_q: 41.566002
[F[K  24861/500000: episode: 421, duration: 0.784s, episode steps: 61, steps per second: 78, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.541 [0.000, 4.000], mean observation: 0.504 [0.440, 0.600], loss: 7.266786, mean_absolute_error: 32.732243, mean_q: 42.398643
[F[K  24912/500000: episode: 422, duration: 0.640s, episode steps: 51, steps per second: 80, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.157 [0.000, 4.000], mean observation: 0.496 [0.370, 0.630], loss: 7.578182, mean_absolute_error: 32.409241, mean_q: 41.834450
[F[K  24970/500000: episode: 423, duration: 0.847s, episode steps: 58, steps per second: 68, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 6.865261, mean_absolute_error: 31.767092, mean_q: 41.061409
[F[K  25024/500000: episode: 424, duration: 0.724s, episode steps: 54, steps per second: 75, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.426 [0.000, 4.000], mean observation: 0.494 [0.360, 0.580], loss: 7.510843, mean_absolute_error: 32.203915, mean_q: 41.549507
[F[K  25072/500000: episode: 425, duration: 0.640s, episode steps: 48, steps per second: 75, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.562 [0.000, 4.000], mean observation: 0.499 [0.360, 0.620], loss: 5.163434, mean_absolute_error: 32.339352, mean_q: 41.861797
[F[K  25127/500000: episode: 426, duration: 0.852s, episode steps: 55, steps per second: 65, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.273 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 6.469337, mean_absolute_error: 32.382278, mean_q: 42.117489
[F[K  25197/500000: episode: 427, duration: 0.996s, episode steps: 70, steps per second: 70, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.488 [0.420, 0.530], loss: 7.182830, mean_absolute_error: 31.861601, mean_q: 41.288891
[F[K  25306/500000: episode: 428, duration: 1.511s, episode steps: 109, steps per second: 72, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.413 [0.000, 4.000], mean observation: 0.478 [0.370, 0.520], loss: 6.916543, mean_absolute_error: 32.022324, mean_q: 41.518005
[F[K  25363/500000: episode: 429, duration: 0.786s, episode steps: 57, steps per second: 73, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.035 [0.000, 4.000], mean observation: 0.508 [0.450, 0.640], loss: 6.836374, mean_absolute_error: 32.047489, mean_q: 41.465057
[F[K  25427/500000: episode: 430, duration: 0.879s, episode steps: 64, steps per second: 73, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.094 [0.000, 4.000], mean observation: 0.516 [0.470, 0.600], loss: 7.133834, mean_absolute_error: 32.090702, mean_q: 41.577702
[F[K  25467/500000: episode: 431, duration: 0.603s, episode steps: 40, steps per second: 66, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.513 [0.470, 0.660], loss: 6.790252, mean_absolute_error: 31.753674, mean_q: 41.238533
[F[K  25545/500000: episode: 432, duration: 1.121s, episode steps: 78, steps per second: 70, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.487 [0.000, 4.000], mean observation: 0.474 [0.360, 0.520], loss: 8.150845, mean_absolute_error: 32.531166, mean_q: 42.212757
[F[K  25609/500000: episode: 433, duration: 0.877s, episode steps: 64, steps per second: 73, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.490 [0.430, 0.520], loss: 7.379170, mean_absolute_error: 32.736443, mean_q: 42.282921
[F[K  25662/500000: episode: 434, duration: 0.779s, episode steps: 53, steps per second: 68, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.623 [0.000, 4.000], mean observation: 0.504 [0.420, 0.650], loss: 7.816740, mean_absolute_error: 32.697464, mean_q: 42.431133
[F[K  25723/500000: episode: 435, duration: 0.896s, episode steps: 61, steps per second: 68, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.836 [0.000, 4.000], mean observation: 0.495 [0.410, 0.550], loss: 7.253281, mean_absolute_error: 32.158295, mean_q: 41.634586
[F[K  25776/500000: episode: 436, duration: 0.713s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.849 [0.000, 4.000], mean observation: 0.495 [0.360, 0.600], loss: 6.875270, mean_absolute_error: 32.257042, mean_q: 41.703419
[F[K  25910/500000: episode: 437, duration: 1.905s, episode steps: 134, steps per second: 70, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.060 [0.000, 4.000], mean observation: 0.512 [0.470, 0.620], loss: 6.522353, mean_absolute_error: 32.034412, mean_q: 41.426189
[F[K  25982/500000: episode: 438, duration: 1.053s, episode steps: 72, steps per second: 68, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.458 [0.000, 4.000], mean observation: 0.499 [0.380, 0.610], loss: 6.798494, mean_absolute_error: 32.276772, mean_q: 41.633125
[F[K  26057/500000: episode: 439, duration: 1.118s, episode steps: 75, steps per second: 67, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.187 [0.000, 4.000], mean observation: 0.482 [0.390, 0.520], loss: 6.808389, mean_absolute_error: 32.579994, mean_q: 42.273659
[F[K  26097/500000: episode: 440, duration: 0.696s, episode steps: 40, steps per second: 57, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.300 [0.000, 3.000], mean observation: 0.502 [0.400, 0.640], loss: 6.654746, mean_absolute_error: 32.285194, mean_q: 41.828674
[F[K  26175/500000: episode: 441, duration: 1.009s, episode steps: 78, steps per second: 77, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.141 [0.000, 4.000], mean observation: 0.502 [0.400, 0.610], loss: 7.279445, mean_absolute_error: 32.050140, mean_q: 41.448406
[F[K  26239/500000: episode: 442, duration: 0.937s, episode steps: 64, steps per second: 68, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.062 [0.000, 4.000], mean observation: 0.481 [0.370, 0.530], loss: 6.838084, mean_absolute_error: 32.282135, mean_q: 41.633347
[F[K  26325/500000: episode: 443, duration: 1.114s, episode steps: 86, steps per second: 77, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.070 [0.000, 4.000], mean observation: 0.511 [0.450, 0.570], loss: 6.988643, mean_absolute_error: 31.931519, mean_q: 41.235737
[F[K  26376/500000: episode: 444, duration: 0.669s, episode steps: 51, steps per second: 76, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.686 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 6.858205, mean_absolute_error: 32.468140, mean_q: 42.054276
[F[K  26445/500000: episode: 445, duration: 0.886s, episode steps: 69, steps per second: 78, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.478 [0.000, 4.000], mean observation: 0.507 [0.450, 0.590], loss: 6.541341, mean_absolute_error: 32.727814, mean_q: 42.407768
[F[K  26586/500000: episode: 446, duration: 1.910s, episode steps: 141, steps per second: 74, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.482 [0.000, 4.000], mean observation: 0.489 [0.410, 0.520], loss: 6.963518, mean_absolute_error: 32.349747, mean_q: 41.829468
[F[K  26624/500000: episode: 447, duration: 0.527s, episode steps: 38, steps per second: 72, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.289 [0.000, 2.000], mean observation: 0.524 [0.470, 0.650], loss: 7.689681, mean_absolute_error: 32.479336, mean_q: 41.935059
[F[K  26664/500000: episode: 448, duration: 0.614s, episode steps: 40, steps per second: 65, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 3.000], mean observation: 0.505 [0.400, 0.650], loss: 7.205657, mean_absolute_error: 32.760479, mean_q: 42.431690
[F[K  26722/500000: episode: 449, duration: 0.869s, episode steps: 58, steps per second: 67, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.491 [0.390, 0.530], loss: 6.744329, mean_absolute_error: 31.887363, mean_q: 41.290348
[F[K  26789/500000: episode: 450, duration: 0.937s, episode steps: 67, steps per second: 72, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.501 [0.470, 0.520], loss: 7.460481, mean_absolute_error: 32.426922, mean_q: 41.875282
[F[K  26852/500000: episode: 451, duration: 0.898s, episode steps: 63, steps per second: 70, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.496 [0.460, 0.520], loss: 7.266120, mean_absolute_error: 32.348267, mean_q: 41.823818
[F[K  26922/500000: episode: 452, duration: 1.049s, episode steps: 70, steps per second: 67, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.114 [0.000, 4.000], mean observation: 0.493 [0.410, 0.530], loss: 6.386498, mean_absolute_error: 31.625051, mean_q: 40.977642
[F[K  26961/500000: episode: 453, duration: 0.620s, episode steps: 39, steps per second: 63, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.521 [0.470, 0.650], loss: 6.268639, mean_absolute_error: 32.744186, mean_q: 42.441326
[F[K  27041/500000: episode: 454, duration: 0.909s, episode steps: 80, steps per second: 88, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.163 [0.000, 4.000], mean observation: 0.487 [0.420, 0.520], loss: 7.633138, mean_absolute_error: 32.706352, mean_q: 42.151020
[F[K  27114/500000: episode: 455, duration: 1.112s, episode steps: 73, steps per second: 66, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.014 [0.000, 4.000], mean observation: 0.511 [0.470, 0.580], loss: 6.741822, mean_absolute_error: 32.479935, mean_q: 41.991165
[F[K  27175/500000: episode: 456, duration: 0.849s, episode steps: 61, steps per second: 72, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.344 [0.000, 4.000], mean observation: 0.492 [0.360, 0.570], loss: 7.046937, mean_absolute_error: 32.345348, mean_q: 41.762787
[F[K  27260/500000: episode: 457, duration: 1.155s, episode steps: 85, steps per second: 74, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.871 [0.000, 4.000], mean observation: 0.501 [0.430, 0.570], loss: 6.846034, mean_absolute_error: 32.586567, mean_q: 42.094292
[F[K  27367/500000: episode: 458, duration: 1.302s, episode steps: 107, steps per second: 82, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.514 [0.000, 4.000], mean observation: 0.492 [0.370, 0.540], loss: 7.608544, mean_absolute_error: 32.591858, mean_q: 42.069485
[F[K  27425/500000: episode: 459, duration: 0.941s, episode steps: 58, steps per second: 62, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.486 [0.360, 0.520], loss: 6.773897, mean_absolute_error: 32.250183, mean_q: 41.825939
[F[K  27485/500000: episode: 460, duration: 1.034s, episode steps: 60, steps per second: 58, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.533 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 6.520942, mean_absolute_error: 32.830498, mean_q: 42.579403
[F[K  27543/500000: episode: 461, duration: 0.805s, episode steps: 58, steps per second: 72, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.509 [0.460, 0.630], loss: 6.859969, mean_absolute_error: 33.352238, mean_q: 43.047852
[F[K  27622/500000: episode: 462, duration: 1.119s, episode steps: 79, steps per second: 71, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.013 [0.000, 4.000], mean observation: 0.508 [0.440, 0.650], loss: 6.461817, mean_absolute_error: 32.653667, mean_q: 42.146622
[F[K  27709/500000: episode: 463, duration: 1.321s, episode steps: 87, steps per second: 66, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.034 [0.000, 4.000], mean observation: 0.512 [0.460, 0.620], loss: 7.083107, mean_absolute_error: 32.824917, mean_q: 42.312759
[F[K  27816/500000: episode: 464, duration: 1.761s, episode steps: 107, steps per second: 61, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.327 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 6.405225, mean_absolute_error: 32.928276, mean_q: 42.453377
[F[K  27865/500000: episode: 465, duration: 0.724s, episode steps: 49, steps per second: 68, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.484 [0.390, 0.530], loss: 6.215185, mean_absolute_error: 32.687366, mean_q: 42.185944
[F[K  27899/500000: episode: 466, duration: 0.500s, episode steps: 34, steps per second: 68, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.412 [0.000, 4.000], mean observation: 0.510 [0.460, 0.650], loss: 7.120855, mean_absolute_error: 33.239685, mean_q: 42.763191
[F[K  27942/500000: episode: 467, duration: 0.688s, episode steps: 43, steps per second: 62, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.512 [0.000, 3.000], mean observation: 0.504 [0.400, 0.640], loss: 6.393944, mean_absolute_error: 32.115913, mean_q: 41.483658
[F[K  28000/500000: episode: 468, duration: 0.954s, episode steps: 58, steps per second: 61, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.672 [0.000, 4.000], mean observation: 0.495 [0.420, 0.540], loss: 6.591856, mean_absolute_error: 33.323853, mean_q: 43.014442
[F[K  28047/500000: episode: 469, duration: 0.761s, episode steps: 47, steps per second: 62, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.404 [0.000, 4.000], mean observation: 0.503 [0.350, 0.650], loss: 5.953319, mean_absolute_error: 33.086079, mean_q: 42.781128
[F[K  28124/500000: episode: 470, duration: 1.094s, episode steps: 77, steps per second: 70, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.455 [0.000, 4.000], mean observation: 0.486 [0.370, 0.520], loss: 6.184224, mean_absolute_error: 33.205215, mean_q: 42.955345
[F[K  28175/500000: episode: 471, duration: 0.750s, episode steps: 51, steps per second: 68, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.961 [0.000, 4.000], mean observation: 0.511 [0.470, 0.650], loss: 6.930194, mean_absolute_error: 32.828152, mean_q: 42.526417
[F[K  28248/500000: episode: 472, duration: 1.134s, episode steps: 73, steps per second: 64, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 7.071003, mean_absolute_error: 33.039268, mean_q: 42.459099
[F[K  28318/500000: episode: 473, duration: 1.001s, episode steps: 70, steps per second: 70, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.504 [0.430, 0.580], loss: 7.125305, mean_absolute_error: 32.301392, mean_q: 41.821655
[F[K  28381/500000: episode: 474, duration: 1.037s, episode steps: 63, steps per second: 61, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.937 [0.000, 4.000], mean observation: 0.480 [0.380, 0.530], loss: 6.914865, mean_absolute_error: 33.504250, mean_q: 43.327217
[F[K  28466/500000: episode: 475, duration: 1.357s, episode steps: 85, steps per second: 63, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.812 [0.000, 4.000], mean observation: 0.524 [0.470, 0.620], loss: 6.501381, mean_absolute_error: 32.839207, mean_q: 42.450836
[F[K  28524/500000: episode: 476, duration: 1.004s, episode steps: 58, steps per second: 58, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.362 [0.000, 4.000], mean observation: 0.512 [0.470, 0.610], loss: 7.034398, mean_absolute_error: 32.811207, mean_q: 42.447517
[F[K  28623/500000: episode: 477, duration: 1.615s, episode steps: 99, steps per second: 61, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.394 [0.000, 4.000], mean observation: 0.486 [0.370, 0.520], loss: 6.606063, mean_absolute_error: 33.183952, mean_q: 42.866661
[F[K  28692/500000: episode: 478, duration: 1.289s, episode steps: 69, steps per second: 54, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.507 [0.000, 4.000], mean observation: 0.520 [0.480, 0.630], loss: 6.880584, mean_absolute_error: 32.303410, mean_q: 41.663265
[F[K  28747/500000: episode: 479, duration: 0.873s, episode steps: 55, steps per second: 63, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 7.409672, mean_absolute_error: 32.293892, mean_q: 41.754101
[F[K  28812/500000: episode: 480, duration: 0.847s, episode steps: 65, steps per second: 77, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.569 [0.000, 4.000], mean observation: 0.499 [0.470, 0.530], loss: 7.050456, mean_absolute_error: 33.124565, mean_q: 42.768669
[F[K  28863/500000: episode: 481, duration: 0.699s, episode steps: 51, steps per second: 73, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.078 [0.000, 4.000], mean observation: 0.513 [0.490, 0.560], loss: 7.021407, mean_absolute_error: 32.055683, mean_q: 41.484322
[F[K  28917/500000: episode: 482, duration: 0.809s, episode steps: 54, steps per second: 67, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 6.371143, mean_absolute_error: 32.669590, mean_q: 42.198025
[F[K  28995/500000: episode: 483, duration: 1.397s, episode steps: 78, steps per second: 56, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.692 [0.000, 4.000], mean observation: 0.505 [0.470, 0.540], loss: 6.563138, mean_absolute_error: 32.490601, mean_q: 41.923702
[F[K  29062/500000: episode: 484, duration: 1.124s, episode steps: 67, steps per second: 60, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.746 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 6.583462, mean_absolute_error: 32.547321, mean_q: 42.040092
[F[K  29120/500000: episode: 485, duration: 1.050s, episode steps: 58, steps per second: 55, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.569 [0.000, 4.000], mean observation: 0.492 [0.450, 0.520], loss: 7.052084, mean_absolute_error: 33.159637, mean_q: 42.800686
[F[K  29184/500000: episode: 486, duration: 1.068s, episode steps: 64, steps per second: 60, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.507 [0.470, 0.560], loss: 7.171471, mean_absolute_error: 32.728973, mean_q: 42.120422
[F[K  29264/500000: episode: 487, duration: 1.250s, episode steps: 80, steps per second: 64, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.587 [0.000, 4.000], mean observation: 0.489 [0.410, 0.530], loss: 6.839220, mean_absolute_error: 32.127903, mean_q: 41.489368
[F[K  29348/500000: episode: 488, duration: 1.256s, episode steps: 84, steps per second: 67, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.486 [0.400, 0.530], loss: 7.213036, mean_absolute_error: 32.742577, mean_q: 42.187626
[F[K  29423/500000: episode: 489, duration: 1.347s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.560 [0.000, 4.000], mean observation: 0.476 [0.360, 0.510], loss: 6.820023, mean_absolute_error: 32.628414, mean_q: 42.222980
[F[K  29491/500000: episode: 490, duration: 1.088s, episode steps: 68, steps per second: 62, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.779 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 6.814783, mean_absolute_error: 32.067253, mean_q: 41.429970
[F[K  29542/500000: episode: 491, duration: 0.856s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.686 [0.000, 4.000], mean observation: 0.512 [0.470, 0.630], loss: 6.228592, mean_absolute_error: 32.891220, mean_q: 42.298229
[F[K  29622/500000: episode: 492, duration: 1.360s, episode steps: 80, steps per second: 59, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.498 [0.430, 0.550], loss: 7.472673, mean_absolute_error: 32.625587, mean_q: 42.011555
[F[K  29661/500000: episode: 493, duration: 0.754s, episode steps: 39, steps per second: 52, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.462 [0.000, 4.000], mean observation: 0.499 [0.370, 0.620], loss: 5.882514, mean_absolute_error: 32.479050, mean_q: 42.021015
[F[K  29719/500000: episode: 494, duration: 1.100s, episode steps: 58, steps per second: 53, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.948 [0.000, 4.000], mean observation: 0.499 [0.430, 0.530], loss: 5.888043, mean_absolute_error: 32.892918, mean_q: 42.595417
[F[K  29794/500000: episode: 495, duration: 1.342s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.488 [0.370, 0.520], loss: 6.650421, mean_absolute_error: 32.232388, mean_q: 41.530327
[F[K  29861/500000: episode: 496, duration: 1.331s, episode steps: 67, steps per second: 50, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.433 [0.000, 4.000], mean observation: 0.492 [0.380, 0.530], loss: 6.944656, mean_absolute_error: 32.798462, mean_q: 42.218548
[F[K  29925/500000: episode: 497, duration: 1.172s, episode steps: 64, steps per second: 55, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.499 [0.390, 0.570], loss: 6.777335, mean_absolute_error: 32.852482, mean_q: 42.334747
[F[K  29998/500000: episode: 498, duration: 1.386s, episode steps: 73, steps per second: 53, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.616 [0.000, 4.000], mean observation: 0.496 [0.390, 0.580], loss: 7.756411, mean_absolute_error: 32.662800, mean_q: 42.040367
[F[K  30047/500000: episode: 499, duration: 0.763s, episode steps: 49, steps per second: 64, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.020 [0.000, 4.000], mean observation: 0.501 [0.350, 0.650], loss: 6.790166, mean_absolute_error: 32.998993, mean_q: 42.514214
[F[K  30105/500000: episode: 500, duration: 1.018s, episode steps: 58, steps per second: 57, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.138 [0.000, 4.000], mean observation: 0.491 [0.440, 0.530], loss: 7.239130, mean_absolute_error: 32.967495, mean_q: 42.570648
[F[K  30167/500000: episode: 501, duration: 1.039s, episode steps: 62, steps per second: 60, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.677 [0.000, 4.000], mean observation: 0.476 [0.360, 0.510], loss: 7.622503, mean_absolute_error: 32.706734, mean_q: 42.090492
[F[K  30225/500000: episode: 502, duration: 1.074s, episode steps: 58, steps per second: 54, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 0.513 [0.500, 0.580], loss: 6.330790, mean_absolute_error: 32.496162, mean_q: 42.049610
[F[K  30279/500000: episode: 503, duration: 0.866s, episode steps: 54, steps per second: 62, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.519 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 7.383067, mean_absolute_error: 32.859203, mean_q: 42.291828
[F[K  30374/500000: episode: 504, duration: 1.468s, episode steps: 95, steps per second: 65, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.842 [0.000, 4.000], mean observation: 0.510 [0.460, 0.600], loss: 7.229906, mean_absolute_error: 32.313499, mean_q: 41.923317
[F[K  30412/500000: episode: 505, duration: 0.680s, episode steps: 38, steps per second: 56, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.842 [0.000, 4.000], mean observation: 0.516 [0.470, 0.640], loss: 7.505024, mean_absolute_error: 32.993652, mean_q: 42.480751
[F[K  30477/500000: episode: 506, duration: 1.127s, episode steps: 65, steps per second: 58, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.477 [0.000, 4.000], mean observation: 0.516 [0.480, 0.620], loss: 7.828338, mean_absolute_error: 31.882936, mean_q: 41.147583
[F[K  30544/500000: episode: 507, duration: 1.222s, episode steps: 67, steps per second: 55, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.119 [0.000, 4.000], mean observation: 0.489 [0.420, 0.520], loss: 6.285261, mean_absolute_error: 32.093487, mean_q: 41.577549
[F[K  30619/500000: episode: 508, duration: 1.245s, episode steps: 75, steps per second: 60, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.693 [0.000, 4.000], mean observation: 0.487 [0.370, 0.530], loss: 7.205559, mean_absolute_error: 32.797363, mean_q: 42.363224
[F[K  30677/500000: episode: 509, duration: 0.924s, episode steps: 58, steps per second: 63, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.638 [0.000, 4.000], mean observation: 0.502 [0.440, 0.590], loss: 7.213269, mean_absolute_error: 32.393219, mean_q: 41.650478
[F[K  30740/500000: episode: 510, duration: 0.952s, episode steps: 63, steps per second: 66, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.889 [0.000, 4.000], mean observation: 0.493 [0.390, 0.530], loss: 7.691432, mean_absolute_error: 31.892853, mean_q: 41.213219
[F[K  30812/500000: episode: 511, duration: 1.042s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.944 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 6.500491, mean_absolute_error: 32.066669, mean_q: 41.469299
[F[K  30872/500000: episode: 512, duration: 0.986s, episode steps: 60, steps per second: 61, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.533 [0.000, 4.000], mean observation: 0.493 [0.450, 0.520], loss: 6.204349, mean_absolute_error: 32.738659, mean_q: 42.366463
[F[K  30931/500000: episode: 513, duration: 1.001s, episode steps: 59, steps per second: 59, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.661 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 7.032190, mean_absolute_error: 32.894398, mean_q: 42.460743
[F[K  30986/500000: episode: 514, duration: 0.739s, episode steps: 55, steps per second: 74, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.509 [0.000, 4.000], mean observation: 0.518 [0.470, 0.600], loss: 6.250786, mean_absolute_error: 33.309593, mean_q: 43.026131
[F[K  31019/500000: episode: 515, duration: 0.602s, episode steps: 33, steps per second: 55, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.515 [0.000, 4.000], mean observation: 0.529 [0.470, 0.650], loss: 7.877086, mean_absolute_error: 32.021584, mean_q: 41.246567
[F[K  31081/500000: episode: 516, duration: 0.976s, episode steps: 62, steps per second: 64, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.774 [0.000, 4.000], mean observation: 0.485 [0.370, 0.520], loss: 7.186603, mean_absolute_error: 32.131084, mean_q: 41.482193
[F[K  31138/500000: episode: 517, duration: 0.898s, episode steps: 57, steps per second: 63, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.140 [0.000, 4.000], mean observation: 0.486 [0.380, 0.520], loss: 6.781368, mean_absolute_error: 32.472576, mean_q: 41.909973
[F[K  31216/500000: episode: 518, duration: 1.224s, episode steps: 78, steps per second: 64, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.603 [0.000, 4.000], mean observation: 0.509 [0.440, 0.650], loss: 7.507965, mean_absolute_error: 32.771198, mean_q: 42.231007
[F[K  31286/500000: episode: 519, duration: 1.105s, episode steps: 70, steps per second: 63, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.498 [0.430, 0.530], loss: 6.489201, mean_absolute_error: 33.289795, mean_q: 43.008198
[F[K  31357/500000: episode: 520, duration: 1.144s, episode steps: 71, steps per second: 62, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.451 [0.000, 4.000], mean observation: 0.502 [0.450, 0.580], loss: 5.852083, mean_absolute_error: 32.724113, mean_q: 42.271198
[F[K  31426/500000: episode: 521, duration: 1.102s, episode steps: 69, steps per second: 63, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.246 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 7.528317, mean_absolute_error: 32.505936, mean_q: 41.860355
[F[K  31502/500000: episode: 522, duration: 1.490s, episode steps: 76, steps per second: 51, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.053 [0.000, 4.000], mean observation: 0.516 [0.460, 0.590], loss: 7.075277, mean_absolute_error: 32.750626, mean_q: 42.228481
[F[K  31574/500000: episode: 523, duration: 1.008s, episode steps: 72, steps per second: 71, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.505 [0.470, 0.580], loss: 7.202960, mean_absolute_error: 32.359329, mean_q: 41.735733
[F[K  31631/500000: episode: 524, duration: 0.928s, episode steps: 57, steps per second: 61, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.484 [0.360, 0.510], loss: 6.955624, mean_absolute_error: 32.993778, mean_q: 42.607113
[F[K  31690/500000: episode: 525, duration: 0.869s, episode steps: 59, steps per second: 68, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.492 [0.000, 4.000], mean observation: 0.498 [0.400, 0.590], loss: 7.242697, mean_absolute_error: 31.921137, mean_q: 41.273640
[F[K  31741/500000: episode: 526, duration: 0.783s, episode steps: 51, steps per second: 65, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.784 [0.000, 4.000], mean observation: 0.495 [0.370, 0.580], loss: 7.688806, mean_absolute_error: 32.550648, mean_q: 41.992077
[F[K  31795/500000: episode: 527, duration: 1.022s, episode steps: 54, steps per second: 53, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.574 [0.000, 4.000], mean observation: 0.482 [0.360, 0.510], loss: 7.009366, mean_absolute_error: 32.828785, mean_q: 42.303638
[F[K  31859/500000: episode: 528, duration: 1.146s, episode steps: 64, steps per second: 56, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.609 [0.000, 4.000], mean observation: 0.505 [0.420, 0.650], loss: 6.799933, mean_absolute_error: 32.636822, mean_q: 42.217583
[F[K  31905/500000: episode: 529, duration: 0.747s, episode steps: 46, steps per second: 62, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.457 [0.000, 4.000], mean observation: 0.501 [0.350, 0.630], loss: 6.280911, mean_absolute_error: 32.676319, mean_q: 42.160465
[F[K  31983/500000: episode: 530, duration: 1.276s, episode steps: 78, steps per second: 61, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.769 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 6.727055, mean_absolute_error: 32.862381, mean_q: 42.315544
[F[K  32042/500000: episode: 531, duration: 1.014s, episode steps: 59, steps per second: 58, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.513 [0.470, 0.580], loss: 6.421870, mean_absolute_error: 32.466911, mean_q: 41.938358
[F[K  32150/500000: episode: 532, duration: 1.891s, episode steps: 108, steps per second: 57, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.398 [0.000, 4.000], mean observation: 0.493 [0.410, 0.560], loss: 6.569229, mean_absolute_error: 33.053864, mean_q: 42.597004
[F[K  32213/500000: episode: 533, duration: 1.036s, episode steps: 63, steps per second: 61, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.762 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 6.299146, mean_absolute_error: 32.428265, mean_q: 41.759926
[F[K  32281/500000: episode: 534, duration: 1.161s, episode steps: 68, steps per second: 59, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.265 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 7.251789, mean_absolute_error: 32.415020, mean_q: 41.763683
[F[K  32340/500000: episode: 535, duration: 1.001s, episode steps: 59, steps per second: 59, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.502 [0.410, 0.600], loss: 6.992939, mean_absolute_error: 33.366150, mean_q: 42.962124
[F[K  32375/500000: episode: 536, duration: 0.525s, episode steps: 35, steps per second: 67, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.524 [0.470, 0.640], loss: 7.689710, mean_absolute_error: 31.322670, mean_q: 40.559658
[F[K  32442/500000: episode: 537, duration: 1.071s, episode steps: 67, steps per second: 63, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.687 [0.000, 4.000], mean observation: 0.484 [0.370, 0.530], loss: 7.142883, mean_absolute_error: 31.910496, mean_q: 41.068150
[F[K  32534/500000: episode: 538, duration: 1.408s, episode steps: 92, steps per second: 65, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.293 [0.000, 4.000], mean observation: 0.503 [0.420, 0.590], loss: 6.669806, mean_absolute_error: 33.021732, mean_q: 42.623291
[F[K  32625/500000: episode: 539, duration: 1.544s, episode steps: 91, steps per second: 59, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.758 [0.000, 4.000], mean observation: 0.507 [0.460, 0.590], loss: 7.801382, mean_absolute_error: 32.411324, mean_q: 41.798649
[F[K  32672/500000: episode: 540, duration: 0.685s, episode steps: 47, steps per second: 69, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.340 [0.000, 4.000], mean observation: 0.504 [0.390, 0.640], loss: 6.873294, mean_absolute_error: 32.419682, mean_q: 41.696819
[F[K  32747/500000: episode: 541, duration: 1.322s, episode steps: 75, steps per second: 57, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.680 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 6.941433, mean_absolute_error: 32.226154, mean_q: 41.617939
[F[K  32807/500000: episode: 542, duration: 1.153s, episode steps: 60, steps per second: 52, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.067 [0.000, 4.000], mean observation: 0.519 [0.490, 0.600], loss: 7.840165, mean_absolute_error: 32.428143, mean_q: 41.780399
[F[K  32863/500000: episode: 543, duration: 0.900s, episode steps: 56, steps per second: 62, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.839 [0.000, 4.000], mean observation: 0.501 [0.360, 0.640], loss: 6.645829, mean_absolute_error: 32.788872, mean_q: 42.379963
[F[K  32912/500000: episode: 544, duration: 0.875s, episode steps: 49, steps per second: 56, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.776 [0.000, 4.000], mean observation: 0.510 [0.470, 0.570], loss: 6.071425, mean_absolute_error: 32.433929, mean_q: 41.928272
[F[K  32993/500000: episode: 545, duration: 1.360s, episode steps: 81, steps per second: 60, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.951 [0.000, 4.000], mean observation: 0.489 [0.410, 0.530], loss: 7.502013, mean_absolute_error: 32.763779, mean_q: 42.171520
[F[K  33049/500000: episode: 546, duration: 0.746s, episode steps: 56, steps per second: 75, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.857 [0.000, 4.000], mean observation: 0.504 [0.370, 0.650], loss: 7.605680, mean_absolute_error: 32.275661, mean_q: 41.739159
[F[K  33134/500000: episode: 547, duration: 1.170s, episode steps: 85, steps per second: 73, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.506 [0.000, 4.000], mean observation: 0.503 [0.470, 0.540], loss: 7.429387, mean_absolute_error: 32.767830, mean_q: 42.312984
[F[K  33206/500000: episode: 548, duration: 0.993s, episode steps: 72, steps per second: 73, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.361 [0.000, 4.000], mean observation: 0.497 [0.380, 0.570], loss: 6.390696, mean_absolute_error: 32.367622, mean_q: 41.737148
[F[K  33263/500000: episode: 549, duration: 0.776s, episode steps: 57, steps per second: 73, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.421 [0.000, 4.000], mean observation: 0.513 [0.470, 0.630], loss: 6.403009, mean_absolute_error: 33.136429, mean_q: 42.613800
[F[K  33318/500000: episode: 550, duration: 0.836s, episode steps: 55, steps per second: 66, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.545 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 7.452799, mean_absolute_error: 32.492519, mean_q: 41.844833
[F[K  33381/500000: episode: 551, duration: 0.990s, episode steps: 63, steps per second: 64, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 0.502 [0.360, 0.660], loss: 6.891512, mean_absolute_error: 32.216934, mean_q: 41.601982
[F[K  33421/500000: episode: 552, duration: 0.727s, episode steps: 40, steps per second: 55, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.505 [0.410, 0.660], loss: 8.387975, mean_absolute_error: 32.451103, mean_q: 41.752865
[F[K  33492/500000: episode: 553, duration: 1.069s, episode steps: 71, steps per second: 66, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.634 [0.000, 4.000], mean observation: 0.511 [0.480, 0.600], loss: 7.227019, mean_absolute_error: 32.967079, mean_q: 42.441807
[F[K  33568/500000: episode: 554, duration: 1.110s, episode steps: 76, steps per second: 68, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.395 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 6.899919, mean_absolute_error: 32.168354, mean_q: 41.453548
[F[K  33621/500000: episode: 555, duration: 0.710s, episode steps: 53, steps per second: 75, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.151 [0.000, 4.000], mean observation: 0.512 [0.470, 0.650], loss: 7.361931, mean_absolute_error: 33.220669, mean_q: 42.764278
[F[K  33705/500000: episode: 556, duration: 1.267s, episode steps: 84, steps per second: 66, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.499 [0.410, 0.550], loss: 7.079241, mean_absolute_error: 32.374413, mean_q: 41.746380
[F[K  33764/500000: episode: 557, duration: 0.950s, episode steps: 59, steps per second: 62, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.500 [0.370, 0.620], loss: 6.527539, mean_absolute_error: 32.182201, mean_q: 41.623020
[F[K  33847/500000: episode: 558, duration: 1.253s, episode steps: 83, steps per second: 66, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.024 [0.000, 4.000], mean observation: 0.490 [0.390, 0.530], loss: 6.409442, mean_absolute_error: 32.388641, mean_q: 41.776108
[F[K  33908/500000: episode: 559, duration: 1.006s, episode steps: 61, steps per second: 61, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 8.070268, mean_absolute_error: 32.353436, mean_q: 41.762360
[F[K  34072/500000: episode: 560, duration: 2.722s, episode steps: 164, steps per second: 60, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.707 [0.000, 4.000], mean observation: 0.491 [0.420, 0.520], loss: 6.967979, mean_absolute_error: 32.528046, mean_q: 42.045471
[F[K  34124/500000: episode: 561, duration: 0.823s, episode steps: 52, steps per second: 63, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.404 [0.000, 4.000], mean observation: 0.474 [0.350, 0.530], loss: 6.327648, mean_absolute_error: 32.979588, mean_q: 42.566460
[F[K  34194/500000: episode: 562, duration: 1.119s, episode steps: 70, steps per second: 63, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.929 [0.000, 4.000], mean observation: 0.499 [0.420, 0.560], loss: 6.888008, mean_absolute_error: 32.618614, mean_q: 42.119194
[F[K  34313/500000: episode: 563, duration: 1.690s, episode steps: 119, steps per second: 70, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.008 [0.000, 4.000], mean observation: 0.490 [0.390, 0.530], loss: 6.516085, mean_absolute_error: 32.829739, mean_q: 42.354156
[F[K  34415/500000: episode: 564, duration: 1.511s, episode steps: 102, steps per second: 67, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.255 [0.000, 4.000], mean observation: 0.509 [0.480, 0.560], loss: 6.610120, mean_absolute_error: 33.322445, mean_q: 42.996109
[F[K  34480/500000: episode: 565, duration: 0.966s, episode steps: 65, steps per second: 67, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.508 [0.460, 0.620], loss: 7.864338, mean_absolute_error: 33.354103, mean_q: 42.903534
[F[K  34539/500000: episode: 566, duration: 0.861s, episode steps: 59, steps per second: 69, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.509 [0.490, 0.570], loss: 7.032303, mean_absolute_error: 32.135921, mean_q: 41.482697
[F[K  34588/500000: episode: 567, duration: 0.918s, episode steps: 49, steps per second: 53, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.653 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 6.821255, mean_absolute_error: 33.036404, mean_q: 42.461948
[F[K  34650/500000: episode: 568, duration: 0.968s, episode steps: 62, steps per second: 64, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.419 [0.000, 4.000], mean observation: 0.511 [0.470, 0.620], loss: 7.093364, mean_absolute_error: 33.794254, mean_q: 43.458687
[F[K  34687/500000: episode: 569, duration: 0.499s, episode steps: 37, steps per second: 74, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.405 [0.000, 4.000], mean observation: 0.515 [0.470, 0.650], loss: 6.902080, mean_absolute_error: 32.120781, mean_q: 41.477112
[F[K  34740/500000: episode: 570, duration: 0.759s, episode steps: 53, steps per second: 70, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.566 [0.000, 4.000], mean observation: 0.503 [0.470, 0.520], loss: 7.369252, mean_absolute_error: 32.927723, mean_q: 42.329407
[F[K  34809/500000: episode: 571, duration: 1.110s, episode steps: 69, steps per second: 62, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 6.867334, mean_absolute_error: 32.390301, mean_q: 41.630848
[F[K  34879/500000: episode: 572, duration: 1.231s, episode steps: 70, steps per second: 57, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.114 [0.000, 4.000], mean observation: 0.506 [0.440, 0.620], loss: 6.124451, mean_absolute_error: 32.736649, mean_q: 42.312763
[F[K  34968/500000: episode: 573, duration: 1.180s, episode steps: 89, steps per second: 75, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.427 [0.000, 4.000], mean observation: 0.497 [0.360, 0.600], loss: 6.961177, mean_absolute_error: 33.159149, mean_q: 42.750225
[F[K  35041/500000: episode: 574, duration: 1.115s, episode steps: 73, steps per second: 65, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.493 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 7.522821, mean_absolute_error: 33.226574, mean_q: 42.785927
[F[K  35125/500000: episode: 575, duration: 1.595s, episode steps: 84, steps per second: 53, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.485 [0.370, 0.530], loss: 7.212514, mean_absolute_error: 32.278000, mean_q: 41.683392
[F[K  35204/500000: episode: 576, duration: 1.363s, episode steps: 79, steps per second: 58, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.177 [0.000, 4.000], mean observation: 0.494 [0.390, 0.530], loss: 6.137192, mean_absolute_error: 33.350609, mean_q: 43.060154
[F[K  35256/500000: episode: 577, duration: 0.840s, episode steps: 52, steps per second: 62, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.077 [0.000, 4.000], mean observation: 0.477 [0.360, 0.510], loss: 8.305326, mean_absolute_error: 32.980511, mean_q: 42.442505
[F[K  35329/500000: episode: 578, duration: 1.176s, episode steps: 73, steps per second: 62, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.890 [0.000, 4.000], mean observation: 0.518 [0.490, 0.640], loss: 6.923439, mean_absolute_error: 33.489120, mean_q: 43.101501
[F[K  35410/500000: episode: 579, duration: 1.259s, episode steps: 81, steps per second: 64, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.210 [0.000, 4.000], mean observation: 0.499 [0.410, 0.560], loss: 7.154980, mean_absolute_error: 32.903236, mean_q: 42.461842
[F[K  35449/500000: episode: 580, duration: 0.711s, episode steps: 39, steps per second: 55, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.692 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 7.048663, mean_absolute_error: 33.143913, mean_q: 42.715046
[F[K  35483/500000: episode: 581, duration: 0.633s, episode steps: 34, steps per second: 54, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.706 [0.000, 4.000], mean observation: 0.525 [0.470, 0.630], loss: 7.225416, mean_absolute_error: 32.564301, mean_q: 41.949944
[F[K  35596/500000: episode: 582, duration: 2.253s, episode steps: 113, steps per second: 50, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.507 [0.470, 0.550], loss: 6.808949, mean_absolute_error: 32.558941, mean_q: 41.991905
[F[K  35716/500000: episode: 583, duration: 1.836s, episode steps: 120, steps per second: 65, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.450 [0.000, 4.000], mean observation: 0.503 [0.460, 0.550], loss: 6.260818, mean_absolute_error: 32.651363, mean_q: 42.199619
[F[K  35808/500000: episode: 584, duration: 1.418s, episode steps: 92, steps per second: 65, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.483 [0.410, 0.530], loss: 6.491560, mean_absolute_error: 32.744884, mean_q: 42.277840
[F[K  35865/500000: episode: 585, duration: 0.948s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.579 [0.000, 3.000], mean observation: 0.475 [0.350, 0.510], loss: 6.291553, mean_absolute_error: 32.633797, mean_q: 42.138103
[F[K  35947/500000: episode: 586, duration: 1.563s, episode steps: 82, steps per second: 52, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.890 [0.000, 4.000], mean observation: 0.513 [0.490, 0.590], loss: 7.209206, mean_absolute_error: 33.363392, mean_q: 42.920811
[F[K  36003/500000: episode: 587, duration: 1.036s, episode steps: 56, steps per second: 54, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.536 [0.000, 4.000], mean observation: 0.506 [0.440, 0.620], loss: 7.238225, mean_absolute_error: 33.054661, mean_q: 42.635925
[F[K  36060/500000: episode: 588, duration: 1.014s, episode steps: 57, steps per second: 56, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.479 [0.380, 0.500], loss: 6.990194, mean_absolute_error: 32.591885, mean_q: 42.009403
[F[K  36135/500000: episode: 589, duration: 1.409s, episode steps: 75, steps per second: 53, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.013 [0.000, 4.000], mean observation: 0.485 [0.380, 0.530], loss: 6.450394, mean_absolute_error: 32.768250, mean_q: 42.376064
[F[K  36189/500000: episode: 590, duration: 0.954s, episode steps: 54, steps per second: 57, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.037 [0.000, 4.000], mean observation: 0.506 [0.470, 0.550], loss: 6.429981, mean_absolute_error: 33.473324, mean_q: 43.209873
[F[K  36229/500000: episode: 591, duration: 0.745s, episode steps: 40, steps per second: 54, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.675 [0.000, 3.000], mean observation: 0.529 [0.470, 0.650], loss: 8.853994, mean_absolute_error: 32.384617, mean_q: 41.781307
[F[K  36335/500000: episode: 592, duration: 1.740s, episode steps: 106, steps per second: 61, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.113 [0.000, 4.000], mean observation: 0.480 [0.380, 0.530], loss: 6.720509, mean_absolute_error: 32.946117, mean_q: 42.532810
[F[K  36415/500000: episode: 593, duration: 1.415s, episode steps: 80, steps per second: 57, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.863 [0.000, 4.000], mean observation: 0.505 [0.470, 0.540], loss: 7.339622, mean_absolute_error: 33.702709, mean_q: 43.312984
[F[K  36450/500000: episode: 594, duration: 0.582s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.657 [0.000, 4.000], mean observation: 0.516 [0.470, 0.640], loss: 6.861295, mean_absolute_error: 32.681919, mean_q: 42.061443
[F[K  36510/500000: episode: 595, duration: 0.968s, episode steps: 60, steps per second: 62, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.350 [0.000, 4.000], mean observation: 0.501 [0.390, 0.630], loss: 7.255494, mean_absolute_error: 33.733471, mean_q: 43.488464
[F[K  36593/500000: episode: 596, duration: 1.516s, episode steps: 83, steps per second: 55, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.783 [0.000, 4.000], mean observation: 0.505 [0.450, 0.630], loss: 7.139926, mean_absolute_error: 33.046780, mean_q: 42.653645
[F[K  36644/500000: episode: 597, duration: 0.997s, episode steps: 51, steps per second: 51, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.503 [0.460, 0.540], loss: 6.563695, mean_absolute_error: 32.710480, mean_q: 42.351925
[F[K  36688/500000: episode: 598, duration: 0.777s, episode steps: 44, steps per second: 57, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.136 [0.000, 4.000], mean observation: 0.492 [0.370, 0.560], loss: 7.135811, mean_absolute_error: 33.077404, mean_q: 42.645622
[F[K  36759/500000: episode: 599, duration: 1.417s, episode steps: 71, steps per second: 50, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.502 [0.410, 0.580], loss: 7.302557, mean_absolute_error: 33.162632, mean_q: 42.823746
[F[K  36838/500000: episode: 600, duration: 1.156s, episode steps: 79, steps per second: 68, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 0.488 [0.380, 0.520], loss: 7.187745, mean_absolute_error: 32.861725, mean_q: 42.324520
[F[K  36878/500000: episode: 601, duration: 0.747s, episode steps: 40, steps per second: 54, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.650 [0.000, 4.000], mean observation: 0.520 [0.470, 0.640], loss: 5.514205, mean_absolute_error: 33.236065, mean_q: 42.854198
[F[K  36945/500000: episode: 602, duration: 1.212s, episode steps: 67, steps per second: 55, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.388 [0.000, 4.000], mean observation: 0.513 [0.470, 0.600], loss: 6.815567, mean_absolute_error: 32.896053, mean_q: 42.418415
[F[K  37053/500000: episode: 603, duration: 1.933s, episode steps: 108, steps per second: 56, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.926 [0.000, 4.000], mean observation: 0.493 [0.390, 0.550], loss: 7.213546, mean_absolute_error: 33.709759, mean_q: 43.396980
[F[K  37113/500000: episode: 604, duration: 1.136s, episode steps: 60, steps per second: 53, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.492 [0.380, 0.530], loss: 7.269808, mean_absolute_error: 32.996300, mean_q: 42.627277
[F[K  37147/500000: episode: 605, duration: 0.682s, episode steps: 34, steps per second: 50, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.524 [0.470, 0.640], loss: 6.881671, mean_absolute_error: 32.893074, mean_q: 42.395737
[F[K  37210/500000: episode: 606, duration: 1.172s, episode steps: 63, steps per second: 54, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.498 [0.370, 0.600], loss: 7.026285, mean_absolute_error: 33.424286, mean_q: 43.094448
[F[K  37277/500000: episode: 607, duration: 1.278s, episode steps: 67, steps per second: 52, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.015 [0.000, 4.000], mean observation: 0.483 [0.380, 0.520], loss: 7.091552, mean_absolute_error: 33.410076, mean_q: 43.032696
[F[K  37338/500000: episode: 608, duration: 1.204s, episode steps: 61, steps per second: 51, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.721 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 7.866737, mean_absolute_error: 33.669491, mean_q: 43.321373
[F[K  37414/500000: episode: 609, duration: 1.351s, episode steps: 76, steps per second: 56, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.329 [0.000, 4.000], mean observation: 0.481 [0.390, 0.530], loss: 7.655848, mean_absolute_error: 33.063366, mean_q: 42.553562
[F[K  37508/500000: episode: 610, duration: 1.694s, episode steps: 94, steps per second: 55, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.085 [0.000, 4.000], mean observation: 0.502 [0.370, 0.630], loss: 6.758432, mean_absolute_error: 33.351276, mean_q: 42.946548
[F[K  37587/500000: episode: 611, duration: 1.460s, episode steps: 79, steps per second: 54, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.480 [0.360, 0.510], loss: 6.801140, mean_absolute_error: 33.540707, mean_q: 43.208759
[F[K  37650/500000: episode: 612, duration: 0.995s, episode steps: 63, steps per second: 63, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.222 [0.000, 4.000], mean observation: 0.489 [0.360, 0.530], loss: 6.791060, mean_absolute_error: 33.705082, mean_q: 43.395710
[F[K  37727/500000: episode: 613, duration: 1.441s, episode steps: 77, steps per second: 53, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.805 [0.000, 4.000], mean observation: 0.508 [0.470, 0.600], loss: 5.933116, mean_absolute_error: 33.973751, mean_q: 43.834705
[F[K  37814/500000: episode: 614, duration: 1.211s, episode steps: 87, steps per second: 72, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.491 [0.370, 0.550], loss: 6.706150, mean_absolute_error: 33.340843, mean_q: 42.981483
[F[K  37884/500000: episode: 615, duration: 1.060s, episode steps: 70, steps per second: 66, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.629 [0.000, 4.000], mean observation: 0.491 [0.390, 0.540], loss: 7.710241, mean_absolute_error: 33.512802, mean_q: 43.092152
[F[K  37962/500000: episode: 616, duration: 1.573s, episode steps: 78, steps per second: 50, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.385 [0.000, 4.000], mean observation: 0.501 [0.470, 0.520], loss: 7.413935, mean_absolute_error: 33.050011, mean_q: 42.514080
[F[K  38023/500000: episode: 617, duration: 1.167s, episode steps: 61, steps per second: 52, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.049 [0.000, 4.000], mean observation: 0.482 [0.390, 0.530], loss: 6.859160, mean_absolute_error: 33.402962, mean_q: 43.081551
[F[K  38146/500000: episode: 618, duration: 2.666s, episode steps: 123, steps per second: 46, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.927 [0.000, 4.000], mean observation: 0.493 [0.380, 0.560], loss: 6.572037, mean_absolute_error: 34.086456, mean_q: 43.856922
[F[K  38232/500000: episode: 619, duration: 1.602s, episode steps: 86, steps per second: 54, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.860 [0.000, 4.000], mean observation: 0.497 [0.400, 0.550], loss: 7.235960, mean_absolute_error: 33.882717, mean_q: 43.602543
[F[K  38286/500000: episode: 620, duration: 1.040s, episode steps: 54, steps per second: 52, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.741 [0.000, 4.000], mean observation: 0.473 [0.350, 0.530], loss: 7.253485, mean_absolute_error: 33.553913, mean_q: 43.222321
[F[K  38341/500000: episode: 621, duration: 1.010s, episode steps: 55, steps per second: 54, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.055 [0.000, 4.000], mean observation: 0.498 [0.430, 0.570], loss: 8.245598, mean_absolute_error: 33.127586, mean_q: 42.499939
[F[K  38375/500000: episode: 622, duration: 0.734s, episode steps: 34, steps per second: 46, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.706 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 7.833344, mean_absolute_error: 33.793797, mean_q: 43.427856
[F[K  38430/500000: episode: 623, duration: 1.165s, episode steps: 55, steps per second: 47, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.509 [0.460, 0.640], loss: 7.270304, mean_absolute_error: 33.972908, mean_q: 43.742821
[F[K  38496/500000: episode: 624, duration: 1.240s, episode steps: 66, steps per second: 53, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.712 [0.000, 4.000], mean observation: 0.492 [0.400, 0.530], loss: 7.895766, mean_absolute_error: 34.643879, mean_q: 44.603901
[F[K  38539/500000: episode: 625, duration: 0.795s, episode steps: 43, steps per second: 54, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.442 [0.000, 4.000], mean observation: 0.502 [0.400, 0.630], loss: 7.261788, mean_absolute_error: 33.198868, mean_q: 42.717045
[F[K  38611/500000: episode: 626, duration: 1.413s, episode steps: 72, steps per second: 51, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.493 [0.420, 0.530], loss: 6.955918, mean_absolute_error: 34.015396, mean_q: 43.830002
[F[K  38679/500000: episode: 627, duration: 1.280s, episode steps: 68, steps per second: 53, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.559 [0.000, 4.000], mean observation: 0.478 [0.380, 0.530], loss: 7.085588, mean_absolute_error: 33.759434, mean_q: 43.354851
[F[K  38725/500000: episode: 628, duration: 0.782s, episode steps: 46, steps per second: 59, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.130 [0.000, 4.000], mean observation: 0.497 [0.380, 0.590], loss: 8.176556, mean_absolute_error: 33.453773, mean_q: 43.009056
[F[K  38761/500000: episode: 629, duration: 0.596s, episode steps: 36, steps per second: 60, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.639 [0.000, 3.000], mean observation: 0.502 [0.370, 0.650], loss: 8.061509, mean_absolute_error: 33.056358, mean_q: 42.583118
[F[K  38826/500000: episode: 630, duration: 1.281s, episode steps: 65, steps per second: 51, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.969 [0.000, 4.000], mean observation: 0.507 [0.420, 0.640], loss: 6.199723, mean_absolute_error: 34.055359, mean_q: 43.843624
[F[K  38871/500000: episode: 631, duration: 0.806s, episode steps: 45, steps per second: 56, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 0.500 [0.370, 0.630], loss: 8.040756, mean_absolute_error: 33.851082, mean_q: 43.490276
[F[K  38959/500000: episode: 632, duration: 1.731s, episode steps: 88, steps per second: 51, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.487 [0.410, 0.520], loss: 6.570430, mean_absolute_error: 33.500912, mean_q: 43.148312
[F[K  39023/500000: episode: 633, duration: 1.246s, episode steps: 64, steps per second: 51, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.507 [0.470, 0.570], loss: 7.881549, mean_absolute_error: 33.798920, mean_q: 43.478527
[F[K  39066/500000: episode: 634, duration: 0.927s, episode steps: 43, steps per second: 46, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.512 [0.000, 4.000], mean observation: 0.502 [0.350, 0.640], loss: 6.505945, mean_absolute_error: 33.593449, mean_q: 43.273640
[F[K  39142/500000: episode: 635, duration: 1.485s, episode steps: 76, steps per second: 51, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.737 [0.000, 4.000], mean observation: 0.520 [0.500, 0.600], loss: 7.715109, mean_absolute_error: 34.020981, mean_q: 43.637539
[F[K  39190/500000: episode: 636, duration: 1.029s, episode steps: 48, steps per second: 47, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.417 [0.000, 4.000], mean observation: 0.485 [0.410, 0.530], loss: 7.425217, mean_absolute_error: 34.126625, mean_q: 43.788013
[F[K  39258/500000: episode: 637, duration: 1.325s, episode steps: 68, steps per second: 51, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.485 [0.400, 0.530], loss: 7.586465, mean_absolute_error: 33.666824, mean_q: 43.210552
[F[K  39303/500000: episode: 638, duration: 0.767s, episode steps: 45, steps per second: 59, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.556 [0.000, 4.000], mean observation: 0.483 [0.360, 0.530], loss: 7.890407, mean_absolute_error: 34.371410, mean_q: 44.144444
[F[K  39342/500000: episode: 639, duration: 0.768s, episode steps: 39, steps per second: 51, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.476 [0.370, 0.530], loss: 7.288988, mean_absolute_error: 33.706287, mean_q: 43.220928
[F[K  39380/500000: episode: 640, duration: 0.760s, episode steps: 38, steps per second: 50, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.506 [0.410, 0.650], loss: 8.098776, mean_absolute_error: 33.882248, mean_q: 43.450146
[F[K  39463/500000: episode: 641, duration: 1.619s, episode steps: 83, steps per second: 51, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.675 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 7.310702, mean_absolute_error: 33.486908, mean_q: 42.962280
[F[K  39518/500000: episode: 642, duration: 1.021s, episode steps: 55, steps per second: 54, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.691 [0.000, 4.000], mean observation: 0.477 [0.350, 0.530], loss: 7.491937, mean_absolute_error: 33.273201, mean_q: 42.696625
[F[K  39585/500000: episode: 643, duration: 1.253s, episode steps: 67, steps per second: 53, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.687 [0.000, 4.000], mean observation: 0.481 [0.360, 0.510], loss: 8.671549, mean_absolute_error: 32.823845, mean_q: 42.086758
[F[K  39634/500000: episode: 644, duration: 0.950s, episode steps: 49, steps per second: 52, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.510 [0.000, 4.000], mean observation: 0.503 [0.400, 0.630], loss: 7.129864, mean_absolute_error: 33.785728, mean_q: 43.457344
[F[K  39690/500000: episode: 645, duration: 1.121s, episode steps: 56, steps per second: 50, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.375 [0.000, 4.000], mean observation: 0.485 [0.350, 0.520], loss: 6.960770, mean_absolute_error: 32.923870, mean_q: 42.479397
[F[K  39741/500000: episode: 646, duration: 1.026s, episode steps: 51, steps per second: 50, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.495 [0.460, 0.530], loss: 7.774726, mean_absolute_error: 33.777615, mean_q: 43.520340
[F[K  39781/500000: episode: 647, duration: 0.884s, episode steps: 40, steps per second: 45, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.575 [0.000, 4.000], mean observation: 0.496 [0.350, 0.590], loss: 7.109281, mean_absolute_error: 33.078777, mean_q: 42.732430
[F[K  39855/500000: episode: 648, duration: 1.546s, episode steps: 74, steps per second: 48, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.757 [0.000, 4.000], mean observation: 0.476 [0.360, 0.530], loss: 7.020782, mean_absolute_error: 33.222290, mean_q: 42.819660
[F[K  39920/500000: episode: 649, duration: 1.466s, episode steps: 65, steps per second: 44, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 7.859684, mean_absolute_error: 32.874966, mean_q: 42.423203
[F[K  39976/500000: episode: 650, duration: 1.095s, episode steps: 56, steps per second: 51, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.321 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 7.659383, mean_absolute_error: 33.807583, mean_q: 43.562656
[F[K  40014/500000: episode: 651, duration: 0.607s, episode steps: 38, steps per second: 63, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.605 [0.000, 4.000], mean observation: 0.514 [0.470, 0.640], loss: 7.580317, mean_absolute_error: 33.470856, mean_q: 43.005219
[F[K  40073/500000: episode: 652, duration: 1.039s, episode steps: 59, steps per second: 57, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.519 [0.470, 0.600], loss: 7.341636, mean_absolute_error: 33.629818, mean_q: 43.310223
[F[K  40145/500000: episode: 653, duration: 1.411s, episode steps: 72, steps per second: 51, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 0.504 [0.470, 0.540], loss: 7.013945, mean_absolute_error: 33.482468, mean_q: 43.203106
[F[K  40218/500000: episode: 654, duration: 1.185s, episode steps: 73, steps per second: 62, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.452 [0.000, 4.000], mean observation: 0.518 [0.480, 0.600], loss: 6.773786, mean_absolute_error: 33.603218, mean_q: 43.285378
[F[K  40281/500000: episode: 655, duration: 1.377s, episode steps: 63, steps per second: 46, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.873 [0.000, 4.000], mean observation: 0.492 [0.440, 0.520], loss: 6.639144, mean_absolute_error: 34.278732, mean_q: 44.111027
[F[K  40381/500000: episode: 656, duration: 1.969s, episode steps: 100, steps per second: 51, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.630 [0.000, 4.000], mean observation: 0.498 [0.460, 0.520], loss: 6.927851, mean_absolute_error: 33.719120, mean_q: 43.430702
[F[K  40454/500000: episode: 657, duration: 1.343s, episode steps: 73, steps per second: 54, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.904 [0.000, 4.000], mean observation: 0.490 [0.380, 0.530], loss: 7.798903, mean_absolute_error: 33.433090, mean_q: 42.978374
[F[K  40532/500000: episode: 658, duration: 1.311s, episode steps: 78, steps per second: 60, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.385 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 7.355482, mean_absolute_error: 33.577042, mean_q: 43.266365
[F[K  40589/500000: episode: 659, duration: 1.103s, episode steps: 57, steps per second: 52, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.281 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 7.183122, mean_absolute_error: 33.090668, mean_q: 42.441666
[F[K  40626/500000: episode: 660, duration: 0.740s, episode steps: 37, steps per second: 50, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.811 [0.000, 4.000], mean observation: 0.517 [0.470, 0.640], loss: 6.616217, mean_absolute_error: 33.206841, mean_q: 42.803795
[F[K  40682/500000: episode: 661, duration: 0.946s, episode steps: 56, steps per second: 59, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.499 [0.400, 0.570], loss: 7.012268, mean_absolute_error: 33.289928, mean_q: 42.797474
[F[K  40760/500000: episode: 662, duration: 1.618s, episode steps: 78, steps per second: 48, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.962 [0.000, 4.000], mean observation: 0.498 [0.410, 0.590], loss: 7.452739, mean_absolute_error: 33.061733, mean_q: 42.580818
[F[K  40832/500000: episode: 663, duration: 1.308s, episode steps: 72, steps per second: 55, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.931 [0.000, 4.000], mean observation: 0.510 [0.460, 0.600], loss: 8.004757, mean_absolute_error: 33.648148, mean_q: 43.291794
[F[K  40892/500000: episode: 664, duration: 1.027s, episode steps: 60, steps per second: 58, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.517 [0.000, 4.000], mean observation: 0.515 [0.470, 0.640], loss: 7.272231, mean_absolute_error: 33.688416, mean_q: 43.332821
[F[K  40955/500000: episode: 665, duration: 1.210s, episode steps: 63, steps per second: 52, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.381 [0.000, 4.000], mean observation: 0.489 [0.390, 0.530], loss: 6.430815, mean_absolute_error: 34.204437, mean_q: 44.105747
[F[K  41012/500000: episode: 666, duration: 1.088s, episode steps: 57, steps per second: 52, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.486 [0.350, 0.520], loss: 7.458460, mean_absolute_error: 33.119389, mean_q: 42.695000
[F[K  41082/500000: episode: 667, duration: 1.102s, episode steps: 70, steps per second: 64, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.314 [0.000, 4.000], mean observation: 0.509 [0.470, 0.570], loss: 6.709987, mean_absolute_error: 34.020107, mean_q: 43.845470
[F[K  41136/500000: episode: 668, duration: 0.939s, episode steps: 54, steps per second: 58, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.704 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 7.431130, mean_absolute_error: 34.025639, mean_q: 43.735260
[F[K  41200/500000: episode: 669, duration: 1.001s, episode steps: 64, steps per second: 64, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.109 [0.000, 4.000], mean observation: 0.499 [0.440, 0.560], loss: 7.284459, mean_absolute_error: 33.056580, mean_q: 42.594986
[F[K  41337/500000: episode: 670, duration: 2.483s, episode steps: 137, steps per second: 55, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.358 [0.000, 4.000], mean observation: 0.516 [0.480, 0.580], loss: 6.601000, mean_absolute_error: 33.391201, mean_q: 42.969383
[F[K  41384/500000: episode: 671, duration: 0.883s, episode steps: 47, steps per second: 53, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.660 [0.000, 4.000], mean observation: 0.497 [0.360, 0.600], loss: 8.195226, mean_absolute_error: 33.854435, mean_q: 43.513153
[F[K  41481/500000: episode: 672, duration: 1.738s, episode steps: 97, steps per second: 56, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.454 [0.000, 4.000], mean observation: 0.511 [0.460, 0.650], loss: 6.891311, mean_absolute_error: 34.488098, mean_q: 44.367107
[F[K  41544/500000: episode: 673, duration: 1.173s, episode steps: 63, steps per second: 54, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.873 [0.000, 4.000], mean observation: 0.509 [0.470, 0.590], loss: 7.032918, mean_absolute_error: 33.734470, mean_q: 43.486767
[F[K  41615/500000: episode: 674, duration: 1.272s, episode steps: 71, steps per second: 56, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.775 [0.000, 4.000], mean observation: 0.480 [0.380, 0.530], loss: 7.487533, mean_absolute_error: 33.590820, mean_q: 43.295307
[F[K  41664/500000: episode: 675, duration: 0.979s, episode steps: 49, steps per second: 50, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 7.384152, mean_absolute_error: 33.617748, mean_q: 43.285919
[F[K  41767/500000: episode: 676, duration: 2.045s, episode steps: 103, steps per second: 50, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.806 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 7.424376, mean_absolute_error: 33.849129, mean_q: 43.567871
[F[K  41861/500000: episode: 677, duration: 1.490s, episode steps: 94, steps per second: 63, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.518 [0.490, 0.600], loss: 7.581399, mean_absolute_error: 33.301758, mean_q: 42.817524
[F[K  41910/500000: episode: 678, duration: 1.068s, episode steps: 49, steps per second: 46, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.571 [0.000, 4.000], mean observation: 0.498 [0.400, 0.580], loss: 6.192968, mean_absolute_error: 34.680107, mean_q: 44.617512
[F[K  41975/500000: episode: 679, duration: 1.444s, episode steps: 65, steps per second: 45, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.369 [0.000, 4.000], mean observation: 0.509 [0.470, 0.580], loss: 6.848940, mean_absolute_error: 33.445797, mean_q: 43.208427
[F[K  42038/500000: episode: 680, duration: 1.196s, episode steps: 63, steps per second: 53, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.841 [0.000, 4.000], mean observation: 0.486 [0.360, 0.530], loss: 7.038531, mean_absolute_error: 33.902187, mean_q: 43.631592
[F[K  42095/500000: episode: 681, duration: 0.970s, episode steps: 57, steps per second: 59, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.456 [0.000, 4.000], mean observation: 0.509 [0.460, 0.640], loss: 8.394102, mean_absolute_error: 33.599049, mean_q: 43.186657
[F[K  42177/500000: episode: 682, duration: 1.767s, episode steps: 82, steps per second: 46, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.502 [0.410, 0.590], loss: 8.326180, mean_absolute_error: 34.190681, mean_q: 43.841152
[F[K  42212/500000: episode: 683, duration: 0.727s, episode steps: 35, steps per second: 48, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.400 [0.000, 4.000], mean observation: 0.527 [0.470, 0.650], loss: 7.305960, mean_absolute_error: 33.954247, mean_q: 43.679661
[F[K  42309/500000: episode: 684, duration: 1.857s, episode steps: 97, steps per second: 52, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.814 [0.000, 4.000], mean observation: 0.502 [0.460, 0.540], loss: 6.795717, mean_absolute_error: 34.154373, mean_q: 43.838074
[F[K  42370/500000: episode: 685, duration: 1.084s, episode steps: 61, steps per second: 56, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.525 [0.000, 4.000], mean observation: 0.497 [0.450, 0.530], loss: 7.593721, mean_absolute_error: 33.628040, mean_q: 43.138493
[F[K  42424/500000: episode: 686, duration: 1.275s, episode steps: 54, steps per second: 42, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.741 [0.000, 3.000], mean observation: 0.512 [0.470, 0.620], loss: 7.074923, mean_absolute_error: 33.892902, mean_q: 43.550739
[F[K  42494/500000: episode: 687, duration: 1.602s, episode steps: 70, steps per second: 44, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.514 [0.480, 0.610], loss: 6.990828, mean_absolute_error: 33.617573, mean_q: 43.375416
[F[K  42556/500000: episode: 688, duration: 1.251s, episode steps: 62, steps per second: 50, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.290 [0.000, 4.000], mean observation: 0.493 [0.440, 0.520], loss: 7.835794, mean_absolute_error: 33.790817, mean_q: 43.525234
[F[K  42615/500000: episode: 689, duration: 1.196s, episode steps: 59, steps per second: 49, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.153 [0.000, 4.000], mean observation: 0.514 [0.470, 0.620], loss: 8.189783, mean_absolute_error: 33.743576, mean_q: 43.371914
[F[K  42688/500000: episode: 690, duration: 1.527s, episode steps: 73, steps per second: 48, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.712 [0.000, 4.000], mean observation: 0.502 [0.380, 0.620], loss: 7.003098, mean_absolute_error: 33.202435, mean_q: 42.781479
[F[K  42749/500000: episode: 691, duration: 1.183s, episode steps: 61, steps per second: 52, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.885 [0.000, 4.000], mean observation: 0.494 [0.360, 0.600], loss: 6.672740, mean_absolute_error: 33.084682, mean_q: 42.612251
[F[K  42827/500000: episode: 692, duration: 1.638s, episode steps: 78, steps per second: 48, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.859 [0.000, 4.000], mean observation: 0.507 [0.490, 0.550], loss: 6.128401, mean_absolute_error: 33.512459, mean_q: 43.096027
[F[K  42897/500000: episode: 693, duration: 1.499s, episode steps: 70, steps per second: 47, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.229 [0.000, 4.000], mean observation: 0.500 [0.470, 0.520], loss: 6.032387, mean_absolute_error: 33.808956, mean_q: 43.571659
[F[K  42951/500000: episode: 694, duration: 1.085s, episode steps: 54, steps per second: 50, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 8.052201, mean_absolute_error: 33.188446, mean_q: 42.722805
[F[K  42996/500000: episode: 695, duration: 0.842s, episode steps: 45, steps per second: 53, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.733 [0.000, 4.000], mean observation: 0.524 [0.470, 0.640], loss: 7.221125, mean_absolute_error: 34.626316, mean_q: 44.612637
[F[K  43052/500000: episode: 696, duration: 1.185s, episode steps: 56, steps per second: 47, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.804 [0.000, 4.000], mean observation: 0.518 [0.490, 0.610], loss: 6.840360, mean_absolute_error: 34.245228, mean_q: 44.173279
[F[K  43119/500000: episode: 697, duration: 1.298s, episode steps: 67, steps per second: 52, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.493 [0.000, 4.000], mean observation: 0.475 [0.360, 0.510], loss: 7.118385, mean_absolute_error: 33.610943, mean_q: 43.347904
[F[K  43154/500000: episode: 698, duration: 0.761s, episode steps: 35, steps per second: 46, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.519 [0.470, 0.640], loss: 7.631795, mean_absolute_error: 34.037617, mean_q: 43.730923
[F[K  43213/500000: episode: 699, duration: 1.235s, episode steps: 59, steps per second: 48, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.525 [0.000, 4.000], mean observation: 0.509 [0.450, 0.640], loss: 7.300697, mean_absolute_error: 33.630112, mean_q: 43.301208
[F[K  43279/500000: episode: 700, duration: 1.232s, episode steps: 66, steps per second: 54, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.486 [0.430, 0.510], loss: 6.784496, mean_absolute_error: 33.624237, mean_q: 43.281109
[F[K  43382/500000: episode: 701, duration: 1.418s, episode steps: 103, steps per second: 73, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.699 [0.000, 4.000], mean observation: 0.504 [0.470, 0.580], loss: 7.023204, mean_absolute_error: 33.924389, mean_q: 43.640846
[F[K  43456/500000: episode: 702, duration: 1.036s, episode steps: 74, steps per second: 71, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.784 [0.000, 4.000], mean observation: 0.493 [0.410, 0.530], loss: 6.848051, mean_absolute_error: 33.997841, mean_q: 43.716789
[F[K  43518/500000: episode: 703, duration: 1.263s, episode steps: 62, steps per second: 49, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.758 [0.000, 4.000], mean observation: 0.496 [0.380, 0.590], loss: 7.964422, mean_absolute_error: 34.528976, mean_q: 44.368969
[F[K  43610/500000: episode: 704, duration: 1.734s, episode steps: 92, steps per second: 53, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 0.508 [0.480, 0.560], loss: 7.882782, mean_absolute_error: 34.426777, mean_q: 44.251705
[F[K  43677/500000: episode: 705, duration: 1.252s, episode steps: 67, steps per second: 54, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.791 [0.000, 4.000], mean observation: 0.506 [0.490, 0.530], loss: 7.729029, mean_absolute_error: 34.622520, mean_q: 44.600086
[F[K  43770/500000: episode: 706, duration: 1.967s, episode steps: 93, steps per second: 47, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.473 [0.360, 0.510], loss: 6.595705, mean_absolute_error: 33.879887, mean_q: 43.714115
[F[K  43825/500000: episode: 707, duration: 1.275s, episode steps: 55, steps per second: 43, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 0.514 [0.490, 0.570], loss: 7.385752, mean_absolute_error: 34.664272, mean_q: 44.668819
[F[K  43876/500000: episode: 708, duration: 1.190s, episode steps: 51, steps per second: 43, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 0.518 [0.470, 0.620], loss: 8.054588, mean_absolute_error: 34.705093, mean_q: 44.693184
[F[K  43936/500000: episode: 709, duration: 0.996s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.067 [0.000, 4.000], mean observation: 0.498 [0.360, 0.600], loss: 8.052087, mean_absolute_error: 33.739204, mean_q: 43.237381
[F[K  44011/500000: episode: 710, duration: 1.568s, episode steps: 75, steps per second: 48, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 0.516 [0.470, 0.600], loss: 7.847856, mean_absolute_error: 34.150429, mean_q: 43.943684
[F[K  44089/500000: episode: 711, duration: 1.772s, episode steps: 78, steps per second: 44, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.192 [0.000, 4.000], mean observation: 0.508 [0.490, 0.550], loss: 7.049277, mean_absolute_error: 35.028412, mean_q: 45.105598
[F[K  44120/500000: episode: 712, duration: 0.626s, episode steps: 31, steps per second: 49, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.419 [0.000, 3.000], mean observation: 0.525 [0.470, 0.650], loss: 7.848083, mean_absolute_error: 34.074459, mean_q: 43.943729
[F[K  44192/500000: episode: 713, duration: 1.324s, episode steps: 72, steps per second: 54, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.611 [0.000, 4.000], mean observation: 0.505 [0.490, 0.530], loss: 8.156789, mean_absolute_error: 33.946262, mean_q: 43.659279
[F[K  44244/500000: episode: 714, duration: 1.087s, episode steps: 52, steps per second: 48, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.327 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 6.687219, mean_absolute_error: 34.166157, mean_q: 43.992771
[F[K  44314/500000: episode: 715, duration: 1.476s, episode steps: 70, steps per second: 47, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.512 [0.460, 0.640], loss: 7.057974, mean_absolute_error: 34.608562, mean_q: 44.513466
[F[K  44397/500000: episode: 716, duration: 1.708s, episode steps: 83, steps per second: 49, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.723 [0.000, 4.000], mean observation: 0.505 [0.420, 0.610], loss: 8.399649, mean_absolute_error: 34.392525, mean_q: 44.179661
[F[K  44438/500000: episode: 717, duration: 0.808s, episode steps: 41, steps per second: 51, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.499 [0.370, 0.610], loss: 7.214740, mean_absolute_error: 34.520302, mean_q: 44.586010
[F[K  44489/500000: episode: 718, duration: 1.003s, episode steps: 51, steps per second: 51, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.804 [0.000, 4.000], mean observation: 0.498 [0.410, 0.530], loss: 7.377209, mean_absolute_error: 34.767776, mean_q: 44.543381
[F[K  44569/500000: episode: 719, duration: 1.529s, episode steps: 80, steps per second: 52, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.675 [0.000, 4.000], mean observation: 0.495 [0.450, 0.530], loss: 8.027674, mean_absolute_error: 33.978188, mean_q: 43.730705
[F[K  44630/500000: episode: 720, duration: 1.290s, episode steps: 61, steps per second: 47, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.934 [0.000, 4.000], mean observation: 0.517 [0.500, 0.570], loss: 7.026193, mean_absolute_error: 33.909927, mean_q: 43.733250
[F[K  44697/500000: episode: 721, duration: 1.249s, episode steps: 67, steps per second: 54, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.746 [0.000, 4.000], mean observation: 0.497 [0.440, 0.530], loss: 7.664381, mean_absolute_error: 34.487602, mean_q: 44.437801
[F[K  44745/500000: episode: 722, duration: 0.897s, episode steps: 48, steps per second: 54, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.646 [0.000, 4.000], mean observation: 0.507 [0.450, 0.630], loss: 6.558449, mean_absolute_error: 34.141682, mean_q: 44.045788
[F[K  44811/500000: episode: 723, duration: 1.345s, episode steps: 66, steps per second: 49, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.955 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 7.042640, mean_absolute_error: 35.000259, mean_q: 45.027523
[F[K  44898/500000: episode: 724, duration: 1.782s, episode steps: 87, steps per second: 49, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.511 [0.460, 0.650], loss: 7.631938, mean_absolute_error: 34.119045, mean_q: 43.978935
[F[K  44932/500000: episode: 725, duration: 0.777s, episode steps: 34, steps per second: 44, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.618 [0.000, 4.000], mean observation: 0.513 [0.470, 0.640], loss: 7.226535, mean_absolute_error: 34.332592, mean_q: 44.265072
[F[K  44999/500000: episode: 726, duration: 1.283s, episode steps: 67, steps per second: 52, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.284 [0.000, 4.000], mean observation: 0.503 [0.470, 0.570], loss: 8.023012, mean_absolute_error: 34.306763, mean_q: 43.986397
[F[K  45059/500000: episode: 727, duration: 1.307s, episode steps: 60, steps per second: 46, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.550 [0.000, 4.000], mean observation: 0.515 [0.470, 0.650], loss: 7.415081, mean_absolute_error: 34.834194, mean_q: 44.874229
[F[K  45128/500000: episode: 728, duration: 1.637s, episode steps: 69, steps per second: 42, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.232 [0.000, 4.000], mean observation: 0.502 [0.400, 0.600], loss: 8.089157, mean_absolute_error: 33.850552, mean_q: 43.558891
[F[K  45210/500000: episode: 729, duration: 1.735s, episode steps: 82, steps per second: 47, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.732 [0.000, 4.000], mean observation: 0.488 [0.400, 0.530], loss: 7.257740, mean_absolute_error: 34.268559, mean_q: 44.046459
[F[K  45342/500000: episode: 730, duration: 2.702s, episode steps: 132, steps per second: 49, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.356 [0.000, 4.000], mean observation: 0.516 [0.460, 0.640], loss: 7.055757, mean_absolute_error: 33.841942, mean_q: 43.586624
[F[K  45499/500000: episode: 731, duration: 2.944s, episode steps: 157, steps per second: 53, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.561 [0.000, 4.000], mean observation: 0.512 [0.450, 0.630], loss: 7.239924, mean_absolute_error: 34.764107, mean_q: 44.700245
[F[K  45573/500000: episode: 732, duration: 1.521s, episode steps: 74, steps per second: 49, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.568 [0.000, 4.000], mean observation: 0.488 [0.390, 0.530], loss: 7.130670, mean_absolute_error: 34.875965, mean_q: 44.919270
[F[K  45643/500000: episode: 733, duration: 1.314s, episode steps: 70, steps per second: 53, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.886 [0.000, 4.000], mean observation: 0.514 [0.470, 0.590], loss: 8.615720, mean_absolute_error: 34.618206, mean_q: 44.438541
[F[K  45723/500000: episode: 734, duration: 1.554s, episode steps: 80, steps per second: 51, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.508 [0.470, 0.580], loss: 7.992358, mean_absolute_error: 34.308167, mean_q: 44.146080
[F[K  45787/500000: episode: 735, duration: 1.040s, episode steps: 64, steps per second: 62, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.906 [0.000, 4.000], mean observation: 0.492 [0.380, 0.540], loss: 8.243723, mean_absolute_error: 34.030701, mean_q: 43.748764
[F[K  45853/500000: episode: 736, duration: 1.228s, episode steps: 66, steps per second: 54, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.517 [0.490, 0.620], loss: 8.718695, mean_absolute_error: 34.088203, mean_q: 43.697712
[F[K  45935/500000: episode: 737, duration: 1.603s, episode steps: 82, steps per second: 51, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.671 [0.000, 3.000], mean observation: 0.491 [0.380, 0.550], loss: 7.223481, mean_absolute_error: 33.956501, mean_q: 43.751160
[F[K  46015/500000: episode: 738, duration: 1.716s, episode steps: 80, steps per second: 47, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.112 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 7.840089, mean_absolute_error: 34.815723, mean_q: 44.744461
[F[K  46125/500000: episode: 739, duration: 1.969s, episode steps: 110, steps per second: 56, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.255 [0.000, 4.000], mean observation: 0.495 [0.400, 0.530], loss: 7.714468, mean_absolute_error: 34.414841, mean_q: 44.153679
[F[K  46161/500000: episode: 740, duration: 0.698s, episode steps: 36, steps per second: 52, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.515 [0.470, 0.640], loss: 7.353700, mean_absolute_error: 33.151745, mean_q: 42.745785
[F[K  46332/500000: episode: 741, duration: 2.803s, episode steps: 171, steps per second: 61, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.596 [0.000, 4.000], mean observation: 0.496 [0.440, 0.560], loss: 8.062405, mean_absolute_error: 34.429611, mean_q: 44.280239
[F[K  46500/500000: episode: 742, duration: 3.344s, episode steps: 168, steps per second: 50, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.030 [0.000, 4.000], mean observation: 0.486 [0.380, 0.540], loss: 7.442426, mean_absolute_error: 34.092731, mean_q: 43.853951
[F[K  46591/500000: episode: 743, duration: 1.687s, episode steps: 91, steps per second: 54, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.500 [0.440, 0.580], loss: 7.932396, mean_absolute_error: 34.890541, mean_q: 44.772076
[F[K  46692/500000: episode: 744, duration: 1.744s, episode steps: 101, steps per second: 58, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.287 [0.000, 4.000], mean observation: 0.506 [0.470, 0.610], loss: 7.003779, mean_absolute_error: 34.750061, mean_q: 44.537025
[F[K  46741/500000: episode: 745, duration: 0.759s, episode steps: 49, steps per second: 65, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.776 [0.000, 4.000], mean observation: 0.503 [0.410, 0.630], loss: 6.427719, mean_absolute_error: 34.303753, mean_q: 44.188866
[F[K  46808/500000: episode: 746, duration: 1.263s, episode steps: 67, steps per second: 53, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.821 [0.000, 4.000], mean observation: 0.496 [0.380, 0.590], loss: 6.476997, mean_absolute_error: 34.137291, mean_q: 43.856342
[F[K  46846/500000: episode: 747, duration: 0.828s, episode steps: 38, steps per second: 46, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.474 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 7.155987, mean_absolute_error: 35.119923, mean_q: 45.273724
[F[K  46905/500000: episode: 748, duration: 1.304s, episode steps: 59, steps per second: 45, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 7.124753, mean_absolute_error: 34.906185, mean_q: 44.978821
[F[K  46983/500000: episode: 749, duration: 1.642s, episode steps: 78, steps per second: 48, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.423 [0.000, 4.000], mean observation: 0.502 [0.390, 0.630], loss: 7.558753, mean_absolute_error: 34.855495, mean_q: 44.769901
[F[K  47066/500000: episode: 750, duration: 1.723s, episode steps: 83, steps per second: 48, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.627 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 6.978390, mean_absolute_error: 34.729073, mean_q: 44.660820
[F[K  47112/500000: episode: 751, duration: 0.943s, episode steps: 46, steps per second: 49, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.304 [0.000, 4.000], mean observation: 0.516 [0.470, 0.620], loss: 7.581903, mean_absolute_error: 34.541245, mean_q: 44.312077
[F[K  47193/500000: episode: 752, duration: 1.565s, episode steps: 81, steps per second: 52, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.012 [0.000, 4.000], mean observation: 0.495 [0.400, 0.560], loss: 8.033206, mean_absolute_error: 33.922779, mean_q: 43.561615
[F[K  47246/500000: episode: 753, duration: 1.192s, episode steps: 53, steps per second: 44, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.717 [0.000, 4.000], mean observation: 0.497 [0.390, 0.570], loss: 7.678075, mean_absolute_error: 34.564686, mean_q: 44.489391
[F[K  47284/500000: episode: 754, duration: 0.810s, episode steps: 38, steps per second: 47, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.632 [0.000, 3.000], mean observation: 0.513 [0.470, 0.640], loss: 8.544219, mean_absolute_error: 34.919048, mean_q: 44.703003
[F[K  47347/500000: episode: 755, duration: 1.318s, episode steps: 63, steps per second: 48, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.683 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 7.979121, mean_absolute_error: 35.069908, mean_q: 45.004776
[F[K  47383/500000: episode: 756, duration: 0.770s, episode steps: 36, steps per second: 47, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 3.000], mean observation: 0.523 [0.470, 0.650], loss: 7.605871, mean_absolute_error: 34.580574, mean_q: 44.140034
[F[K  47461/500000: episode: 757, duration: 1.499s, episode steps: 78, steps per second: 52, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.936 [0.000, 4.000], mean observation: 0.517 [0.480, 0.600], loss: 8.195465, mean_absolute_error: 34.133690, mean_q: 43.856766
[F[K  47536/500000: episode: 758, duration: 1.589s, episode steps: 75, steps per second: 47, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 8.479720, mean_absolute_error: 34.223984, mean_q: 43.985622
[F[K  47609/500000: episode: 759, duration: 1.472s, episode steps: 73, steps per second: 50, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 7.434518, mean_absolute_error: 34.715431, mean_q: 44.680733
[F[K  47654/500000: episode: 760, duration: 0.997s, episode steps: 45, steps per second: 45, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.556 [0.000, 4.000], mean observation: 0.492 [0.360, 0.560], loss: 7.158182, mean_absolute_error: 33.753716, mean_q: 43.267330
[F[K  47725/500000: episode: 761, duration: 1.416s, episode steps: 71, steps per second: 50, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.296 [0.000, 4.000], mean observation: 0.515 [0.470, 0.630], loss: 7.098831, mean_absolute_error: 34.234207, mean_q: 44.084625
[F[K  47786/500000: episode: 762, duration: 1.192s, episode steps: 61, steps per second: 51, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.131 [0.000, 4.000], mean observation: 0.495 [0.470, 0.510], loss: 7.904441, mean_absolute_error: 33.543385, mean_q: 43.091217
[F[K  47890/500000: episode: 763, duration: 2.025s, episode steps: 104, steps per second: 51, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.769 [0.000, 4.000], mean observation: 0.485 [0.350, 0.540], loss: 7.077397, mean_absolute_error: 34.681572, mean_q: 44.480305
[F[K  47971/500000: episode: 764, duration: 1.684s, episode steps: 81, steps per second: 48, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.210 [0.000, 4.000], mean observation: 0.506 [0.480, 0.530], loss: 7.651598, mean_absolute_error: 34.528099, mean_q: 44.370152
[F[K  48044/500000: episode: 765, duration: 1.525s, episode steps: 73, steps per second: 48, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.808 [0.000, 4.000], mean observation: 0.492 [0.430, 0.530], loss: 7.752793, mean_absolute_error: 33.471382, mean_q: 43.072620
[F[K  48093/500000: episode: 766, duration: 1.096s, episode steps: 49, steps per second: 45, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.499 [0.430, 0.530], loss: 7.049418, mean_absolute_error: 34.575329, mean_q: 44.609955
[F[K  48174/500000: episode: 767, duration: 1.600s, episode steps: 81, steps per second: 51, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.938 [0.000, 4.000], mean observation: 0.500 [0.420, 0.580], loss: 7.219399, mean_absolute_error: 34.302948, mean_q: 44.181969
[F[K  48243/500000: episode: 768, duration: 1.396s, episode steps: 69, steps per second: 49, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.512 [0.500, 0.590], loss: 7.035538, mean_absolute_error: 34.063740, mean_q: 43.801025
[F[K  48301/500000: episode: 769, duration: 1.061s, episode steps: 58, steps per second: 55, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 0.518 [0.480, 0.650], loss: 8.119130, mean_absolute_error: 34.744530, mean_q: 44.664803
[F[K  48341/500000: episode: 770, duration: 0.811s, episode steps: 40, steps per second: 49, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.450 [0.000, 4.000], mean observation: 0.525 [0.470, 0.640], loss: 7.180073, mean_absolute_error: 33.720379, mean_q: 43.313011
[F[K  48403/500000: episode: 771, duration: 1.287s, episode steps: 62, steps per second: 48, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.726 [0.000, 4.000], mean observation: 0.505 [0.490, 0.530], loss: 7.434239, mean_absolute_error: 35.406620, mean_q: 45.499210
[F[K  48489/500000: episode: 772, duration: 1.840s, episode steps: 86, steps per second: 47, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.500 [0.380, 0.620], loss: 7.854024, mean_absolute_error: 34.160305, mean_q: 43.901638
[F[K  48552/500000: episode: 773, duration: 1.209s, episode steps: 63, steps per second: 52, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 7.046174, mean_absolute_error: 34.363865, mean_q: 44.074188
[F[K  48630/500000: episode: 774, duration: 1.421s, episode steps: 78, steps per second: 55, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.244 [0.000, 4.000], mean observation: 0.502 [0.470, 0.540], loss: 8.026861, mean_absolute_error: 34.165272, mean_q: 43.861427
[F[K  48770/500000: episode: 775, duration: 3.037s, episode steps: 140, steps per second: 46, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.497 [0.410, 0.550], loss: 7.464861, mean_absolute_error: 33.893520, mean_q: 43.583008
[F[K  48816/500000: episode: 776, duration: 0.916s, episode steps: 46, steps per second: 50, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.152 [0.000, 4.000], mean observation: 0.500 [0.350, 0.630], loss: 7.391712, mean_absolute_error: 34.630993, mean_q: 44.489136
[F[K  48919/500000: episode: 777, duration: 2.095s, episode steps: 103, steps per second: 49, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.223 [0.000, 4.000], mean observation: 0.519 [0.480, 0.650], loss: 7.342690, mean_absolute_error: 34.128227, mean_q: 43.870659
[F[K  48986/500000: episode: 778, duration: 1.298s, episode steps: 67, steps per second: 52, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.821 [0.000, 4.000], mean observation: 0.485 [0.400, 0.530], loss: 8.496563, mean_absolute_error: 33.753567, mean_q: 43.460335
[F[K  49039/500000: episode: 779, duration: 1.011s, episode steps: 53, steps per second: 52, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.170 [0.000, 4.000], mean observation: 0.516 [0.480, 0.650], loss: 7.938752, mean_absolute_error: 34.342297, mean_q: 44.026363
[F[K  49096/500000: episode: 780, duration: 1.220s, episode steps: 57, steps per second: 47, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 0.501 [0.420, 0.570], loss: 7.986433, mean_absolute_error: 34.487747, mean_q: 44.252888
[F[K  49150/500000: episode: 781, duration: 1.317s, episode steps: 54, steps per second: 41, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.056 [0.000, 4.000], mean observation: 0.504 [0.470, 0.580], loss: 8.388984, mean_absolute_error: 34.503197, mean_q: 44.308750
[F[K  49241/500000: episode: 782, duration: 1.791s, episode steps: 91, steps per second: 51, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.934 [0.000, 4.000], mean observation: 0.499 [0.430, 0.540], loss: 7.287845, mean_absolute_error: 34.161316, mean_q: 43.935669
[F[K  49405/500000: episode: 783, duration: 3.731s, episode steps: 164, steps per second: 44, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.232 [0.000, 4.000], mean observation: 0.505 [0.400, 0.640], loss: 7.221245, mean_absolute_error: 33.950256, mean_q: 43.682056
[F[K  49487/500000: episode: 784, duration: 1.663s, episode steps: 82, steps per second: 49, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.659 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 6.667495, mean_absolute_error: 34.484734, mean_q: 44.320713
[F[K  49591/500000: episode: 785, duration: 1.920s, episode steps: 104, steps per second: 54, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.356 [0.000, 4.000], mean observation: 0.485 [0.370, 0.530], loss: 7.137654, mean_absolute_error: 34.189049, mean_q: 44.098087
[F[K  49654/500000: episode: 786, duration: 1.185s, episode steps: 63, steps per second: 53, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.413 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 7.329424, mean_absolute_error: 34.781113, mean_q: 44.719410
[F[K  49689/500000: episode: 787, duration: 0.757s, episode steps: 35, steps per second: 46, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.514 [0.470, 0.650], loss: 7.507113, mean_absolute_error: 34.214542, mean_q: 44.028873
[F[K  49760/500000: episode: 788, duration: 1.667s, episode steps: 71, steps per second: 43, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.563 [0.000, 4.000], mean observation: 0.511 [0.480, 0.590], loss: 7.108869, mean_absolute_error: 34.109768, mean_q: 44.008106
[F[K  49818/500000: episode: 789, duration: 1.233s, episode steps: 58, steps per second: 47, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.483 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 7.382496, mean_absolute_error: 34.436634, mean_q: 44.366585
[F[K  49892/500000: episode: 790, duration: 1.499s, episode steps: 74, steps per second: 49, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.257 [0.000, 4.000], mean observation: 0.501 [0.410, 0.630], loss: 7.065861, mean_absolute_error: 34.747101, mean_q: 44.787952
[F[K  49958/500000: episode: 791, duration: 1.438s, episode steps: 66, steps per second: 46, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.288 [0.000, 4.000], mean observation: 0.503 [0.430, 0.610], loss: 7.897582, mean_absolute_error: 34.054798, mean_q: 43.833008
[F[K  50022/500000: episode: 792, duration: 1.464s, episode steps: 64, steps per second: 44, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.479 [0.350, 0.530], loss: 6.744348, mean_absolute_error: 34.363136, mean_q: 44.269367
[F[K  50099/500000: episode: 793, duration: 1.666s, episode steps: 77, steps per second: 46, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.468 [0.000, 4.000], mean observation: 0.491 [0.350, 0.570], loss: 7.504512, mean_absolute_error: 34.603306, mean_q: 44.495941
[F[K  50153/500000: episode: 794, duration: 0.996s, episode steps: 54, steps per second: 54, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.037 [0.000, 4.000], mean observation: 0.493 [0.450, 0.530], loss: 8.835320, mean_absolute_error: 34.700829, mean_q: 44.483456
[F[K  50238/500000: episode: 795, duration: 1.672s, episode steps: 85, steps per second: 51, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 4.000], mean observation: 0.501 [0.380, 0.610], loss: 7.978591, mean_absolute_error: 35.133526, mean_q: 45.212490
[F[K  50294/500000: episode: 796, duration: 1.180s, episode steps: 56, steps per second: 47, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.786 [0.000, 4.000], mean observation: 0.482 [0.390, 0.530], loss: 7.037892, mean_absolute_error: 34.582527, mean_q: 44.523518
[F[K  50377/500000: episode: 797, duration: 1.713s, episode steps: 83, steps per second: 48, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.490 [0.380, 0.540], loss: 7.557799, mean_absolute_error: 34.902046, mean_q: 44.851658
[F[K  50425/500000: episode: 798, duration: 1.171s, episode steps: 48, steps per second: 41, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.516 [0.470, 0.620], loss: 7.619213, mean_absolute_error: 35.168766, mean_q: 45.159149
[F[K  50483/500000: episode: 799, duration: 1.232s, episode steps: 58, steps per second: 47, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.052 [0.000, 4.000], mean observation: 0.522 [0.470, 0.610], loss: 8.600104, mean_absolute_error: 34.283245, mean_q: 43.997066
[F[K  50520/500000: episode: 800, duration: 0.756s, episode steps: 37, steps per second: 49, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.946 [0.000, 4.000], mean observation: 0.488 [0.350, 0.530], loss: 6.407843, mean_absolute_error: 34.758121, mean_q: 44.533245
[F[K  50592/500000: episode: 801, duration: 1.613s, episode steps: 72, steps per second: 45, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.486 [0.000, 4.000], mean observation: 0.506 [0.480, 0.550], loss: 6.937479, mean_absolute_error: 35.152939, mean_q: 45.228230
[F[K  50663/500000: episode: 802, duration: 1.479s, episode steps: 71, steps per second: 48, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.662 [0.000, 4.000], mean observation: 0.490 [0.400, 0.530], loss: 7.854507, mean_absolute_error: 35.304497, mean_q: 45.376129
[F[K  50742/500000: episode: 803, duration: 1.774s, episode steps: 79, steps per second: 45, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.823 [0.000, 4.000], mean observation: 0.506 [0.400, 0.630], loss: 7.107417, mean_absolute_error: 35.293148, mean_q: 45.301899
[F[K  50808/500000: episode: 804, duration: 1.317s, episode steps: 66, steps per second: 50, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.364 [0.000, 4.000], mean observation: 0.507 [0.480, 0.540], loss: 7.792009, mean_absolute_error: 35.378654, mean_q: 45.381710
[F[K  50872/500000: episode: 805, duration: 1.385s, episode steps: 64, steps per second: 46, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.515 [0.490, 0.560], loss: 7.178245, mean_absolute_error: 34.980011, mean_q: 44.879642
[F[K  50952/500000: episode: 806, duration: 1.481s, episode steps: 80, steps per second: 54, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.517 [0.480, 0.600], loss: 7.857471, mean_absolute_error: 35.038860, mean_q: 44.872650
[F[K  51062/500000: episode: 807, duration: 2.280s, episode steps: 110, steps per second: 48, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.318 [0.000, 4.000], mean observation: 0.491 [0.390, 0.530], loss: 7.390130, mean_absolute_error: 34.730362, mean_q: 44.591824
[F[K  51096/500000: episode: 808, duration: 0.757s, episode steps: 34, steps per second: 45, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.527 [0.470, 0.650], loss: 7.499736, mean_absolute_error: 34.643559, mean_q: 44.435074
[F[K  51183/500000: episode: 809, duration: 1.561s, episode steps: 87, steps per second: 56, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.839 [0.000, 4.000], mean observation: 0.503 [0.450, 0.560], loss: 8.060885, mean_absolute_error: 35.012024, mean_q: 44.871662
[F[K  51251/500000: episode: 810, duration: 1.123s, episode steps: 68, steps per second: 61, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.510 [0.490, 0.560], loss: 7.608834, mean_absolute_error: 35.508320, mean_q: 45.635384
[F[K  51410/500000: episode: 811, duration: 3.392s, episode steps: 159, steps per second: 47, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.969 [0.000, 4.000], mean observation: 0.503 [0.400, 0.650], loss: 7.869883, mean_absolute_error: 34.720974, mean_q: 44.524807
[F[K  51492/500000: episode: 812, duration: 1.739s, episode steps: 82, steps per second: 47, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.490 [0.430, 0.520], loss: 7.706496, mean_absolute_error: 35.395271, mean_q: 45.376766
[F[K  51552/500000: episode: 813, duration: 1.072s, episode steps: 60, steps per second: 56, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.317 [0.000, 4.000], mean observation: 0.504 [0.470, 0.570], loss: 7.351961, mean_absolute_error: 34.778393, mean_q: 44.598984
[F[K  51612/500000: episode: 814, duration: 1.211s, episode steps: 60, steps per second: 50, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.950 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 8.170401, mean_absolute_error: 34.617851, mean_q: 44.434376
[F[K  51653/500000: episode: 815, duration: 0.863s, episode steps: 41, steps per second: 48, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.503 [0.370, 0.650], loss: 8.364278, mean_absolute_error: 35.469307, mean_q: 45.496254
[F[K  51721/500000: episode: 816, duration: 1.308s, episode steps: 68, steps per second: 52, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.985 [0.000, 4.000], mean observation: 0.495 [0.360, 0.590], loss: 7.754906, mean_absolute_error: 35.335178, mean_q: 45.335602
[F[K  51794/500000: episode: 817, duration: 1.251s, episode steps: 73, steps per second: 58, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.384 [0.000, 4.000], mean observation: 0.505 [0.440, 0.590], loss: 7.449700, mean_absolute_error: 34.416225, mean_q: 44.120258
[F[K  51872/500000: episode: 818, duration: 1.394s, episode steps: 78, steps per second: 56, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.923 [0.000, 4.000], mean observation: 0.510 [0.460, 0.570], loss: 7.555325, mean_absolute_error: 34.526634, mean_q: 44.356396
[F[K  51914/500000: episode: 819, duration: 0.776s, episode steps: 42, steps per second: 54, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.381 [0.000, 4.000], mean observation: 0.491 [0.350, 0.540], loss: 7.453361, mean_absolute_error: 34.494099, mean_q: 44.364822
[F[K  51973/500000: episode: 820, duration: 1.041s, episode steps: 59, steps per second: 57, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.576 [0.000, 4.000], mean observation: 0.502 [0.460, 0.570], loss: 8.442176, mean_absolute_error: 35.113052, mean_q: 45.053410
[F[K  52053/500000: episode: 821, duration: 1.609s, episode steps: 80, steps per second: 50, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 7.777185, mean_absolute_error: 34.573109, mean_q: 44.428558
[F[K  52114/500000: episode: 822, duration: 1.264s, episode steps: 61, steps per second: 48, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.492 [0.000, 4.000], mean observation: 0.519 [0.480, 0.640], loss: 7.210432, mean_absolute_error: 34.825817, mean_q: 44.848545
[F[K  52187/500000: episode: 823, duration: 1.457s, episode steps: 73, steps per second: 50, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.822 [0.000, 4.000], mean observation: 0.484 [0.370, 0.510], loss: 7.477589, mean_absolute_error: 34.037766, mean_q: 43.819893
[F[K  52245/500000: episode: 824, duration: 1.060s, episode steps: 58, steps per second: 55, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.515 [0.470, 0.610], loss: 8.157165, mean_absolute_error: 34.772419, mean_q: 44.634293
[F[K  52309/500000: episode: 825, duration: 1.357s, episode steps: 64, steps per second: 47, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.266 [0.000, 4.000], mean observation: 0.480 [0.360, 0.520], loss: 8.076740, mean_absolute_error: 34.243973, mean_q: 43.920753
[F[K  52384/500000: episode: 826, duration: 1.592s, episode steps: 75, steps per second: 47, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.280 [0.000, 4.000], mean observation: 0.487 [0.390, 0.530], loss: 6.931108, mean_absolute_error: 34.525867, mean_q: 44.471737
[F[K  52451/500000: episode: 827, duration: 1.156s, episode steps: 67, steps per second: 58, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.896 [0.000, 4.000], mean observation: 0.511 [0.460, 0.620], loss: 8.142455, mean_absolute_error: 34.688004, mean_q: 44.481556
[F[K  52524/500000: episode: 828, duration: 1.290s, episode steps: 73, steps per second: 57, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.501 [0.450, 0.550], loss: 6.779034, mean_absolute_error: 34.967331, mean_q: 44.986668
[F[K  52600/500000: episode: 829, duration: 1.400s, episode steps: 76, steps per second: 54, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.026 [0.000, 4.000], mean observation: 0.512 [0.490, 0.570], loss: 7.848519, mean_absolute_error: 35.024067, mean_q: 44.983318
[F[K  52793/500000: episode: 830, duration: 3.329s, episode steps: 193, steps per second: 58, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.280 [0.000, 4.000], mean observation: 0.513 [0.440, 0.690], loss: 8.111430, mean_absolute_error: 34.685204, mean_q: 44.497765
[F[K  52872/500000: episode: 831, duration: 1.380s, episode steps: 79, steps per second: 57, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.506 [0.490, 0.540], loss: 7.906239, mean_absolute_error: 34.698853, mean_q: 44.519836
[F[K  52931/500000: episode: 832, duration: 1.163s, episode steps: 59, steps per second: 51, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.220 [0.000, 4.000], mean observation: 0.515 [0.470, 0.620], loss: 7.269881, mean_absolute_error: 34.848465, mean_q: 44.877106
[F[K  52992/500000: episode: 833, duration: 1.135s, episode steps: 61, steps per second: 54, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.738 [0.000, 4.000], mean observation: 0.483 [0.400, 0.530], loss: 7.716764, mean_absolute_error: 34.262169, mean_q: 44.092278
[F[K  53156/500000: episode: 834, duration: 2.981s, episode steps: 164, steps per second: 55, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.494 [0.390, 0.570], loss: 7.311197, mean_absolute_error: 34.863548, mean_q: 44.708664
[F[K  53253/500000: episode: 835, duration: 1.599s, episode steps: 97, steps per second: 61, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.509 [0.470, 0.560], loss: 7.965772, mean_absolute_error: 34.872570, mean_q: 44.748348
[F[K  53342/500000: episode: 836, duration: 1.408s, episode steps: 89, steps per second: 63, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.011 [0.000, 4.000], mean observation: 0.489 [0.410, 0.530], loss: 8.174542, mean_absolute_error: 34.696697, mean_q: 44.463406
[F[K  53398/500000: episode: 837, duration: 0.916s, episode steps: 56, steps per second: 61, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.232 [0.000, 4.000], mean observation: 0.493 [0.400, 0.530], loss: 7.936101, mean_absolute_error: 34.317337, mean_q: 44.017178
[F[K  53507/500000: episode: 838, duration: 2.074s, episode steps: 109, steps per second: 53, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.670 [0.000, 4.000], mean observation: 0.522 [0.470, 0.650], loss: 8.115240, mean_absolute_error: 34.971939, mean_q: 44.875332
[F[K  53568/500000: episode: 839, duration: 1.027s, episode steps: 61, steps per second: 59, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.525 [0.470, 0.640], loss: 7.159617, mean_absolute_error: 34.853130, mean_q: 44.811535
[F[K  53672/500000: episode: 840, duration: 1.889s, episode steps: 104, steps per second: 55, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.548 [0.000, 4.000], mean observation: 0.506 [0.470, 0.540], loss: 7.361448, mean_absolute_error: 34.816494, mean_q: 44.746140
[F[K  53745/500000: episode: 841, duration: 1.334s, episode steps: 73, steps per second: 55, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.425 [0.000, 4.000], mean observation: 0.484 [0.400, 0.520], loss: 8.205077, mean_absolute_error: 35.076790, mean_q: 44.939110
[F[K  53868/500000: episode: 842, duration: 1.999s, episode steps: 123, steps per second: 62, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.163 [0.000, 4.000], mean observation: 0.499 [0.410, 0.590], loss: 7.519848, mean_absolute_error: 34.932083, mean_q: 44.780998
[F[K  53941/500000: episode: 843, duration: 1.242s, episode steps: 73, steps per second: 59, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.959 [0.000, 4.000], mean observation: 0.481 [0.370, 0.530], loss: 7.727954, mean_absolute_error: 35.223907, mean_q: 45.220600
[F[K  54003/500000: episode: 844, duration: 1.201s, episode steps: 62, steps per second: 52, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.968 [0.000, 4.000], mean observation: 0.505 [0.480, 0.540], loss: 6.994012, mean_absolute_error: 34.999180, mean_q: 44.745289
[F[K  54107/500000: episode: 845, duration: 1.979s, episode steps: 104, steps per second: 53, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.240 [0.000, 4.000], mean observation: 0.498 [0.440, 0.550], loss: 7.743270, mean_absolute_error: 34.975548, mean_q: 44.881554
[F[K  54275/500000: episode: 846, duration: 2.851s, episode steps: 168, steps per second: 59, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.345 [0.000, 4.000], mean observation: 0.502 [0.470, 0.570], loss: 7.883965, mean_absolute_error: 35.090988, mean_q: 45.013355
[F[K  54348/500000: episode: 847, duration: 1.425s, episode steps: 73, steps per second: 51, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.822 [0.000, 4.000], mean observation: 0.496 [0.420, 0.530], loss: 7.342245, mean_absolute_error: 34.698174, mean_q: 44.450783
[F[K  54414/500000: episode: 848, duration: 1.245s, episode steps: 66, steps per second: 53, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.303 [0.000, 4.000], mean observation: 0.517 [0.480, 0.620], loss: 7.646614, mean_absolute_error: 34.769169, mean_q: 44.589520
[F[K  54510/500000: episode: 849, duration: 1.490s, episode steps: 96, steps per second: 64, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.504 [0.420, 0.620], loss: 7.987226, mean_absolute_error: 35.021832, mean_q: 44.929821
[F[K  54676/500000: episode: 850, duration: 3.524s, episode steps: 166, steps per second: 47, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.367 [0.000, 4.000], mean observation: 0.489 [0.390, 0.530], loss: 7.201501, mean_absolute_error: 34.854279, mean_q: 44.767109
[F[K  54733/500000: episode: 851, duration: 1.184s, episode steps: 57, steps per second: 48, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.737 [0.000, 4.000], mean observation: 0.504 [0.420, 0.630], loss: 7.694706, mean_absolute_error: 34.930180, mean_q: 44.841423
[F[K  54837/500000: episode: 852, duration: 2.041s, episode steps: 104, steps per second: 51, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.212 [0.000, 4.000], mean observation: 0.498 [0.430, 0.550], loss: 6.731908, mean_absolute_error: 35.165798, mean_q: 45.149132
[F[K  54917/500000: episode: 853, duration: 1.377s, episode steps: 80, steps per second: 58, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.837 [0.000, 4.000], mean observation: 0.504 [0.480, 0.530], loss: 8.234860, mean_absolute_error: 35.276451, mean_q: 45.258053
[F[K  54989/500000: episode: 854, duration: 1.173s, episode steps: 72, steps per second: 61, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.389 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 8.009673, mean_absolute_error: 35.297909, mean_q: 45.226524
[F[K  55060/500000: episode: 855, duration: 1.315s, episode steps: 71, steps per second: 54, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.690 [0.000, 4.000], mean observation: 0.505 [0.460, 0.590], loss: 7.229211, mean_absolute_error: 35.661488, mean_q: 45.757504
[F[K  55094/500000: episode: 856, duration: 0.678s, episode steps: 34, steps per second: 50, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.706 [0.000, 4.000], mean observation: 0.516 [0.470, 0.640], loss: 7.237543, mean_absolute_error: 35.508984, mean_q: 45.606579
[F[K  55159/500000: episode: 857, duration: 1.116s, episode steps: 65, steps per second: 58, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.046 [0.000, 4.000], mean observation: 0.497 [0.370, 0.600], loss: 7.952047, mean_absolute_error: 35.498207, mean_q: 45.475906
[F[K  55217/500000: episode: 858, duration: 0.953s, episode steps: 58, steps per second: 61, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.845 [0.000, 4.000], mean observation: 0.478 [0.350, 0.520], loss: 6.786199, mean_absolute_error: 35.539608, mean_q: 45.644447
[F[K  55314/500000: episode: 859, duration: 1.663s, episode steps: 97, steps per second: 58, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.500 [0.370, 0.620], loss: 7.558444, mean_absolute_error: 35.936844, mean_q: 46.084625
[F[K  55386/500000: episode: 860, duration: 1.262s, episode steps: 72, steps per second: 57, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.972 [0.000, 4.000], mean observation: 0.502 [0.390, 0.660], loss: 7.260788, mean_absolute_error: 35.645069, mean_q: 45.751675
[F[K  55458/500000: episode: 861, duration: 1.413s, episode steps: 72, steps per second: 51, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.486 [0.000, 4.000], mean observation: 0.516 [0.480, 0.580], loss: 8.863292, mean_absolute_error: 35.502789, mean_q: 45.483383
[F[K  55536/500000: episode: 862, duration: 1.402s, episode steps: 78, steps per second: 56, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.192 [0.000, 4.000], mean observation: 0.472 [0.350, 0.530], loss: 7.078201, mean_absolute_error: 35.115570, mean_q: 45.085079
[F[K  55595/500000: episode: 863, duration: 1.121s, episode steps: 59, steps per second: 53, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.153 [0.000, 4.000], mean observation: 0.503 [0.450, 0.560], loss: 7.568272, mean_absolute_error: 34.816437, mean_q: 44.846188
[F[K  55710/500000: episode: 864, duration: 2.031s, episode steps: 115, steps per second: 57, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.617 [0.000, 4.000], mean observation: 0.497 [0.380, 0.570], loss: 7.645303, mean_absolute_error: 36.077328, mean_q: 46.245567
[F[K  55808/500000: episode: 865, duration: 1.473s, episode steps: 98, steps per second: 67, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.439 [0.000, 4.000], mean observation: 0.495 [0.450, 0.530], loss: 7.674554, mean_absolute_error: 35.347763, mean_q: 45.348858
[F[K  55937/500000: episode: 866, duration: 2.703s, episode steps: 129, steps per second: 48, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.496 [0.000, 4.000], mean observation: 0.498 [0.420, 0.550], loss: 8.968201, mean_absolute_error: 35.704018, mean_q: 45.812225
[F[K  56003/500000: episode: 867, duration: 1.201s, episode steps: 66, steps per second: 55, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.515 [0.000, 4.000], mean observation: 0.494 [0.370, 0.570], loss: 8.749043, mean_absolute_error: 35.797138, mean_q: 45.847214
[F[K  56039/500000: episode: 868, duration: 0.609s, episode steps: 36, steps per second: 59, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 3.000], mean observation: 0.522 [0.470, 0.640], loss: 6.901206, mean_absolute_error: 34.915672, mean_q: 44.804264
[F[K  56103/500000: episode: 869, duration: 1.192s, episode steps: 64, steps per second: 54, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.953 [0.000, 4.000], mean observation: 0.489 [0.450, 0.520], loss: 7.588519, mean_absolute_error: 35.425499, mean_q: 45.350883
[F[K  56143/500000: episode: 870, duration: 0.752s, episode steps: 40, steps per second: 53, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.522 [0.470, 0.620], loss: 8.206390, mean_absolute_error: 34.468315, mean_q: 44.293308
[F[K  56220/500000: episode: 871, duration: 1.428s, episode steps: 77, steps per second: 54, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 0.482 [0.390, 0.520], loss: 7.566442, mean_absolute_error: 34.946079, mean_q: 44.938152
[F[K  56291/500000: episode: 872, duration: 1.360s, episode steps: 71, steps per second: 52, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.803 [0.000, 4.000], mean observation: 0.495 [0.430, 0.530], loss: 7.952116, mean_absolute_error: 35.690849, mean_q: 45.755474
[F[K  56336/500000: episode: 873, duration: 0.927s, episode steps: 45, steps per second: 49, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.644 [0.000, 4.000], mean observation: 0.495 [0.390, 0.530], loss: 9.417961, mean_absolute_error: 35.543808, mean_q: 45.499863
[F[K  56390/500000: episode: 874, duration: 1.166s, episode steps: 54, steps per second: 46, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.519 [0.000, 4.000], mean observation: 0.484 [0.410, 0.520], loss: 7.356079, mean_absolute_error: 35.674847, mean_q: 45.763424
[F[K  56484/500000: episode: 875, duration: 1.952s, episode steps: 94, steps per second: 48, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.504 [0.480, 0.550], loss: 8.174586, mean_absolute_error: 35.088982, mean_q: 44.978588
[F[K  56529/500000: episode: 876, duration: 0.874s, episode steps: 45, steps per second: 51, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.523 [0.470, 0.620], loss: 9.309205, mean_absolute_error: 34.772236, mean_q: 44.601101
[F[K  56586/500000: episode: 877, duration: 1.265s, episode steps: 57, steps per second: 45, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.175 [0.000, 4.000], mean observation: 0.525 [0.470, 0.640], loss: 7.884142, mean_absolute_error: 35.602562, mean_q: 45.645458
[F[K  56673/500000: episode: 878, duration: 1.641s, episode steps: 87, steps per second: 53, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.126 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 7.311065, mean_absolute_error: 35.360611, mean_q: 45.463783
[F[K  56709/500000: episode: 879, duration: 0.695s, episode steps: 36, steps per second: 52, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.502 [0.380, 0.650], loss: 7.930288, mean_absolute_error: 35.908791, mean_q: 46.102158
[F[K  56777/500000: episode: 880, duration: 1.194s, episode steps: 68, steps per second: 57, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.486 [0.360, 0.530], loss: 7.323227, mean_absolute_error: 35.737118, mean_q: 45.898853
[F[K  56829/500000: episode: 881, duration: 0.940s, episode steps: 52, steps per second: 55, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.288 [0.000, 4.000], mean observation: 0.510 [0.470, 0.630], loss: 6.646014, mean_absolute_error: 36.199566, mean_q: 46.505440
[F[K  56933/500000: episode: 882, duration: 1.650s, episode steps: 104, steps per second: 63, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.489 [0.380, 0.540], loss: 8.017292, mean_absolute_error: 35.501320, mean_q: 45.684883
[F[K  56996/500000: episode: 883, duration: 1.227s, episode steps: 63, steps per second: 51, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.498 [0.410, 0.540], loss: 7.433886, mean_absolute_error: 35.607819, mean_q: 45.694054
[F[K  57144/500000: episode: 884, duration: 3.082s, episode steps: 148, steps per second: 48, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.081 [0.000, 4.000], mean observation: 0.502 [0.400, 0.620], loss: 7.431193, mean_absolute_error: 35.559559, mean_q: 45.652454
[F[K  57222/500000: episode: 885, duration: 1.603s, episode steps: 78, steps per second: 49, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.910 [0.000, 4.000], mean observation: 0.502 [0.460, 0.550], loss: 8.960170, mean_absolute_error: 35.747955, mean_q: 45.677834
[F[K  57296/500000: episode: 886, duration: 1.232s, episode steps: 74, steps per second: 60, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.216 [0.000, 4.000], mean observation: 0.489 [0.420, 0.530], loss: 8.129012, mean_absolute_error: 35.977432, mean_q: 46.131847
[F[K  57379/500000: episode: 887, duration: 1.287s, episode steps: 83, steps per second: 64, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.169 [0.000, 4.000], mean observation: 0.501 [0.410, 0.580], loss: 7.822322, mean_absolute_error: 35.913471, mean_q: 45.946117
[F[K  57416/500000: episode: 888, duration: 0.722s, episode steps: 37, steps per second: 51, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.622 [0.000, 3.000], mean observation: 0.524 [0.470, 0.650], loss: 11.326131, mean_absolute_error: 35.272560, mean_q: 45.300598
[F[K  57484/500000: episode: 889, duration: 1.479s, episode steps: 68, steps per second: 46, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.471 [0.000, 4.000], mean observation: 0.503 [0.390, 0.620], loss: 8.387528, mean_absolute_error: 35.314392, mean_q: 45.259838
[F[K  57557/500000: episode: 890, duration: 1.646s, episode steps: 73, steps per second: 44, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.500 [0.490, 0.530], loss: 7.327677, mean_absolute_error: 35.510338, mean_q: 45.604427
[F[K  57627/500000: episode: 891, duration: 1.390s, episode steps: 70, steps per second: 50, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.485 [0.420, 0.530], loss: 8.230000, mean_absolute_error: 36.222130, mean_q: 46.463512
[F[K  57695/500000: episode: 892, duration: 1.303s, episode steps: 68, steps per second: 52, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.265 [0.000, 4.000], mean observation: 0.500 [0.380, 0.590], loss: 7.422351, mean_absolute_error: 36.124687, mean_q: 46.373428
[F[K  57761/500000: episode: 893, duration: 1.567s, episode steps: 66, steps per second: 42, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.818 [0.000, 4.000], mean observation: 0.505 [0.440, 0.580], loss: 8.109621, mean_absolute_error: 35.950588, mean_q: 46.201118
[F[K  57852/500000: episode: 894, duration: 1.959s, episode steps: 91, steps per second: 46, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.593 [0.000, 4.000], mean observation: 0.489 [0.380, 0.540], loss: 7.998176, mean_absolute_error: 35.636806, mean_q: 45.779957
[F[K  57922/500000: episode: 895, duration: 1.430s, episode steps: 70, steps per second: 49, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.900 [0.000, 4.000], mean observation: 0.473 [0.350, 0.530], loss: 7.945731, mean_absolute_error: 35.766502, mean_q: 45.962082
[F[K  57979/500000: episode: 896, duration: 1.191s, episode steps: 57, steps per second: 48, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.489 [0.380, 0.530], loss: 6.747471, mean_absolute_error: 35.513458, mean_q: 45.572433
[F[K  58037/500000: episode: 897, duration: 1.133s, episode steps: 58, steps per second: 51, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.534 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 7.993853, mean_absolute_error: 35.354534, mean_q: 45.323917
[F[K  58122/500000: episode: 898, duration: 1.614s, episode steps: 85, steps per second: 53, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.059 [0.000, 4.000], mean observation: 0.486 [0.420, 0.510], loss: 7.672853, mean_absolute_error: 35.962849, mean_q: 46.232475
[F[K  58207/500000: episode: 899, duration: 1.824s, episode steps: 85, steps per second: 47, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.494 [0.000, 4.000], mean observation: 0.496 [0.470, 0.530], loss: 8.495115, mean_absolute_error: 35.965164, mean_q: 46.133049
[F[K  58266/500000: episode: 900, duration: 1.065s, episode steps: 59, steps per second: 55, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.119 [0.000, 4.000], mean observation: 0.513 [0.500, 0.580], loss: 7.988081, mean_absolute_error: 36.107571, mean_q: 46.347637
[F[K  58322/500000: episode: 901, duration: 0.984s, episode steps: 56, steps per second: 57, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.507 [0.460, 0.570], loss: 8.276744, mean_absolute_error: 36.396065, mean_q: 46.679783
[F[K  58361/500000: episode: 902, duration: 0.780s, episode steps: 39, steps per second: 50, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.513 [0.470, 0.660], loss: 7.215409, mean_absolute_error: 36.209167, mean_q: 46.378654
[F[K  58426/500000: episode: 903, duration: 1.539s, episode steps: 65, steps per second: 42, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.385 [0.000, 4.000], mean observation: 0.509 [0.490, 0.550], loss: 7.605264, mean_absolute_error: 35.716652, mean_q: 45.812927
[F[K  58486/500000: episode: 904, duration: 1.110s, episode steps: 60, steps per second: 54, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.517 [0.000, 4.000], mean observation: 0.479 [0.350, 0.530], loss: 8.127101, mean_absolute_error: 35.403122, mean_q: 45.432110
[F[K  58656/500000: episode: 905, duration: 3.730s, episode steps: 170, steps per second: 46, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 0.499 [0.380, 0.620], loss: 8.025623, mean_absolute_error: 36.009029, mean_q: 46.076431
[F[K  58728/500000: episode: 906, duration: 1.525s, episode steps: 72, steps per second: 47, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.653 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 7.260181, mean_absolute_error: 35.618488, mean_q: 45.639000
[F[K  58841/500000: episode: 907, duration: 2.259s, episode steps: 113, steps per second: 50, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.805 [0.000, 4.000], mean observation: 0.497 [0.430, 0.540], loss: 7.946879, mean_absolute_error: 36.016533, mean_q: 46.171318
[F[K  58897/500000: episode: 908, duration: 0.996s, episode steps: 56, steps per second: 56, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.492 [0.360, 0.560], loss: 7.230253, mean_absolute_error: 35.970158, mean_q: 46.200466
[F[K  58993/500000: episode: 909, duration: 1.933s, episode steps: 96, steps per second: 50, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.698 [0.000, 4.000], mean observation: 0.494 [0.370, 0.580], loss: 7.519129, mean_absolute_error: 36.246319, mean_q: 46.518951
[F[K  59076/500000: episode: 910, duration: 1.634s, episode steps: 83, steps per second: 51, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.337 [0.000, 4.000], mean observation: 0.497 [0.420, 0.580], loss: 7.602048, mean_absolute_error: 35.571136, mean_q: 45.669327
[F[K  59138/500000: episode: 911, duration: 1.047s, episode steps: 62, steps per second: 59, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 4.000], mean observation: 0.484 [0.390, 0.530], loss: 7.947596, mean_absolute_error: 36.234669, mean_q: 46.367271
[F[K  59199/500000: episode: 912, duration: 1.337s, episode steps: 61, steps per second: 46, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.656 [0.000, 4.000], mean observation: 0.520 [0.470, 0.610], loss: 8.081296, mean_absolute_error: 35.174751, mean_q: 45.039070
[F[K  59261/500000: episode: 913, duration: 1.280s, episode steps: 62, steps per second: 48, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.855 [0.000, 4.000], mean observation: 0.481 [0.370, 0.530], loss: 7.710639, mean_absolute_error: 35.807606, mean_q: 45.807892
[F[K  59339/500000: episode: 914, duration: 1.682s, episode steps: 78, steps per second: 46, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.872 [0.000, 4.000], mean observation: 0.510 [0.490, 0.580], loss: 8.361415, mean_absolute_error: 36.267307, mean_q: 46.467720
[F[K  59428/500000: episode: 915, duration: 1.727s, episode steps: 89, steps per second: 52, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.764 [0.000, 4.000], mean observation: 0.506 [0.430, 0.650], loss: 8.379219, mean_absolute_error: 35.812973, mean_q: 45.943588
[F[K  59511/500000: episode: 916, duration: 1.546s, episode steps: 83, steps per second: 54, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.193 [0.000, 4.000], mean observation: 0.492 [0.430, 0.520], loss: 8.045428, mean_absolute_error: 36.152546, mean_q: 46.340195
[F[K  59566/500000: episode: 917, duration: 1.059s, episode steps: 55, steps per second: 52, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.127 [0.000, 4.000], mean observation: 0.505 [0.430, 0.630], loss: 7.858178, mean_absolute_error: 36.082043, mean_q: 46.251865
[F[K  59621/500000: episode: 918, duration: 1.155s, episode steps: 55, steps per second: 48, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.364 [0.000, 4.000], mean observation: 0.502 [0.400, 0.630], loss: 7.600737, mean_absolute_error: 35.604233, mean_q: 45.654953
[F[K  59694/500000: episode: 919, duration: 1.340s, episode steps: 73, steps per second: 54, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.658 [0.000, 4.000], mean observation: 0.509 [0.480, 0.610], loss: 7.555024, mean_absolute_error: 36.659534, mean_q: 46.855698
[F[K  59735/500000: episode: 920, duration: 0.892s, episode steps: 41, steps per second: 46, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.483 [0.350, 0.530], loss: 8.434437, mean_absolute_error: 35.431664, mean_q: 45.499466
[F[K  59798/500000: episode: 921, duration: 1.149s, episode steps: 63, steps per second: 55, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.762 [0.000, 4.000], mean observation: 0.505 [0.460, 0.580], loss: 7.139026, mean_absolute_error: 35.764008, mean_q: 45.947704
[F[K  59923/500000: episode: 922, duration: 2.471s, episode steps: 125, steps per second: 51, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.568 [0.000, 4.000], mean observation: 0.512 [0.490, 0.600], loss: 8.049401, mean_absolute_error: 36.336052, mean_q: 46.677563
[F[K  60064/500000: episode: 923, duration: 2.801s, episode steps: 141, steps per second: 50, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.312 [0.000, 4.000], mean observation: 0.490 [0.410, 0.530], loss: 8.853945, mean_absolute_error: 35.758919, mean_q: 45.805046
[F[K  60131/500000: episode: 924, duration: 1.458s, episode steps: 67, steps per second: 46, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 7.801408, mean_absolute_error: 35.728786, mean_q: 45.886715
[F[K  60176/500000: episode: 925, duration: 0.981s, episode steps: 45, steps per second: 46, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.519 [0.470, 0.630], loss: 9.018831, mean_absolute_error: 35.884464, mean_q: 45.986229
[F[K  60252/500000: episode: 926, duration: 1.380s, episode steps: 76, steps per second: 55, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.605 [0.000, 4.000], mean observation: 0.507 [0.430, 0.630], loss: 8.215113, mean_absolute_error: 35.576534, mean_q: 45.621552
[F[K  60300/500000: episode: 927, duration: 0.982s, episode steps: 48, steps per second: 49, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.917 [0.000, 4.000], mean observation: 0.491 [0.420, 0.530], loss: 7.689135, mean_absolute_error: 34.286179, mean_q: 44.018906
[F[K  60416/500000: episode: 928, duration: 2.249s, episode steps: 116, steps per second: 52, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.845 [0.000, 4.000], mean observation: 0.498 [0.400, 0.600], loss: 7.566944, mean_absolute_error: 35.639088, mean_q: 45.750572
[F[K  60469/500000: episode: 929, duration: 1.100s, episode steps: 53, steps per second: 48, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.508 [0.470, 0.610], loss: 7.612275, mean_absolute_error: 35.832012, mean_q: 46.012474
[F[K  60566/500000: episode: 930, duration: 2.171s, episode steps: 97, steps per second: 45, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.433 [0.000, 4.000], mean observation: 0.492 [0.400, 0.530], loss: 7.568338, mean_absolute_error: 35.451641, mean_q: 45.478233
[F[K  60645/500000: episode: 931, duration: 1.593s, episode steps: 79, steps per second: 50, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.810 [0.000, 4.000], mean observation: 0.487 [0.380, 0.530], loss: 7.282636, mean_absolute_error: 35.461361, mean_q: 45.554844
[F[K  60704/500000: episode: 932, duration: 1.161s, episode steps: 59, steps per second: 51, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.814 [0.000, 4.000], mean observation: 0.481 [0.400, 0.530], loss: 7.989978, mean_absolute_error: 35.611355, mean_q: 45.686295
[F[K  60759/500000: episode: 933, duration: 1.152s, episode steps: 55, steps per second: 48, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.927 [0.000, 4.000], mean observation: 0.509 [0.470, 0.600], loss: 8.036233, mean_absolute_error: 35.762840, mean_q: 45.938957
[F[K  60820/500000: episode: 934, duration: 0.928s, episode steps: 61, steps per second: 66, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.967 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 7.077669, mean_absolute_error: 34.532906, mean_q: 44.461708
[F[K  60899/500000: episode: 935, duration: 1.316s, episode steps: 79, steps per second: 60, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.405 [0.000, 4.000], mean observation: 0.502 [0.470, 0.530], loss: 7.914698, mean_absolute_error: 36.133881, mean_q: 46.272541
[F[K  60978/500000: episode: 936, duration: 1.603s, episode steps: 79, steps per second: 49, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.823 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 8.532451, mean_absolute_error: 35.925056, mean_q: 46.068466
[F[K  61049/500000: episode: 937, duration: 1.429s, episode steps: 71, steps per second: 50, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.817 [0.000, 4.000], mean observation: 0.514 [0.470, 0.650], loss: 6.961649, mean_absolute_error: 36.089127, mean_q: 46.177193
[F[K  61118/500000: episode: 938, duration: 1.548s, episode steps: 69, steps per second: 45, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.174 [0.000, 4.000], mean observation: 0.496 [0.450, 0.520], loss: 8.123474, mean_absolute_error: 35.217686, mean_q: 45.192425
[F[K  61192/500000: episode: 939, duration: 1.543s, episode steps: 74, steps per second: 48, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.635 [0.000, 4.000], mean observation: 0.489 [0.370, 0.540], loss: 7.491621, mean_absolute_error: 35.718658, mean_q: 45.827370
[F[K  61261/500000: episode: 940, duration: 1.296s, episode steps: 69, steps per second: 53, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.855 [0.000, 4.000], mean observation: 0.496 [0.390, 0.540], loss: 7.109246, mean_absolute_error: 36.273129, mean_q: 46.555962
[F[K  61371/500000: episode: 941, duration: 1.859s, episode steps: 110, steps per second: 59, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.573 [0.000, 4.000], mean observation: 0.500 [0.460, 0.530], loss: 7.690400, mean_absolute_error: 36.020130, mean_q: 46.179245
[F[K  61430/500000: episode: 942, duration: 1.124s, episode steps: 59, steps per second: 52, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.390 [0.000, 4.000], mean observation: 0.509 [0.490, 0.540], loss: 7.106688, mean_absolute_error: 36.195049, mean_q: 46.517883
[F[K  61496/500000: episode: 943, duration: 1.335s, episode steps: 66, steps per second: 49, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.258 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 7.463731, mean_absolute_error: 36.994488, mean_q: 47.424492
[F[K  61570/500000: episode: 944, duration: 1.310s, episode steps: 74, steps per second: 57, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.581 [0.000, 4.000], mean observation: 0.515 [0.480, 0.630], loss: 7.480061, mean_absolute_error: 35.961048, mean_q: 46.179150
[F[K  61631/500000: episode: 945, duration: 0.916s, episode steps: 61, steps per second: 67, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.574 [0.000, 4.000], mean observation: 0.498 [0.410, 0.570], loss: 7.782881, mean_absolute_error: 36.437870, mean_q: 46.634308
[F[K  61756/500000: episode: 946, duration: 1.963s, episode steps: 125, steps per second: 64, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.497 [0.390, 0.590], loss: 7.696715, mean_absolute_error: 35.933968, mean_q: 46.041824
[F[K  61856/500000: episode: 947, duration: 1.780s, episode steps: 100, steps per second: 56, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.480 [0.000, 4.000], mean observation: 0.491 [0.410, 0.530], loss: 7.643228, mean_absolute_error: 35.598442, mean_q: 45.661686
[F[K  61970/500000: episode: 948, duration: 1.886s, episode steps: 114, steps per second: 60, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.035 [0.000, 4.000], mean observation: 0.515 [0.470, 0.640], loss: 7.609598, mean_absolute_error: 35.808079, mean_q: 45.869667
[F[K  62046/500000: episode: 949, duration: 0.942s, episode steps: 76, steps per second: 81, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.276 [0.000, 4.000], mean observation: 0.500 [0.390, 0.620], loss: 8.279766, mean_absolute_error: 36.099125, mean_q: 46.306446
[F[K  62166/500000: episode: 950, duration: 2.086s, episode steps: 120, steps per second: 58, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.867 [0.000, 4.000], mean observation: 0.479 [0.370, 0.520], loss: 7.582388, mean_absolute_error: 36.336109, mean_q: 46.588184
[F[K  62342/500000: episode: 951, duration: 3.118s, episode steps: 176, steps per second: 56, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.386 [0.000, 4.000], mean observation: 0.482 [0.360, 0.520], loss: 8.187650, mean_absolute_error: 36.398258, mean_q: 46.565884
[F[K  62394/500000: episode: 952, duration: 0.954s, episode steps: 52, steps per second: 54, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.654 [0.000, 4.000], mean observation: 0.499 [0.360, 0.620], loss: 7.521459, mean_absolute_error: 36.652309, mean_q: 46.989395
[F[K  62470/500000: episode: 953, duration: 1.317s, episode steps: 76, steps per second: 58, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.013 [0.000, 4.000], mean observation: 0.478 [0.360, 0.520], loss: 6.858184, mean_absolute_error: 35.568443, mean_q: 45.675735
[F[K  62538/500000: episode: 954, duration: 1.260s, episode steps: 68, steps per second: 54, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.824 [0.000, 4.000], mean observation: 0.500 [0.440, 0.560], loss: 7.517963, mean_absolute_error: 35.873394, mean_q: 46.038315
[F[K  62619/500000: episode: 955, duration: 1.298s, episode steps: 81, steps per second: 62, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.926 [0.000, 4.000], mean observation: 0.503 [0.440, 0.590], loss: 7.706513, mean_absolute_error: 36.374599, mean_q: 46.628197
[F[K  62666/500000: episode: 956, duration: 0.805s, episode steps: 47, steps per second: 58, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.957 [0.000, 4.000], mean observation: 0.490 [0.360, 0.540], loss: 6.723012, mean_absolute_error: 36.143867, mean_q: 46.276867
[F[K  62741/500000: episode: 957, duration: 1.341s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.653 [0.000, 4.000], mean observation: 0.516 [0.470, 0.590], loss: 6.670481, mean_absolute_error: 36.334053, mean_q: 46.609146
[F[K  62830/500000: episode: 958, duration: 1.421s, episode steps: 89, steps per second: 63, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.135 [0.000, 4.000], mean observation: 0.493 [0.380, 0.550], loss: 7.867419, mean_absolute_error: 36.581051, mean_q: 46.921295
[F[K  62869/500000: episode: 959, duration: 0.639s, episode steps: 39, steps per second: 61, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.462 [0.000, 4.000], mean observation: 0.501 [0.370, 0.650], loss: 7.579072, mean_absolute_error: 36.989311, mean_q: 47.660603
[F[K  62931/500000: episode: 960, duration: 1.079s, episode steps: 62, steps per second: 57, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.242 [0.000, 4.000], mean observation: 0.511 [0.490, 0.590], loss: 8.005846, mean_absolute_error: 35.949245, mean_q: 46.112427
[F[K  62999/500000: episode: 961, duration: 1.160s, episode steps: 68, steps per second: 59, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 0.521 [0.470, 0.610], loss: 7.994473, mean_absolute_error: 35.857624, mean_q: 45.945179
[F[K  63173/500000: episode: 962, duration: 2.636s, episode steps: 174, steps per second: 66, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.052 [0.000, 4.000], mean observation: 0.487 [0.380, 0.540], loss: 7.725415, mean_absolute_error: 36.483410, mean_q: 46.836792
[F[K  63217/500000: episode: 963, duration: 0.742s, episode steps: 44, steps per second: 59, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.114 [0.000, 4.000], mean observation: 0.498 [0.360, 0.590], loss: 7.575351, mean_absolute_error: 36.519463, mean_q: 46.775681
[F[K  63292/500000: episode: 964, duration: 1.326s, episode steps: 75, steps per second: 57, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.120 [0.000, 4.000], mean observation: 0.483 [0.370, 0.530], loss: 7.223053, mean_absolute_error: 36.702637, mean_q: 47.041092
[F[K  63436/500000: episode: 965, duration: 2.688s, episode steps: 144, steps per second: 54, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.181 [0.000, 4.000], mean observation: 0.499 [0.430, 0.580], loss: 7.779141, mean_absolute_error: 37.023300, mean_q: 47.484077
[F[K  63498/500000: episode: 966, duration: 1.288s, episode steps: 62, steps per second: 48, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.048 [0.000, 4.000], mean observation: 0.517 [0.470, 0.590], loss: 8.363169, mean_absolute_error: 36.606674, mean_q: 46.974720
[F[K  63535/500000: episode: 967, duration: 0.672s, episode steps: 37, steps per second: 55, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.757 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 6.846853, mean_absolute_error: 37.353493, mean_q: 47.866047
[F[K  63631/500000: episode: 968, duration: 1.746s, episode steps: 96, steps per second: 55, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.505 [0.420, 0.650], loss: 7.272679, mean_absolute_error: 37.408436, mean_q: 47.909885
[F[K  63667/500000: episode: 969, duration: 0.774s, episode steps: 36, steps per second: 46, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.639 [0.000, 4.000], mean observation: 0.520 [0.470, 0.640], loss: 7.906667, mean_absolute_error: 36.411961, mean_q: 46.727966
[F[K  63736/500000: episode: 970, duration: 1.203s, episode steps: 69, steps per second: 57, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.826 [0.000, 4.000], mean observation: 0.482 [0.390, 0.530], loss: 8.341633, mean_absolute_error: 36.903557, mean_q: 47.187813
[F[K  63794/500000: episode: 971, duration: 0.965s, episode steps: 58, steps per second: 60, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.138 [0.000, 4.000], mean observation: 0.515 [0.500, 0.580], loss: 8.560104, mean_absolute_error: 36.451107, mean_q: 46.757294
[F[K  63852/500000: episode: 972, duration: 1.235s, episode steps: 58, steps per second: 47, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.483 [0.000, 4.000], mean observation: 0.492 [0.380, 0.530], loss: 7.589161, mean_absolute_error: 36.743668, mean_q: 47.052132
[F[K  63937/500000: episode: 973, duration: 1.576s, episode steps: 85, steps per second: 54, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.953 [0.000, 4.000], mean observation: 0.501 [0.460, 0.550], loss: 7.759595, mean_absolute_error: 36.961418, mean_q: 47.364960
[F[K  63993/500000: episode: 974, duration: 1.118s, episode steps: 56, steps per second: 50, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.643 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 8.026806, mean_absolute_error: 37.153706, mean_q: 47.605389
[F[K  64064/500000: episode: 975, duration: 1.219s, episode steps: 71, steps per second: 58, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.873 [0.000, 4.000], mean observation: 0.501 [0.480, 0.530], loss: 7.468478, mean_absolute_error: 36.755772, mean_q: 46.932629
[F[K  64108/500000: episode: 976, duration: 0.804s, episode steps: 44, steps per second: 55, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.523 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 9.067187, mean_absolute_error: 36.738163, mean_q: 47.039516
[F[K  64174/500000: episode: 977, duration: 1.272s, episode steps: 66, steps per second: 52, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.121 [0.000, 4.000], mean observation: 0.518 [0.490, 0.610], loss: 8.602469, mean_absolute_error: 36.642910, mean_q: 46.874413
[F[K  64262/500000: episode: 978, duration: 1.577s, episode steps: 88, steps per second: 56, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.852 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 7.043571, mean_absolute_error: 36.850368, mean_q: 47.128464
[F[K  64332/500000: episode: 979, duration: 1.286s, episode steps: 70, steps per second: 54, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.643 [0.000, 4.000], mean observation: 0.494 [0.380, 0.530], loss: 8.036534, mean_absolute_error: 36.124451, mean_q: 46.205341
[F[K  64407/500000: episode: 980, duration: 1.351s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.493 [0.350, 0.550], loss: 8.447545, mean_absolute_error: 37.549088, mean_q: 48.022781
[F[K  64475/500000: episode: 981, duration: 1.345s, episode steps: 68, steps per second: 51, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.721 [0.000, 4.000], mean observation: 0.490 [0.440, 0.530], loss: 8.284348, mean_absolute_error: 37.054153, mean_q: 47.485458
[F[K  64549/500000: episode: 982, duration: 1.245s, episode steps: 74, steps per second: 59, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.517 [0.480, 0.610], loss: 8.167820, mean_absolute_error: 37.066803, mean_q: 47.361290
[F[K  64749/500000: episode: 983, duration: 3.106s, episode steps: 200, steps per second: 64, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.290 [0.000, 4.000], mean observation: 0.500 [0.420, 0.570], loss: 8.217923, mean_absolute_error: 37.062260, mean_q: 47.359932
[F[K  64786/500000: episode: 984, duration: 0.672s, episode steps: 37, steps per second: 55, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.541 [0.000, 3.000], mean observation: 0.514 [0.470, 0.660], loss: 6.805785, mean_absolute_error: 36.605610, mean_q: 46.875038
[F[K  64852/500000: episode: 985, duration: 1.210s, episode steps: 66, steps per second: 55, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.516 [0.470, 0.630], loss: 9.201460, mean_absolute_error: 37.540340, mean_q: 47.924030
[F[K  65003/500000: episode: 986, duration: 2.733s, episode steps: 151, steps per second: 55, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.046 [0.000, 4.000], mean observation: 0.486 [0.340, 0.530], loss: 7.806988, mean_absolute_error: 36.686745, mean_q: 46.984619
[F[K  65078/500000: episode: 987, duration: 1.332s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.067 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 8.601482, mean_absolute_error: 36.995235, mean_q: 47.345497
[F[K  65179/500000: episode: 988, duration: 1.649s, episode steps: 101, steps per second: 61, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.505 [0.000, 4.000], mean observation: 0.480 [0.360, 0.530], loss: 7.739793, mean_absolute_error: 37.128311, mean_q: 47.559841
[F[K  65213/500000: episode: 989, duration: 0.516s, episode steps: 34, steps per second: 66, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.765 [0.000, 3.000], mean observation: 0.513 [0.470, 0.640], loss: 7.876762, mean_absolute_error: 35.774574, mean_q: 45.811703
[F[K  65394/500000: episode: 990, duration: 2.612s, episode steps: 181, steps per second: 69, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.193 [0.000, 4.000], mean observation: 0.500 [0.410, 0.600], loss: 8.463318, mean_absolute_error: 36.779846, mean_q: 47.016251
[F[K  65441/500000: episode: 991, duration: 0.861s, episode steps: 47, steps per second: 55, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.957 [0.000, 4.000], mean observation: 0.477 [0.350, 0.530], loss: 7.762117, mean_absolute_error: 36.057613, mean_q: 46.174576
[F[K  65543/500000: episode: 992, duration: 1.495s, episode steps: 102, steps per second: 68, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.451 [0.000, 4.000], mean observation: 0.484 [0.380, 0.530], loss: 7.807794, mean_absolute_error: 36.413326, mean_q: 46.587208
[F[K  65617/500000: episode: 993, duration: 1.163s, episode steps: 74, steps per second: 64, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.784 [0.000, 4.000], mean observation: 0.500 [0.410, 0.580], loss: 7.782185, mean_absolute_error: 37.136658, mean_q: 47.549938
[F[K  65688/500000: episode: 994, duration: 1.163s, episode steps: 71, steps per second: 61, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.310 [0.000, 4.000], mean observation: 0.508 [0.470, 0.580], loss: 8.272297, mean_absolute_error: 37.207211, mean_q: 47.595051
[F[K  65756/500000: episode: 995, duration: 1.208s, episode steps: 68, steps per second: 56, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.338 [0.000, 4.000], mean observation: 0.503 [0.470, 0.540], loss: 7.792277, mean_absolute_error: 37.099277, mean_q: 47.409401
[F[K  65804/500000: episode: 996, duration: 0.974s, episode steps: 48, steps per second: 49, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.506 [0.500, 0.530], loss: 8.036181, mean_absolute_error: 36.729649, mean_q: 47.176285
[F[K  65839/500000: episode: 997, duration: 0.652s, episode steps: 35, steps per second: 54, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.510 [0.460, 0.650], loss: 8.346547, mean_absolute_error: 36.957092, mean_q: 47.460918
[F[K  65896/500000: episode: 998, duration: 1.050s, episode steps: 57, steps per second: 54, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.702 [0.000, 4.000], mean observation: 0.505 [0.370, 0.650], loss: 8.234649, mean_absolute_error: 37.438938, mean_q: 47.867455
[F[K  65947/500000: episode: 999, duration: 0.854s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.980 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 8.055692, mean_absolute_error: 37.589912, mean_q: 48.123043
[F[K  66009/500000: episode: 1000, duration: 0.973s, episode steps: 62, steps per second: 64, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.242 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 6.947259, mean_absolute_error: 37.480747, mean_q: 47.884537
[F[K  66127/500000: episode: 1001, duration: 1.401s, episode steps: 118, steps per second: 84, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.195 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 8.040653, mean_absolute_error: 36.905624, mean_q: 47.252815
[F[K  66213/500000: episode: 1002, duration: 1.119s, episode steps: 86, steps per second: 77, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.942 [0.000, 4.000], mean observation: 0.505 [0.470, 0.580], loss: 8.109224, mean_absolute_error: 37.665203, mean_q: 48.272648
[F[K  66304/500000: episode: 1003, duration: 1.157s, episode steps: 91, steps per second: 79, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.330 [0.000, 4.000], mean observation: 0.484 [0.390, 0.530], loss: 8.103089, mean_absolute_error: 37.012619, mean_q: 47.365971
[F[K  66355/500000: episode: 1004, duration: 0.762s, episode steps: 51, steps per second: 67, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.511 [0.470, 0.630], loss: 7.413884, mean_absolute_error: 37.084381, mean_q: 47.473610
[F[K  66451/500000: episode: 1005, duration: 1.413s, episode steps: 96, steps per second: 68, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.094 [0.000, 4.000], mean observation: 0.491 [0.420, 0.530], loss: 7.805643, mean_absolute_error: 37.459721, mean_q: 47.974316
[F[K  66485/500000: episode: 1006, duration: 0.498s, episode steps: 34, steps per second: 68, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.676 [0.000, 3.000], mean observation: 0.515 [0.470, 0.650], loss: 7.974094, mean_absolute_error: 37.469700, mean_q: 47.962334
[F[K  66528/500000: episode: 1007, duration: 0.651s, episode steps: 43, steps per second: 66, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.791 [0.000, 4.000], mean observation: 0.509 [0.460, 0.630], loss: 9.076440, mean_absolute_error: 37.602318, mean_q: 48.145813
[F[K  66577/500000: episode: 1008, duration: 0.848s, episode steps: 49, steps per second: 58, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.347 [0.000, 4.000], mean observation: 0.500 [0.370, 0.640], loss: 8.030179, mean_absolute_error: 37.949238, mean_q: 48.551807
[F[K  66631/500000: episode: 1009, duration: 0.860s, episode steps: 54, steps per second: 63, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.504 [0.410, 0.600], loss: 7.739352, mean_absolute_error: 37.061001, mean_q: 47.441093
[F[K  66682/500000: episode: 1010, duration: 0.899s, episode steps: 51, steps per second: 57, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.843 [0.000, 4.000], mean observation: 0.507 [0.490, 0.530], loss: 8.439180, mean_absolute_error: 37.177013, mean_q: 47.620525
[F[K  66747/500000: episode: 1011, duration: 1.136s, episode steps: 65, steps per second: 57, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.503 [0.450, 0.570], loss: 7.041429, mean_absolute_error: 37.008137, mean_q: 47.446682
[F[K  66845/500000: episode: 1012, duration: 1.798s, episode steps: 98, steps per second: 55, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.255 [0.000, 4.000], mean observation: 0.477 [0.360, 0.540], loss: 7.760535, mean_absolute_error: 37.079327, mean_q: 47.517593
[F[K  66902/500000: episode: 1013, duration: 0.874s, episode steps: 57, steps per second: 65, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.053 [0.000, 4.000], mean observation: 0.503 [0.440, 0.640], loss: 8.026352, mean_absolute_error: 37.238720, mean_q: 47.621826
[F[K  66978/500000: episode: 1014, duration: 1.475s, episode steps: 76, steps per second: 52, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.566 [0.000, 4.000], mean observation: 0.508 [0.470, 0.600], loss: 8.297976, mean_absolute_error: 36.908501, mean_q: 47.322937
[F[K  67028/500000: episode: 1015, duration: 0.877s, episode steps: 50, steps per second: 57, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.460 [0.000, 4.000], mean observation: 0.508 [0.470, 0.550], loss: 8.436048, mean_absolute_error: 37.535824, mean_q: 48.182724
[F[K  67086/500000: episode: 1016, duration: 1.000s, episode steps: 58, steps per second: 58, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.519 [0.500, 0.620], loss: 8.708068, mean_absolute_error: 37.447746, mean_q: 47.838295
[F[K  67134/500000: episode: 1017, duration: 0.892s, episode steps: 48, steps per second: 54, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.604 [0.000, 4.000], mean observation: 0.495 [0.360, 0.600], loss: 8.016509, mean_absolute_error: 37.032681, mean_q: 47.411346
[F[K  67218/500000: episode: 1018, duration: 1.448s, episode steps: 84, steps per second: 58, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.893 [0.000, 4.000], mean observation: 0.515 [0.480, 0.570], loss: 7.878855, mean_absolute_error: 37.523594, mean_q: 47.969208
[F[K  67329/500000: episode: 1019, duration: 1.597s, episode steps: 111, steps per second: 70, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.775 [0.000, 4.000], mean observation: 0.492 [0.380, 0.540], loss: 8.219757, mean_absolute_error: 37.854298, mean_q: 48.456554
[F[K  67391/500000: episode: 1020, duration: 0.903s, episode steps: 62, steps per second: 69, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.758 [0.000, 4.000], mean observation: 0.521 [0.480, 0.630], loss: 8.740952, mean_absolute_error: 36.990696, mean_q: 47.385860
[F[K  67457/500000: episode: 1021, duration: 1.166s, episode steps: 66, steps per second: 57, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.227 [0.000, 4.000], mean observation: 0.480 [0.390, 0.520], loss: 8.703103, mean_absolute_error: 37.279774, mean_q: 47.809841
[F[K  67527/500000: episode: 1022, duration: 1.300s, episode steps: 70, steps per second: 54, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.483 [0.400, 0.530], loss: 7.075491, mean_absolute_error: 37.711254, mean_q: 48.267982
[F[K  67562/500000: episode: 1023, duration: 0.620s, episode steps: 35, steps per second: 56, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.486 [0.000, 4.000], mean observation: 0.525 [0.470, 0.630], loss: 8.013271, mean_absolute_error: 38.363750, mean_q: 49.129822
[F[K  67635/500000: episode: 1024, duration: 1.201s, episode steps: 73, steps per second: 61, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.753 [0.000, 4.000], mean observation: 0.512 [0.500, 0.570], loss: 7.380051, mean_absolute_error: 37.413643, mean_q: 47.927071
[F[K  67704/500000: episode: 1025, duration: 1.172s, episode steps: 69, steps per second: 59, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.391 [0.000, 4.000], mean observation: 0.496 [0.400, 0.560], loss: 8.477862, mean_absolute_error: 37.726643, mean_q: 48.209248
[F[K  67773/500000: episode: 1026, duration: 1.008s, episode steps: 69, steps per second: 68, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.739 [0.000, 4.000], mean observation: 0.496 [0.370, 0.590], loss: 8.001731, mean_absolute_error: 37.107712, mean_q: 47.559444
[F[K  67935/500000: episode: 1027, duration: 2.722s, episode steps: 162, steps per second: 60, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.031 [0.000, 4.000], mean observation: 0.489 [0.360, 0.530], loss: 8.763946, mean_absolute_error: 37.945927, mean_q: 48.612404
[F[K  68033/500000: episode: 1028, duration: 1.719s, episode steps: 98, steps per second: 57, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.487 [0.430, 0.530], loss: 8.482140, mean_absolute_error: 37.049248, mean_q: 47.519222
[F[K  68211/500000: episode: 1029, duration: 3.177s, episode steps: 178, steps per second: 56, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.202 [0.000, 4.000], mean observation: 0.502 [0.380, 0.640], loss: 8.007108, mean_absolute_error: 38.042389, mean_q: 48.703827
[F[K  68290/500000: episode: 1030, duration: 1.157s, episode steps: 79, steps per second: 68, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.304 [0.000, 4.000], mean observation: 0.497 [0.390, 0.590], loss: 8.552994, mean_absolute_error: 37.683159, mean_q: 48.167740
[F[K  68379/500000: episode: 1031, duration: 1.513s, episode steps: 89, steps per second: 59, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.629 [0.000, 4.000], mean observation: 0.482 [0.380, 0.530], loss: 8.763359, mean_absolute_error: 37.290836, mean_q: 47.594116
[F[K  68452/500000: episode: 1032, duration: 1.200s, episode steps: 73, steps per second: 61, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.959 [0.000, 4.000], mean observation: 0.484 [0.360, 0.520], loss: 7.625041, mean_absolute_error: 37.758442, mean_q: 48.263138
[F[K  68572/500000: episode: 1033, duration: 2.010s, episode steps: 120, steps per second: 60, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.450 [0.000, 4.000], mean observation: 0.495 [0.420, 0.540], loss: 7.449238, mean_absolute_error: 37.758732, mean_q: 48.348030
[F[K  68710/500000: episode: 1034, duration: 2.390s, episode steps: 138, steps per second: 58, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.964 [0.000, 4.000], mean observation: 0.507 [0.480, 0.550], loss: 7.824064, mean_absolute_error: 37.622593, mean_q: 48.140068
[F[K  68778/500000: episode: 1035, duration: 1.182s, episode steps: 68, steps per second: 58, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.147 [0.000, 4.000], mean observation: 0.498 [0.420, 0.580], loss: 8.395879, mean_absolute_error: 37.069393, mean_q: 47.420414
[F[K  68836/500000: episode: 1036, duration: 1.071s, episode steps: 58, steps per second: 54, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.328 [0.000, 4.000], mean observation: 0.523 [0.470, 0.650], loss: 8.509295, mean_absolute_error: 37.853062, mean_q: 48.340202
[F[K  68885/500000: episode: 1037, duration: 0.764s, episode steps: 49, steps per second: 64, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 7.806828, mean_absolute_error: 36.914185, mean_q: 47.291233
[F[K  68978/500000: episode: 1038, duration: 1.624s, episode steps: 93, steps per second: 57, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.194 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 8.524289, mean_absolute_error: 38.243046, mean_q: 48.934219
[F[K  69083/500000: episode: 1039, duration: 1.507s, episode steps: 105, steps per second: 70, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.543 [0.000, 4.000], mean observation: 0.504 [0.450, 0.580], loss: 6.896013, mean_absolute_error: 37.788303, mean_q: 48.347599
[F[K  69148/500000: episode: 1040, duration: 0.938s, episode steps: 65, steps per second: 69, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.123 [0.000, 4.000], mean observation: 0.484 [0.420, 0.510], loss: 8.096920, mean_absolute_error: 37.855911, mean_q: 48.485802
[F[K  69213/500000: episode: 1041, duration: 1.132s, episode steps: 65, steps per second: 57, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.723 [0.000, 3.000], mean observation: 0.497 [0.370, 0.630], loss: 7.629385, mean_absolute_error: 37.242466, mean_q: 47.717133
[F[K  69265/500000: episode: 1042, duration: 0.996s, episode steps: 52, steps per second: 52, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.288 [0.000, 4.000], mean observation: 0.497 [0.350, 0.630], loss: 7.532983, mean_absolute_error: 37.396152, mean_q: 48.000004
[F[K  69373/500000: episode: 1043, duration: 1.954s, episode steps: 108, steps per second: 55, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.741 [0.000, 4.000], mean observation: 0.486 [0.410, 0.540], loss: 7.768952, mean_absolute_error: 37.567165, mean_q: 48.187759
[F[K  69463/500000: episode: 1044, duration: 1.547s, episode steps: 90, steps per second: 58, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.508 [0.470, 0.560], loss: 7.961321, mean_absolute_error: 37.975918, mean_q: 48.738350
[F[K  69533/500000: episode: 1045, duration: 1.235s, episode steps: 70, steps per second: 57, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.271 [0.000, 4.000], mean observation: 0.477 [0.370, 0.530], loss: 6.930812, mean_absolute_error: 37.779434, mean_q: 48.422150
[F[K  69602/500000: episode: 1046, duration: 1.309s, episode steps: 69, steps per second: 53, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.486 [0.390, 0.510], loss: 8.053619, mean_absolute_error: 37.640392, mean_q: 48.211010
[F[K  69693/500000: episode: 1047, duration: 1.394s, episode steps: 91, steps per second: 65, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.473 [0.000, 4.000], mean observation: 0.515 [0.460, 0.640], loss: 8.220077, mean_absolute_error: 37.974541, mean_q: 48.511154
[F[K  69797/500000: episode: 1048, duration: 1.723s, episode steps: 104, steps per second: 60, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.404 [0.000, 4.000], mean observation: 0.486 [0.360, 0.530], loss: 8.805339, mean_absolute_error: 38.041790, mean_q: 48.673164
[F[K  69862/500000: episode: 1049, duration: 1.218s, episode steps: 65, steps per second: 53, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.369 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 8.645482, mean_absolute_error: 37.785961, mean_q: 48.363541
[F[K  69915/500000: episode: 1050, duration: 0.924s, episode steps: 53, steps per second: 57, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.472 [0.000, 4.000], mean observation: 0.506 [0.470, 0.530], loss: 6.631842, mean_absolute_error: 38.189114, mean_q: 48.914497
[F[K  70018/500000: episode: 1051, duration: 1.813s, episode steps: 103, steps per second: 57, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.680 [0.000, 4.000], mean observation: 0.502 [0.420, 0.570], loss: 7.721673, mean_absolute_error: 37.306931, mean_q: 47.672466
[F[K  70087/500000: episode: 1052, duration: 1.202s, episode steps: 69, steps per second: 57, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.516 [0.490, 0.570], loss: 8.183246, mean_absolute_error: 38.087097, mean_q: 48.679634
[F[K  70132/500000: episode: 1053, duration: 0.893s, episode steps: 45, steps per second: 50, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.490 [0.350, 0.550], loss: 6.712640, mean_absolute_error: 37.400078, mean_q: 47.932632
[F[K  70192/500000: episode: 1054, duration: 1.011s, episode steps: 60, steps per second: 59, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.583 [0.000, 4.000], mean observation: 0.510 [0.440, 0.620], loss: 6.782004, mean_absolute_error: 38.577393, mean_q: 49.514706
[F[K  70254/500000: episode: 1055, duration: 1.040s, episode steps: 62, steps per second: 60, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.984 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 7.339128, mean_absolute_error: 37.469578, mean_q: 48.039009
[F[K  70334/500000: episode: 1056, duration: 1.355s, episode steps: 80, steps per second: 59, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.497 [0.360, 0.590], loss: 7.836249, mean_absolute_error: 38.103951, mean_q: 48.723186
[F[K  70393/500000: episode: 1057, duration: 1.110s, episode steps: 59, steps per second: 53, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.458 [0.000, 4.000], mean observation: 0.481 [0.390, 0.530], loss: 8.903154, mean_absolute_error: 38.169025, mean_q: 48.881344
[F[K  70455/500000: episode: 1058, duration: 1.157s, episode steps: 62, steps per second: 54, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.129 [0.000, 4.000], mean observation: 0.492 [0.420, 0.520], loss: 9.589348, mean_absolute_error: 37.806797, mean_q: 48.331337
[F[K  70581/500000: episode: 1059, duration: 2.201s, episode steps: 126, steps per second: 57, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.373 [0.000, 4.000], mean observation: 0.500 [0.410, 0.570], loss: 7.550334, mean_absolute_error: 38.325344, mean_q: 49.148575
[F[K  70671/500000: episode: 1060, duration: 1.495s, episode steps: 90, steps per second: 60, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.144 [0.000, 4.000], mean observation: 0.509 [0.470, 0.580], loss: 8.261645, mean_absolute_error: 37.899635, mean_q: 48.551781
[F[K  70742/500000: episode: 1061, duration: 1.249s, episode steps: 71, steps per second: 57, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.732 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 10.040406, mean_absolute_error: 38.374641, mean_q: 49.076397
[F[K  70810/500000: episode: 1062, duration: 1.161s, episode steps: 68, steps per second: 59, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.956 [0.000, 4.000], mean observation: 0.509 [0.490, 0.540], loss: 8.622457, mean_absolute_error: 37.929295, mean_q: 48.637909
[F[K  70885/500000: episode: 1063, duration: 1.227s, episode steps: 75, steps per second: 61, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 0.498 [0.390, 0.580], loss: 8.054165, mean_absolute_error: 38.799572, mean_q: 49.776707
[F[K  71019/500000: episode: 1064, duration: 2.101s, episode steps: 134, steps per second: 64, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.104 [0.000, 4.000], mean observation: 0.502 [0.420, 0.590], loss: 8.514862, mean_absolute_error: 37.968437, mean_q: 48.590492
[F[K  71081/500000: episode: 1065, duration: 1.068s, episode steps: 62, steps per second: 58, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.032 [0.000, 4.000], mean observation: 0.506 [0.430, 0.590], loss: 8.455853, mean_absolute_error: 37.340237, mean_q: 47.881329
[F[K  71145/500000: episode: 1066, duration: 1.134s, episode steps: 64, steps per second: 56, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.969 [0.000, 4.000], mean observation: 0.496 [0.380, 0.580], loss: 8.065401, mean_absolute_error: 37.784248, mean_q: 48.422924
[F[K  71219/500000: episode: 1067, duration: 1.414s, episode steps: 74, steps per second: 52, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.243 [0.000, 4.000], mean observation: 0.503 [0.430, 0.580], loss: 8.026587, mean_absolute_error: 38.326351, mean_q: 49.116596
[F[K  71281/500000: episode: 1068, duration: 1.140s, episode steps: 62, steps per second: 54, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.839 [0.000, 4.000], mean observation: 0.498 [0.440, 0.540], loss: 7.425272, mean_absolute_error: 37.884617, mean_q: 48.607910
[F[K  71366/500000: episode: 1069, duration: 1.477s, episode steps: 85, steps per second: 58, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.176 [0.000, 4.000], mean observation: 0.494 [0.410, 0.560], loss: 8.267940, mean_absolute_error: 38.204491, mean_q: 48.884129
[F[K  71442/500000: episode: 1070, duration: 1.276s, episode steps: 76, steps per second: 60, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.671 [0.000, 4.000], mean observation: 0.520 [0.490, 0.600], loss: 7.719083, mean_absolute_error: 38.049614, mean_q: 48.748081
[F[K  71501/500000: episode: 1071, duration: 1.065s, episode steps: 59, steps per second: 55, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.102 [0.000, 4.000], mean observation: 0.485 [0.410, 0.520], loss: 7.946907, mean_absolute_error: 38.798077, mean_q: 49.679058
[F[K  71541/500000: episode: 1072, duration: 0.554s, episode steps: 40, steps per second: 72, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.472 [0.350, 0.530], loss: 7.964816, mean_absolute_error: 38.428837, mean_q: 49.317741
[F[K  71610/500000: episode: 1073, duration: 1.456s, episode steps: 69, steps per second: 47, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.725 [0.000, 4.000], mean observation: 0.487 [0.360, 0.550], loss: 7.082137, mean_absolute_error: 38.743900, mean_q: 49.628231
[F[K  71689/500000: episode: 1074, duration: 1.360s, episode steps: 79, steps per second: 58, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.747 [0.000, 4.000], mean observation: 0.503 [0.450, 0.590], loss: 7.017073, mean_absolute_error: 38.587238, mean_q: 49.547127
[F[K  71757/500000: episode: 1075, duration: 1.167s, episode steps: 68, steps per second: 58, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.853 [0.000, 4.000], mean observation: 0.504 [0.430, 0.640], loss: 8.656878, mean_absolute_error: 39.027374, mean_q: 49.928108
[F[K  71939/500000: episode: 1076, duration: 2.976s, episode steps: 182, steps per second: 61, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.491 [0.370, 0.540], loss: 8.915586, mean_absolute_error: 38.395508, mean_q: 49.118057
[F[K  72029/500000: episode: 1077, duration: 1.622s, episode steps: 90, steps per second: 55, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.922 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 8.232569, mean_absolute_error: 38.773659, mean_q: 49.654980
[F[K  72098/500000: episode: 1078, duration: 1.209s, episode steps: 69, steps per second: 57, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.174 [0.000, 4.000], mean observation: 0.479 [0.360, 0.520], loss: 7.467395, mean_absolute_error: 38.486061, mean_q: 49.299168
[F[K  72161/500000: episode: 1079, duration: 1.140s, episode steps: 63, steps per second: 55, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 0.492 [0.420, 0.530], loss: 8.774341, mean_absolute_error: 38.990963, mean_q: 49.695698
[F[K  72235/500000: episode: 1080, duration: 1.269s, episode steps: 74, steps per second: 58, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.297 [0.000, 4.000], mean observation: 0.486 [0.400, 0.520], loss: 6.849914, mean_absolute_error: 38.586956, mean_q: 49.401558
[F[K  72302/500000: episode: 1081, duration: 1.224s, episode steps: 67, steps per second: 55, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.896 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 8.236984, mean_absolute_error: 38.645954, mean_q: 49.458126
[F[K  72379/500000: episode: 1082, duration: 1.328s, episode steps: 77, steps per second: 58, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.727 [0.000, 4.000], mean observation: 0.502 [0.440, 0.610], loss: 8.905463, mean_absolute_error: 39.086979, mean_q: 49.890701
[F[K  72476/500000: episode: 1083, duration: 1.664s, episode steps: 97, steps per second: 58, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.433 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 8.790019, mean_absolute_error: 38.744877, mean_q: 49.524960
[F[K  72612/500000: episode: 1084, duration: 2.231s, episode steps: 136, steps per second: 61, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.492 [0.350, 0.560], loss: 8.324336, mean_absolute_error: 39.144192, mean_q: 49.998772
[F[K  72712/500000: episode: 1085, duration: 1.705s, episode steps: 100, steps per second: 59, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.710 [0.000, 4.000], mean observation: 0.513 [0.490, 0.580], loss: 8.097229, mean_absolute_error: 38.570282, mean_q: 49.302120
[F[K  72768/500000: episode: 1086, duration: 0.938s, episode steps: 56, steps per second: 60, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.520 [0.470, 0.600], loss: 8.331294, mean_absolute_error: 38.683941, mean_q: 49.408100
[F[K  72809/500000: episode: 1087, duration: 0.713s, episode steps: 41, steps per second: 57, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.561 [0.000, 4.000], mean observation: 0.512 [0.470, 0.650], loss: 8.013219, mean_absolute_error: 39.219681, mean_q: 50.257454
[F[K  72916/500000: episode: 1088, duration: 1.817s, episode steps: 107, steps per second: 59, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.150 [0.000, 4.000], mean observation: 0.497 [0.360, 0.600], loss: 9.349026, mean_absolute_error: 38.122421, mean_q: 48.765781
[F[K  72953/500000: episode: 1089, duration: 0.696s, episode steps: 37, steps per second: 53, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.432 [0.000, 3.000], mean observation: 0.499 [0.360, 0.640], loss: 8.507438, mean_absolute_error: 38.045025, mean_q: 48.640644
[F[K  73009/500000: episode: 1090, duration: 1.018s, episode steps: 56, steps per second: 55, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.488 [0.360, 0.530], loss: 8.871112, mean_absolute_error: 38.342541, mean_q: 49.031597
[F[K  73082/500000: episode: 1091, duration: 1.414s, episode steps: 73, steps per second: 52, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.521 [0.000, 4.000], mean observation: 0.511 [0.490, 0.570], loss: 8.273741, mean_absolute_error: 39.159824, mean_q: 50.069424
[F[K  73126/500000: episode: 1092, duration: 0.886s, episode steps: 44, steps per second: 50, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.864 [0.000, 4.000], mean observation: 0.502 [0.360, 0.650], loss: 8.220406, mean_absolute_error: 39.573769, mean_q: 50.745762
[F[K  73209/500000: episode: 1093, duration: 1.366s, episode steps: 83, steps per second: 61, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.157 [0.000, 4.000], mean observation: 0.489 [0.420, 0.520], loss: 8.449357, mean_absolute_error: 38.723816, mean_q: 49.470787
[F[K  73315/500000: episode: 1094, duration: 1.919s, episode steps: 106, steps per second: 55, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.877 [0.000, 4.000], mean observation: 0.494 [0.400, 0.530], loss: 8.934677, mean_absolute_error: 38.601868, mean_q: 49.323795
[F[K  73391/500000: episode: 1095, duration: 1.419s, episode steps: 76, steps per second: 54, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.882 [0.000, 4.000], mean observation: 0.518 [0.480, 0.650], loss: 8.406828, mean_absolute_error: 39.010727, mean_q: 49.936844
[F[K  73444/500000: episode: 1096, duration: 0.803s, episode steps: 53, steps per second: 66, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.189 [0.000, 4.000], mean observation: 0.523 [0.470, 0.650], loss: 8.095146, mean_absolute_error: 38.850971, mean_q: 49.658833
[F[K  73624/500000: episode: 1097, duration: 3.016s, episode steps: 180, steps per second: 60, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.956 [0.000, 4.000], mean observation: 0.513 [0.490, 0.620], loss: 8.962168, mean_absolute_error: 38.584919, mean_q: 49.380024
[F[K  73686/500000: episode: 1098, duration: 1.232s, episode steps: 62, steps per second: 50, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.968 [0.000, 4.000], mean observation: 0.518 [0.470, 0.640], loss: 9.400935, mean_absolute_error: 38.615356, mean_q: 49.436764
[F[K  73855/500000: episode: 1099, duration: 2.811s, episode steps: 169, steps per second: 60, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.504 [0.470, 0.560], loss: 8.243433, mean_absolute_error: 39.047909, mean_q: 49.937187
[F[K  73931/500000: episode: 1100, duration: 1.282s, episode steps: 76, steps per second: 59, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.487 [0.000, 4.000], mean observation: 0.478 [0.350, 0.540], loss: 7.633979, mean_absolute_error: 39.161205, mean_q: 50.074738
[F[K  73980/500000: episode: 1101, duration: 0.887s, episode steps: 49, steps per second: 55, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.518 [0.470, 0.610], loss: 7.102519, mean_absolute_error: 39.318939, mean_q: 50.329155
[F[K  74078/500000: episode: 1102, duration: 1.778s, episode steps: 98, steps per second: 55, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.112 [0.000, 4.000], mean observation: 0.489 [0.370, 0.550], loss: 8.415304, mean_absolute_error: 38.663658, mean_q: 49.405106
[F[K  74172/500000: episode: 1103, duration: 1.544s, episode steps: 94, steps per second: 61, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.830 [0.000, 4.000], mean observation: 0.484 [0.350, 0.530], loss: 7.406907, mean_absolute_error: 39.631508, mean_q: 50.744495
[F[K  74211/500000: episode: 1104, duration: 0.715s, episode steps: 39, steps per second: 55, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.436 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 8.713573, mean_absolute_error: 38.379124, mean_q: 49.199821
[F[K  74275/500000: episode: 1105, duration: 1.159s, episode steps: 64, steps per second: 55, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.844 [0.000, 4.000], mean observation: 0.497 [0.460, 0.530], loss: 7.593164, mean_absolute_error: 38.907478, mean_q: 49.836899
[F[K  74342/500000: episode: 1106, duration: 1.093s, episode steps: 67, steps per second: 61, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.496 [0.430, 0.530], loss: 8.282420, mean_absolute_error: 39.507294, mean_q: 50.481987
[F[K  74417/500000: episode: 1107, duration: 1.196s, episode steps: 75, steps per second: 63, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.067 [0.000, 4.000], mean observation: 0.497 [0.440, 0.520], loss: 8.811440, mean_absolute_error: 38.423496, mean_q: 49.188644
[F[K  74479/500000: episode: 1108, duration: 1.130s, episode steps: 62, steps per second: 55, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.500 [0.410, 0.570], loss: 8.541931, mean_absolute_error: 39.303185, mean_q: 50.312191
[F[K  74556/500000: episode: 1109, duration: 1.430s, episode steps: 77, steps per second: 54, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.478 [0.340, 0.520], loss: 7.985294, mean_absolute_error: 39.222633, mean_q: 50.164261
[F[K  74689/500000: episode: 1110, duration: 2.357s, episode steps: 133, steps per second: 56, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.053 [0.000, 4.000], mean observation: 0.495 [0.420, 0.540], loss: 8.451158, mean_absolute_error: 38.650490, mean_q: 49.448730
[F[K  74761/500000: episode: 1111, duration: 1.451s, episode steps: 72, steps per second: 50, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.847 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 9.255208, mean_absolute_error: 39.453213, mean_q: 50.568035
[F[K  74818/500000: episode: 1112, duration: 1.227s, episode steps: 57, steps per second: 46, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.825 [0.000, 4.000], mean observation: 0.502 [0.410, 0.620], loss: 9.108809, mean_absolute_error: 38.915813, mean_q: 49.782444
[F[K  74882/500000: episode: 1113, duration: 1.117s, episode steps: 64, steps per second: 57, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.094 [0.000, 4.000], mean observation: 0.491 [0.420, 0.520], loss: 7.442029, mean_absolute_error: 39.164291, mean_q: 50.160816
[F[K  74951/500000: episode: 1114, duration: 1.418s, episode steps: 69, steps per second: 49, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.101 [0.000, 4.000], mean observation: 0.492 [0.380, 0.580], loss: 9.158974, mean_absolute_error: 38.814285, mean_q: 49.698242
[F[K  75039/500000: episode: 1115, duration: 1.400s, episode steps: 88, steps per second: 63, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.443 [0.000, 4.000], mean observation: 0.507 [0.470, 0.570], loss: 8.858876, mean_absolute_error: 38.480667, mean_q: 49.232742
[F[K  75117/500000: episode: 1116, duration: 1.501s, episode steps: 78, steps per second: 52, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.090 [0.000, 4.000], mean observation: 0.506 [0.430, 0.620], loss: 9.735548, mean_absolute_error: 39.242268, mean_q: 50.180931
[F[K  75197/500000: episode: 1117, duration: 1.250s, episode steps: 80, steps per second: 64, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.337 [0.000, 4.000], mean observation: 0.479 [0.370, 0.520], loss: 8.244248, mean_absolute_error: 38.507725, mean_q: 49.286850
[F[K  75244/500000: episode: 1118, duration: 0.730s, episode steps: 47, steps per second: 64, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.298 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 9.048993, mean_absolute_error: 38.777184, mean_q: 49.479290
[F[K  75338/500000: episode: 1119, duration: 1.722s, episode steps: 94, steps per second: 55, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 7.594390, mean_absolute_error: 38.684013, mean_q: 49.609722
[F[K  75405/500000: episode: 1120, duration: 1.279s, episode steps: 67, steps per second: 52, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.119 [0.000, 4.000], mean observation: 0.494 [0.360, 0.610], loss: 7.489552, mean_absolute_error: 39.451992, mean_q: 50.473644
[F[K  75463/500000: episode: 1121, duration: 1.388s, episode steps: 58, steps per second: 42, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.519 [0.490, 0.620], loss: 8.105545, mean_absolute_error: 38.056015, mean_q: 48.684082
[F[K  75619/500000: episode: 1122, duration: 2.700s, episode steps: 156, steps per second: 58, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.923 [0.000, 4.000], mean observation: 0.510 [0.440, 0.680], loss: 8.788227, mean_absolute_error: 39.130589, mean_q: 50.036098
[F[K  75688/500000: episode: 1123, duration: 1.112s, episode steps: 69, steps per second: 62, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.510 [0.470, 0.570], loss: 8.000974, mean_absolute_error: 39.254627, mean_q: 50.251808
[F[K  75757/500000: episode: 1124, duration: 1.255s, episode steps: 69, steps per second: 55, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.500 [0.370, 0.620], loss: 7.159629, mean_absolute_error: 38.754845, mean_q: 49.692623
[F[K  75840/500000: episode: 1125, duration: 1.139s, episode steps: 83, steps per second: 73, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.783 [0.000, 4.000], mean observation: 0.516 [0.490, 0.590], loss: 7.872100, mean_absolute_error: 39.352699, mean_q: 50.442005
[F[K  75962/500000: episode: 1126, duration: 1.902s, episode steps: 122, steps per second: 64, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.148 [0.000, 4.000], mean observation: 0.503 [0.380, 0.670], loss: 8.082274, mean_absolute_error: 38.906178, mean_q: 49.770458
[F[K  76002/500000: episode: 1127, duration: 0.634s, episode steps: 40, steps per second: 63, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.521 [0.470, 0.650], loss: 7.857207, mean_absolute_error: 39.632172, mean_q: 50.768604
[F[K  76137/500000: episode: 1128, duration: 1.939s, episode steps: 135, steps per second: 70, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.492 [0.350, 0.580], loss: 8.955974, mean_absolute_error: 38.680622, mean_q: 49.451958
[F[K  76226/500000: episode: 1129, duration: 1.348s, episode steps: 89, steps per second: 66, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.112 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 8.246691, mean_absolute_error: 39.576984, mean_q: 50.629662
[F[K  76279/500000: episode: 1130, duration: 0.747s, episode steps: 53, steps per second: 71, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.489 [0.380, 0.530], loss: 7.044270, mean_absolute_error: 39.398651, mean_q: 50.558620
[F[K  76336/500000: episode: 1131, duration: 0.772s, episode steps: 57, steps per second: 74, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.982 [0.000, 4.000], mean observation: 0.481 [0.350, 0.520], loss: 7.075105, mean_absolute_error: 39.197140, mean_q: 50.283401
[F[K  76411/500000: episode: 1132, duration: 1.087s, episode steps: 75, steps per second: 69, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.760 [0.000, 4.000], mean observation: 0.511 [0.470, 0.620], loss: 8.370778, mean_absolute_error: 38.558361, mean_q: 49.299877
[F[K  76484/500000: episode: 1133, duration: 1.006s, episode steps: 73, steps per second: 73, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.603 [0.000, 4.000], mean observation: 0.491 [0.410, 0.530], loss: 9.257611, mean_absolute_error: 39.234699, mean_q: 50.107281
[F[K  76539/500000: episode: 1134, duration: 0.896s, episode steps: 55, steps per second: 61, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.018 [0.000, 4.000], mean observation: 0.514 [0.480, 0.600], loss: 9.385301, mean_absolute_error: 39.216274, mean_q: 50.005039
[F[K  76619/500000: episode: 1135, duration: 1.260s, episode steps: 80, steps per second: 63, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.481 [0.390, 0.530], loss: 9.557469, mean_absolute_error: 38.583035, mean_q: 49.487602
[F[K  76733/500000: episode: 1136, duration: 1.515s, episode steps: 114, steps per second: 75, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.974 [0.000, 4.000], mean observation: 0.495 [0.410, 0.550], loss: 8.106645, mean_absolute_error: 39.140919, mean_q: 50.190331
[F[K  76809/500000: episode: 1137, duration: 1.050s, episode steps: 76, steps per second: 72, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.697 [0.000, 4.000], mean observation: 0.502 [0.470, 0.530], loss: 8.713711, mean_absolute_error: 39.223358, mean_q: 50.209667
[F[K  76898/500000: episode: 1138, duration: 1.301s, episode steps: 89, steps per second: 68, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.202 [0.000, 4.000], mean observation: 0.502 [0.420, 0.610], loss: 8.259740, mean_absolute_error: 39.497021, mean_q: 50.555351
[F[K  76967/500000: episode: 1139, duration: 1.132s, episode steps: 69, steps per second: 61, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.483 [0.390, 0.520], loss: 8.702149, mean_absolute_error: 39.536098, mean_q: 50.581486
[F[K  77080/500000: episode: 1140, duration: 1.690s, episode steps: 113, steps per second: 67, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.142 [0.000, 4.000], mean observation: 0.499 [0.450, 0.540], loss: 8.108480, mean_absolute_error: 39.699699, mean_q: 50.783993
[F[K  77156/500000: episode: 1141, duration: 1.458s, episode steps: 76, steps per second: 52, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.868 [0.000, 4.000], mean observation: 0.511 [0.490, 0.550], loss: 9.472160, mean_absolute_error: 39.213943, mean_q: 50.174469
[F[K  77195/500000: episode: 1142, duration: 0.722s, episode steps: 39, steps per second: 54, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.523 [0.470, 0.650], loss: 6.791849, mean_absolute_error: 39.940643, mean_q: 51.060898
[F[K  77255/500000: episode: 1143, duration: 0.939s, episode steps: 60, steps per second: 64, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.417 [0.000, 4.000], mean observation: 0.488 [0.380, 0.530], loss: 8.537908, mean_absolute_error: 39.579308, mean_q: 50.460129
[F[K  77328/500000: episode: 1144, duration: 1.134s, episode steps: 73, steps per second: 64, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.795 [0.000, 4.000], mean observation: 0.510 [0.470, 0.570], loss: 8.197021, mean_absolute_error: 38.766670, mean_q: 49.739948
[F[K  77438/500000: episode: 1145, duration: 1.578s, episode steps: 110, steps per second: 70, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.516 [0.470, 0.600], loss: 8.174692, mean_absolute_error: 39.850147, mean_q: 51.119911
[F[K  77474/500000: episode: 1146, duration: 0.583s, episode steps: 36, steps per second: 62, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.778 [0.000, 4.000], mean observation: 0.519 [0.470, 0.650], loss: 8.162386, mean_absolute_error: 39.658310, mean_q: 50.749084
[F[K  77536/500000: episode: 1147, duration: 0.992s, episode steps: 62, steps per second: 63, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.145 [0.000, 4.000], mean observation: 0.524 [0.470, 0.640], loss: 8.088748, mean_absolute_error: 39.895439, mean_q: 51.040333
[F[K  77660/500000: episode: 1148, duration: 1.771s, episode steps: 124, steps per second: 70, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.089 [0.000, 4.000], mean observation: 0.507 [0.460, 0.590], loss: 7.850625, mean_absolute_error: 39.555016, mean_q: 50.586815
[F[K  77808/500000: episode: 1149, duration: 1.924s, episode steps: 148, steps per second: 77, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.020 [0.000, 4.000], mean observation: 0.517 [0.480, 0.680], loss: 8.297929, mean_absolute_error: 40.055237, mean_q: 51.249985
[F[K  77847/500000: episode: 1150, duration: 0.627s, episode steps: 39, steps per second: 62, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.718 [0.000, 4.000], mean observation: 0.504 [0.410, 0.640], loss: 9.044702, mean_absolute_error: 39.732578, mean_q: 50.832489
[F[K  77913/500000: episode: 1151, duration: 1.032s, episode steps: 66, steps per second: 64, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.697 [0.000, 4.000], mean observation: 0.485 [0.380, 0.530], loss: 8.511533, mean_absolute_error: 39.626778, mean_q: 50.720459
[F[K  78003/500000: episode: 1152, duration: 1.387s, episode steps: 90, steps per second: 65, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.189 [0.000, 4.000], mean observation: 0.488 [0.440, 0.530], loss: 8.677912, mean_absolute_error: 39.140724, mean_q: 50.044880
[F[K  78099/500000: episode: 1153, duration: 1.381s, episode steps: 96, steps per second: 70, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.979 [0.000, 4.000], mean observation: 0.486 [0.420, 0.540], loss: 9.193563, mean_absolute_error: 39.071377, mean_q: 49.807831
[F[K  78162/500000: episode: 1154, duration: 0.956s, episode steps: 63, steps per second: 66, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.175 [0.000, 4.000], mean observation: 0.481 [0.380, 0.520], loss: 7.599359, mean_absolute_error: 39.808414, mean_q: 51.037537
[F[K  78230/500000: episode: 1155, duration: 1.092s, episode steps: 68, steps per second: 62, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.517 [0.490, 0.580], loss: 7.988581, mean_absolute_error: 39.599297, mean_q: 50.657150
[F[K  78396/500000: episode: 1156, duration: 2.226s, episode steps: 166, steps per second: 75, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.072 [0.000, 4.000], mean observation: 0.471 [0.330, 0.530], loss: 8.327221, mean_absolute_error: 39.547985, mean_q: 50.697582
[F[K  78480/500000: episode: 1157, duration: 1.480s, episode steps: 84, steps per second: 57, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.881 [0.000, 4.000], mean observation: 0.498 [0.410, 0.550], loss: 9.060392, mean_absolute_error: 39.593086, mean_q: 50.591633
[F[K  78514/500000: episode: 1158, duration: 0.556s, episode steps: 34, steps per second: 61, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.647 [0.000, 3.000], mean observation: 0.509 [0.450, 0.650], loss: 8.022200, mean_absolute_error: 39.359806, mean_q: 50.412750
[F[K  78622/500000: episode: 1159, duration: 1.721s, episode steps: 108, steps per second: 63, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.185 [0.000, 4.000], mean observation: 0.513 [0.470, 0.600], loss: 8.712982, mean_absolute_error: 39.050358, mean_q: 49.974789
[F[K  78683/500000: episode: 1160, duration: 0.901s, episode steps: 61, steps per second: 68, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.672 [0.000, 4.000], mean observation: 0.479 [0.350, 0.520], loss: 8.421292, mean_absolute_error: 40.032700, mean_q: 51.152065
[F[K  78773/500000: episode: 1161, duration: 1.348s, episode steps: 90, steps per second: 67, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.322 [0.000, 4.000], mean observation: 0.484 [0.370, 0.540], loss: 8.414730, mean_absolute_error: 38.866486, mean_q: 49.642387
[F[K  78955/500000: episode: 1162, duration: 3.083s, episode steps: 182, steps per second: 59, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.510 [0.420, 0.670], loss: 7.534478, mean_absolute_error: 39.589172, mean_q: 50.676880
[F[K  79004/500000: episode: 1163, duration: 0.837s, episode steps: 49, steps per second: 59, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.449 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 8.158730, mean_absolute_error: 38.881351, mean_q: 49.750031
[F[K  79090/500000: episode: 1164, duration: 1.246s, episode steps: 86, steps per second: 69, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.872 [0.000, 4.000], mean observation: 0.488 [0.390, 0.520], loss: 8.903535, mean_absolute_error: 39.950279, mean_q: 51.012112
[F[K  79164/500000: episode: 1165, duration: 1.143s, episode steps: 74, steps per second: 65, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.297 [0.000, 4.000], mean observation: 0.499 [0.430, 0.570], loss: 9.254346, mean_absolute_error: 39.578903, mean_q: 50.579330
[F[K  79201/500000: episode: 1166, duration: 0.649s, episode steps: 37, steps per second: 57, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.595 [0.000, 4.000], mean observation: 0.527 [0.470, 0.650], loss: 7.446776, mean_absolute_error: 39.924301, mean_q: 51.095352
[F[K  79332/500000: episode: 1167, duration: 1.941s, episode steps: 131, steps per second: 67, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.206 [0.000, 4.000], mean observation: 0.508 [0.420, 0.650], loss: 8.241327, mean_absolute_error: 39.710110, mean_q: 50.822601
[F[K  79402/500000: episode: 1168, duration: 1.266s, episode steps: 70, steps per second: 55, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.886 [0.000, 4.000], mean observation: 0.488 [0.360, 0.530], loss: 7.579233, mean_absolute_error: 39.392864, mean_q: 50.456940
[F[K  79470/500000: episode: 1169, duration: 1.083s, episode steps: 68, steps per second: 63, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.044 [0.000, 4.000], mean observation: 0.510 [0.450, 0.640], loss: 8.022052, mean_absolute_error: 39.679005, mean_q: 50.681217
[F[K  79524/500000: episode: 1170, duration: 0.669s, episode steps: 54, steps per second: 81, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.537 [0.000, 4.000], mean observation: 0.512 [0.480, 0.580], loss: 7.731574, mean_absolute_error: 41.314190, mean_q: 52.723320
[F[K  79573/500000: episode: 1171, duration: 0.687s, episode steps: 49, steps per second: 71, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.837 [0.000, 4.000], mean observation: 0.499 [0.410, 0.570], loss: 7.346291, mean_absolute_error: 39.538528, mean_q: 50.586273
[F[K  79651/500000: episode: 1172, duration: 1.066s, episode steps: 78, steps per second: 73, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.485 [0.370, 0.510], loss: 9.300019, mean_absolute_error: 39.913998, mean_q: 50.996273
[F[K  79780/500000: episode: 1173, duration: 1.932s, episode steps: 129, steps per second: 67, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.341 [0.000, 4.000], mean observation: 0.482 [0.400, 0.510], loss: 8.391736, mean_absolute_error: 39.979958, mean_q: 51.066582
[F[K  79837/500000: episode: 1174, duration: 0.872s, episode steps: 57, steps per second: 65, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.930 [0.000, 4.000], mean observation: 0.504 [0.380, 0.640], loss: 7.466283, mean_absolute_error: 39.915550, mean_q: 51.024578
[F[K  79899/500000: episode: 1175, duration: 1.084s, episode steps: 62, steps per second: 57, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.968 [0.000, 4.000], mean observation: 0.495 [0.460, 0.530], loss: 9.160763, mean_absolute_error: 39.956982, mean_q: 51.028538
[F[K  79967/500000: episode: 1176, duration: 1.032s, episode steps: 68, steps per second: 66, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.691 [0.000, 4.000], mean observation: 0.476 [0.350, 0.530], loss: 9.023720, mean_absolute_error: 39.925529, mean_q: 51.041477
[F[K  80056/500000: episode: 1177, duration: 1.071s, episode steps: 89, steps per second: 83, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.146 [0.000, 4.000], mean observation: 0.495 [0.470, 0.530], loss: 8.769073, mean_absolute_error: 39.808784, mean_q: 50.836372
[F[K  80132/500000: episode: 1178, duration: 1.199s, episode steps: 76, steps per second: 63, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.684 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 9.426555, mean_absolute_error: 39.507519, mean_q: 50.404797
[F[K  80192/500000: episode: 1179, duration: 0.835s, episode steps: 60, steps per second: 72, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.487 [0.380, 0.530], loss: 7.841678, mean_absolute_error: 40.133102, mean_q: 51.217213
[F[K  80256/500000: episode: 1180, duration: 0.892s, episode steps: 64, steps per second: 72, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.406 [0.000, 4.000], mean observation: 0.498 [0.470, 0.520], loss: 7.882943, mean_absolute_error: 39.797806, mean_q: 50.947914
[F[K  80350/500000: episode: 1181, duration: 1.289s, episode steps: 94, steps per second: 73, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.149 [0.000, 4.000], mean observation: 0.488 [0.360, 0.530], loss: 7.611494, mean_absolute_error: 39.837311, mean_q: 50.928772
[F[K  80406/500000: episode: 1182, duration: 0.764s, episode steps: 56, steps per second: 73, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.214 [0.000, 4.000], mean observation: 0.483 [0.380, 0.530], loss: 7.294712, mean_absolute_error: 40.352436, mean_q: 51.647457
[F[K  80473/500000: episode: 1183, duration: 0.994s, episode steps: 67, steps per second: 67, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.075 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 8.204686, mean_absolute_error: 39.358032, mean_q: 50.379467
[F[K  80571/500000: episode: 1184, duration: 1.630s, episode steps: 98, steps per second: 60, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.449 [0.000, 4.000], mean observation: 0.513 [0.480, 0.590], loss: 9.201200, mean_absolute_error: 39.730873, mean_q: 50.731117
[F[K  80610/500000: episode: 1185, duration: 0.649s, episode steps: 39, steps per second: 60, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.524 [0.470, 0.630], loss: 8.038132, mean_absolute_error: 39.570950, mean_q: 50.451317
[F[K  80678/500000: episode: 1186, duration: 0.892s, episode steps: 68, steps per second: 76, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.971 [0.000, 4.000], mean observation: 0.498 [0.390, 0.600], loss: 8.346642, mean_absolute_error: 39.453777, mean_q: 50.414375
[F[K  80747/500000: episode: 1187, duration: 1.145s, episode steps: 69, steps per second: 60, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.478 [0.360, 0.510], loss: 8.994018, mean_absolute_error: 39.713520, mean_q: 50.767548
[F[K  80888/500000: episode: 1188, duration: 2.163s, episode steps: 141, steps per second: 65, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.936 [0.000, 4.000], mean observation: 0.501 [0.450, 0.530], loss: 8.208741, mean_absolute_error: 40.552906, mean_q: 51.820293
[F[K  80940/500000: episode: 1189, duration: 0.867s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.192 [0.000, 4.000], mean observation: 0.496 [0.370, 0.570], loss: 8.498760, mean_absolute_error: 39.791462, mean_q: 50.859123
[F[K  81015/500000: episode: 1190, duration: 1.098s, episode steps: 75, steps per second: 68, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.987 [0.000, 4.000], mean observation: 0.500 [0.430, 0.570], loss: 8.260731, mean_absolute_error: 39.604942, mean_q: 50.648323
[F[K  81085/500000: episode: 1191, duration: 1.133s, episode steps: 70, steps per second: 62, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.499 [0.470, 0.530], loss: 8.986227, mean_absolute_error: 40.136612, mean_q: 51.304329
[F[K  81142/500000: episode: 1192, duration: 1.014s, episode steps: 57, steps per second: 56, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.018 [0.000, 4.000], mean observation: 0.511 [0.490, 0.570], loss: 8.494948, mean_absolute_error: 40.071438, mean_q: 51.067001
[F[K  81220/500000: episode: 1193, duration: 1.068s, episode steps: 78, steps per second: 73, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.449 [0.000, 4.000], mean observation: 0.495 [0.410, 0.530], loss: 7.894575, mean_absolute_error: 40.047390, mean_q: 51.214058
[F[K  81274/500000: episode: 1194, duration: 0.851s, episode steps: 54, steps per second: 63, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.296 [0.000, 4.000], mean observation: 0.497 [0.430, 0.570], loss: 7.036682, mean_absolute_error: 40.221363, mean_q: 51.432831
[F[K  81352/500000: episode: 1195, duration: 1.305s, episode steps: 78, steps per second: 60, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.505 [0.480, 0.530], loss: 9.401997, mean_absolute_error: 40.276405, mean_q: 51.478146
[F[K  81434/500000: episode: 1196, duration: 1.336s, episode steps: 82, steps per second: 61, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.098 [0.000, 4.000], mean observation: 0.485 [0.350, 0.520], loss: 7.983968, mean_absolute_error: 39.979038, mean_q: 51.116116
[F[K  81498/500000: episode: 1197, duration: 1.010s, episode steps: 64, steps per second: 63, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.969 [0.000, 4.000], mean observation: 0.490 [0.400, 0.520], loss: 8.980042, mean_absolute_error: 39.369404, mean_q: 50.408394
[F[K  81559/500000: episode: 1198, duration: 0.882s, episode steps: 61, steps per second: 69, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.967 [0.000, 4.000], mean observation: 0.507 [0.460, 0.580], loss: 8.301266, mean_absolute_error: 40.078068, mean_q: 51.177711
[F[K  81630/500000: episode: 1199, duration: 1.037s, episode steps: 71, steps per second: 68, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.423 [0.000, 4.000], mean observation: 0.516 [0.500, 0.590], loss: 8.886814, mean_absolute_error: 40.494228, mean_q: 51.745342
[F[K  81743/500000: episode: 1200, duration: 1.860s, episode steps: 113, steps per second: 61, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.611 [0.000, 4.000], mean observation: 0.498 [0.430, 0.530], loss: 8.008855, mean_absolute_error: 40.495140, mean_q: 51.733456
[F[K  81780/500000: episode: 1201, duration: 0.646s, episode steps: 37, steps per second: 57, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.514 [0.000, 4.000], mean observation: 0.507 [0.430, 0.640], loss: 7.923364, mean_absolute_error: 39.859093, mean_q: 50.939140
[F[K  81863/500000: episode: 1202, duration: 1.187s, episode steps: 83, steps per second: 70, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.892 [0.000, 4.000], mean observation: 0.520 [0.480, 0.620], loss: 8.783232, mean_absolute_error: 40.094337, mean_q: 51.227943
[F[K  81924/500000: episode: 1203, duration: 0.892s, episode steps: 61, steps per second: 68, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.098 [0.000, 4.000], mean observation: 0.482 [0.390, 0.530], loss: 8.628882, mean_absolute_error: 40.270206, mean_q: 51.573212
[F[K  82002/500000: episode: 1204, duration: 1.248s, episode steps: 78, steps per second: 62, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.474 [0.360, 0.540], loss: 8.135424, mean_absolute_error: 40.812656, mean_q: 52.152615
[F[K  82062/500000: episode: 1205, duration: 0.909s, episode steps: 60, steps per second: 66, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.383 [0.000, 4.000], mean observation: 0.509 [0.470, 0.630], loss: 7.860618, mean_absolute_error: 39.941902, mean_q: 51.166813
[F[K  82117/500000: episode: 1206, duration: 0.918s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.782 [0.000, 4.000], mean observation: 0.508 [0.490, 0.550], loss: 8.324279, mean_absolute_error: 40.537239, mean_q: 51.755337
[F[K  82237/500000: episode: 1207, duration: 1.711s, episode steps: 120, steps per second: 70, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.892 [0.000, 4.000], mean observation: 0.510 [0.470, 0.590], loss: 8.084146, mean_absolute_error: 40.228195, mean_q: 51.477879
[F[K  82309/500000: episode: 1208, duration: 1.189s, episode steps: 72, steps per second: 61, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 0.501 [0.350, 0.630], loss: 8.310962, mean_absolute_error: 40.520153, mean_q: 51.699768
[F[K  82384/500000: episode: 1209, duration: 1.302s, episode steps: 75, steps per second: 58, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.240 [0.000, 4.000], mean observation: 0.492 [0.380, 0.530], loss: 8.846974, mean_absolute_error: 40.264259, mean_q: 51.536530
[F[K  82454/500000: episode: 1210, duration: 1.149s, episode steps: 70, steps per second: 61, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.757 [0.000, 4.000], mean observation: 0.510 [0.500, 0.580], loss: 8.175703, mean_absolute_error: 40.713734, mean_q: 52.040497
[F[K  82654/500000: episode: 1211, duration: 3.109s, episode steps: 200, steps per second: 64, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.486 [0.320, 0.540], loss: 8.214563, mean_absolute_error: 39.846359, mean_q: 50.899609
[F[K  82728/500000: episode: 1212, duration: 1.106s, episode steps: 74, steps per second: 67, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.492 [0.430, 0.530], loss: 8.293849, mean_absolute_error: 39.943237, mean_q: 51.120552
[F[K  82856/500000: episode: 1213, duration: 1.896s, episode steps: 128, steps per second: 68, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.094 [0.000, 4.000], mean observation: 0.493 [0.430, 0.540], loss: 8.467999, mean_absolute_error: 40.411583, mean_q: 51.628647
[F[K  82929/500000: episode: 1214, duration: 1.175s, episode steps: 73, steps per second: 62, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.110 [0.000, 4.000], mean observation: 0.505 [0.440, 0.650], loss: 7.591974, mean_absolute_error: 39.845860, mean_q: 51.014244
[F[K  83004/500000: episode: 1215, duration: 1.154s, episode steps: 75, steps per second: 65, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.506 [0.480, 0.550], loss: 8.056463, mean_absolute_error: 40.334576, mean_q: 51.626411
[F[K  83136/500000: episode: 1216, duration: 2.418s, episode steps: 132, steps per second: 55, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.509 [0.470, 0.600], loss: 7.999475, mean_absolute_error: 40.249207, mean_q: 51.563606
[F[K  83214/500000: episode: 1217, duration: 1.322s, episode steps: 78, steps per second: 59, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.026 [0.000, 4.000], mean observation: 0.485 [0.360, 0.530], loss: 9.227147, mean_absolute_error: 40.056183, mean_q: 51.193382
[F[K  83271/500000: episode: 1218, duration: 0.829s, episode steps: 57, steps per second: 69, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.053 [0.000, 4.000], mean observation: 0.519 [0.470, 0.600], loss: 9.045513, mean_absolute_error: 40.121899, mean_q: 51.311378
[F[K  83342/500000: episode: 1219, duration: 1.112s, episode steps: 71, steps per second: 64, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.620 [0.000, 4.000], mean observation: 0.508 [0.470, 0.560], loss: 8.454289, mean_absolute_error: 39.966255, mean_q: 51.059437
[F[K  83418/500000: episode: 1220, duration: 1.301s, episode steps: 76, steps per second: 58, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.211 [0.000, 4.000], mean observation: 0.487 [0.420, 0.520], loss: 8.769508, mean_absolute_error: 40.277752, mean_q: 51.402874
[F[K  83486/500000: episode: 1221, duration: 1.141s, episode steps: 68, steps per second: 60, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 7.198115, mean_absolute_error: 40.232609, mean_q: 51.444107
[F[K  83557/500000: episode: 1222, duration: 1.205s, episode steps: 71, steps per second: 59, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.282 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 8.107762, mean_absolute_error: 39.997879, mean_q: 51.124325
[F[K  83638/500000: episode: 1223, duration: 1.266s, episode steps: 81, steps per second: 64, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.593 [0.000, 4.000], mean observation: 0.482 [0.400, 0.530], loss: 7.846445, mean_absolute_error: 40.501373, mean_q: 51.860188
[F[K  83729/500000: episode: 1224, duration: 1.424s, episode steps: 91, steps per second: 64, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.352 [0.000, 4.000], mean observation: 0.515 [0.470, 0.590], loss: 8.797659, mean_absolute_error: 41.257301, mean_q: 52.723686
[F[K  83804/500000: episode: 1225, duration: 1.193s, episode steps: 75, steps per second: 63, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.600 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 8.090926, mean_absolute_error: 40.310204, mean_q: 51.555553
[F[K  83865/500000: episode: 1226, duration: 0.915s, episode steps: 61, steps per second: 67, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.770 [0.000, 4.000], mean observation: 0.498 [0.360, 0.610], loss: 7.626336, mean_absolute_error: 39.969322, mean_q: 51.034046
[F[K  83933/500000: episode: 1227, duration: 0.926s, episode steps: 68, steps per second: 73, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.191 [0.000, 4.000], mean observation: 0.486 [0.420, 0.510], loss: 8.145318, mean_absolute_error: 40.565392, mean_q: 51.852665
[F[K  84017/500000: episode: 1228, duration: 1.342s, episode steps: 84, steps per second: 63, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.509 [0.490, 0.550], loss: 7.930500, mean_absolute_error: 39.814514, mean_q: 50.963959
[F[K  84095/500000: episode: 1229, duration: 0.985s, episode steps: 78, steps per second: 79, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.321 [0.000, 4.000], mean observation: 0.486 [0.410, 0.530], loss: 9.471631, mean_absolute_error: 40.750484, mean_q: 52.102314
[F[K  84162/500000: episode: 1230, duration: 0.965s, episode steps: 67, steps per second: 69, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.851 [0.000, 4.000], mean observation: 0.496 [0.380, 0.550], loss: 8.198730, mean_absolute_error: 40.226292, mean_q: 51.405525
[F[K  84216/500000: episode: 1231, duration: 1.001s, episode steps: 54, steps per second: 54, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.259 [0.000, 4.000], mean observation: 0.514 [0.470, 0.610], loss: 8.469130, mean_absolute_error: 40.848331, mean_q: 52.153194
[F[K  84327/500000: episode: 1232, duration: 1.726s, episode steps: 111, steps per second: 64, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.955 [0.000, 4.000], mean observation: 0.495 [0.410, 0.560], loss: 7.674222, mean_absolute_error: 41.119865, mean_q: 52.629204
[F[K  84396/500000: episode: 1233, duration: 1.121s, episode steps: 69, steps per second: 62, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.475 [0.380, 0.520], loss: 8.687126, mean_absolute_error: 41.255074, mean_q: 52.637791
[F[K  84573/500000: episode: 1234, duration: 2.331s, episode steps: 177, steps per second: 76, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.056 [0.000, 4.000], mean observation: 0.484 [0.330, 0.530], loss: 7.802561, mean_absolute_error: 40.664940, mean_q: 52.041080
[F[K  84640/500000: episode: 1235, duration: 0.992s, episode steps: 67, steps per second: 68, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.209 [0.000, 4.000], mean observation: 0.503 [0.420, 0.650], loss: 9.804852, mean_absolute_error: 40.483917, mean_q: 51.652023
[F[K  84719/500000: episode: 1236, duration: 0.988s, episode steps: 79, steps per second: 80, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.392 [0.000, 4.000], mean observation: 0.521 [0.480, 0.610], loss: 8.442189, mean_absolute_error: 41.334770, mean_q: 52.931942
[F[K  84800/500000: episode: 1237, duration: 1.146s, episode steps: 81, steps per second: 71, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.593 [0.000, 4.000], mean observation: 0.498 [0.440, 0.550], loss: 8.523302, mean_absolute_error: 40.668259, mean_q: 52.007477
[F[K  84857/500000: episode: 1238, duration: 0.850s, episode steps: 57, steps per second: 67, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.316 [0.000, 4.000], mean observation: 0.499 [0.360, 0.610], loss: 7.854536, mean_absolute_error: 41.078857, mean_q: 52.665073
[F[K  84890/500000: episode: 1239, duration: 0.580s, episode steps: 33, steps per second: 57, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.788 [0.000, 4.000], mean observation: 0.521 [0.470, 0.640], loss: 9.650423, mean_absolute_error: 41.515614, mean_q: 53.075100
[F[K  84964/500000: episode: 1240, duration: 1.072s, episode steps: 74, steps per second: 69, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.527 [0.000, 4.000], mean observation: 0.492 [0.370, 0.560], loss: 9.031820, mean_absolute_error: 40.609791, mean_q: 52.032413
[F[K  85152/500000: episode: 1241, duration: 2.574s, episode steps: 188, steps per second: 73, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.053 [0.000, 4.000], mean observation: 0.511 [0.460, 0.600], loss: 9.340897, mean_absolute_error: 40.597450, mean_q: 51.846603
[F[K  85243/500000: episode: 1242, duration: 1.359s, episode steps: 91, steps per second: 67, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.132 [0.000, 4.000], mean observation: 0.493 [0.380, 0.570], loss: 8.285409, mean_absolute_error: 40.639519, mean_q: 51.922745
[F[K  85297/500000: episode: 1243, duration: 0.791s, episode steps: 54, steps per second: 68, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.648 [0.000, 4.000], mean observation: 0.509 [0.470, 0.600], loss: 9.465474, mean_absolute_error: 41.077606, mean_q: 52.471516
[F[K  85355/500000: episode: 1244, duration: 0.813s, episode steps: 58, steps per second: 71, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.501 [0.480, 0.530], loss: 8.796000, mean_absolute_error: 39.429317, mean_q: 50.295696
[F[K  85433/500000: episode: 1245, duration: 1.094s, episode steps: 78, steps per second: 71, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.564 [0.000, 4.000], mean observation: 0.498 [0.400, 0.560], loss: 8.310173, mean_absolute_error: 40.644936, mean_q: 51.979607
[F[K  85493/500000: episode: 1246, duration: 0.829s, episode steps: 60, steps per second: 72, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.583 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 9.258700, mean_absolute_error: 40.624374, mean_q: 51.940140
[F[K  85684/500000: episode: 1247, duration: 2.707s, episode steps: 191, steps per second: 71, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.963 [0.000, 4.000], mean observation: 0.490 [0.370, 0.540], loss: 8.043146, mean_absolute_error: 40.851410, mean_q: 52.144482
[F[K  85763/500000: episode: 1248, duration: 1.036s, episode steps: 79, steps per second: 76, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.962 [0.000, 4.000], mean observation: 0.496 [0.350, 0.620], loss: 7.901068, mean_absolute_error: 40.429260, mean_q: 51.659039
[F[K  85839/500000: episode: 1249, duration: 1.151s, episode steps: 76, steps per second: 66, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.724 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 7.979915, mean_absolute_error: 40.417637, mean_q: 51.769226
[F[K  85904/500000: episode: 1250, duration: 1.031s, episode steps: 65, steps per second: 63, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.498 [0.350, 0.600], loss: 9.449705, mean_absolute_error: 40.750721, mean_q: 52.122238
[F[K  85988/500000: episode: 1251, duration: 1.330s, episode steps: 84, steps per second: 63, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.976 [0.000, 4.000], mean observation: 0.509 [0.470, 0.600], loss: 8.482369, mean_absolute_error: 40.646145, mean_q: 51.978325
[F[K  86132/500000: episode: 1252, duration: 2.011s, episode steps: 144, steps per second: 72, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.174 [0.000, 4.000], mean observation: 0.507 [0.450, 0.590], loss: 8.901956, mean_absolute_error: 41.118710, mean_q: 52.493919
[F[K  86205/500000: episode: 1253, duration: 0.962s, episode steps: 73, steps per second: 76, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.315 [0.000, 4.000], mean observation: 0.506 [0.460, 0.580], loss: 8.679053, mean_absolute_error: 40.990074, mean_q: 52.446064
[F[K  86261/500000: episode: 1254, duration: 0.773s, episode steps: 56, steps per second: 72, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.232 [0.000, 4.000], mean observation: 0.514 [0.490, 0.580], loss: 9.121997, mean_absolute_error: 40.528339, mean_q: 51.886009
[F[K  86321/500000: episode: 1255, duration: 1.080s, episode steps: 60, steps per second: 56, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.633 [0.000, 4.000], mean observation: 0.497 [0.450, 0.530], loss: 10.303744, mean_absolute_error: 40.873711, mean_q: 52.269417
[F[K  86393/500000: episode: 1256, duration: 1.190s, episode steps: 72, steps per second: 61, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.444 [0.000, 4.000], mean observation: 0.508 [0.470, 0.590], loss: 8.643028, mean_absolute_error: 40.784119, mean_q: 52.199471
[F[K  86465/500000: episode: 1257, duration: 1.100s, episode steps: 72, steps per second: 65, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.264 [0.000, 4.000], mean observation: 0.495 [0.470, 0.520], loss: 7.618835, mean_absolute_error: 40.798725, mean_q: 52.198662
[F[K  86525/500000: episode: 1258, duration: 0.984s, episode steps: 60, steps per second: 61, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.317 [0.000, 4.000], mean observation: 0.511 [0.470, 0.620], loss: 7.975983, mean_absolute_error: 41.159653, mean_q: 52.506794
[F[K  86626/500000: episode: 1259, duration: 1.707s, episode steps: 101, steps per second: 59, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.970 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 8.862906, mean_absolute_error: 40.984051, mean_q: 52.402020
[F[K  86767/500000: episode: 1260, duration: 2.089s, episode steps: 141, steps per second: 67, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.908 [0.000, 4.000], mean observation: 0.501 [0.400, 0.580], loss: 8.379747, mean_absolute_error: 40.440693, mean_q: 51.654690
[F[K  86862/500000: episode: 1261, duration: 1.325s, episode steps: 95, steps per second: 72, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 7.562217, mean_absolute_error: 40.781017, mean_q: 52.275654
[F[K  86943/500000: episode: 1262, duration: 1.236s, episode steps: 81, steps per second: 66, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.099 [0.000, 4.000], mean observation: 0.504 [0.390, 0.610], loss: 7.657585, mean_absolute_error: 41.441975, mean_q: 53.058544
[F[K  87000/500000: episode: 1263, duration: 0.814s, episode steps: 57, steps per second: 70, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.684 [0.000, 3.000], mean observation: 0.520 [0.470, 0.620], loss: 8.245446, mean_absolute_error: 41.001087, mean_q: 52.355515
[F[K  87150/500000: episode: 1264, duration: 2.143s, episode steps: 150, steps per second: 70, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.313 [0.000, 4.000], mean observation: 0.472 [0.320, 0.510], loss: 8.850786, mean_absolute_error: 41.135864, mean_q: 52.588390
[F[K  87204/500000: episode: 1265, duration: 0.715s, episode steps: 54, steps per second: 76, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.426 [0.000, 4.000], mean observation: 0.488 [0.430, 0.510], loss: 8.288189, mean_absolute_error: 41.423046, mean_q: 53.041477
[F[K  87248/500000: episode: 1266, duration: 0.657s, episode steps: 44, steps per second: 67, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.513 [0.470, 0.650], loss: 8.776036, mean_absolute_error: 41.655399, mean_q: 53.186501
[F[K  87312/500000: episode: 1267, duration: 0.987s, episode steps: 64, steps per second: 65, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.501 [0.470, 0.550], loss: 8.290119, mean_absolute_error: 40.975620, mean_q: 52.323860
[F[K  87390/500000: episode: 1268, duration: 1.078s, episode steps: 78, steps per second: 72, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.744 [0.000, 4.000], mean observation: 0.505 [0.460, 0.550], loss: 10.178967, mean_absolute_error: 40.928860, mean_q: 52.132008
[F[K  87445/500000: episode: 1269, duration: 0.740s, episode steps: 55, steps per second: 74, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.499 [0.350, 0.630], loss: 9.219547, mean_absolute_error: 40.253300, mean_q: 51.534691
[F[K  87520/500000: episode: 1270, duration: 0.954s, episode steps: 75, steps per second: 79, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.867 [0.000, 4.000], mean observation: 0.506 [0.480, 0.560], loss: 8.641455, mean_absolute_error: 40.594238, mean_q: 51.749794
[F[K  87601/500000: episode: 1271, duration: 1.025s, episode steps: 81, steps per second: 79, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 0.501 [0.460, 0.560], loss: 8.306244, mean_absolute_error: 40.899231, mean_q: 52.205200
[F[K  87673/500000: episode: 1272, duration: 0.991s, episode steps: 72, steps per second: 73, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.485 [0.380, 0.510], loss: 9.631684, mean_absolute_error: 41.440849, mean_q: 52.901299
[F[K  87775/500000: episode: 1273, duration: 1.268s, episode steps: 102, steps per second: 80, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.284 [0.000, 4.000], mean observation: 0.501 [0.390, 0.610], loss: 9.018636, mean_absolute_error: 40.544411, mean_q: 51.868874
[F[K  87836/500000: episode: 1274, duration: 0.730s, episode steps: 61, steps per second: 84, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.082 [0.000, 4.000], mean observation: 0.501 [0.470, 0.540], loss: 8.498958, mean_absolute_error: 40.047398, mean_q: 51.232834
[F[K  87925/500000: episode: 1275, duration: 1.255s, episode steps: 89, steps per second: 71, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.382 [0.000, 4.000], mean observation: 0.498 [0.430, 0.550], loss: 8.272706, mean_absolute_error: 40.649406, mean_q: 52.018040
[F[K  88015/500000: episode: 1276, duration: 1.173s, episode steps: 90, steps per second: 77, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.493 [0.390, 0.550], loss: 9.271839, mean_absolute_error: 40.825771, mean_q: 52.108891
[F[K  88107/500000: episode: 1277, duration: 1.429s, episode steps: 92, steps per second: 64, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.011 [0.000, 4.000], mean observation: 0.473 [0.360, 0.510], loss: 8.687647, mean_absolute_error: 39.921494, mean_q: 50.925114
[F[K  88192/500000: episode: 1278, duration: 1.353s, episode steps: 85, steps per second: 63, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.176 [0.000, 4.000], mean observation: 0.487 [0.410, 0.520], loss: 7.426284, mean_absolute_error: 40.547668, mean_q: 51.857719
[F[K  88240/500000: episode: 1279, duration: 0.739s, episode steps: 48, steps per second: 65, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.292 [0.000, 4.000], mean observation: 0.518 [0.470, 0.600], loss: 7.456224, mean_absolute_error: 40.405720, mean_q: 51.697765
[F[K  88311/500000: episode: 1280, duration: 1.081s, episode steps: 71, steps per second: 66, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.423 [0.000, 4.000], mean observation: 0.514 [0.480, 0.600], loss: 9.018023, mean_absolute_error: 40.719471, mean_q: 52.089046
[F[K  88462/500000: episode: 1281, duration: 2.238s, episode steps: 151, steps per second: 67, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.013 [0.000, 4.000], mean observation: 0.508 [0.470, 0.570], loss: 8.842776, mean_absolute_error: 40.980042, mean_q: 52.402554
[F[K  88571/500000: episode: 1282, duration: 1.569s, episode steps: 109, steps per second: 69, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.780 [0.000, 4.000], mean observation: 0.495 [0.430, 0.530], loss: 8.785978, mean_absolute_error: 40.134586, mean_q: 51.318741
[F[K  88639/500000: episode: 1283, duration: 0.965s, episode steps: 68, steps per second: 70, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.441 [0.000, 4.000], mean observation: 0.502 [0.420, 0.610], loss: 7.795586, mean_absolute_error: 41.016258, mean_q: 52.402161
[F[K  88704/500000: episode: 1284, duration: 0.916s, episode steps: 65, steps per second: 71, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.046 [0.000, 4.000], mean observation: 0.492 [0.370, 0.540], loss: 9.152058, mean_absolute_error: 41.058430, mean_q: 52.609322
[F[K  88757/500000: episode: 1285, duration: 0.885s, episode steps: 53, steps per second: 60, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.132 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 8.709194, mean_absolute_error: 40.950748, mean_q: 52.349396
[F[K  88815/500000: episode: 1286, duration: 0.953s, episode steps: 58, steps per second: 61, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 7.345023, mean_absolute_error: 41.483864, mean_q: 53.151081
[F[K  88878/500000: episode: 1287, duration: 0.934s, episode steps: 63, steps per second: 67, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.381 [0.000, 4.000], mean observation: 0.510 [0.470, 0.550], loss: 8.505820, mean_absolute_error: 41.198063, mean_q: 52.782009
[F[K  88914/500000: episode: 1288, duration: 0.589s, episode steps: 36, steps per second: 61, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.306 [0.000, 3.000], mean observation: 0.516 [0.470, 0.650], loss: 7.793623, mean_absolute_error: 40.728355, mean_q: 52.202858
[F[K  88949/500000: episode: 1289, duration: 0.517s, episode steps: 35, steps per second: 68, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.714 [0.000, 3.000], mean observation: 0.524 [0.470, 0.650], loss: 7.598464, mean_absolute_error: 41.060394, mean_q: 52.461708
[F[K  89033/500000: episode: 1290, duration: 1.100s, episode steps: 84, steps per second: 76, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.690 [0.000, 4.000], mean observation: 0.492 [0.360, 0.560], loss: 8.424076, mean_absolute_error: 41.872353, mean_q: 53.536518
[F[K  89121/500000: episode: 1291, duration: 1.042s, episode steps: 88, steps per second: 84, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.580 [0.000, 4.000], mean observation: 0.479 [0.360, 0.530], loss: 8.253422, mean_absolute_error: 41.239201, mean_q: 52.785789
[F[K  89186/500000: episode: 1292, duration: 0.888s, episode steps: 65, steps per second: 73, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.923 [0.000, 4.000], mean observation: 0.523 [0.490, 0.630], loss: 8.605423, mean_absolute_error: 41.262730, mean_q: 52.763931
[F[K  89231/500000: episode: 1293, duration: 0.750s, episode steps: 45, steps per second: 60, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.578 [0.000, 4.000], mean observation: 0.500 [0.350, 0.640], loss: 8.902403, mean_absolute_error: 41.025955, mean_q: 52.542793
[F[K  89306/500000: episode: 1294, duration: 1.056s, episode steps: 75, steps per second: 71, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 0.488 [0.400, 0.530], loss: 8.495098, mean_absolute_error: 41.427727, mean_q: 52.970707
[F[K  89407/500000: episode: 1295, duration: 1.721s, episode steps: 101, steps per second: 59, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.040 [0.000, 4.000], mean observation: 0.515 [0.470, 0.620], loss: 8.217073, mean_absolute_error: 41.376842, mean_q: 52.928772
[F[K  89473/500000: episode: 1296, duration: 1.108s, episode steps: 66, steps per second: 60, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.576 [0.000, 4.000], mean observation: 0.505 [0.470, 0.580], loss: 9.172333, mean_absolute_error: 41.357998, mean_q: 52.833836
[F[K  89576/500000: episode: 1297, duration: 1.561s, episode steps: 103, steps per second: 66, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.039 [0.000, 4.000], mean observation: 0.505 [0.460, 0.600], loss: 8.747866, mean_absolute_error: 41.411041, mean_q: 52.968548
[F[K  89649/500000: episode: 1298, duration: 1.038s, episode steps: 73, steps per second: 70, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.515 [0.470, 0.640], loss: 8.363056, mean_absolute_error: 41.128502, mean_q: 52.696411
[F[K  89768/500000: episode: 1299, duration: 1.960s, episode steps: 119, steps per second: 61, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.798 [0.000, 4.000], mean observation: 0.475 [0.370, 0.510], loss: 8.331756, mean_absolute_error: 40.743774, mean_q: 52.093151
[F[K  89830/500000: episode: 1300, duration: 1.017s, episode steps: 62, steps per second: 61, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.490 [0.440, 0.510], loss: 8.824732, mean_absolute_error: 41.486103, mean_q: 53.020439
[F[K  89952/500000: episode: 1301, duration: 1.799s, episode steps: 122, steps per second: 68, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.631 [0.000, 4.000], mean observation: 0.514 [0.470, 0.640], loss: 9.578188, mean_absolute_error: 40.783298, mean_q: 52.080807
[F[K  90006/500000: episode: 1302, duration: 0.798s, episode steps: 54, steps per second: 68, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.944 [0.000, 4.000], mean observation: 0.514 [0.470, 0.610], loss: 9.940220, mean_absolute_error: 41.247711, mean_q: 52.500137
[F[K  90085/500000: episode: 1303, duration: 1.210s, episode steps: 79, steps per second: 65, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.139 [0.000, 4.000], mean observation: 0.522 [0.490, 0.610], loss: 8.795839, mean_absolute_error: 41.249470, mean_q: 52.737602
[F[K  90175/500000: episode: 1304, duration: 1.506s, episode steps: 90, steps per second: 60, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 0.496 [0.380, 0.580], loss: 8.677335, mean_absolute_error: 41.188248, mean_q: 52.719204
[F[K  90349/500000: episode: 1305, duration: 2.323s, episode steps: 174, steps per second: 75, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.482 [0.330, 0.510], loss: 8.599979, mean_absolute_error: 41.005463, mean_q: 52.401924
[F[K  90397/500000: episode: 1306, duration: 0.728s, episode steps: 48, steps per second: 66, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.104 [0.000, 4.000], mean observation: 0.504 [0.430, 0.620], loss: 10.568002, mean_absolute_error: 41.419155, mean_q: 53.025177
[F[K  90470/500000: episode: 1307, duration: 1.150s, episode steps: 73, steps per second: 63, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.041 [0.000, 4.000], mean observation: 0.503 [0.470, 0.580], loss: 8.607943, mean_absolute_error: 41.652676, mean_q: 53.289127
[F[K  90537/500000: episode: 1308, duration: 0.882s, episode steps: 67, steps per second: 76, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 0.511 [0.480, 0.600], loss: 8.346602, mean_absolute_error: 41.070950, mean_q: 52.570286
[F[K  90618/500000: episode: 1309, duration: 1.074s, episode steps: 81, steps per second: 75, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.136 [0.000, 4.000], mean observation: 0.499 [0.430, 0.580], loss: 8.840624, mean_absolute_error: 41.666286, mean_q: 53.201733
[F[K  90728/500000: episode: 1310, duration: 1.831s, episode steps: 110, steps per second: 60, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.391 [0.000, 4.000], mean observation: 0.510 [0.480, 0.590], loss: 7.938234, mean_absolute_error: 42.523575, mean_q: 54.417015
[F[K  90762/500000: episode: 1311, duration: 0.605s, episode steps: 34, steps per second: 56, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.382 [0.000, 3.000], mean observation: 0.516 [0.470, 0.650], loss: 8.440324, mean_absolute_error: 41.275509, mean_q: 52.812046
[F[K  90844/500000: episode: 1312, duration: 1.373s, episode steps: 82, steps per second: 60, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.476 [0.000, 4.000], mean observation: 0.479 [0.370, 0.520], loss: 8.938241, mean_absolute_error: 41.098633, mean_q: 52.547203
[F[K  90883/500000: episode: 1313, duration: 0.662s, episode steps: 39, steps per second: 59, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.564 [0.000, 3.000], mean observation: 0.523 [0.470, 0.640], loss: 7.459030, mean_absolute_error: 42.397434, mean_q: 54.218849
[F[K  90971/500000: episode: 1314, duration: 1.497s, episode steps: 88, steps per second: 59, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.795 [0.000, 4.000], mean observation: 0.521 [0.490, 0.620], loss: 7.984433, mean_absolute_error: 41.491154, mean_q: 53.046768
[F[K  91035/500000: episode: 1315, duration: 1.096s, episode steps: 64, steps per second: 58, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.766 [0.000, 4.000], mean observation: 0.491 [0.430, 0.520], loss: 7.045365, mean_absolute_error: 41.326027, mean_q: 52.921547
[F[K  91197/500000: episode: 1316, duration: 2.720s, episode steps: 162, steps per second: 60, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.508 [0.430, 0.610], loss: 8.465945, mean_absolute_error: 41.795250, mean_q: 53.510284
[F[K  91249/500000: episode: 1317, duration: 0.876s, episode steps: 52, steps per second: 59, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.495 [0.350, 0.590], loss: 8.282979, mean_absolute_error: 41.664307, mean_q: 53.318245
[F[K  91283/500000: episode: 1318, duration: 0.581s, episode steps: 34, steps per second: 59, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.824 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 8.110683, mean_absolute_error: 40.703194, mean_q: 52.185116
[F[K  91354/500000: episode: 1319, duration: 1.228s, episode steps: 71, steps per second: 58, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.496 [0.370, 0.620], loss: 7.828717, mean_absolute_error: 42.282524, mean_q: 54.129322
[F[K  91431/500000: episode: 1320, duration: 1.564s, episode steps: 77, steps per second: 49, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.156 [0.000, 4.000], mean observation: 0.507 [0.470, 0.620], loss: 7.922684, mean_absolute_error: 41.750103, mean_q: 53.439758
[F[K  91498/500000: episode: 1321, duration: 1.160s, episode steps: 67, steps per second: 58, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.896 [0.000, 4.000], mean observation: 0.515 [0.470, 0.600], loss: 9.157527, mean_absolute_error: 41.271687, mean_q: 52.814220
[F[K  91572/500000: episode: 1322, duration: 1.220s, episode steps: 74, steps per second: 61, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.757 [0.000, 4.000], mean observation: 0.506 [0.430, 0.640], loss: 8.725949, mean_absolute_error: 41.741100, mean_q: 53.452473
[F[K  91648/500000: episode: 1323, duration: 1.383s, episode steps: 76, steps per second: 55, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.092 [0.000, 4.000], mean observation: 0.475 [0.370, 0.520], loss: 8.702597, mean_absolute_error: 41.541927, mean_q: 53.205006
[F[K  91708/500000: episode: 1324, duration: 1.236s, episode steps: 60, steps per second: 49, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.492 [0.350, 0.560], loss: 8.726111, mean_absolute_error: 41.025776, mean_q: 52.535263
[F[K  91843/500000: episode: 1325, duration: 2.093s, episode steps: 135, steps per second: 64, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.495 [0.380, 0.590], loss: 8.505380, mean_absolute_error: 41.468048, mean_q: 53.015633
[F[K  91932/500000: episode: 1326, duration: 1.265s, episode steps: 89, steps per second: 70, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.292 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 8.279430, mean_absolute_error: 41.586445, mean_q: 53.176636
[F[K  91986/500000: episode: 1327, duration: 0.829s, episode steps: 54, steps per second: 65, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.407 [0.000, 4.000], mean observation: 0.521 [0.470, 0.640], loss: 9.434672, mean_absolute_error: 41.170559, mean_q: 52.649315
[F[K  92045/500000: episode: 1328, duration: 0.992s, episode steps: 59, steps per second: 59, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.508 [0.000, 4.000], mean observation: 0.512 [0.470, 0.610], loss: 9.966992, mean_absolute_error: 41.455254, mean_q: 52.950081
[F[K  92129/500000: episode: 1329, duration: 1.340s, episode steps: 84, steps per second: 63, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.493 [0.330, 0.580], loss: 8.457333, mean_absolute_error: 41.414898, mean_q: 52.926533
[F[K  92202/500000: episode: 1330, duration: 1.117s, episode steps: 73, steps per second: 65, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.486 [0.400, 0.520], loss: 7.991409, mean_absolute_error: 41.353195, mean_q: 52.934719
[F[K  92287/500000: episode: 1331, duration: 1.553s, episode steps: 85, steps per second: 55, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.499 [0.470, 0.530], loss: 9.611777, mean_absolute_error: 41.247593, mean_q: 52.795170
[F[K  92340/500000: episode: 1332, duration: 0.947s, episode steps: 53, steps per second: 56, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.496 [0.410, 0.580], loss: 10.348165, mean_absolute_error: 41.770515, mean_q: 53.139065
[F[K  92393/500000: episode: 1333, duration: 0.793s, episode steps: 53, steps per second: 67, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.585 [0.000, 3.000], mean observation: 0.516 [0.470, 0.620], loss: 7.996378, mean_absolute_error: 42.009766, mean_q: 53.725185
[F[K  92479/500000: episode: 1334, duration: 1.194s, episode steps: 86, steps per second: 72, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.640 [0.000, 4.000], mean observation: 0.504 [0.470, 0.580], loss: 9.938556, mean_absolute_error: 41.533154, mean_q: 53.102375
[F[K  92679/500000: episode: 1335, duration: 3.077s, episode steps: 200, steps per second: 65, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.980 [0.000, 4.000], mean observation: 0.517 [0.490, 0.700], loss: 8.373003, mean_absolute_error: 41.143379, mean_q: 52.648727
[F[K  92761/500000: episode: 1336, duration: 1.268s, episode steps: 82, steps per second: 65, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.268 [0.000, 4.000], mean observation: 0.475 [0.370, 0.510], loss: 8.672738, mean_absolute_error: 41.408852, mean_q: 52.914314
[F[K  92839/500000: episode: 1337, duration: 1.278s, episode steps: 78, steps per second: 61, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.846 [0.000, 4.000], mean observation: 0.505 [0.430, 0.600], loss: 7.610996, mean_absolute_error: 41.801620, mean_q: 53.412518
[F[K  92900/500000: episode: 1338, duration: 0.986s, episode steps: 61, steps per second: 62, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.066 [0.000, 4.000], mean observation: 0.495 [0.380, 0.600], loss: 9.351020, mean_absolute_error: 41.448265, mean_q: 52.798805
[F[K  92952/500000: episode: 1339, duration: 0.824s, episode steps: 52, steps per second: 63, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.212 [0.000, 4.000], mean observation: 0.497 [0.360, 0.610], loss: 7.752101, mean_absolute_error: 41.952415, mean_q: 53.656105
[F[K  93016/500000: episode: 1340, duration: 0.938s, episode steps: 64, steps per second: 68, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 9.182743, mean_absolute_error: 41.465157, mean_q: 53.023674
[F[K  93076/500000: episode: 1341, duration: 1.126s, episode steps: 60, steps per second: 53, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.497 [0.390, 0.610], loss: 8.235900, mean_absolute_error: 41.785576, mean_q: 53.480446
[F[K  93131/500000: episode: 1342, duration: 0.985s, episode steps: 55, steps per second: 56, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.018 [0.000, 4.000], mean observation: 0.498 [0.460, 0.560], loss: 8.489116, mean_absolute_error: 41.914379, mean_q: 53.467430
[F[K  93197/500000: episode: 1343, duration: 1.276s, episode steps: 66, steps per second: 52, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.636 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 8.963351, mean_absolute_error: 41.373154, mean_q: 52.816017
[F[K  93299/500000: episode: 1344, duration: 1.723s, episode steps: 102, steps per second: 59, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.108 [0.000, 4.000], mean observation: 0.508 [0.470, 0.590], loss: 9.034772, mean_absolute_error: 41.163181, mean_q: 52.649021
[F[K  93401/500000: episode: 1345, duration: 1.520s, episode steps: 102, steps per second: 67, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.127 [0.000, 4.000], mean observation: 0.474 [0.370, 0.530], loss: 8.623766, mean_absolute_error: 41.091339, mean_q: 52.524395
[F[K  93511/500000: episode: 1346, duration: 1.847s, episode steps: 110, steps per second: 60, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.486 [0.370, 0.540], loss: 8.334448, mean_absolute_error: 41.991108, mean_q: 53.704235
[F[K  93598/500000: episode: 1347, duration: 1.461s, episode steps: 87, steps per second: 60, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.184 [0.000, 4.000], mean observation: 0.493 [0.410, 0.540], loss: 7.800689, mean_absolute_error: 41.960262, mean_q: 53.770470
[F[K  93671/500000: episode: 1348, duration: 0.977s, episode steps: 73, steps per second: 75, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.781 [0.000, 4.000], mean observation: 0.502 [0.440, 0.570], loss: 9.809741, mean_absolute_error: 41.256390, mean_q: 52.736973
[F[K  93749/500000: episode: 1349, duration: 1.306s, episode steps: 78, steps per second: 60, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.026 [0.000, 4.000], mean observation: 0.482 [0.410, 0.510], loss: 9.081122, mean_absolute_error: 41.610310, mean_q: 53.135349
[F[K  93819/500000: episode: 1350, duration: 1.186s, episode steps: 70, steps per second: 59, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.489 [0.450, 0.510], loss: 8.461322, mean_absolute_error: 41.446636, mean_q: 52.908119
[F[K  93878/500000: episode: 1351, duration: 1.049s, episode steps: 59, steps per second: 56, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.322 [0.000, 4.000], mean observation: 0.479 [0.380, 0.520], loss: 8.232744, mean_absolute_error: 42.574051, mean_q: 54.368904
[F[K  93952/500000: episode: 1352, duration: 1.176s, episode steps: 74, steps per second: 63, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.595 [0.000, 4.000], mean observation: 0.519 [0.500, 0.580], loss: 8.604634, mean_absolute_error: 41.542324, mean_q: 53.164764
[F[K  93993/500000: episode: 1353, duration: 0.744s, episode steps: 41, steps per second: 55, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.415 [0.000, 4.000], mean observation: 0.525 [0.470, 0.630], loss: 9.566341, mean_absolute_error: 41.294563, mean_q: 52.766804
[F[K  94076/500000: episode: 1354, duration: 1.177s, episode steps: 83, steps per second: 70, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.422 [0.000, 4.000], mean observation: 0.476 [0.370, 0.530], loss: 7.919273, mean_absolute_error: 41.534019, mean_q: 53.257885
[F[K  94154/500000: episode: 1355, duration: 1.214s, episode steps: 78, steps per second: 64, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.423 [0.000, 4.000], mean observation: 0.508 [0.480, 0.550], loss: 9.023631, mean_absolute_error: 41.277851, mean_q: 52.684746
[F[K  94207/500000: episode: 1356, duration: 0.658s, episode steps: 53, steps per second: 81, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.480 [0.380, 0.520], loss: 8.593470, mean_absolute_error: 41.182308, mean_q: 52.565910
[F[K  94254/500000: episode: 1357, duration: 0.841s, episode steps: 47, steps per second: 56, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.702 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 8.602882, mean_absolute_error: 41.222347, mean_q: 52.682686
[F[K  94321/500000: episode: 1358, duration: 1.060s, episode steps: 67, steps per second: 63, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.299 [0.000, 4.000], mean observation: 0.509 [0.480, 0.550], loss: 8.316628, mean_absolute_error: 41.684547, mean_q: 53.348053
[F[K  94377/500000: episode: 1359, duration: 0.974s, episode steps: 56, steps per second: 57, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.506 [0.460, 0.630], loss: 9.420883, mean_absolute_error: 41.893940, mean_q: 53.490925
[F[K  94447/500000: episode: 1360, duration: 1.098s, episode steps: 70, steps per second: 64, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.457 [0.000, 4.000], mean observation: 0.476 [0.350, 0.520], loss: 7.865927, mean_absolute_error: 41.543945, mean_q: 53.130299
[F[K  94540/500000: episode: 1361, duration: 1.327s, episode steps: 93, steps per second: 70, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.774 [0.000, 4.000], mean observation: 0.501 [0.400, 0.620], loss: 8.193712, mean_absolute_error: 41.516701, mean_q: 53.086884
[F[K  94595/500000: episode: 1362, duration: 0.822s, episode steps: 55, steps per second: 67, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.309 [0.000, 4.000], mean observation: 0.482 [0.370, 0.520], loss: 10.231982, mean_absolute_error: 41.260120, mean_q: 52.800854
[F[K  94707/500000: episode: 1363, duration: 1.689s, episode steps: 112, steps per second: 66, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.438 [0.000, 4.000], mean observation: 0.490 [0.340, 0.550], loss: 7.851205, mean_absolute_error: 42.231365, mean_q: 53.945587
[F[K  94766/500000: episode: 1364, duration: 0.879s, episode steps: 59, steps per second: 67, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.390 [0.000, 4.000], mean observation: 0.509 [0.470, 0.610], loss: 6.863405, mean_absolute_error: 42.221733, mean_q: 53.997620
[F[K  94849/500000: episode: 1365, duration: 1.299s, episode steps: 83, steps per second: 64, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.229 [0.000, 4.000], mean observation: 0.475 [0.350, 0.520], loss: 9.231382, mean_absolute_error: 41.666786, mean_q: 53.273090
[F[K  94974/500000: episode: 1366, duration: 2.181s, episode steps: 125, steps per second: 57, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.736 [0.000, 4.000], mean observation: 0.505 [0.440, 0.620], loss: 8.963315, mean_absolute_error: 42.262039, mean_q: 54.053101
[F[K  95062/500000: episode: 1367, duration: 1.428s, episode steps: 88, steps per second: 62, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.227 [0.000, 4.000], mean observation: 0.481 [0.350, 0.520], loss: 8.825942, mean_absolute_error: 41.344952, mean_q: 52.950230
[F[K  95116/500000: episode: 1368, duration: 0.739s, episode steps: 54, steps per second: 73, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.852 [0.000, 4.000], mean observation: 0.512 [0.480, 0.610], loss: 8.631407, mean_absolute_error: 42.028839, mean_q: 53.725220
[F[K  95184/500000: episode: 1369, duration: 1.079s, episode steps: 68, steps per second: 63, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.221 [0.000, 4.000], mean observation: 0.479 [0.390, 0.510], loss: 9.570408, mean_absolute_error: 41.518803, mean_q: 53.250740
[F[K  95220/500000: episode: 1370, duration: 0.588s, episode steps: 36, steps per second: 61, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.361 [0.000, 3.000], mean observation: 0.506 [0.420, 0.650], loss: 8.287323, mean_absolute_error: 41.649300, mean_q: 53.283615
[F[K  95279/500000: episode: 1371, duration: 1.007s, episode steps: 59, steps per second: 59, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.288 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 9.498762, mean_absolute_error: 41.486584, mean_q: 53.122288
[F[K  95409/500000: episode: 1372, duration: 1.695s, episode steps: 130, steps per second: 77, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.292 [0.000, 4.000], mean observation: 0.492 [0.380, 0.550], loss: 9.810637, mean_absolute_error: 41.216000, mean_q: 52.739567
[F[K  95451/500000: episode: 1373, duration: 0.608s, episode steps: 42, steps per second: 69, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.810 [0.000, 4.000], mean observation: 0.501 [0.370, 0.650], loss: 8.108993, mean_absolute_error: 41.189735, mean_q: 52.777397
[F[K  95512/500000: episode: 1374, duration: 1.161s, episode steps: 61, steps per second: 53, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.489 [0.420, 0.530], loss: 8.726056, mean_absolute_error: 40.865898, mean_q: 52.285961
[F[K  95599/500000: episode: 1375, duration: 1.513s, episode steps: 87, steps per second: 58, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.498 [0.390, 0.590], loss: 8.958302, mean_absolute_error: 41.040085, mean_q: 52.573811
[F[K  95671/500000: episode: 1376, duration: 1.155s, episode steps: 72, steps per second: 62, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.488 [0.430, 0.520], loss: 8.635624, mean_absolute_error: 41.215557, mean_q: 52.860157
[F[K  95744/500000: episode: 1377, duration: 1.127s, episode steps: 73, steps per second: 65, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.329 [0.000, 4.000], mean observation: 0.481 [0.400, 0.520], loss: 9.020684, mean_absolute_error: 41.979248, mean_q: 53.684464
[F[K  95924/500000: episode: 1378, duration: 2.225s, episode steps: 180, steps per second: 81, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.756 [0.000, 4.000], mean observation: 0.504 [0.470, 0.540], loss: 8.430386, mean_absolute_error: 41.365017, mean_q: 52.973732
[F[K  95996/500000: episode: 1379, duration: 0.893s, episode steps: 72, steps per second: 81, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.153 [0.000, 4.000], mean observation: 0.489 [0.380, 0.530], loss: 10.104458, mean_absolute_error: 41.507027, mean_q: 53.095047
[F[K  96068/500000: episode: 1380, duration: 1.014s, episode steps: 72, steps per second: 71, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.498 [0.430, 0.570], loss: 9.243908, mean_absolute_error: 41.373447, mean_q: 52.899223
[F[K  96168/500000: episode: 1381, duration: 1.386s, episode steps: 100, steps per second: 72, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.830 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 8.138285, mean_absolute_error: 41.670807, mean_q: 53.259361
[F[K  96226/500000: episode: 1382, duration: 0.834s, episode steps: 58, steps per second: 70, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.914 [0.000, 4.000], mean observation: 0.473 [0.350, 0.510], loss: 8.508593, mean_absolute_error: 41.815174, mean_q: 53.506077
[F[K  96286/500000: episode: 1383, duration: 0.768s, episode steps: 60, steps per second: 78, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.495 [0.460, 0.520], loss: 8.858115, mean_absolute_error: 40.658974, mean_q: 52.106441
[F[K  96373/500000: episode: 1384, duration: 1.089s, episode steps: 87, steps per second: 80, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.184 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 8.288429, mean_absolute_error: 41.018749, mean_q: 52.515148
[F[K  96436/500000: episode: 1385, duration: 0.827s, episode steps: 63, steps per second: 76, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.317 [0.000, 4.000], mean observation: 0.497 [0.390, 0.580], loss: 8.832056, mean_absolute_error: 41.475536, mean_q: 53.141663
[F[K  96540/500000: episode: 1386, duration: 1.414s, episode steps: 104, steps per second: 74, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.010 [0.000, 4.000], mean observation: 0.505 [0.450, 0.560], loss: 8.376693, mean_absolute_error: 40.814846, mean_q: 52.311325
[F[K  96652/500000: episode: 1387, duration: 1.630s, episode steps: 112, steps per second: 69, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.502 [0.380, 0.660], loss: 8.820245, mean_absolute_error: 41.235249, mean_q: 52.783569
[F[K  96784/500000: episode: 1388, duration: 1.705s, episode steps: 132, steps per second: 77, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.788 [0.000, 4.000], mean observation: 0.494 [0.360, 0.560], loss: 7.849482, mean_absolute_error: 41.576153, mean_q: 53.203552
[F[K  96860/500000: episode: 1389, duration: 0.961s, episode steps: 76, steps per second: 79, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.132 [0.000, 4.000], mean observation: 0.473 [0.350, 0.530], loss: 8.880862, mean_absolute_error: 40.931751, mean_q: 52.461613
[F[K  96992/500000: episode: 1390, duration: 1.594s, episode steps: 132, steps per second: 83, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.970 [0.000, 4.000], mean observation: 0.477 [0.330, 0.530], loss: 8.277284, mean_absolute_error: 41.068394, mean_q: 52.562695
[F[K  97062/500000: episode: 1391, duration: 0.890s, episode steps: 70, steps per second: 79, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.243 [0.000, 4.000], mean observation: 0.496 [0.380, 0.580], loss: 8.275245, mean_absolute_error: 41.681290, mean_q: 53.376999
[F[K  97200/500000: episode: 1392, duration: 1.720s, episode steps: 138, steps per second: 80, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.283 [0.000, 4.000], mean observation: 0.504 [0.470, 0.550], loss: 9.048491, mean_absolute_error: 41.249229, mean_q: 52.824955
[F[K  97259/500000: episode: 1393, duration: 0.840s, episode steps: 59, steps per second: 70, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.441 [0.000, 4.000], mean observation: 0.502 [0.390, 0.640], loss: 8.811083, mean_absolute_error: 42.120052, mean_q: 53.848145
[F[K  97325/500000: episode: 1394, duration: 1.025s, episode steps: 66, steps per second: 64, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.803 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 7.509100, mean_absolute_error: 41.182602, mean_q: 52.821476
[F[K  97396/500000: episode: 1395, duration: 1.044s, episode steps: 71, steps per second: 68, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.268 [0.000, 4.000], mean observation: 0.494 [0.450, 0.520], loss: 8.122656, mean_absolute_error: 40.981640, mean_q: 52.449379
[F[K  97494/500000: episode: 1396, duration: 1.303s, episode steps: 98, steps per second: 75, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.898 [0.000, 4.000], mean observation: 0.504 [0.480, 0.540], loss: 9.307138, mean_absolute_error: 41.465801, mean_q: 52.995472
[F[K  97546/500000: episode: 1397, duration: 0.773s, episode steps: 52, steps per second: 67, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.981 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 7.757218, mean_absolute_error: 41.348148, mean_q: 52.879330
[F[K  97638/500000: episode: 1398, duration: 1.285s, episode steps: 92, steps per second: 72, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 0.504 [0.430, 0.610], loss: 9.851456, mean_absolute_error: 40.633450, mean_q: 51.889076
[F[K  97721/500000: episode: 1399, duration: 1.183s, episode steps: 83, steps per second: 70, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.145 [0.000, 4.000], mean observation: 0.498 [0.420, 0.560], loss: 8.898288, mean_absolute_error: 40.958439, mean_q: 52.453747
[F[K  97764/500000: episode: 1400, duration: 0.616s, episode steps: 43, steps per second: 70, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.349 [0.000, 4.000], mean observation: 0.499 [0.370, 0.630], loss: 6.662256, mean_absolute_error: 41.110260, mean_q: 52.728130
[F[K  97797/500000: episode: 1401, duration: 0.334s, episode steps: 33, steps per second: 99, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.527 [0.470, 0.650], loss: 9.981034, mean_absolute_error: 40.705246, mean_q: 52.233360
[F[K  97865/500000: episode: 1402, duration: 0.793s, episode steps: 68, steps per second: 86, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.294 [0.000, 4.000], mean observation: 0.475 [0.360, 0.520], loss: 7.678508, mean_absolute_error: 41.995728, mean_q: 53.721569
[F[K  97982/500000: episode: 1403, duration: 1.609s, episode steps: 117, steps per second: 73, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 0.489 [0.400, 0.520], loss: 8.685096, mean_absolute_error: 41.567654, mean_q: 53.220383
[F[K  98065/500000: episode: 1404, duration: 1.154s, episode steps: 83, steps per second: 72, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.843 [0.000, 4.000], mean observation: 0.481 [0.360, 0.530], loss: 8.517567, mean_absolute_error: 41.273479, mean_q: 52.736797
[F[K  98256/500000: episode: 1405, duration: 2.483s, episode steps: 191, steps per second: 77, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.314 [0.000, 4.000], mean observation: 0.492 [0.360, 0.590], loss: 8.452452, mean_absolute_error: 41.179993, mean_q: 52.656361
[F[K  98334/500000: episode: 1406, duration: 0.964s, episode steps: 78, steps per second: 81, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.090 [0.000, 4.000], mean observation: 0.483 [0.380, 0.520], loss: 8.063636, mean_absolute_error: 41.262333, mean_q: 52.906239
[F[K  98462/500000: episode: 1407, duration: 1.647s, episode steps: 128, steps per second: 78, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.445 [0.000, 4.000], mean observation: 0.522 [0.470, 0.680], loss: 8.120527, mean_absolute_error: 41.962418, mean_q: 53.602493
[F[K  98591/500000: episode: 1408, duration: 1.661s, episode steps: 129, steps per second: 78, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.791 [0.000, 4.000], mean observation: 0.498 [0.420, 0.550], loss: 9.100097, mean_absolute_error: 42.038891, mean_q: 53.776779
[F[K  98661/500000: episode: 1409, duration: 0.920s, episode steps: 70, steps per second: 76, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.482 [0.410, 0.520], loss: 8.338864, mean_absolute_error: 41.451958, mean_q: 52.999287
[F[K  98747/500000: episode: 1410, duration: 1.047s, episode steps: 86, steps per second: 82, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.128 [0.000, 4.000], mean observation: 0.492 [0.420, 0.520], loss: 9.262667, mean_absolute_error: 41.558636, mean_q: 53.105843
[F[K  98830/500000: episode: 1411, duration: 1.221s, episode steps: 83, steps per second: 68, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.783 [0.000, 4.000], mean observation: 0.507 [0.470, 0.600], loss: 8.485316, mean_absolute_error: 41.337658, mean_q: 52.899921
[F[K  98920/500000: episode: 1412, duration: 1.235s, episode steps: 90, steps per second: 73, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.144 [0.000, 4.000], mean observation: 0.478 [0.390, 0.530], loss: 8.404091, mean_absolute_error: 41.045319, mean_q: 52.484699
[F[K  98950/500000: episode: 1413, duration: 0.377s, episode steps: 30, steps per second: 80, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.567 [0.000, 3.000], mean observation: 0.515 [0.470, 0.650], loss: 8.824530, mean_absolute_error: 42.987705, mean_q: 55.025230
[F[K  99042/500000: episode: 1414, duration: 1.216s, episode steps: 92, steps per second: 76, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.348 [0.000, 4.000], mean observation: 0.486 [0.420, 0.520], loss: 9.160324, mean_absolute_error: 41.809948, mean_q: 53.500877
[F[K  99118/500000: episode: 1415, duration: 1.001s, episode steps: 76, steps per second: 76, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.592 [0.000, 4.000], mean observation: 0.509 [0.470, 0.560], loss: 8.594912, mean_absolute_error: 41.830704, mean_q: 53.523426
[F[K  99163/500000: episode: 1416, duration: 0.615s, episode steps: 45, steps per second: 73, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.467 [0.000, 4.000], mean observation: 0.522 [0.470, 0.620], loss: 8.502207, mean_absolute_error: 41.360386, mean_q: 52.938900
[F[K  99218/500000: episode: 1417, duration: 0.763s, episode steps: 55, steps per second: 72, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.182 [0.000, 4.000], mean observation: 0.506 [0.400, 0.630], loss: 9.435575, mean_absolute_error: 41.393646, mean_q: 52.925907
[F[K  99298/500000: episode: 1418, duration: 1.228s, episode steps: 80, steps per second: 65, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.438 [0.000, 4.000], mean observation: 0.478 [0.370, 0.510], loss: 8.674688, mean_absolute_error: 41.030010, mean_q: 52.466148
[F[K  99367/500000: episode: 1419, duration: 0.987s, episode steps: 69, steps per second: 70, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.490 [0.370, 0.530], loss: 8.219606, mean_absolute_error: 41.090744, mean_q: 52.611103
[F[K  99443/500000: episode: 1420, duration: 1.101s, episode steps: 76, steps per second: 69, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 8.374636, mean_absolute_error: 41.150082, mean_q: 52.705753
[F[K  99531/500000: episode: 1421, duration: 1.528s, episode steps: 88, steps per second: 58, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.784 [0.000, 4.000], mean observation: 0.483 [0.360, 0.510], loss: 8.769380, mean_absolute_error: 41.732857, mean_q: 53.345448
[F[K  99631/500000: episode: 1422, duration: 1.586s, episode steps: 100, steps per second: 63, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.020 [0.000, 4.000], mean observation: 0.501 [0.360, 0.670], loss: 8.435518, mean_absolute_error: 40.997196, mean_q: 52.612274
[F[K  99727/500000: episode: 1423, duration: 1.250s, episode steps: 96, steps per second: 77, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.552 [0.000, 4.000], mean observation: 0.477 [0.360, 0.540], loss: 9.046256, mean_absolute_error: 41.363113, mean_q: 53.098312
[F[K  99773/500000: episode: 1424, duration: 0.595s, episode steps: 46, steps per second: 77, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.696 [0.000, 4.000], mean observation: 0.507 [0.460, 0.620], loss: 8.737465, mean_absolute_error: 41.162319, mean_q: 52.749958
[F[K  99922/500000: episode: 1425, duration: 2.147s, episode steps: 149, steps per second: 69, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.497 [0.370, 0.600], loss: 8.757813, mean_absolute_error: 40.783184, mean_q: 52.277790
[F[K  99990/500000: episode: 1426, duration: 1.080s, episode steps: 68, steps per second: 63, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.471 [0.000, 4.000], mean observation: 0.500 [0.410, 0.590], loss: 7.547211, mean_absolute_error: 41.650570, mean_q: 53.339520
[F[K 100118/500000: episode: 1427, duration: 1.744s, episode steps: 128, steps per second: 73, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.422 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 8.043425, mean_absolute_error: 41.493469, mean_q: 53.165680
[F[K 100164/500000: episode: 1428, duration: 0.739s, episode steps: 46, steps per second: 62, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.630 [0.000, 4.000], mean observation: 0.497 [0.390, 0.570], loss: 8.245872, mean_absolute_error: 41.883972, mean_q: 53.543530
[F[K 100226/500000: episode: 1429, duration: 0.921s, episode steps: 62, steps per second: 67, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 0.508 [0.480, 0.570], loss: 8.525078, mean_absolute_error: 41.459766, mean_q: 52.962948
[F[K 100352/500000: episode: 1430, duration: 1.851s, episode steps: 126, steps per second: 68, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.897 [0.000, 4.000], mean observation: 0.499 [0.430, 0.590], loss: 8.776041, mean_absolute_error: 41.800804, mean_q: 53.454575
[F[K 100424/500000: episode: 1431, duration: 1.072s, episode steps: 72, steps per second: 67, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.485 [0.350, 0.530], loss: 9.045778, mean_absolute_error: 41.022545, mean_q: 52.463158
[F[K 100498/500000: episode: 1432, duration: 1.094s, episode steps: 74, steps per second: 68, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.494 [0.410, 0.560], loss: 8.780520, mean_absolute_error: 41.428585, mean_q: 52.964806
[F[K 100538/500000: episode: 1433, duration: 0.578s, episode steps: 40, steps per second: 69, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.527 [0.470, 0.640], loss: 8.798247, mean_absolute_error: 41.435425, mean_q: 52.976074
[F[K 100609/500000: episode: 1434, duration: 0.804s, episode steps: 71, steps per second: 88, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.488 [0.420, 0.510], loss: 7.828502, mean_absolute_error: 41.519310, mean_q: 53.176628
[F[K 100675/500000: episode: 1435, duration: 0.968s, episode steps: 66, steps per second: 68, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.212 [0.000, 4.000], mean observation: 0.506 [0.470, 0.560], loss: 8.691517, mean_absolute_error: 41.432755, mean_q: 53.058777
[F[K 100745/500000: episode: 1436, duration: 0.979s, episode steps: 70, steps per second: 72, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 0.497 [0.390, 0.580], loss: 8.385614, mean_absolute_error: 42.185131, mean_q: 53.940048
[F[K 100944/500000: episode: 1437, duration: 2.799s, episode steps: 199, steps per second: 71, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.930 [0.000, 4.000], mean observation: 0.494 [0.380, 0.540], loss: 8.059784, mean_absolute_error: 41.480713, mean_q: 53.095692
[F[K 101075/500000: episode: 1438, duration: 2.139s, episode steps: 131, steps per second: 61, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.502 [0.430, 0.600], loss: 8.714928, mean_absolute_error: 41.984482, mean_q: 53.828251
[F[K 101144/500000: episode: 1439, duration: 1.118s, episode steps: 69, steps per second: 62, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.377 [0.000, 4.000], mean observation: 0.506 [0.480, 0.580], loss: 7.860787, mean_absolute_error: 42.361904, mean_q: 54.141853
[F[K 101206/500000: episode: 1440, duration: 1.047s, episode steps: 62, steps per second: 59, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.097 [0.000, 4.000], mean observation: 0.481 [0.400, 0.520], loss: 9.435363, mean_absolute_error: 41.311260, mean_q: 52.891521
[F[K 101267/500000: episode: 1441, duration: 0.819s, episode steps: 61, steps per second: 75, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.131 [0.000, 4.000], mean observation: 0.471 [0.350, 0.520], loss: 9.124964, mean_absolute_error: 41.781395, mean_q: 53.469067
[F[K 101340/500000: episode: 1442, duration: 1.240s, episode steps: 73, steps per second: 59, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.836 [0.000, 4.000], mean observation: 0.472 [0.360, 0.510], loss: 8.240542, mean_absolute_error: 41.256577, mean_q: 52.843384
[F[K 101437/500000: episode: 1443, duration: 1.296s, episode steps: 97, steps per second: 75, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.711 [0.000, 4.000], mean observation: 0.495 [0.370, 0.570], loss: 8.485421, mean_absolute_error: 41.417221, mean_q: 53.057659
[F[K 101525/500000: episode: 1444, duration: 1.347s, episode steps: 88, steps per second: 65, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.011 [0.000, 4.000], mean observation: 0.491 [0.360, 0.570], loss: 8.300534, mean_absolute_error: 40.946873, mean_q: 52.310787
[F[K 101628/500000: episode: 1445, duration: 1.727s, episode steps: 103, steps per second: 60, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.223 [0.000, 4.000], mean observation: 0.518 [0.480, 0.650], loss: 8.021773, mean_absolute_error: 42.068069, mean_q: 53.886700
[F[K 101751/500000: episode: 1446, duration: 1.943s, episode steps: 123, steps per second: 63, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.642 [0.000, 4.000], mean observation: 0.508 [0.430, 0.620], loss: 7.935297, mean_absolute_error: 41.940205, mean_q: 53.706226
[F[K 101832/500000: episode: 1447, duration: 1.219s, episode steps: 81, steps per second: 66, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.173 [0.000, 4.000], mean observation: 0.520 [0.480, 0.600], loss: 8.158984, mean_absolute_error: 42.315624, mean_q: 54.080379
[F[K 101904/500000: episode: 1448, duration: 1.111s, episode steps: 72, steps per second: 65, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.458 [0.000, 4.000], mean observation: 0.501 [0.360, 0.630], loss: 8.298776, mean_absolute_error: 41.608757, mean_q: 53.319374
[F[K 101970/500000: episode: 1449, duration: 1.206s, episode steps: 66, steps per second: 55, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.258 [0.000, 4.000], mean observation: 0.498 [0.470, 0.530], loss: 7.289274, mean_absolute_error: 41.786716, mean_q: 53.597229
[F[K 102134/500000: episode: 1450, duration: 2.586s, episode steps: 164, steps per second: 63, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.183 [0.000, 4.000], mean observation: 0.486 [0.360, 0.550], loss: 8.171014, mean_absolute_error: 41.633404, mean_q: 53.316849
[F[K 102206/500000: episode: 1451, duration: 1.191s, episode steps: 72, steps per second: 60, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.889 [0.000, 4.000], mean observation: 0.488 [0.390, 0.520], loss: 8.006962, mean_absolute_error: 41.979206, mean_q: 53.783108
[F[K 102322/500000: episode: 1452, duration: 1.806s, episode steps: 116, steps per second: 64, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.276 [0.000, 4.000], mean observation: 0.509 [0.430, 0.680], loss: 8.705550, mean_absolute_error: 41.581818, mean_q: 53.201054
[F[K 102386/500000: episode: 1453, duration: 0.796s, episode steps: 64, steps per second: 80, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.109 [0.000, 4.000], mean observation: 0.502 [0.400, 0.650], loss: 8.576072, mean_absolute_error: 41.451817, mean_q: 53.015068
[F[K 102441/500000: episode: 1454, duration: 0.759s, episode steps: 55, steps per second: 72, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.255 [0.000, 4.000], mean observation: 0.498 [0.410, 0.600], loss: 10.188525, mean_absolute_error: 42.114132, mean_q: 53.697994
[F[K 102501/500000: episode: 1455, duration: 0.902s, episode steps: 60, steps per second: 67, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.350 [0.000, 4.000], mean observation: 0.496 [0.380, 0.580], loss: 6.364283, mean_absolute_error: 41.694344, mean_q: 53.440220
[F[K 102582/500000: episode: 1456, duration: 0.948s, episode steps: 81, steps per second: 85, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.457 [0.000, 4.000], mean observation: 0.487 [0.440, 0.530], loss: 8.675637, mean_absolute_error: 41.960915, mean_q: 53.652851
[F[K 102714/500000: episode: 1457, duration: 1.707s, episode steps: 132, steps per second: 77, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.479 [0.350, 0.530], loss: 8.585805, mean_absolute_error: 41.995029, mean_q: 53.732964
[F[K 102813/500000: episode: 1458, duration: 1.500s, episode steps: 99, steps per second: 66, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.121 [0.000, 4.000], mean observation: 0.483 [0.380, 0.530], loss: 9.517981, mean_absolute_error: 41.470081, mean_q: 53.043137
[F[K 102848/500000: episode: 1459, duration: 0.517s, episode steps: 35, steps per second: 68, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.527 [0.470, 0.640], loss: 9.171579, mean_absolute_error: 42.731018, mean_q: 54.661392
[F[K 102914/500000: episode: 1460, duration: 1.228s, episode steps: 66, steps per second: 54, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.106 [0.000, 4.000], mean observation: 0.514 [0.480, 0.630], loss: 7.715810, mean_absolute_error: 41.696312, mean_q: 53.444160
[F[K 102974/500000: episode: 1461, duration: 0.776s, episode steps: 60, steps per second: 77, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.983 [0.000, 4.000], mean observation: 0.497 [0.410, 0.540], loss: 8.097264, mean_absolute_error: 41.208019, mean_q: 52.813187
[F[K 103045/500000: episode: 1462, duration: 0.968s, episode steps: 71, steps per second: 73, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.324 [0.000, 4.000], mean observation: 0.482 [0.360, 0.530], loss: 9.271013, mean_absolute_error: 41.591396, mean_q: 53.205891
[F[K 103131/500000: episode: 1463, duration: 0.987s, episode steps: 86, steps per second: 87, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.058 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 8.946705, mean_absolute_error: 41.288017, mean_q: 52.881786
[F[K 103227/500000: episode: 1464, duration: 1.308s, episode steps: 96, steps per second: 73, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.906 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 8.789426, mean_absolute_error: 41.725540, mean_q: 53.447144
[F[K 103297/500000: episode: 1465, duration: 1.049s, episode steps: 70, steps per second: 67, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 0.481 [0.390, 0.510], loss: 7.099486, mean_absolute_error: 41.864349, mean_q: 53.714142
[F[K 103344/500000: episode: 1466, duration: 0.855s, episode steps: 47, steps per second: 55, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.723 [0.000, 4.000], mean observation: 0.521 [0.470, 0.610], loss: 8.454966, mean_absolute_error: 42.044495, mean_q: 53.904568
[F[K 103407/500000: episode: 1467, duration: 0.899s, episode steps: 63, steps per second: 70, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.381 [0.000, 4.000], mean observation: 0.509 [0.480, 0.570], loss: 7.833406, mean_absolute_error: 42.057926, mean_q: 53.860954
[F[K 103502/500000: episode: 1468, duration: 1.259s, episode steps: 95, steps per second: 75, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.147 [0.000, 4.000], mean observation: 0.494 [0.380, 0.540], loss: 9.385415, mean_absolute_error: 41.860950, mean_q: 53.606308
[F[K 103597/500000: episode: 1469, duration: 1.168s, episode steps: 95, steps per second: 81, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.274 [0.000, 4.000], mean observation: 0.508 [0.470, 0.590], loss: 8.027951, mean_absolute_error: 41.459541, mean_q: 53.145065
[F[K 103729/500000: episode: 1470, duration: 1.594s, episode steps: 132, steps per second: 83, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.644 [0.000, 4.000], mean observation: 0.509 [0.420, 0.660], loss: 8.732733, mean_absolute_error: 41.706490, mean_q: 53.373741
[F[K 103783/500000: episode: 1471, duration: 0.759s, episode steps: 54, steps per second: 71, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.093 [0.000, 4.000], mean observation: 0.503 [0.490, 0.530], loss: 9.299878, mean_absolute_error: 41.292019, mean_q: 52.885864
[F[K 103879/500000: episode: 1472, duration: 1.334s, episode steps: 96, steps per second: 72, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.365 [0.000, 4.000], mean observation: 0.485 [0.350, 0.530], loss: 8.249845, mean_absolute_error: 42.102001, mean_q: 53.865402
[F[K 104006/500000: episode: 1473, duration: 1.641s, episode steps: 127, steps per second: 77, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.961 [0.000, 4.000], mean observation: 0.469 [0.330, 0.530], loss: 7.929602, mean_absolute_error: 41.678997, mean_q: 53.328232
[F[K 104067/500000: episode: 1474, duration: 0.781s, episode steps: 61, steps per second: 78, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.508 [0.000, 4.000], mean observation: 0.516 [0.490, 0.590], loss: 9.002138, mean_absolute_error: 41.798332, mean_q: 53.487827
[F[K 104130/500000: episode: 1475, duration: 0.893s, episode steps: 63, steps per second: 71, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.206 [0.000, 4.000], mean observation: 0.499 [0.440, 0.570], loss: 8.076269, mean_absolute_error: 42.128849, mean_q: 53.946625
[F[K 104192/500000: episode: 1476, duration: 0.876s, episode steps: 62, steps per second: 71, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.952 [0.000, 4.000], mean observation: 0.523 [0.480, 0.630], loss: 8.334801, mean_absolute_error: 41.944603, mean_q: 53.691696
[F[K 104269/500000: episode: 1477, duration: 1.200s, episode steps: 77, steps per second: 64, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.299 [0.000, 4.000], mean observation: 0.496 [0.430, 0.530], loss: 8.053751, mean_absolute_error: 42.291985, mean_q: 54.080067
[F[K 104337/500000: episode: 1478, duration: 1.044s, episode steps: 68, steps per second: 65, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.985 [0.000, 4.000], mean observation: 0.502 [0.470, 0.540], loss: 8.538221, mean_absolute_error: 42.572601, mean_q: 54.512302
[F[K 104383/500000: episode: 1479, duration: 0.820s, episode steps: 46, steps per second: 56, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.174 [0.000, 4.000], mean observation: 0.523 [0.480, 0.630], loss: 8.226324, mean_absolute_error: 40.945457, mean_q: 52.541840
[F[K 104461/500000: episode: 1480, duration: 1.263s, episode steps: 78, steps per second: 62, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 0.517 [0.470, 0.620], loss: 7.106818, mean_absolute_error: 42.200397, mean_q: 54.071728
[F[K 104518/500000: episode: 1481, duration: 0.804s, episode steps: 57, steps per second: 71, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.088 [0.000, 4.000], mean observation: 0.498 [0.380, 0.590], loss: 7.911502, mean_absolute_error: 40.892891, mean_q: 52.328983
[F[K 104566/500000: episode: 1482, duration: 0.668s, episode steps: 48, steps per second: 72, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.062 [0.000, 4.000], mean observation: 0.503 [0.400, 0.610], loss: 9.303998, mean_absolute_error: 41.798237, mean_q: 53.541779
[F[K 104649/500000: episode: 1483, duration: 1.045s, episode steps: 83, steps per second: 79, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.976 [0.000, 4.000], mean observation: 0.509 [0.480, 0.540], loss: 9.009157, mean_absolute_error: 41.950985, mean_q: 53.642418
[F[K 104687/500000: episode: 1484, duration: 0.534s, episode steps: 38, steps per second: 71, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.421 [0.000, 4.000], mean observation: 0.511 [0.460, 0.640], loss: 8.108377, mean_absolute_error: 41.294426, mean_q: 52.935230
[F[K 104740/500000: episode: 1485, duration: 0.718s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.547 [0.000, 4.000], mean observation: 0.508 [0.470, 0.610], loss: 9.620701, mean_absolute_error: 41.994221, mean_q: 53.744141
[F[K 104827/500000: episode: 1486, duration: 1.052s, episode steps: 87, steps per second: 83, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.092 [0.000, 4.000], mean observation: 0.523 [0.470, 0.660], loss: 7.844765, mean_absolute_error: 41.802200, mean_q: 53.539513
[F[K 104882/500000: episode: 1487, duration: 0.707s, episode steps: 55, steps per second: 78, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.470 [0.350, 0.520], loss: 7.736769, mean_absolute_error: 41.478798, mean_q: 53.032993
[F[K 104968/500000: episode: 1488, duration: 1.225s, episode steps: 86, steps per second: 70, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.547 [0.000, 4.000], mean observation: 0.489 [0.420, 0.530], loss: 8.917152, mean_absolute_error: 41.653934, mean_q: 53.339108
[F[K 105055/500000: episode: 1489, duration: 1.228s, episode steps: 87, steps per second: 71, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.954 [0.000, 4.000], mean observation: 0.501 [0.460, 0.530], loss: 7.622829, mean_absolute_error: 42.106068, mean_q: 53.900352
[F[K 105230/500000: episode: 1490, duration: 2.438s, episode steps: 175, steps per second: 72, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.177 [0.000, 4.000], mean observation: 0.500 [0.340, 0.630], loss: 8.111094, mean_absolute_error: 41.836094, mean_q: 53.581654
[F[K 105317/500000: episode: 1491, duration: 1.229s, episode steps: 87, steps per second: 71, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.264 [0.000, 4.000], mean observation: 0.499 [0.470, 0.550], loss: 8.498372, mean_absolute_error: 42.334244, mean_q: 54.198544
[F[K 105401/500000: episode: 1492, duration: 1.200s, episode steps: 84, steps per second: 70, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.798 [0.000, 4.000], mean observation: 0.515 [0.460, 0.580], loss: 8.143128, mean_absolute_error: 42.080219, mean_q: 53.872662
[F[K 105592/500000: episode: 1493, duration: 2.714s, episode steps: 191, steps per second: 70, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.513 [0.420, 0.720], loss: 9.315015, mean_absolute_error: 41.709942, mean_q: 53.372433
[F[K 105648/500000: episode: 1494, duration: 0.751s, episode steps: 56, steps per second: 75, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.482 [0.000, 4.000], mean observation: 0.508 [0.430, 0.600], loss: 8.643120, mean_absolute_error: 42.216732, mean_q: 53.999500
[F[K 105692/500000: episode: 1495, duration: 0.589s, episode steps: 44, steps per second: 75, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.504 [0.390, 0.640], loss: 9.625306, mean_absolute_error: 41.518784, mean_q: 53.228695
[F[K 105775/500000: episode: 1496, duration: 1.122s, episode steps: 83, steps per second: 74, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.952 [0.000, 4.000], mean observation: 0.517 [0.490, 0.590], loss: 9.275716, mean_absolute_error: 41.272686, mean_q: 52.755238
[F[K 105857/500000: episode: 1497, duration: 1.062s, episode steps: 82, steps per second: 77, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.329 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 8.482515, mean_absolute_error: 42.319530, mean_q: 54.114929
[F[K 105920/500000: episode: 1498, duration: 0.897s, episode steps: 63, steps per second: 70, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.651 [0.000, 4.000], mean observation: 0.479 [0.380, 0.530], loss: 10.215514, mean_absolute_error: 41.697845, mean_q: 53.285801
[F[K 106065/500000: episode: 1499, duration: 2.095s, episode steps: 145, steps per second: 69, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.414 [0.000, 4.000], mean observation: 0.505 [0.430, 0.610], loss: 8.329498, mean_absolute_error: 41.626995, mean_q: 53.371292
[F[K 106155/500000: episode: 1500, duration: 1.460s, episode steps: 90, steps per second: 62, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.505 [0.470, 0.570], loss: 9.596446, mean_absolute_error: 41.788025, mean_q: 53.418625
[F[K 106241/500000: episode: 1501, duration: 1.464s, episode steps: 86, steps per second: 59, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.267 [0.000, 4.000], mean observation: 0.521 [0.490, 0.610], loss: 9.012084, mean_absolute_error: 41.808655, mean_q: 53.473217
[F[K 106288/500000: episode: 1502, duration: 0.733s, episode steps: 47, steps per second: 64, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.574 [0.000, 4.000], mean observation: 0.510 [0.470, 0.620], loss: 9.330798, mean_absolute_error: 41.713703, mean_q: 53.489849
[F[K 106362/500000: episode: 1503, duration: 1.189s, episode steps: 74, steps per second: 62, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.784 [0.000, 3.000], mean observation: 0.501 [0.420, 0.620], loss: 9.410461, mean_absolute_error: 41.785019, mean_q: 53.494652
[F[K 106439/500000: episode: 1504, duration: 1.126s, episode steps: 77, steps per second: 68, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.496 [0.440, 0.530], loss: 8.531407, mean_absolute_error: 41.848206, mean_q: 53.562359
[F[K 106516/500000: episode: 1505, duration: 0.975s, episode steps: 77, steps per second: 79, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.485 [0.370, 0.530], loss: 8.793178, mean_absolute_error: 41.472149, mean_q: 53.089119
[F[K 106608/500000: episode: 1506, duration: 1.068s, episode steps: 92, steps per second: 86, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.076 [0.000, 4.000], mean observation: 0.497 [0.400, 0.540], loss: 8.720675, mean_absolute_error: 41.260071, mean_q: 52.877026
[F[K 106694/500000: episode: 1507, duration: 1.282s, episode steps: 86, steps per second: 67, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.093 [0.000, 4.000], mean observation: 0.483 [0.380, 0.530], loss: 8.421416, mean_absolute_error: 42.105518, mean_q: 53.912029
[F[K 106788/500000: episode: 1508, duration: 1.338s, episode steps: 94, steps per second: 70, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.234 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 9.245012, mean_absolute_error: 41.188335, mean_q: 52.817181
[F[K 106885/500000: episode: 1509, duration: 1.310s, episode steps: 97, steps per second: 74, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.474 [0.000, 4.000], mean observation: 0.516 [0.480, 0.630], loss: 9.062288, mean_absolute_error: 41.727840, mean_q: 53.406490
[F[K 106972/500000: episode: 1510, duration: 1.386s, episode steps: 87, steps per second: 63, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.195 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 8.469184, mean_absolute_error: 42.081272, mean_q: 53.917442
[F[K 107044/500000: episode: 1511, duration: 1.091s, episode steps: 72, steps per second: 66, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.499 [0.430, 0.560], loss: 8.630425, mean_absolute_error: 41.815937, mean_q: 53.564590
[F[K 107099/500000: episode: 1512, duration: 0.777s, episode steps: 55, steps per second: 71, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.511 [0.480, 0.610], loss: 8.515606, mean_absolute_error: 41.479797, mean_q: 53.161720
[F[K 107182/500000: episode: 1513, duration: 1.166s, episode steps: 83, steps per second: 71, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.229 [0.000, 4.000], mean observation: 0.488 [0.380, 0.530], loss: 9.429414, mean_absolute_error: 41.983398, mean_q: 53.763500
[F[K 107254/500000: episode: 1514, duration: 1.025s, episode steps: 72, steps per second: 70, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.528 [0.000, 4.000], mean observation: 0.483 [0.370, 0.530], loss: 8.613132, mean_absolute_error: 41.835625, mean_q: 53.572590
[F[K 107313/500000: episode: 1515, duration: 0.981s, episode steps: 59, steps per second: 60, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.576 [0.000, 4.000], mean observation: 0.512 [0.500, 0.590], loss: 9.101983, mean_absolute_error: 42.057083, mean_q: 53.881920
[F[K 107377/500000: episode: 1516, duration: 1.027s, episode steps: 64, steps per second: 62, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.922 [0.000, 4.000], mean observation: 0.495 [0.360, 0.590], loss: 8.917761, mean_absolute_error: 41.552086, mean_q: 53.196854
[F[K 107450/500000: episode: 1517, duration: 1.106s, episode steps: 73, steps per second: 66, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.517 [0.500, 0.630], loss: 8.545275, mean_absolute_error: 42.001652, mean_q: 53.784916
[F[K 107522/500000: episode: 1518, duration: 1.159s, episode steps: 72, steps per second: 62, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.056 [0.000, 4.000], mean observation: 0.486 [0.390, 0.510], loss: 8.223105, mean_absolute_error: 41.484886, mean_q: 53.251369
[F[K 107587/500000: episode: 1519, duration: 0.956s, episode steps: 65, steps per second: 68, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.185 [0.000, 4.000], mean observation: 0.520 [0.490, 0.600], loss: 9.260225, mean_absolute_error: 41.542713, mean_q: 53.228760
[F[K 107678/500000: episode: 1520, duration: 1.343s, episode steps: 91, steps per second: 68, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.121 [0.000, 4.000], mean observation: 0.491 [0.420, 0.520], loss: 8.864476, mean_absolute_error: 41.877697, mean_q: 53.567001
[F[K 107732/500000: episode: 1521, duration: 0.804s, episode steps: 54, steps per second: 67, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.852 [0.000, 4.000], mean observation: 0.515 [0.470, 0.620], loss: 8.545146, mean_absolute_error: 41.778244, mean_q: 53.421978
[F[K 107785/500000: episode: 1522, duration: 0.885s, episode steps: 53, steps per second: 60, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.585 [0.000, 3.000], mean observation: 0.501 [0.380, 0.640], loss: 9.887103, mean_absolute_error: 41.130196, mean_q: 52.624519
[F[K 107952/500000: episode: 1523, duration: 2.612s, episode steps: 167, steps per second: 64, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 8.410671, mean_absolute_error: 42.313061, mean_q: 54.160664
[F[K 108019/500000: episode: 1524, duration: 0.941s, episode steps: 67, steps per second: 71, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.731 [0.000, 4.000], mean observation: 0.500 [0.430, 0.550], loss: 7.279196, mean_absolute_error: 41.768929, mean_q: 53.440422
[F[K 108074/500000: episode: 1525, duration: 0.859s, episode steps: 55, steps per second: 64, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.891 [0.000, 4.000], mean observation: 0.498 [0.430, 0.530], loss: 8.342525, mean_absolute_error: 42.276222, mean_q: 54.104458
[F[K 108142/500000: episode: 1526, duration: 1.056s, episode steps: 68, steps per second: 64, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.588 [0.000, 4.000], mean observation: 0.488 [0.390, 0.530], loss: 8.254763, mean_absolute_error: 42.410717, mean_q: 54.377697
[F[K 108211/500000: episode: 1527, duration: 0.986s, episode steps: 69, steps per second: 70, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.768 [0.000, 4.000], mean observation: 0.496 [0.370, 0.580], loss: 7.543508, mean_absolute_error: 41.707954, mean_q: 53.359158
[F[K 108279/500000: episode: 1528, duration: 1.128s, episode steps: 68, steps per second: 60, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.838 [0.000, 4.000], mean observation: 0.502 [0.460, 0.550], loss: 8.945714, mean_absolute_error: 41.732864, mean_q: 53.316986
[F[K 108371/500000: episode: 1529, duration: 1.605s, episode steps: 92, steps per second: 57, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.728 [0.000, 4.000], mean observation: 0.490 [0.350, 0.560], loss: 8.693404, mean_absolute_error: 42.227074, mean_q: 53.915035
[F[K 108457/500000: episode: 1530, duration: 1.504s, episode steps: 86, steps per second: 57, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.942 [0.000, 4.000], mean observation: 0.500 [0.420, 0.610], loss: 8.398760, mean_absolute_error: 41.333786, mean_q: 52.929203
[F[K 108522/500000: episode: 1531, duration: 1.015s, episode steps: 65, steps per second: 64, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.631 [0.000, 4.000], mean observation: 0.503 [0.460, 0.550], loss: 8.055435, mean_absolute_error: 41.208546, mean_q: 52.850971
[F[K 108603/500000: episode: 1532, duration: 1.377s, episode steps: 81, steps per second: 59, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.951 [0.000, 4.000], mean observation: 0.485 [0.350, 0.520], loss: 8.484342, mean_absolute_error: 42.237961, mean_q: 54.094261
[F[K 108677/500000: episode: 1533, duration: 1.190s, episode steps: 74, steps per second: 62, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.338 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 9.389102, mean_absolute_error: 41.999798, mean_q: 53.631367
[F[K 108771/500000: episode: 1534, duration: 1.441s, episode steps: 94, steps per second: 65, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.532 [0.000, 4.000], mean observation: 0.513 [0.470, 0.590], loss: 9.161887, mean_absolute_error: 41.888977, mean_q: 53.638958
[F[K 108941/500000: episode: 1535, duration: 2.819s, episode steps: 170, steps per second: 60, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 0.486 [0.340, 0.540], loss: 8.568439, mean_absolute_error: 41.657734, mean_q: 53.396954
[F[K 109000/500000: episode: 1536, duration: 1.137s, episode steps: 59, steps per second: 52, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.424 [0.000, 4.000], mean observation: 0.496 [0.360, 0.600], loss: 9.000000, mean_absolute_error: 41.538204, mean_q: 53.161755
[F[K 109110/500000: episode: 1537, duration: 1.765s, episode steps: 110, steps per second: 62, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.973 [0.000, 4.000], mean observation: 0.499 [0.450, 0.540], loss: 8.518167, mean_absolute_error: 42.271194, mean_q: 54.211906
[F[K 109197/500000: episode: 1538, duration: 1.548s, episode steps: 87, steps per second: 56, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.770 [0.000, 4.000], mean observation: 0.496 [0.420, 0.530], loss: 8.461242, mean_absolute_error: 42.000439, mean_q: 53.682079
[F[K 109281/500000: episode: 1539, duration: 1.483s, episode steps: 84, steps per second: 57, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.357 [0.000, 4.000], mean observation: 0.501 [0.360, 0.640], loss: 9.543650, mean_absolute_error: 41.780804, mean_q: 53.492962
[F[K 109349/500000: episode: 1540, duration: 1.042s, episode steps: 68, steps per second: 65, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.309 [0.000, 4.000], mean observation: 0.493 [0.390, 0.530], loss: 9.628560, mean_absolute_error: 41.716747, mean_q: 53.423767
[F[K 109464/500000: episode: 1541, duration: 1.926s, episode steps: 115, steps per second: 60, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.509 [0.480, 0.600], loss: 7.483738, mean_absolute_error: 41.965279, mean_q: 53.772060
[F[K 109498/500000: episode: 1542, duration: 0.556s, episode steps: 34, steps per second: 61, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.647 [0.000, 3.000], mean observation: 0.512 [0.470, 0.640], loss: 8.463220, mean_absolute_error: 42.666718, mean_q: 54.729469
[F[K 109568/500000: episode: 1543, duration: 1.130s, episode steps: 70, steps per second: 62, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.514 [0.000, 4.000], mean observation: 0.504 [0.400, 0.600], loss: 8.857396, mean_absolute_error: 42.596947, mean_q: 54.606861
[F[K 109631/500000: episode: 1544, duration: 1.225s, episode steps: 63, steps per second: 51, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.476 [0.360, 0.520], loss: 8.969480, mean_absolute_error: 41.987732, mean_q: 53.735344
[F[K 109741/500000: episode: 1545, duration: 1.888s, episode steps: 110, steps per second: 58, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.627 [0.000, 4.000], mean observation: 0.513 [0.490, 0.590], loss: 8.352123, mean_absolute_error: 41.750092, mean_q: 53.474880
[F[K 109832/500000: episode: 1546, duration: 1.581s, episode steps: 91, steps per second: 58, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.758 [0.000, 4.000], mean observation: 0.508 [0.470, 0.590], loss: 9.414008, mean_absolute_error: 41.922123, mean_q: 53.593525
[F[K 109892/500000: episode: 1547, duration: 1.163s, episode steps: 60, steps per second: 52, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.490 [0.400, 0.530], loss: 8.773555, mean_absolute_error: 42.119308, mean_q: 53.952675
[F[K 109949/500000: episode: 1548, duration: 1.004s, episode steps: 57, steps per second: 57, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.632 [0.000, 4.000], mean observation: 0.515 [0.480, 0.590], loss: 8.916449, mean_absolute_error: 41.703384, mean_q: 53.301418
[F[K 110037/500000: episode: 1549, duration: 1.285s, episode steps: 88, steps per second: 68, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.193 [0.000, 4.000], mean observation: 0.507 [0.470, 0.550], loss: 8.588099, mean_absolute_error: 42.299366, mean_q: 54.223667
[F[K 110121/500000: episode: 1550, duration: 1.314s, episode steps: 84, steps per second: 64, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 9.399279, mean_absolute_error: 41.974327, mean_q: 53.705212
[F[K 110183/500000: episode: 1551, duration: 0.916s, episode steps: 62, steps per second: 68, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.498 [0.370, 0.590], loss: 8.359341, mean_absolute_error: 42.151302, mean_q: 53.969059
[F[K 110224/500000: episode: 1552, duration: 0.619s, episode steps: 41, steps per second: 66, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.707 [0.000, 4.000], mean observation: 0.516 [0.470, 0.650], loss: 8.373168, mean_absolute_error: 42.981827, mean_q: 55.070969
[F[K 110298/500000: episode: 1553, duration: 1.279s, episode steps: 74, steps per second: 58, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.662 [0.000, 4.000], mean observation: 0.505 [0.470, 0.580], loss: 7.129295, mean_absolute_error: 41.700722, mean_q: 53.357716
[F[K 110345/500000: episode: 1554, duration: 0.770s, episode steps: 47, steps per second: 61, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.553 [0.000, 4.000], mean observation: 0.506 [0.420, 0.630], loss: 8.096516, mean_absolute_error: 41.003738, mean_q: 52.506741
[F[K 110391/500000: episode: 1555, duration: 0.754s, episode steps: 46, steps per second: 61, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.217 [0.000, 4.000], mean observation: 0.516 [0.470, 0.600], loss: 7.988678, mean_absolute_error: 42.311607, mean_q: 54.260212
[F[K 110454/500000: episode: 1556, duration: 1.077s, episode steps: 63, steps per second: 59, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.794 [0.000, 4.000], mean observation: 0.496 [0.380, 0.610], loss: 8.303231, mean_absolute_error: 41.672707, mean_q: 53.374935
[F[K 110519/500000: episode: 1557, duration: 0.988s, episode steps: 65, steps per second: 66, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.923 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 8.351447, mean_absolute_error: 41.847950, mean_q: 53.670132
[F[K 110607/500000: episode: 1558, duration: 1.488s, episode steps: 88, steps per second: 59, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.375 [0.000, 4.000], mean observation: 0.474 [0.350, 0.530], loss: 8.442652, mean_absolute_error: 41.789394, mean_q: 53.535183
[F[K 110749/500000: episode: 1559, duration: 2.383s, episode steps: 142, steps per second: 60, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.359 [0.000, 4.000], mean observation: 0.497 [0.420, 0.550], loss: 8.750794, mean_absolute_error: 41.915092, mean_q: 53.728294
[F[K 110802/500000: episode: 1560, duration: 0.867s, episode steps: 53, steps per second: 61, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.505 [0.400, 0.630], loss: 9.934606, mean_absolute_error: 41.350304, mean_q: 52.951279
[F[K 110948/500000: episode: 1561, duration: 2.443s, episode steps: 146, steps per second: 60, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.048 [0.000, 4.000], mean observation: 0.484 [0.410, 0.530], loss: 8.327137, mean_absolute_error: 41.745972, mean_q: 53.558781
[F[K 110996/500000: episode: 1562, duration: 0.870s, episode steps: 48, steps per second: 55, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.229 [0.000, 4.000], mean observation: 0.510 [0.480, 0.580], loss: 8.126272, mean_absolute_error: 40.946926, mean_q: 52.609241
[F[K 111062/500000: episode: 1563, duration: 1.175s, episode steps: 66, steps per second: 56, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.503 [0.460, 0.530], loss: 8.227185, mean_absolute_error: 42.174404, mean_q: 54.039913
[F[K 111196/500000: episode: 1564, duration: 2.346s, episode steps: 134, steps per second: 57, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.261 [0.000, 4.000], mean observation: 0.509 [0.470, 0.600], loss: 8.191605, mean_absolute_error: 41.764488, mean_q: 53.509548
[F[K 111250/500000: episode: 1565, duration: 0.978s, episode steps: 54, steps per second: 55, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.648 [0.000, 4.000], mean observation: 0.490 [0.360, 0.530], loss: 8.878793, mean_absolute_error: 41.266460, mean_q: 52.938679
[F[K 111391/500000: episode: 1566, duration: 2.189s, episode steps: 141, steps per second: 64, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.340 [0.000, 4.000], mean observation: 0.479 [0.390, 0.530], loss: 9.486330, mean_absolute_error: 42.356907, mean_q: 54.214138
[F[K 111431/500000: episode: 1567, duration: 0.584s, episode steps: 40, steps per second: 68, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.625 [0.000, 4.000], mean observation: 0.518 [0.470, 0.640], loss: 8.091080, mean_absolute_error: 41.694843, mean_q: 53.414642
[F[K 111516/500000: episode: 1568, duration: 1.032s, episode steps: 85, steps per second: 82, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.489 [0.390, 0.530], loss: 8.022515, mean_absolute_error: 42.029148, mean_q: 53.905090
[F[K 111587/500000: episode: 1569, duration: 1.083s, episode steps: 71, steps per second: 66, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.282 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 9.272305, mean_absolute_error: 41.831341, mean_q: 53.708710
[F[K 111650/500000: episode: 1570, duration: 0.811s, episode steps: 63, steps per second: 78, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.698 [0.000, 4.000], mean observation: 0.498 [0.370, 0.580], loss: 9.657003, mean_absolute_error: 41.539417, mean_q: 53.188828
[F[K 111789/500000: episode: 1571, duration: 1.896s, episode steps: 139, steps per second: 73, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.813 [0.000, 4.000], mean observation: 0.506 [0.490, 0.550], loss: 8.993267, mean_absolute_error: 41.805824, mean_q: 53.545006
[F[K 111907/500000: episode: 1572, duration: 1.769s, episode steps: 118, steps per second: 67, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.771 [0.000, 4.000], mean observation: 0.500 [0.440, 0.550], loss: 9.121021, mean_absolute_error: 41.711670, mean_q: 53.406376
[F[K 111963/500000: episode: 1573, duration: 0.771s, episode steps: 56, steps per second: 73, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.768 [0.000, 4.000], mean observation: 0.509 [0.470, 0.620], loss: 8.200045, mean_absolute_error: 42.189434, mean_q: 54.046413
[F[K 112081/500000: episode: 1574, duration: 1.633s, episode steps: 118, steps per second: 72, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.144 [0.000, 4.000], mean observation: 0.509 [0.430, 0.620], loss: 7.697766, mean_absolute_error: 42.423210, mean_q: 54.351387
[F[K 112168/500000: episode: 1575, duration: 1.045s, episode steps: 87, steps per second: 83, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.480 [0.370, 0.530], loss: 8.924935, mean_absolute_error: 41.661583, mean_q: 53.279129
[F[K 112248/500000: episode: 1576, duration: 1.058s, episode steps: 80, steps per second: 76, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.438 [0.000, 4.000], mean observation: 0.506 [0.470, 0.580], loss: 8.043281, mean_absolute_error: 41.541145, mean_q: 53.195000
[F[K 112348/500000: episode: 1577, duration: 1.188s, episode steps: 100, steps per second: 84, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.050 [0.000, 4.000], mean observation: 0.511 [0.450, 0.580], loss: 9.741253, mean_absolute_error: 41.773350, mean_q: 53.445065
[F[K 112397/500000: episode: 1578, duration: 0.560s, episode steps: 49, steps per second: 87, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.082 [0.000, 4.000], mean observation: 0.521 [0.470, 0.610], loss: 8.038195, mean_absolute_error: 41.782475, mean_q: 53.406590
[F[K 112455/500000: episode: 1579, duration: 0.626s, episode steps: 58, steps per second: 93, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.259 [0.000, 4.000], mean observation: 0.509 [0.440, 0.610], loss: 8.731834, mean_absolute_error: 41.549122, mean_q: 53.162109
[F[K 112522/500000: episode: 1580, duration: 0.736s, episode steps: 67, steps per second: 91, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.015 [0.000, 4.000], mean observation: 0.487 [0.410, 0.540], loss: 8.726113, mean_absolute_error: 41.334049, mean_q: 52.960201
[F[K 112658/500000: episode: 1581, duration: 1.617s, episode steps: 136, steps per second: 84, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.978 [0.000, 4.000], mean observation: 0.508 [0.430, 0.670], loss: 8.256893, mean_absolute_error: 41.704361, mean_q: 53.484104
[F[K 112712/500000: episode: 1582, duration: 0.691s, episode steps: 54, steps per second: 78, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.944 [0.000, 4.000], mean observation: 0.508 [0.470, 0.610], loss: 8.824457, mean_absolute_error: 41.521816, mean_q: 53.112827
[F[K 112787/500000: episode: 1583, duration: 0.966s, episode steps: 75, steps per second: 78, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.813 [0.000, 4.000], mean observation: 0.511 [0.480, 0.580], loss: 8.840781, mean_absolute_error: 42.384552, mean_q: 54.039944
[F[K 112819/500000: episode: 1584, duration: 0.340s, episode steps: 32, steps per second: 94, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.562 [0.000, 4.000], mean observation: 0.528 [0.470, 0.640], loss: 7.752532, mean_absolute_error: 42.016769, mean_q: 53.811050
[F[K 112890/500000: episode: 1585, duration: 0.845s, episode steps: 71, steps per second: 84, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.511 [0.490, 0.590], loss: 7.994705, mean_absolute_error: 41.089680, mean_q: 52.736500
[F[K 112936/500000: episode: 1586, duration: 0.620s, episode steps: 46, steps per second: 74, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.370 [0.000, 4.000], mean observation: 0.509 [0.450, 0.630], loss: 7.472514, mean_absolute_error: 42.014687, mean_q: 53.807266
[F[K 113009/500000: episode: 1587, duration: 0.819s, episode steps: 73, steps per second: 89, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.205 [0.000, 4.000], mean observation: 0.480 [0.400, 0.520], loss: 9.197968, mean_absolute_error: 41.545544, mean_q: 53.118511
[F[K 113108/500000: episode: 1588, duration: 1.384s, episode steps: 99, steps per second: 72, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.511 [0.470, 0.640], loss: 9.144773, mean_absolute_error: 42.117981, mean_q: 53.978157
[F[K 113151/500000: episode: 1589, duration: 0.569s, episode steps: 43, steps per second: 76, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.488 [0.000, 3.000], mean observation: 0.524 [0.470, 0.630], loss: 9.635397, mean_absolute_error: 41.617199, mean_q: 53.066906
[F[K 113208/500000: episode: 1590, duration: 0.688s, episode steps: 57, steps per second: 83, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.982 [0.000, 4.000], mean observation: 0.485 [0.410, 0.510], loss: 9.235918, mean_absolute_error: 41.276585, mean_q: 52.795387
[F[K 113280/500000: episode: 1591, duration: 0.860s, episode steps: 72, steps per second: 84, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.475 [0.370, 0.510], loss: 8.182933, mean_absolute_error: 41.459671, mean_q: 53.087818
[F[K 113342/500000: episode: 1592, duration: 0.800s, episode steps: 62, steps per second: 77, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.565 [0.000, 4.000], mean observation: 0.519 [0.500, 0.580], loss: 7.892147, mean_absolute_error: 41.391075, mean_q: 53.006798
[F[K 113400/500000: episode: 1593, duration: 0.743s, episode steps: 58, steps per second: 78, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.490 [0.440, 0.530], loss: 8.988943, mean_absolute_error: 42.583065, mean_q: 54.639626
[F[K 113478/500000: episode: 1594, duration: 0.958s, episode steps: 78, steps per second: 81, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.821 [0.000, 4.000], mean observation: 0.505 [0.410, 0.620], loss: 9.200310, mean_absolute_error: 41.676319, mean_q: 53.138191
[F[K 113551/500000: episode: 1595, duration: 0.861s, episode steps: 73, steps per second: 85, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.497 [0.380, 0.570], loss: 10.423264, mean_absolute_error: 40.781246, mean_q: 52.217056
[F[K 113751/500000: episode: 1596, duration: 2.253s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.140 [0.000, 4.000], mean observation: 0.511 [0.460, 0.590], loss: 8.861175, mean_absolute_error: 42.134109, mean_q: 53.937943
[F[K 113824/500000: episode: 1597, duration: 0.944s, episode steps: 73, steps per second: 77, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.476 [0.380, 0.510], loss: 7.794166, mean_absolute_error: 41.711578, mean_q: 53.420948
[F[K 113885/500000: episode: 1598, duration: 0.758s, episode steps: 61, steps per second: 80, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.721 [0.000, 4.000], mean observation: 0.504 [0.490, 0.530], loss: 8.865767, mean_absolute_error: 42.601112, mean_q: 54.472218
[F[K 113997/500000: episode: 1599, duration: 1.238s, episode steps: 112, steps per second: 90, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.723 [0.000, 4.000], mean observation: 0.500 [0.400, 0.580], loss: 8.740991, mean_absolute_error: 41.650661, mean_q: 53.388897
[F[K 114044/500000: episode: 1600, duration: 0.449s, episode steps: 47, steps per second: 105, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.809 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 8.650445, mean_absolute_error: 42.098240, mean_q: 53.854607
[F[K 114089/500000: episode: 1601, duration: 0.521s, episode steps: 45, steps per second: 86, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.578 [0.000, 4.000], mean observation: 0.524 [0.470, 0.640], loss: 7.952625, mean_absolute_error: 42.411957, mean_q: 54.376045
[F[K 114289/500000: episode: 1602, duration: 2.217s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.355 [0.000, 4.000], mean observation: 0.496 [0.350, 0.650], loss: 8.386300, mean_absolute_error: 41.728935, mean_q: 53.467846
[F[K 114388/500000: episode: 1603, duration: 1.116s, episode steps: 99, steps per second: 89, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.497 [0.460, 0.530], loss: 7.958208, mean_absolute_error: 41.403507, mean_q: 53.100941
[F[K 114473/500000: episode: 1604, duration: 0.929s, episode steps: 85, steps per second: 91, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.094 [0.000, 4.000], mean observation: 0.507 [0.450, 0.650], loss: 8.395392, mean_absolute_error: 41.436073, mean_q: 53.025879
[F[K 114508/500000: episode: 1605, duration: 0.469s, episode steps: 35, steps per second: 75, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.229 [0.000, 4.000], mean observation: 0.514 [0.470, 0.640], loss: 8.100722, mean_absolute_error: 42.301842, mean_q: 54.210117
[F[K 114554/500000: episode: 1606, duration: 0.589s, episode steps: 46, steps per second: 78, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.261 [0.000, 4.000], mean observation: 0.501 [0.400, 0.610], loss: 9.117208, mean_absolute_error: 41.708874, mean_q: 53.289074
[F[K 114592/500000: episode: 1607, duration: 0.497s, episode steps: 38, steps per second: 76, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.737 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 8.879227, mean_absolute_error: 42.718513, mean_q: 54.653629
[F[K 114627/500000: episode: 1608, duration: 0.515s, episode steps: 35, steps per second: 68, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.343 [0.000, 3.000], mean observation: 0.524 [0.470, 0.660], loss: 11.081144, mean_absolute_error: 41.991646, mean_q: 53.791058
[F[K 114699/500000: episode: 1609, duration: 0.878s, episode steps: 72, steps per second: 82, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 0.493 [0.420, 0.540], loss: 8.560236, mean_absolute_error: 41.835274, mean_q: 53.623379
[F[K 114755/500000: episode: 1610, duration: 0.635s, episode steps: 56, steps per second: 88, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.732 [0.000, 4.000], mean observation: 0.518 [0.490, 0.610], loss: 7.920639, mean_absolute_error: 41.945282, mean_q: 53.739689
[F[K 114805/500000: episode: 1611, duration: 0.587s, episode steps: 50, steps per second: 85, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.680 [0.000, 4.000], mean observation: 0.496 [0.410, 0.560], loss: 9.334740, mean_absolute_error: 41.231293, mean_q: 52.747292
[F[K 114899/500000: episode: 1612, duration: 1.044s, episode steps: 94, steps per second: 90, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.553 [0.000, 4.000], mean observation: 0.494 [0.460, 0.530], loss: 7.855247, mean_absolute_error: 41.405529, mean_q: 52.949978
[F[K 115020/500000: episode: 1613, duration: 1.612s, episode steps: 121, steps per second: 75, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.950 [0.000, 4.000], mean observation: 0.502 [0.380, 0.640], loss: 7.966654, mean_absolute_error: 42.049961, mean_q: 53.872334
[F[K 115167/500000: episode: 1614, duration: 1.730s, episode steps: 147, steps per second: 85, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.612 [0.000, 4.000], mean observation: 0.525 [0.480, 0.650], loss: 8.977594, mean_absolute_error: 41.581505, mean_q: 53.194077
[F[K 115223/500000: episode: 1615, duration: 0.668s, episode steps: 56, steps per second: 84, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.786 [0.000, 4.000], mean observation: 0.501 [0.390, 0.610], loss: 8.591305, mean_absolute_error: 42.400871, mean_q: 54.380424
[F[K 115269/500000: episode: 1616, duration: 0.568s, episode steps: 46, steps per second: 81, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.304 [0.000, 4.000], mean observation: 0.521 [0.480, 0.600], loss: 8.535254, mean_absolute_error: 41.281948, mean_q: 52.789131
[F[K 115334/500000: episode: 1617, duration: 0.774s, episode steps: 65, steps per second: 84, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.785 [0.000, 4.000], mean observation: 0.511 [0.460, 0.610], loss: 8.337811, mean_absolute_error: 41.933353, mean_q: 53.820404
[F[K 115379/500000: episode: 1618, duration: 0.580s, episode steps: 45, steps per second: 78, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.844 [0.000, 4.000], mean observation: 0.496 [0.380, 0.550], loss: 7.694757, mean_absolute_error: 42.347397, mean_q: 54.204449
[F[K 115432/500000: episode: 1619, duration: 0.716s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.491 [0.390, 0.530], loss: 6.910498, mean_absolute_error: 41.734360, mean_q: 53.595955
[F[K 115508/500000: episode: 1620, duration: 0.854s, episode steps: 76, steps per second: 89, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.132 [0.000, 4.000], mean observation: 0.501 [0.490, 0.530], loss: 8.713868, mean_absolute_error: 42.452991, mean_q: 54.354813
[F[K 115615/500000: episode: 1621, duration: 1.270s, episode steps: 107, steps per second: 84, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.402 [0.000, 4.000], mean observation: 0.517 [0.470, 0.670], loss: 8.384278, mean_absolute_error: 42.283733, mean_q: 54.137138
[F[K 115684/500000: episode: 1622, duration: 0.729s, episode steps: 69, steps per second: 95, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.275 [0.000, 4.000], mean observation: 0.482 [0.360, 0.530], loss: 7.799792, mean_absolute_error: 41.987362, mean_q: 53.739468
[F[K 115773/500000: episode: 1623, duration: 1.069s, episode steps: 89, steps per second: 83, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.067 [0.000, 4.000], mean observation: 0.498 [0.380, 0.620], loss: 8.324969, mean_absolute_error: 41.897263, mean_q: 53.667721
[F[K 115804/500000: episode: 1624, duration: 0.395s, episode steps: 31, steps per second: 79, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.548 [0.000, 4.000], mean observation: 0.527 [0.470, 0.650], loss: 9.002177, mean_absolute_error: 41.184151, mean_q: 52.844509
[F[K 115874/500000: episode: 1625, duration: 0.905s, episode steps: 70, steps per second: 77, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.886 [0.000, 4.000], mean observation: 0.491 [0.380, 0.530], loss: 8.304410, mean_absolute_error: 41.794155, mean_q: 53.449200
[F[K 115950/500000: episode: 1626, duration: 0.947s, episode steps: 76, steps per second: 80, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.494 [0.350, 0.580], loss: 7.382019, mean_absolute_error: 41.555916, mean_q: 53.276478
[F[K 116010/500000: episode: 1627, duration: 0.678s, episode steps: 60, steps per second: 89, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.502 [0.380, 0.610], loss: 8.909143, mean_absolute_error: 40.868965, mean_q: 52.326649
[F[K 116079/500000: episode: 1628, duration: 0.709s, episode steps: 69, steps per second: 97, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.580 [0.000, 4.000], mean observation: 0.508 [0.460, 0.620], loss: 9.157097, mean_absolute_error: 41.886436, mean_q: 53.616699
[F[K 116157/500000: episode: 1629, duration: 0.921s, episode steps: 78, steps per second: 85, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.308 [0.000, 4.000], mean observation: 0.494 [0.390, 0.580], loss: 7.501477, mean_absolute_error: 41.842613, mean_q: 53.641281
[F[K 116231/500000: episode: 1630, duration: 0.890s, episode steps: 74, steps per second: 83, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.459 [0.000, 4.000], mean observation: 0.492 [0.440, 0.530], loss: 8.584977, mean_absolute_error: 41.978310, mean_q: 53.774784
[F[K 116285/500000: episode: 1631, duration: 0.731s, episode steps: 54, steps per second: 74, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.370 [0.000, 4.000], mean observation: 0.518 [0.470, 0.590], loss: 8.278955, mean_absolute_error: 41.142117, mean_q: 52.789383
[F[K 116365/500000: episode: 1632, duration: 1.044s, episode steps: 80, steps per second: 77, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.499 [0.390, 0.620], loss: 9.898401, mean_absolute_error: 41.886261, mean_q: 53.455738
[F[K 116437/500000: episode: 1633, duration: 0.954s, episode steps: 72, steps per second: 75, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.736 [0.000, 4.000], mean observation: 0.509 [0.480, 0.580], loss: 9.968852, mean_absolute_error: 41.904884, mean_q: 53.663322
[F[K 116502/500000: episode: 1634, duration: 0.798s, episode steps: 65, steps per second: 81, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.954 [0.000, 4.000], mean observation: 0.492 [0.470, 0.510], loss: 8.462020, mean_absolute_error: 41.688396, mean_q: 53.352394
[F[K 116625/500000: episode: 1635, duration: 1.745s, episode steps: 123, steps per second: 70, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.503 [0.460, 0.590], loss: 9.303783, mean_absolute_error: 41.628036, mean_q: 53.249439
[F[K 116715/500000: episode: 1636, duration: 1.349s, episode steps: 90, steps per second: 67, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.122 [0.000, 4.000], mean observation: 0.487 [0.410, 0.530], loss: 8.539704, mean_absolute_error: 41.639126, mean_q: 53.233860
[F[K 116763/500000: episode: 1637, duration: 0.621s, episode steps: 48, steps per second: 77, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.510 [0.470, 0.560], loss: 8.717538, mean_absolute_error: 40.924606, mean_q: 52.338367
[F[K 116837/500000: episode: 1638, duration: 1.024s, episode steps: 74, steps per second: 72, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.595 [0.000, 4.000], mean observation: 0.504 [0.450, 0.590], loss: 8.356199, mean_absolute_error: 41.954742, mean_q: 53.755066
[F[K 116923/500000: episode: 1639, duration: 1.080s, episode steps: 86, steps per second: 80, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.163 [0.000, 4.000], mean observation: 0.492 [0.420, 0.520], loss: 8.396630, mean_absolute_error: 42.100468, mean_q: 53.879009
[F[K 117006/500000: episode: 1640, duration: 1.092s, episode steps: 83, steps per second: 76, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.157 [0.000, 4.000], mean observation: 0.488 [0.410, 0.530], loss: 9.975402, mean_absolute_error: 41.364380, mean_q: 52.804554
[F[K 117039/500000: episode: 1641, duration: 0.442s, episode steps: 33, steps per second: 75, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.758 [0.000, 3.000], mean observation: 0.522 [0.470, 0.650], loss: 8.265960, mean_absolute_error: 42.643127, mean_q: 54.626644
[F[K 117239/500000: episode: 1642, duration: 2.859s, episode steps: 200, steps per second: 70, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.945 [0.000, 4.000], mean observation: 0.520 [0.470, 0.700], loss: 8.734093, mean_absolute_error: 41.902447, mean_q: 53.599571
[F[K 117439/500000: episode: 1643, duration: 2.440s, episode steps: 200, steps per second: 82, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.110 [0.000, 4.000], mean observation: 0.490 [0.390, 0.530], loss: 8.409679, mean_absolute_error: 41.483734, mean_q: 53.079952
[F[K 117513/500000: episode: 1644, duration: 1.086s, episode steps: 74, steps per second: 68, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.622 [0.000, 4.000], mean observation: 0.507 [0.460, 0.620], loss: 8.900692, mean_absolute_error: 41.566368, mean_q: 53.196167
[F[K 117607/500000: episode: 1645, duration: 1.272s, episode steps: 94, steps per second: 74, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.064 [0.000, 4.000], mean observation: 0.510 [0.460, 0.660], loss: 8.237995, mean_absolute_error: 41.862740, mean_q: 53.536243
[F[K 117669/500000: episode: 1646, duration: 0.951s, episode steps: 62, steps per second: 65, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.242 [0.000, 4.000], mean observation: 0.501 [0.430, 0.550], loss: 8.341281, mean_absolute_error: 41.871456, mean_q: 53.628441
[F[K 117705/500000: episode: 1647, duration: 0.477s, episode steps: 36, steps per second: 75, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.502 [0.390, 0.640], loss: 8.423313, mean_absolute_error: 41.404709, mean_q: 53.096130
[F[K 117751/500000: episode: 1648, duration: 0.689s, episode steps: 46, steps per second: 67, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.457 [0.000, 4.000], mean observation: 0.522 [0.470, 0.620], loss: 8.619639, mean_absolute_error: 41.130169, mean_q: 52.636059
[F[K 117834/500000: episode: 1649, duration: 1.349s, episode steps: 83, steps per second: 62, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.496 [0.450, 0.530], loss: 8.573324, mean_absolute_error: 41.026840, mean_q: 52.577438
[F[K 117925/500000: episode: 1650, duration: 1.402s, episode steps: 91, steps per second: 65, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.725 [0.000, 4.000], mean observation: 0.492 [0.340, 0.560], loss: 8.635384, mean_absolute_error: 41.510849, mean_q: 53.138092
[F[K 117982/500000: episode: 1651, duration: 0.690s, episode steps: 57, steps per second: 83, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.263 [0.000, 4.000], mean observation: 0.497 [0.390, 0.620], loss: 8.443147, mean_absolute_error: 41.987198, mean_q: 53.664871
[F[K 118076/500000: episode: 1652, duration: 1.364s, episode steps: 94, steps per second: 69, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.517 [0.490, 0.610], loss: 8.684833, mean_absolute_error: 41.489483, mean_q: 53.159595
[F[K 118159/500000: episode: 1653, duration: 1.217s, episode steps: 83, steps per second: 68, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.289 [0.000, 4.000], mean observation: 0.493 [0.380, 0.580], loss: 8.679901, mean_absolute_error: 41.266411, mean_q: 52.877308
[F[K 118260/500000: episode: 1654, duration: 1.348s, episode steps: 101, steps per second: 75, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.327 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 8.691012, mean_absolute_error: 41.568302, mean_q: 53.161026
[F[K 118330/500000: episode: 1655, duration: 0.946s, episode steps: 70, steps per second: 74, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.519 [0.490, 0.640], loss: 9.255384, mean_absolute_error: 41.334045, mean_q: 52.943600
[F[K 118394/500000: episode: 1656, duration: 1.000s, episode steps: 64, steps per second: 64, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.438 [0.000, 4.000], mean observation: 0.518 [0.500, 0.600], loss: 8.752617, mean_absolute_error: 41.227539, mean_q: 52.821716
[F[K 118503/500000: episode: 1657, duration: 1.542s, episode steps: 109, steps per second: 71, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.009 [0.000, 4.000], mean observation: 0.502 [0.450, 0.580], loss: 8.472000, mean_absolute_error: 41.246384, mean_q: 52.863934
[F[K 118565/500000: episode: 1658, duration: 0.858s, episode steps: 62, steps per second: 72, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 0.512 [0.490, 0.590], loss: 7.435376, mean_absolute_error: 41.725319, mean_q: 53.497284
[F[K 118620/500000: episode: 1659, duration: 0.861s, episode steps: 55, steps per second: 64, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.891 [0.000, 4.000], mean observation: 0.515 [0.470, 0.580], loss: 7.463626, mean_absolute_error: 41.540363, mean_q: 53.242054
[F[K 118654/500000: episode: 1660, duration: 0.440s, episode steps: 34, steps per second: 77, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.647 [0.000, 3.000], mean observation: 0.528 [0.470, 0.650], loss: 9.443172, mean_absolute_error: 41.629700, mean_q: 53.317524
[F[K 118709/500000: episode: 1661, duration: 0.701s, episode steps: 55, steps per second: 78, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.479 [0.380, 0.530], loss: 7.921212, mean_absolute_error: 41.857563, mean_q: 53.782902
[F[K 118835/500000: episode: 1662, duration: 2.031s, episode steps: 126, steps per second: 62, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.521 [0.460, 0.700], loss: 7.749742, mean_absolute_error: 41.424580, mean_q: 53.133678
[F[K 119031/500000: episode: 1663, duration: 2.995s, episode steps: 196, steps per second: 65, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.158 [0.000, 4.000], mean observation: 0.493 [0.380, 0.570], loss: 8.452241, mean_absolute_error: 42.022900, mean_q: 53.766521
[F[K 119111/500000: episode: 1664, duration: 1.151s, episode steps: 80, steps per second: 70, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.488 [0.410, 0.530], loss: 8.642006, mean_absolute_error: 41.423599, mean_q: 53.112724
[F[K 119187/500000: episode: 1665, duration: 1.216s, episode steps: 76, steps per second: 63, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 0.501 [0.450, 0.530], loss: 7.937274, mean_absolute_error: 42.288544, mean_q: 54.185642
[F[K 119305/500000: episode: 1666, duration: 1.580s, episode steps: 118, steps per second: 75, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.059 [0.000, 4.000], mean observation: 0.496 [0.340, 0.630], loss: 8.289091, mean_absolute_error: 41.931400, mean_q: 53.757980
[F[K 119413/500000: episode: 1667, duration: 1.398s, episode steps: 108, steps per second: 77, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.278 [0.000, 4.000], mean observation: 0.495 [0.430, 0.530], loss: 9.133407, mean_absolute_error: 41.643368, mean_q: 53.309818
[F[K 119473/500000: episode: 1668, duration: 0.857s, episode steps: 60, steps per second: 70, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.506 [0.440, 0.610], loss: 9.265080, mean_absolute_error: 42.001339, mean_q: 53.802002
[F[K 119530/500000: episode: 1669, duration: 0.848s, episode steps: 57, steps per second: 67, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.877 [0.000, 4.000], mean observation: 0.492 [0.350, 0.530], loss: 9.352757, mean_absolute_error: 41.565350, mean_q: 53.343678
[F[K 119603/500000: episode: 1670, duration: 1.264s, episode steps: 73, steps per second: 58, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.890 [0.000, 4.000], mean observation: 0.520 [0.490, 0.650], loss: 7.546779, mean_absolute_error: 41.401081, mean_q: 53.135540
[F[K 119704/500000: episode: 1671, duration: 1.626s, episode steps: 101, steps per second: 62, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.149 [0.000, 4.000], mean observation: 0.471 [0.360, 0.510], loss: 9.197269, mean_absolute_error: 41.947124, mean_q: 53.768143
[F[K 119777/500000: episode: 1672, duration: 1.179s, episode steps: 73, steps per second: 62, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.041 [0.000, 4.000], mean observation: 0.516 [0.490, 0.580], loss: 8.480909, mean_absolute_error: 41.866692, mean_q: 53.675632
[F[K 119845/500000: episode: 1673, duration: 0.968s, episode steps: 68, steps per second: 70, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.480 [0.400, 0.520], loss: 7.684605, mean_absolute_error: 42.255424, mean_q: 54.135189
[F[K 119940/500000: episode: 1674, duration: 1.169s, episode steps: 95, steps per second: 81, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.379 [0.000, 4.000], mean observation: 0.512 [0.470, 0.580], loss: 8.799802, mean_absolute_error: 41.608711, mean_q: 53.273857
[F[K 120105/500000: episode: 1675, duration: 2.441s, episode steps: 165, steps per second: 68, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.483 [0.390, 0.510], loss: 9.447287, mean_absolute_error: 41.912678, mean_q: 53.676834
[F[K 120141/500000: episode: 1676, duration: 0.582s, episode steps: 36, steps per second: 62, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.556 [0.000, 4.000], mean observation: 0.523 [0.470, 0.640], loss: 9.487625, mean_absolute_error: 41.964237, mean_q: 53.778915
[F[K 120288/500000: episode: 1677, duration: 2.407s, episode steps: 147, steps per second: 61, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.122 [0.000, 4.000], mean observation: 0.492 [0.370, 0.560], loss: 8.875362, mean_absolute_error: 41.594368, mean_q: 53.244690
[F[K 120373/500000: episode: 1678, duration: 1.236s, episode steps: 85, steps per second: 69, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.294 [0.000, 4.000], mean observation: 0.503 [0.480, 0.540], loss: 10.157474, mean_absolute_error: 41.196095, mean_q: 52.763943
[F[K 120438/500000: episode: 1679, duration: 1.124s, episode steps: 65, steps per second: 58, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.862 [0.000, 4.000], mean observation: 0.492 [0.380, 0.530], loss: 8.632635, mean_absolute_error: 41.747257, mean_q: 53.439770
[F[K 120480/500000: episode: 1680, duration: 0.748s, episode steps: 42, steps per second: 56, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.619 [0.000, 4.000], mean observation: 0.503 [0.390, 0.630], loss: 8.735958, mean_absolute_error: 41.186596, mean_q: 52.688721
[F[K 120539/500000: episode: 1681, duration: 0.885s, episode steps: 59, steps per second: 67, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.695 [0.000, 4.000], mean observation: 0.509 [0.490, 0.580], loss: 7.677230, mean_absolute_error: 41.345379, mean_q: 53.010441
[F[K 120614/500000: episode: 1682, duration: 0.880s, episode steps: 75, steps per second: 85, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 8.560723, mean_absolute_error: 41.429085, mean_q: 53.092159
[F[K 120704/500000: episode: 1683, duration: 1.416s, episode steps: 90, steps per second: 64, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.867 [0.000, 4.000], mean observation: 0.493 [0.390, 0.570], loss: 8.347720, mean_absolute_error: 41.223289, mean_q: 52.940380
[F[K 120769/500000: episode: 1684, duration: 0.896s, episode steps: 65, steps per second: 73, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.501 [0.380, 0.620], loss: 8.106263, mean_absolute_error: 41.395496, mean_q: 52.902374
[F[K 120949/500000: episode: 1685, duration: 2.512s, episode steps: 180, steps per second: 72, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.989 [0.000, 4.000], mean observation: 0.520 [0.480, 0.680], loss: 8.187943, mean_absolute_error: 41.604065, mean_q: 53.353558
[F[K 121036/500000: episode: 1686, duration: 1.295s, episode steps: 87, steps per second: 67, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.069 [0.000, 4.000], mean observation: 0.481 [0.360, 0.510], loss: 9.734818, mean_absolute_error: 41.156876, mean_q: 52.747322
[F[K 121098/500000: episode: 1687, duration: 1.118s, episode steps: 62, steps per second: 55, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.210 [0.000, 4.000], mean observation: 0.483 [0.390, 0.520], loss: 8.124279, mean_absolute_error: 41.229546, mean_q: 52.878395
[F[K 121160/500000: episode: 1688, duration: 1.115s, episode steps: 62, steps per second: 56, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.645 [0.000, 4.000], mean observation: 0.497 [0.430, 0.530], loss: 10.303232, mean_absolute_error: 41.053139, mean_q: 52.516823
[F[K 121226/500000: episode: 1689, duration: 1.202s, episode steps: 66, steps per second: 55, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.152 [0.000, 4.000], mean observation: 0.478 [0.360, 0.510], loss: 7.956120, mean_absolute_error: 41.188198, mean_q: 52.683731
[F[K 121279/500000: episode: 1690, duration: 0.906s, episode steps: 53, steps per second: 58, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.396 [0.000, 4.000], mean observation: 0.499 [0.420, 0.540], loss: 8.815778, mean_absolute_error: 41.756676, mean_q: 53.450130
[F[K 121350/500000: episode: 1691, duration: 1.183s, episode steps: 71, steps per second: 60, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.761 [0.000, 4.000], mean observation: 0.491 [0.400, 0.530], loss: 8.092868, mean_absolute_error: 41.326347, mean_q: 53.015614
[F[K 121453/500000: episode: 1692, duration: 1.684s, episode steps: 103, steps per second: 61, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.107 [0.000, 4.000], mean observation: 0.481 [0.370, 0.530], loss: 9.208734, mean_absolute_error: 40.918190, mean_q: 52.332478
[F[K 121515/500000: episode: 1693, duration: 0.881s, episode steps: 62, steps per second: 70, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.968 [0.000, 4.000], mean observation: 0.512 [0.490, 0.550], loss: 7.334446, mean_absolute_error: 40.999367, mean_q: 52.574181
[F[K 121559/500000: episode: 1694, duration: 0.660s, episode steps: 44, steps per second: 67, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.614 [0.000, 4.000], mean observation: 0.518 [0.470, 0.620], loss: 8.420753, mean_absolute_error: 41.690594, mean_q: 53.434811
[F[K 121747/500000: episode: 1695, duration: 2.267s, episode steps: 188, steps per second: 83, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.176 [0.000, 4.000], mean observation: 0.512 [0.470, 0.660], loss: 7.939010, mean_absolute_error: 41.456642, mean_q: 53.129208
[F[K 121838/500000: episode: 1696, duration: 1.215s, episode steps: 91, steps per second: 75, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.352 [0.000, 4.000], mean observation: 0.497 [0.420, 0.550], loss: 8.197913, mean_absolute_error: 41.865742, mean_q: 53.639446
[F[K 121876/500000: episode: 1697, duration: 0.501s, episode steps: 38, steps per second: 76, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.368 [0.000, 4.000], mean observation: 0.525 [0.470, 0.630], loss: 8.250432, mean_absolute_error: 41.269848, mean_q: 52.905014
[F[K 121948/500000: episode: 1698, duration: 0.962s, episode steps: 72, steps per second: 75, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.097 [0.000, 4.000], mean observation: 0.502 [0.390, 0.600], loss: 8.229054, mean_absolute_error: 41.685940, mean_q: 53.399574
[F[K 122000/500000: episode: 1699, duration: 0.655s, episode steps: 52, steps per second: 79, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.519 [0.000, 4.000], mean observation: 0.506 [0.420, 0.640], loss: 7.428149, mean_absolute_error: 41.209171, mean_q: 52.735531
[F[K 122087/500000: episode: 1700, duration: 1.147s, episode steps: 87, steps per second: 76, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 8.965034, mean_absolute_error: 41.200718, mean_q: 52.791328
[F[K 122124/500000: episode: 1701, duration: 0.521s, episode steps: 37, steps per second: 71, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.568 [0.000, 3.000], mean observation: 0.520 [0.470, 0.640], loss: 8.174644, mean_absolute_error: 41.216866, mean_q: 52.802227
[F[K 122201/500000: episode: 1702, duration: 1.105s, episode steps: 77, steps per second: 70, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.961 [0.000, 4.000], mean observation: 0.509 [0.450, 0.620], loss: 9.125764, mean_absolute_error: 41.094353, mean_q: 52.728996
[F[K 122241/500000: episode: 1703, duration: 0.539s, episode steps: 40, steps per second: 74, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.675 [0.000, 4.000], mean observation: 0.526 [0.470, 0.640], loss: 8.609515, mean_absolute_error: 41.265209, mean_q: 52.827950
[F[K 122318/500000: episode: 1704, duration: 1.097s, episode steps: 77, steps per second: 70, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.221 [0.000, 4.000], mean observation: 0.503 [0.470, 0.570], loss: 8.821265, mean_absolute_error: 41.331753, mean_q: 53.022003
[F[K 122351/500000: episode: 1705, duration: 0.465s, episode steps: 33, steps per second: 71, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.727 [0.000, 4.000], mean observation: 0.525 [0.470, 0.650], loss: 9.665488, mean_absolute_error: 41.754810, mean_q: 53.442692
[F[K 122417/500000: episode: 1706, duration: 0.917s, episode steps: 66, steps per second: 72, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.394 [0.000, 4.000], mean observation: 0.489 [0.380, 0.530], loss: 9.425533, mean_absolute_error: 40.844048, mean_q: 52.246178
[F[K 122537/500000: episode: 1707, duration: 1.454s, episode steps: 120, steps per second: 83, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.442 [0.000, 4.000], mean observation: 0.508 [0.480, 0.580], loss: 8.182353, mean_absolute_error: 41.447788, mean_q: 53.128559
[F[K 122624/500000: episode: 1708, duration: 1.331s, episode steps: 87, steps per second: 65, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.126 [0.000, 4.000], mean observation: 0.490 [0.430, 0.520], loss: 8.486938, mean_absolute_error: 41.383804, mean_q: 52.962109
[F[K 122702/500000: episode: 1709, duration: 1.161s, episode steps: 78, steps per second: 67, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.897 [0.000, 4.000], mean observation: 0.518 [0.470, 0.640], loss: 7.162284, mean_absolute_error: 41.191063, mean_q: 52.864128
[F[K 122767/500000: episode: 1710, duration: 0.827s, episode steps: 65, steps per second: 79, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.231 [0.000, 4.000], mean observation: 0.498 [0.400, 0.610], loss: 7.508692, mean_absolute_error: 41.303154, mean_q: 52.923019
[F[K 122803/500000: episode: 1711, duration: 0.503s, episode steps: 36, steps per second: 72, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.861 [0.000, 4.000], mean observation: 0.517 [0.470, 0.640], loss: 7.619618, mean_absolute_error: 40.030407, mean_q: 51.501270
[F[K 122858/500000: episode: 1712, duration: 0.747s, episode steps: 55, steps per second: 74, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.182 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 9.286440, mean_absolute_error: 40.413277, mean_q: 51.769783
[F[K 122901/500000: episode: 1713, duration: 0.580s, episode steps: 43, steps per second: 74, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.721 [0.000, 4.000], mean observation: 0.525 [0.470, 0.630], loss: 7.134647, mean_absolute_error: 40.820766, mean_q: 52.367580
[F[K 122961/500000: episode: 1714, duration: 0.706s, episode steps: 60, steps per second: 85, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.733 [0.000, 4.000], mean observation: 0.508 [0.440, 0.620], loss: 8.809522, mean_absolute_error: 41.707890, mean_q: 53.384914
[F[K 123110/500000: episode: 1715, duration: 1.575s, episode steps: 149, steps per second: 95, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.389 [0.000, 4.000], mean observation: 0.492 [0.350, 0.590], loss: 7.949501, mean_absolute_error: 41.215195, mean_q: 52.828880
[F[K 123195/500000: episode: 1716, duration: 0.882s, episode steps: 85, steps per second: 96, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 0.483 [0.410, 0.530], loss: 7.632818, mean_absolute_error: 41.055721, mean_q: 52.627678
[F[K 123257/500000: episode: 1717, duration: 0.699s, episode steps: 62, steps per second: 89, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.677 [0.000, 4.000], mean observation: 0.492 [0.390, 0.530], loss: 7.722550, mean_absolute_error: 42.359127, mean_q: 54.260853
[F[K 123338/500000: episode: 1718, duration: 0.966s, episode steps: 81, steps per second: 84, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.877 [0.000, 4.000], mean observation: 0.491 [0.350, 0.580], loss: 8.069776, mean_absolute_error: 40.993542, mean_q: 52.562767
[F[K 123444/500000: episode: 1719, duration: 1.230s, episode steps: 106, steps per second: 86, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.981 [0.000, 4.000], mean observation: 0.498 [0.380, 0.580], loss: 8.985911, mean_absolute_error: 41.410175, mean_q: 53.032398
[F[K 123563/500000: episode: 1720, duration: 1.333s, episode steps: 119, steps per second: 89, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.109 [0.000, 4.000], mean observation: 0.477 [0.380, 0.520], loss: 8.199087, mean_absolute_error: 41.345165, mean_q: 52.926105
[F[K 123634/500000: episode: 1721, duration: 0.803s, episode steps: 71, steps per second: 88, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.127 [0.000, 4.000], mean observation: 0.471 [0.360, 0.520], loss: 8.232390, mean_absolute_error: 41.347919, mean_q: 53.162052
[F[K 123675/500000: episode: 1722, duration: 0.531s, episode steps: 41, steps per second: 77, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.732 [0.000, 4.000], mean observation: 0.522 [0.470, 0.630], loss: 8.577079, mean_absolute_error: 41.136673, mean_q: 52.692104
[F[K 123809/500000: episode: 1723, duration: 1.636s, episode steps: 134, steps per second: 82, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 0.499 [0.360, 0.630], loss: 8.392799, mean_absolute_error: 41.684994, mean_q: 53.426613
[F[K 123906/500000: episode: 1724, duration: 1.059s, episode steps: 97, steps per second: 92, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.072 [0.000, 4.000], mean observation: 0.491 [0.420, 0.530], loss: 8.217193, mean_absolute_error: 41.814510, mean_q: 53.545712
[F[K 123972/500000: episode: 1725, duration: 0.775s, episode steps: 66, steps per second: 85, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.479 [0.380, 0.530], loss: 8.499644, mean_absolute_error: 41.562851, mean_q: 53.198311
[F[K 124008/500000: episode: 1726, duration: 0.506s, episode steps: 36, steps per second: 71, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.028 [0.000, 3.000], mean observation: 0.512 [0.470, 0.650], loss: 8.221920, mean_absolute_error: 41.190201, mean_q: 52.893887
[F[K 124147/500000: episode: 1727, duration: 1.662s, episode steps: 139, steps per second: 84, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.432 [0.000, 4.000], mean observation: 0.479 [0.310, 0.510], loss: 7.835280, mean_absolute_error: 41.649746, mean_q: 53.348007
[F[K 124228/500000: episode: 1728, duration: 1.032s, episode steps: 81, steps per second: 78, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.728 [0.000, 4.000], mean observation: 0.508 [0.450, 0.600], loss: 7.603584, mean_absolute_error: 41.203083, mean_q: 52.924152
[F[K 124346/500000: episode: 1729, duration: 1.505s, episode steps: 118, steps per second: 78, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.504 [0.480, 0.530], loss: 7.435614, mean_absolute_error: 41.729546, mean_q: 53.518959
[F[K 124438/500000: episode: 1730, duration: 1.123s, episode steps: 92, steps per second: 82, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.946 [0.000, 4.000], mean observation: 0.500 [0.460, 0.560], loss: 8.777667, mean_absolute_error: 41.690281, mean_q: 53.501587
[F[K 124531/500000: episode: 1731, duration: 1.108s, episode steps: 93, steps per second: 84, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.097 [0.000, 4.000], mean observation: 0.507 [0.430, 0.650], loss: 8.433528, mean_absolute_error: 41.705803, mean_q: 53.383427
[F[K 124595/500000: episode: 1732, duration: 0.716s, episode steps: 64, steps per second: 89, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.438 [0.000, 4.000], mean observation: 0.491 [0.360, 0.580], loss: 8.350902, mean_absolute_error: 40.895996, mean_q: 52.435234
[F[K 124683/500000: episode: 1733, duration: 1.122s, episode steps: 88, steps per second: 78, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.502 [0.430, 0.580], loss: 8.351777, mean_absolute_error: 41.636967, mean_q: 53.425571
[F[K 124831/500000: episode: 1734, duration: 1.547s, episode steps: 148, steps per second: 96, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.503 [0.450, 0.570], loss: 8.950302, mean_absolute_error: 42.053024, mean_q: 53.799107
[F[K 124894/500000: episode: 1735, duration: 0.654s, episode steps: 63, steps per second: 96, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.489 [0.450, 0.510], loss: 7.362218, mean_absolute_error: 42.065250, mean_q: 53.833820
[F[K 124964/500000: episode: 1736, duration: 0.785s, episode steps: 70, steps per second: 89, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.086 [0.000, 4.000], mean observation: 0.477 [0.370, 0.530], loss: 8.014879, mean_absolute_error: 41.649910, mean_q: 53.413662
[F[K 125000/500000: episode: 1737, duration: 0.446s, episode steps: 36, steps per second: 81, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.722 [0.000, 3.000], mean observation: 0.515 [0.470, 0.640], loss: 7.448174, mean_absolute_error: 42.033413, mean_q: 53.905731
[F[K 125084/500000: episode: 1738, duration: 1.020s, episode steps: 84, steps per second: 82, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.516 [0.490, 0.650], loss: 8.724808, mean_absolute_error: 41.889130, mean_q: 53.707172
[F[K 125152/500000: episode: 1739, duration: 0.892s, episode steps: 68, steps per second: 76, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.501 [0.440, 0.560], loss: 7.459430, mean_absolute_error: 41.369568, mean_q: 53.005043
[F[K 125240/500000: episode: 1740, duration: 0.965s, episode steps: 88, steps per second: 91, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.486 [0.370, 0.520], loss: 8.619302, mean_absolute_error: 41.969315, mean_q: 53.827114
[F[K 125333/500000: episode: 1741, duration: 0.967s, episode steps: 93, steps per second: 96, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.935 [0.000, 4.000], mean observation: 0.501 [0.390, 0.600], loss: 7.991338, mean_absolute_error: 42.469334, mean_q: 54.433174
[F[K 125420/500000: episode: 1742, duration: 1.053s, episode steps: 87, steps per second: 83, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.563 [0.000, 4.000], mean observation: 0.506 [0.460, 0.590], loss: 7.979945, mean_absolute_error: 41.237984, mean_q: 52.758480
[F[K 125578/500000: episode: 1743, duration: 1.665s, episode steps: 158, steps per second: 95, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.329 [0.000, 4.000], mean observation: 0.480 [0.370, 0.530], loss: 8.092492, mean_absolute_error: 41.645844, mean_q: 53.432369
[F[K 125675/500000: episode: 1744, duration: 1.069s, episode steps: 97, steps per second: 91, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.660 [0.000, 4.000], mean observation: 0.494 [0.350, 0.600], loss: 7.588972, mean_absolute_error: 41.705177, mean_q: 53.495167
[F[K 125836/500000: episode: 1745, duration: 1.820s, episode steps: 161, steps per second: 88, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 0.514 [0.470, 0.590], loss: 8.215817, mean_absolute_error: 41.513824, mean_q: 53.315926
[F[K 125909/500000: episode: 1746, duration: 0.903s, episode steps: 73, steps per second: 81, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.575 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 8.260011, mean_absolute_error: 41.797791, mean_q: 53.585052
[F[K 126031/500000: episode: 1747, duration: 1.484s, episode steps: 122, steps per second: 82, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.516 [0.000, 4.000], mean observation: 0.498 [0.440, 0.540], loss: 8.299162, mean_absolute_error: 42.173340, mean_q: 53.998215
[F[K 126075/500000: episode: 1748, duration: 0.591s, episode steps: 44, steps per second: 74, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.514 [0.470, 0.650], loss: 7.651472, mean_absolute_error: 41.469120, mean_q: 53.204239
[F[K 126151/500000: episode: 1749, duration: 0.998s, episode steps: 76, steps per second: 76, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.263 [0.000, 4.000], mean observation: 0.513 [0.490, 0.550], loss: 8.568513, mean_absolute_error: 42.123726, mean_q: 54.033936
[F[K 126317/500000: episode: 1750, duration: 1.956s, episode steps: 166, steps per second: 85, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.247 [0.000, 4.000], mean observation: 0.499 [0.410, 0.600], loss: 8.251480, mean_absolute_error: 42.223331, mean_q: 54.131020
[F[K 126461/500000: episode: 1751, duration: 1.826s, episode steps: 144, steps per second: 79, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.160 [0.000, 4.000], mean observation: 0.475 [0.350, 0.530], loss: 7.921156, mean_absolute_error: 41.967014, mean_q: 53.752876
[F[K 126495/500000: episode: 1752, duration: 0.425s, episode steps: 34, steps per second: 80, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.382 [0.000, 4.000], mean observation: 0.503 [0.380, 0.650], loss: 9.817197, mean_absolute_error: 42.567837, mean_q: 54.343624
[F[K 126552/500000: episode: 1753, duration: 0.757s, episode steps: 57, steps per second: 75, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.123 [0.000, 4.000], mean observation: 0.521 [0.470, 0.630], loss: 8.092558, mean_absolute_error: 42.268066, mean_q: 54.236576
[F[K 126646/500000: episode: 1754, duration: 1.359s, episode steps: 94, steps per second: 69, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.957 [0.000, 4.000], mean observation: 0.521 [0.490, 0.640], loss: 8.636757, mean_absolute_error: 41.761387, mean_q: 53.507553
[F[K 126711/500000: episode: 1755, duration: 0.856s, episode steps: 65, steps per second: 76, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.908 [0.000, 4.000], mean observation: 0.479 [0.400, 0.510], loss: 6.802022, mean_absolute_error: 42.261154, mean_q: 54.261787
[F[K 126767/500000: episode: 1756, duration: 0.726s, episode steps: 56, steps per second: 77, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 0.508 [0.490, 0.530], loss: 8.486258, mean_absolute_error: 41.058846, mean_q: 52.729378
[F[K 126834/500000: episode: 1757, duration: 0.883s, episode steps: 67, steps per second: 76, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.791 [0.000, 4.000], mean observation: 0.504 [0.390, 0.640], loss: 7.729850, mean_absolute_error: 42.108738, mean_q: 54.039810
[F[K 126885/500000: episode: 1758, duration: 0.693s, episode steps: 51, steps per second: 74, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.765 [0.000, 4.000], mean observation: 0.513 [0.480, 0.610], loss: 7.938538, mean_absolute_error: 41.581211, mean_q: 53.412487
[F[K 126922/500000: episode: 1759, duration: 0.489s, episode steps: 37, steps per second: 76, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.622 [0.000, 3.000], mean observation: 0.520 [0.470, 0.640], loss: 8.624458, mean_absolute_error: 41.967175, mean_q: 53.830067
[F[K 126991/500000: episode: 1760, duration: 0.846s, episode steps: 69, steps per second: 82, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.174 [0.000, 4.000], mean observation: 0.508 [0.480, 0.560], loss: 9.441848, mean_absolute_error: 41.490585, mean_q: 53.224487
[F[K 127082/500000: episode: 1761, duration: 1.029s, episode steps: 91, steps per second: 88, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.945 [0.000, 4.000], mean observation: 0.487 [0.440, 0.520], loss: 8.056583, mean_absolute_error: 42.509445, mean_q: 54.407944
[F[K 127145/500000: episode: 1762, duration: 0.587s, episode steps: 63, steps per second: 107, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.921 [0.000, 4.000], mean observation: 0.487 [0.420, 0.530], loss: 8.491418, mean_absolute_error: 42.192196, mean_q: 53.887230
[F[K 127223/500000: episode: 1763, duration: 0.951s, episode steps: 78, steps per second: 82, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.397 [0.000, 4.000], mean observation: 0.477 [0.370, 0.510], loss: 8.831160, mean_absolute_error: 42.373600, mean_q: 54.282497
[F[K 127285/500000: episode: 1764, duration: 0.757s, episode steps: 62, steps per second: 82, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 8.757184, mean_absolute_error: 41.124966, mean_q: 52.798832
[F[K 127348/500000: episode: 1765, duration: 0.771s, episode steps: 63, steps per second: 82, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.470 [0.360, 0.510], loss: 7.572144, mean_absolute_error: 41.379402, mean_q: 53.079132
[F[K 127412/500000: episode: 1766, duration: 0.877s, episode steps: 64, steps per second: 73, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.062 [0.000, 4.000], mean observation: 0.503 [0.490, 0.530], loss: 8.264338, mean_absolute_error: 41.797073, mean_q: 53.544704
[F[K 127459/500000: episode: 1767, duration: 0.567s, episode steps: 47, steps per second: 83, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.660 [0.000, 3.000], mean observation: 0.520 [0.470, 0.610], loss: 6.592779, mean_absolute_error: 42.102844, mean_q: 54.057320
[F[K 127566/500000: episode: 1768, duration: 1.353s, episode steps: 107, steps per second: 79, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.159 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 8.336605, mean_absolute_error: 41.903412, mean_q: 53.767303
[F[K 127627/500000: episode: 1769, duration: 0.735s, episode steps: 61, steps per second: 83, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.934 [0.000, 4.000], mean observation: 0.502 [0.430, 0.570], loss: 8.296221, mean_absolute_error: 41.349422, mean_q: 53.085587
[F[K 127744/500000: episode: 1770, duration: 1.383s, episode steps: 117, steps per second: 85, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.897 [0.000, 4.000], mean observation: 0.502 [0.440, 0.550], loss: 7.981449, mean_absolute_error: 41.836960, mean_q: 53.662544
[F[K 127854/500000: episode: 1771, duration: 1.423s, episode steps: 110, steps per second: 77, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.164 [0.000, 4.000], mean observation: 0.479 [0.380, 0.530], loss: 9.013375, mean_absolute_error: 41.968128, mean_q: 53.831299
[F[K 127964/500000: episode: 1772, duration: 1.533s, episode steps: 110, steps per second: 72, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.127 [0.000, 4.000], mean observation: 0.490 [0.390, 0.530], loss: 8.322099, mean_absolute_error: 41.902859, mean_q: 53.702328
[F[K 128039/500000: episode: 1773, duration: 1.071s, episode steps: 75, steps per second: 70, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.827 [0.000, 4.000], mean observation: 0.501 [0.390, 0.590], loss: 8.159688, mean_absolute_error: 42.151955, mean_q: 54.098782
[F[K 128083/500000: episode: 1774, duration: 0.555s, episode steps: 44, steps per second: 79, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.386 [0.000, 4.000], mean observation: 0.506 [0.410, 0.650], loss: 9.609597, mean_absolute_error: 42.434437, mean_q: 54.471237
[F[K 128149/500000: episode: 1775, duration: 0.749s, episode steps: 66, steps per second: 88, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.909 [0.000, 4.000], mean observation: 0.473 [0.350, 0.510], loss: 7.997082, mean_absolute_error: 42.042370, mean_q: 53.865322
[F[K 128229/500000: episode: 1776, duration: 0.969s, episode steps: 80, steps per second: 83, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.514 [0.490, 0.600], loss: 8.184841, mean_absolute_error: 42.275578, mean_q: 53.994560
[F[K 128298/500000: episode: 1777, duration: 0.790s, episode steps: 69, steps per second: 87, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.246 [0.000, 4.000], mean observation: 0.484 [0.420, 0.520], loss: 8.097007, mean_absolute_error: 42.015800, mean_q: 53.762367
[F[K 128359/500000: episode: 1778, duration: 0.737s, episode steps: 61, steps per second: 83, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.951 [0.000, 4.000], mean observation: 0.479 [0.360, 0.520], loss: 10.990601, mean_absolute_error: 42.017841, mean_q: 53.890232
[F[K 128421/500000: episode: 1779, duration: 0.702s, episode steps: 62, steps per second: 88, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.258 [0.000, 4.000], mean observation: 0.506 [0.490, 0.530], loss: 8.683196, mean_absolute_error: 41.522297, mean_q: 53.193325
[F[K 128566/500000: episode: 1780, duration: 1.960s, episode steps: 145, steps per second: 74, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.869 [0.000, 4.000], mean observation: 0.521 [0.480, 0.690], loss: 8.805690, mean_absolute_error: 41.954876, mean_q: 53.609219
[F[K 128650/500000: episode: 1781, duration: 1.074s, episode steps: 84, steps per second: 78, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 7.875447, mean_absolute_error: 42.567898, mean_q: 54.583172
[F[K 128712/500000: episode: 1782, duration: 0.807s, episode steps: 62, steps per second: 77, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.145 [0.000, 4.000], mean observation: 0.508 [0.480, 0.590], loss: 9.118540, mean_absolute_error: 41.602764, mean_q: 53.238300
[F[K 128823/500000: episode: 1783, duration: 1.349s, episode steps: 111, steps per second: 82, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.507 [0.440, 0.620], loss: 9.398947, mean_absolute_error: 41.699539, mean_q: 53.424385
[F[K 128899/500000: episode: 1784, duration: 1.066s, episode steps: 76, steps per second: 71, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.503 [0.460, 0.550], loss: 7.390463, mean_absolute_error: 42.085072, mean_q: 53.928883
[F[K 129008/500000: episode: 1785, duration: 1.474s, episode steps: 109, steps per second: 74, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.596 [0.000, 4.000], mean observation: 0.510 [0.460, 0.640], loss: 8.565447, mean_absolute_error: 41.386948, mean_q: 53.038956
[F[K 129087/500000: episode: 1786, duration: 0.825s, episode steps: 79, steps per second: 96, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.354 [0.000, 4.000], mean observation: 0.503 [0.390, 0.640], loss: 8.338309, mean_absolute_error: 41.419235, mean_q: 53.095894
[F[K 129158/500000: episode: 1787, duration: 0.940s, episode steps: 71, steps per second: 76, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.099 [0.000, 4.000], mean observation: 0.515 [0.490, 0.600], loss: 9.640873, mean_absolute_error: 41.564972, mean_q: 53.153755
[F[K 129205/500000: episode: 1788, duration: 0.637s, episode steps: 47, steps per second: 74, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.277 [0.000, 3.000], mean observation: 0.512 [0.470, 0.650], loss: 7.492839, mean_absolute_error: 42.393791, mean_q: 54.412098argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv
[F[K 129241/500000: episode: 1789, duration: 0.496s, episode steps: 36, steps per second: 73, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.639 [0.000, 4.000], mean observation: 0.501 [0.350, 0.650], loss: 7.599058, mean_absolute_error: 41.335590, mean_q: 52.850258
[F[K 129301/500000: episode: 1790, duration: 0.811s, episode steps: 60, steps per second: 74, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.850 [0.000, 4.000], mean observation: 0.504 [0.440, 0.610], loss: 8.804485, mean_absolute_error: 40.876690, mean_q: 52.400372
[F[K 129419/500000: episode: 1791, duration: 1.426s, episode steps: 118, steps per second: 83, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.212 [0.000, 4.000], mean observation: 0.512 [0.430, 0.670], loss: 9.092462, mean_absolute_error: 41.002293, mean_q: 52.530991
[F[K 129466/500000: episode: 1792, duration: 0.569s, episode steps: 47, steps per second: 83, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.128 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 8.155529, mean_absolute_error: 42.085701, mean_q: 53.987080
[F[K 129584/500000: episode: 1793, duration: 1.423s, episode steps: 118, steps per second: 83, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.186 [0.000, 4.000], mean observation: 0.512 [0.480, 0.600], loss: 7.818076, mean_absolute_error: 41.521538, mean_q: 53.168079
[F[K 129683/500000: episode: 1794, duration: 1.283s, episode steps: 99, steps per second: 77, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.020 [0.000, 4.000], mean observation: 0.484 [0.340, 0.510], loss: 7.910017, mean_absolute_error: 41.472855, mean_q: 53.164066
[F[K 129743/500000: episode: 1795, duration: 0.818s, episode steps: 60, steps per second: 73, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.267 [0.000, 4.000], mean observation: 0.512 [0.480, 0.600], loss: 7.659768, mean_absolute_error: 41.381851, mean_q: 53.034077
[F[K 129804/500000: episode: 1796, duration: 0.853s, episode steps: 61, steps per second: 72, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.502 [0.460, 0.580], loss: 8.282681, mean_absolute_error: 42.195000, mean_q: 53.950546
[F[K 129886/500000: episode: 1797, duration: 0.949s, episode steps: 82, steps per second: 86, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.305 [0.000, 4.000], mean observation: 0.491 [0.430, 0.520], loss: 8.557961, mean_absolute_error: 41.656601, mean_q: 53.402523
[F[K 129970/500000: episode: 1798, duration: 0.882s, episode steps: 84, steps per second: 95, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.505 [0.470, 0.570], loss: 7.715655, mean_absolute_error: 42.529411, mean_q: 54.449200
[F[K 130015/500000: episode: 1799, duration: 0.517s, episode steps: 45, steps per second: 87, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.222 [0.000, 4.000], mean observation: 0.498 [0.360, 0.620], loss: 9.489430, mean_absolute_error: 41.246643, mean_q: 52.819031
[F[K 130092/500000: episode: 1800, duration: 0.869s, episode steps: 77, steps per second: 89, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.156 [0.000, 4.000], mean observation: 0.482 [0.380, 0.520], loss: 6.993929, mean_absolute_error: 41.970375, mean_q: 53.882553
[F[K 130164/500000: episode: 1801, duration: 0.852s, episode steps: 72, steps per second: 85, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.069 [0.000, 4.000], mean observation: 0.484 [0.370, 0.530], loss: 7.446589, mean_absolute_error: 41.694031, mean_q: 53.538589
[F[K 130279/500000: episode: 1802, duration: 1.174s, episode steps: 115, steps per second: 98, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.957 [0.000, 4.000], mean observation: 0.496 [0.320, 0.640], loss: 8.419600, mean_absolute_error: 41.395626, mean_q: 53.068466
[F[K 130331/500000: episode: 1803, duration: 0.661s, episode steps: 52, steps per second: 79, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 0.488 [0.360, 0.550], loss: 6.311773, mean_absolute_error: 41.396446, mean_q: 53.080326
[F[K 130432/500000: episode: 1804, duration: 1.182s, episode steps: 101, steps per second: 85, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.228 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 8.182836, mean_absolute_error: 41.963348, mean_q: 53.709656
[F[K 130532/500000: episode: 1805, duration: 1.077s, episode steps: 100, steps per second: 93, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.110 [0.000, 4.000], mean observation: 0.499 [0.450, 0.550], loss: 8.842907, mean_absolute_error: 41.556129, mean_q: 53.218781
[F[K 130647/500000: episode: 1806, duration: 1.312s, episode steps: 115, steps per second: 88, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.052 [0.000, 4.000], mean observation: 0.504 [0.450, 0.550], loss: 9.080678, mean_absolute_error: 41.575092, mean_q: 53.252842
[F[K 130716/500000: episode: 1807, duration: 0.831s, episode steps: 69, steps per second: 83, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.217 [0.000, 4.000], mean observation: 0.517 [0.490, 0.580], loss: 7.817504, mean_absolute_error: 41.853420, mean_q: 53.570923
[F[K 130756/500000: episode: 1808, duration: 0.489s, episode steps: 40, steps per second: 82, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.350 [0.000, 3.000], mean observation: 0.513 [0.470, 0.650], loss: 8.101389, mean_absolute_error: 41.737061, mean_q: 53.528862
[F[K 130817/500000: episode: 1809, duration: 0.710s, episode steps: 61, steps per second: 86, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.033 [0.000, 4.000], mean observation: 0.505 [0.490, 0.530], loss: 7.717659, mean_absolute_error: 41.198563, mean_q: 52.738361
[F[K 130878/500000: episode: 1810, duration: 0.768s, episode steps: 61, steps per second: 79, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.770 [0.000, 4.000], mean observation: 0.514 [0.480, 0.620], loss: 10.493016, mean_absolute_error: 41.432243, mean_q: 53.097301
[F[K 131010/500000: episode: 1811, duration: 1.451s, episode steps: 132, steps per second: 91, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.470 [0.000, 4.000], mean observation: 0.519 [0.500, 0.600], loss: 7.932002, mean_absolute_error: 41.451431, mean_q: 53.132687
[F[K 131071/500000: episode: 1812, duration: 0.773s, episode steps: 61, steps per second: 79, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.115 [0.000, 4.000], mean observation: 0.515 [0.480, 0.580], loss: 7.321120, mean_absolute_error: 41.087406, mean_q: 52.673031
[F[K 131170/500000: episode: 1813, duration: 1.171s, episode steps: 99, steps per second: 85, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.051 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 8.005751, mean_absolute_error: 41.584805, mean_q: 53.361664
[F[K 131234/500000: episode: 1814, duration: 0.832s, episode steps: 64, steps per second: 77, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.422 [0.000, 4.000], mean observation: 0.522 [0.480, 0.630], loss: 10.102697, mean_absolute_error: 41.816467, mean_q: 53.407780
[F[K 131307/500000: episode: 1815, duration: 0.891s, episode steps: 73, steps per second: 82, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.490 [0.370, 0.530], loss: 7.683396, mean_absolute_error: 41.712742, mean_q: 53.621750
[F[K 131374/500000: episode: 1816, duration: 0.853s, episode steps: 67, steps per second: 79, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.239 [0.000, 4.000], mean observation: 0.489 [0.450, 0.520], loss: 7.889479, mean_absolute_error: 41.288418, mean_q: 53.018364
[F[K 131461/500000: episode: 1817, duration: 1.154s, episode steps: 87, steps per second: 75, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.736 [0.000, 4.000], mean observation: 0.519 [0.470, 0.660], loss: 7.954031, mean_absolute_error: 41.272305, mean_q: 52.937439
[F[K 131544/500000: episode: 1818, duration: 0.954s, episode steps: 83, steps per second: 87, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.503 [0.440, 0.570], loss: 8.120890, mean_absolute_error: 42.052315, mean_q: 53.865452
[F[K 131613/500000: episode: 1819, duration: 0.789s, episode steps: 69, steps per second: 87, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.116 [0.000, 4.000], mean observation: 0.494 [0.360, 0.560], loss: 8.583694, mean_absolute_error: 42.068756, mean_q: 53.876545
[F[K 131671/500000: episode: 1820, duration: 0.652s, episode steps: 58, steps per second: 89, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.690 [0.000, 4.000], mean observation: 0.498 [0.360, 0.590], loss: 7.701550, mean_absolute_error: 42.095924, mean_q: 53.957943
[F[K 131719/500000: episode: 1821, duration: 0.654s, episode steps: 48, steps per second: 73, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 0.494 [0.370, 0.570], loss: 9.579062, mean_absolute_error: 41.893612, mean_q: 53.579227
[F[K 131778/500000: episode: 1822, duration: 0.827s, episode steps: 59, steps per second: 71, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.424 [0.000, 4.000], mean observation: 0.495 [0.380, 0.550], loss: 9.070043, mean_absolute_error: 41.931244, mean_q: 53.851055
[F[K 131857/500000: episode: 1823, duration: 0.964s, episode steps: 79, steps per second: 82, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.456 [0.000, 4.000], mean observation: 0.498 [0.390, 0.560], loss: 7.236028, mean_absolute_error: 41.418083, mean_q: 53.232552
[F[K 131923/500000: episode: 1824, duration: 0.748s, episode steps: 66, steps per second: 88, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.455 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 8.345715, mean_absolute_error: 42.153744, mean_q: 54.104813
[F[K 131986/500000: episode: 1825, duration: 0.691s, episode steps: 63, steps per second: 91, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.921 [0.000, 4.000], mean observation: 0.525 [0.500, 0.640], loss: 7.978335, mean_absolute_error: 42.222942, mean_q: 54.051025
[F[K 132049/500000: episode: 1826, duration: 0.722s, episode steps: 63, steps per second: 87, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.873 [0.000, 4.000], mean observation: 0.520 [0.470, 0.600], loss: 8.418961, mean_absolute_error: 41.463390, mean_q: 53.167416
[F[K 132129/500000: episode: 1827, duration: 0.968s, episode steps: 80, steps per second: 83, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.475 [0.000, 4.000], mean observation: 0.502 [0.430, 0.600], loss: 8.604062, mean_absolute_error: 41.170860, mean_q: 52.777046
[F[K 132248/500000: episode: 1828, duration: 1.499s, episode steps: 119, steps per second: 79, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.992 [0.000, 4.000], mean observation: 0.488 [0.360, 0.530], loss: 8.572993, mean_absolute_error: 41.772907, mean_q: 53.545090
[F[K 132328/500000: episode: 1829, duration: 0.950s, episode steps: 80, steps per second: 84, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.975 [0.000, 4.000], mean observation: 0.484 [0.350, 0.520], loss: 7.468776, mean_absolute_error: 42.389729, mean_q: 54.271065
[F[K 132379/500000: episode: 1830, duration: 0.515s, episode steps: 51, steps per second: 99, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.503 [0.430, 0.560], loss: 7.495896, mean_absolute_error: 41.920460, mean_q: 53.818298
[F[K 132420/500000: episode: 1831, duration: 0.530s, episode steps: 41, steps per second: 77, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.854 [0.000, 4.000], mean observation: 0.500 [0.350, 0.650], loss: 7.645854, mean_absolute_error: 41.557972, mean_q: 53.298065
[F[K 132493/500000: episode: 1832, duration: 0.915s, episode steps: 73, steps per second: 80, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.918 [0.000, 4.000], mean observation: 0.502 [0.430, 0.580], loss: 7.612961, mean_absolute_error: 42.404636, mean_q: 54.325909
[F[K 132570/500000: episode: 1833, duration: 0.878s, episode steps: 77, steps per second: 88, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.597 [0.000, 4.000], mean observation: 0.501 [0.430, 0.560], loss: 7.729202, mean_absolute_error: 42.076180, mean_q: 53.942692
[F[K 132627/500000: episode: 1834, duration: 0.657s, episode steps: 57, steps per second: 87, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.158 [0.000, 4.000], mean observation: 0.504 [0.470, 0.560], loss: 8.150455, mean_absolute_error: 41.665142, mean_q: 53.246159
[F[K 132704/500000: episode: 1835, duration: 0.919s, episode steps: 77, steps per second: 84, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.495 [0.420, 0.530], loss: 8.536024, mean_absolute_error: 42.000175, mean_q: 53.711697
[F[K 132904/500000: episode: 1836, duration: 2.383s, episode steps: 200, steps per second: 84, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.270 [0.000, 4.000], mean observation: 0.480 [0.370, 0.530], loss: 8.413091, mean_absolute_error: 41.855743, mean_q: 53.645454
[F[K 132945/500000: episode: 1837, duration: 0.495s, episode steps: 41, steps per second: 83, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.561 [0.000, 4.000], mean observation: 0.499 [0.350, 0.630], loss: 8.178609, mean_absolute_error: 42.149876, mean_q: 53.905453
[F[K 132990/500000: episode: 1838, duration: 0.579s, episode steps: 45, steps per second: 78, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 6.879511, mean_absolute_error: 41.879513, mean_q: 53.776611
[F[K 133058/500000: episode: 1839, duration: 0.792s, episode steps: 68, steps per second: 86, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.868 [0.000, 4.000], mean observation: 0.477 [0.380, 0.520], loss: 9.363792, mean_absolute_error: 42.033821, mean_q: 53.728485
[F[K 133139/500000: episode: 1840, duration: 0.886s, episode steps: 81, steps per second: 91, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.593 [0.000, 4.000], mean observation: 0.484 [0.410, 0.530], loss: 8.670938, mean_absolute_error: 41.809235, mean_q: 53.505104
[F[K 133218/500000: episode: 1841, duration: 1.064s, episode steps: 79, steps per second: 74, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.848 [0.000, 4.000], mean observation: 0.476 [0.370, 0.510], loss: 8.656525, mean_absolute_error: 42.173820, mean_q: 53.997089
[F[K 133285/500000: episode: 1842, duration: 0.840s, episode steps: 67, steps per second: 80, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.179 [0.000, 4.000], mean observation: 0.502 [0.390, 0.620], loss: 8.436760, mean_absolute_error: 42.093262, mean_q: 53.910358
[F[K 133365/500000: episode: 1843, duration: 1.058s, episode steps: 80, steps per second: 76, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.505 [0.490, 0.530], loss: 9.342751, mean_absolute_error: 41.327148, mean_q: 52.958862
[F[K 133440/500000: episode: 1844, duration: 0.918s, episode steps: 75, steps per second: 82, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.560 [0.000, 4.000], mean observation: 0.495 [0.400, 0.530], loss: 8.424697, mean_absolute_error: 41.621807, mean_q: 53.361973
[F[K 133529/500000: episode: 1845, duration: 1.194s, episode steps: 89, steps per second: 75, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.697 [0.000, 4.000], mean observation: 0.499 [0.430, 0.550], loss: 8.990382, mean_absolute_error: 41.968437, mean_q: 53.755569
[F[K 133618/500000: episode: 1846, duration: 1.086s, episode steps: 89, steps per second: 82, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.056 [0.000, 4.000], mean observation: 0.477 [0.360, 0.520], loss: 7.465194, mean_absolute_error: 42.614010, mean_q: 54.586201
[F[K 133692/500000: episode: 1847, duration: 1.088s, episode steps: 74, steps per second: 68, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.568 [0.000, 4.000], mean observation: 0.497 [0.440, 0.530], loss: 9.103016, mean_absolute_error: 42.046471, mean_q: 53.810165
[F[K 133762/500000: episode: 1848, duration: 0.955s, episode steps: 70, steps per second: 73, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.114 [0.000, 4.000], mean observation: 0.516 [0.480, 0.640], loss: 7.170367, mean_absolute_error: 42.070911, mean_q: 53.929096
[F[K 133879/500000: episode: 1849, duration: 1.272s, episode steps: 117, steps per second: 92, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.949 [0.000, 4.000], mean observation: 0.498 [0.360, 0.620], loss: 8.604804, mean_absolute_error: 41.740372, mean_q: 53.514118
[F[K 133937/500000: episode: 1850, duration: 0.778s, episode steps: 58, steps per second: 75, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 7.961356, mean_absolute_error: 42.081299, mean_q: 53.881729
[F[K 134023/500000: episode: 1851, duration: 0.961s, episode steps: 86, steps per second: 89, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.953 [0.000, 4.000], mean observation: 0.518 [0.470, 0.650], loss: 8.396868, mean_absolute_error: 42.039604, mean_q: 53.795723
[F[K 134087/500000: episode: 1852, duration: 0.788s, episode steps: 64, steps per second: 81, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.281 [0.000, 4.000], mean observation: 0.495 [0.380, 0.550], loss: 8.560648, mean_absolute_error: 41.836178, mean_q: 53.597614
[F[K 134144/500000: episode: 1853, duration: 0.868s, episode steps: 57, steps per second: 66, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.175 [0.000, 4.000], mean observation: 0.502 [0.370, 0.650], loss: 8.883596, mean_absolute_error: 41.516937, mean_q: 53.019939
[F[K 134344/500000: episode: 1854, duration: 2.363s, episode steps: 200, steps per second: 85, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.205 [0.000, 4.000], mean observation: 0.506 [0.410, 0.660], loss: 8.296808, mean_absolute_error: 41.838764, mean_q: 53.622284
[F[K 134422/500000: episode: 1855, duration: 0.935s, episode steps: 78, steps per second: 83, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.731 [0.000, 4.000], mean observation: 0.485 [0.430, 0.510], loss: 8.386024, mean_absolute_error: 42.016407, mean_q: 53.816914
[F[K 134507/500000: episode: 1856, duration: 1.078s, episode steps: 85, steps per second: 79, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.894 [0.000, 4.000], mean observation: 0.498 [0.450, 0.530], loss: 8.634593, mean_absolute_error: 42.247211, mean_q: 53.953770
[F[K 134620/500000: episode: 1857, duration: 1.313s, episode steps: 113, steps per second: 86, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.301 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 7.778928, mean_absolute_error: 41.627087, mean_q: 53.261940
[F[K 134680/500000: episode: 1858, duration: 0.789s, episode steps: 60, steps per second: 76, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.700 [0.000, 4.000], mean observation: 0.494 [0.410, 0.550], loss: 9.191284, mean_absolute_error: 41.817326, mean_q: 53.404007
[F[K 134765/500000: episode: 1859, duration: 1.085s, episode steps: 85, steps per second: 78, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.516 [0.480, 0.620], loss: 8.673414, mean_absolute_error: 41.901169, mean_q: 53.646271
[F[K 134842/500000: episode: 1860, duration: 0.893s, episode steps: 77, steps per second: 86, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.662 [0.000, 4.000], mean observation: 0.479 [0.360, 0.520], loss: 7.721303, mean_absolute_error: 41.943012, mean_q: 53.660664
[F[K 134883/500000: episode: 1861, duration: 0.486s, episode steps: 41, steps per second: 84, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.561 [0.000, 4.000], mean observation: 0.510 [0.470, 0.630], loss: 8.395874, mean_absolute_error: 42.545807, mean_q: 54.343346
[F[K 134923/500000: episode: 1862, duration: 0.452s, episode steps: 40, steps per second: 89, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.524 [0.470, 0.650], loss: 8.409407, mean_absolute_error: 42.592163, mean_q: 54.458263
[F[K 134991/500000: episode: 1863, duration: 0.918s, episode steps: 68, steps per second: 74, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.794 [0.000, 4.000], mean observation: 0.518 [0.470, 0.660], loss: 7.830127, mean_absolute_error: 42.341232, mean_q: 54.249645
[F[K 135109/500000: episode: 1864, duration: 1.438s, episode steps: 118, steps per second: 82, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.517 [0.000, 4.000], mean observation: 0.514 [0.480, 0.590], loss: 9.119662, mean_absolute_error: 41.930798, mean_q: 53.736446
[F[K 135171/500000: episode: 1865, duration: 0.820s, episode steps: 62, steps per second: 76, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.113 [0.000, 4.000], mean observation: 0.501 [0.400, 0.580], loss: 8.010652, mean_absolute_error: 41.076626, mean_q: 52.604389
[F[K 135221/500000: episode: 1866, duration: 0.744s, episode steps: 50, steps per second: 67, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.700 [0.000, 4.000], mean observation: 0.527 [0.470, 0.630], loss: 8.316294, mean_absolute_error: 42.155746, mean_q: 54.067440
[F[K 135317/500000: episode: 1867, duration: 1.288s, episode steps: 96, steps per second: 75, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.854 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 9.470177, mean_absolute_error: 41.636169, mean_q: 53.298889
[F[K 135440/500000: episode: 1868, duration: 1.582s, episode steps: 123, steps per second: 78, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.098 [0.000, 4.000], mean observation: 0.490 [0.340, 0.580], loss: 8.236390, mean_absolute_error: 42.045036, mean_q: 53.884659
[F[K 135530/500000: episode: 1869, duration: 1.199s, episode steps: 90, steps per second: 75, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 0.495 [0.350, 0.590], loss: 8.161563, mean_absolute_error: 41.969639, mean_q: 53.738811
[F[K 135610/500000: episode: 1870, duration: 1.039s, episode steps: 80, steps per second: 77, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.625 [0.000, 4.000], mean observation: 0.512 [0.500, 0.580], loss: 8.034365, mean_absolute_error: 42.216228, mean_q: 54.107471
[F[K 135688/500000: episode: 1871, duration: 1.061s, episode steps: 78, steps per second: 74, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.987 [0.000, 4.000], mean observation: 0.511 [0.490, 0.590], loss: 8.171223, mean_absolute_error: 42.313251, mean_q: 54.182644
[F[K 135727/500000: episode: 1872, duration: 0.533s, episode steps: 39, steps per second: 73, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.513 [0.000, 3.000], mean observation: 0.514 [0.470, 0.640], loss: 9.260004, mean_absolute_error: 42.567219, mean_q: 54.561317
[F[K 135766/500000: episode: 1873, duration: 0.558s, episode steps: 39, steps per second: 70, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.872 [0.000, 4.000], mean observation: 0.510 [0.440, 0.650], loss: 5.976532, mean_absolute_error: 41.821735, mean_q: 53.550423
[F[K 135859/500000: episode: 1874, duration: 1.246s, episode steps: 93, steps per second: 75, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.011 [0.000, 4.000], mean observation: 0.500 [0.410, 0.570], loss: 8.552502, mean_absolute_error: 42.018429, mean_q: 53.846424
[F[K 135932/500000: episode: 1875, duration: 1.165s, episode steps: 73, steps per second: 63, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.863 [0.000, 4.000], mean observation: 0.475 [0.370, 0.520], loss: 9.053808, mean_absolute_error: 41.406197, mean_q: 52.973724
[F[K 135995/500000: episode: 1876, duration: 0.898s, episode steps: 63, steps per second: 70, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.498 [0.370, 0.600], loss: 8.770900, mean_absolute_error: 42.049068, mean_q: 53.771400
[F[K 136141/500000: episode: 1877, duration: 2.168s, episode steps: 146, steps per second: 67, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.349 [0.000, 4.000], mean observation: 0.479 [0.370, 0.510], loss: 7.863056, mean_absolute_error: 41.776947, mean_q: 53.499832
[F[K 136196/500000: episode: 1878, duration: 0.764s, episode steps: 55, steps per second: 72, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.236 [0.000, 4.000], mean observation: 0.511 [0.490, 0.570], loss: 8.970704, mean_absolute_error: 41.841557, mean_q: 53.671772
[F[K 136264/500000: episode: 1879, duration: 0.831s, episode steps: 68, steps per second: 82, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.824 [0.000, 4.000], mean observation: 0.498 [0.360, 0.610], loss: 8.423378, mean_absolute_error: 41.563457, mean_q: 53.190228
[F[K 136305/500000: episode: 1880, duration: 0.625s, episode steps: 41, steps per second: 66, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.927 [1.000, 4.000], mean observation: 0.522 [0.470, 0.650], loss: 8.173844, mean_absolute_error: 41.697914, mean_q: 53.311546
[F[K 136378/500000: episode: 1881, duration: 1.115s, episode steps: 73, steps per second: 65, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.438 [0.000, 4.000], mean observation: 0.511 [0.470, 0.570], loss: 7.478366, mean_absolute_error: 41.856243, mean_q: 53.595787
[F[K 136438/500000: episode: 1882, duration: 0.910s, episode steps: 60, steps per second: 66, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.733 [0.000, 4.000], mean observation: 0.495 [0.420, 0.530], loss: 7.914572, mean_absolute_error: 41.925671, mean_q: 53.797276
[F[K 136541/500000: episode: 1883, duration: 1.357s, episode steps: 103, steps per second: 76, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.990 [0.000, 4.000], mean observation: 0.502 [0.430, 0.580], loss: 9.550713, mean_absolute_error: 41.395054, mean_q: 53.100803
[F[K 136601/500000: episode: 1884, duration: 0.894s, episode steps: 60, steps per second: 67, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.496 [0.460, 0.530], loss: 9.882789, mean_absolute_error: 41.455566, mean_q: 53.122856
[F[K 136642/500000: episode: 1885, duration: 0.627s, episode steps: 41, steps per second: 65, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.707 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 6.747355, mean_absolute_error: 42.352486, mean_q: 54.358715
[F[K 136787/500000: episode: 1886, duration: 2.241s, episode steps: 145, steps per second: 65, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.517 [0.000, 4.000], mean observation: 0.482 [0.330, 0.510], loss: 8.153642, mean_absolute_error: 41.933399, mean_q: 53.736408
[F[K 136837/500000: episode: 1887, duration: 0.653s, episode steps: 50, steps per second: 77, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.860 [0.000, 4.000], mean observation: 0.515 [0.490, 0.580], loss: 8.072954, mean_absolute_error: 42.060532, mean_q: 53.815029
[F[K 136929/500000: episode: 1888, duration: 1.256s, episode steps: 92, steps per second: 73, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.098 [0.000, 4.000], mean observation: 0.495 [0.370, 0.600], loss: 7.911735, mean_absolute_error: 42.077492, mean_q: 53.884251
[F[K 136989/500000: episode: 1889, duration: 0.865s, episode steps: 60, steps per second: 69, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.501 [0.420, 0.570], loss: 7.794363, mean_absolute_error: 42.215961, mean_q: 54.070541
[F[K 137044/500000: episode: 1890, duration: 0.708s, episode steps: 55, steps per second: 78, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.182 [0.000, 4.000], mean observation: 0.483 [0.400, 0.510], loss: 7.131213, mean_absolute_error: 42.668785, mean_q: 54.533310
[F[K 137112/500000: episode: 1891, duration: 0.895s, episode steps: 68, steps per second: 76, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.221 [0.000, 4.000], mean observation: 0.503 [0.370, 0.640], loss: 8.907372, mean_absolute_error: 41.291576, mean_q: 52.856834
[F[K 137178/500000: episode: 1892, duration: 0.974s, episode steps: 66, steps per second: 68, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.496 [0.460, 0.530], loss: 8.972182, mean_absolute_error: 41.010643, mean_q: 52.553627
[F[K 137242/500000: episode: 1893, duration: 0.876s, episode steps: 64, steps per second: 73, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.516 [0.000, 4.000], mean observation: 0.498 [0.420, 0.570], loss: 10.054862, mean_absolute_error: 41.215599, mean_q: 52.655235
[F[K 137301/500000: episode: 1894, duration: 0.848s, episode steps: 59, steps per second: 70, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.017 [0.000, 4.000], mean observation: 0.511 [0.490, 0.540], loss: 8.708409, mean_absolute_error: 41.783401, mean_q: 53.406315
[F[K 137366/500000: episode: 1895, duration: 1.045s, episode steps: 65, steps per second: 62, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.215 [0.000, 4.000], mean observation: 0.515 [0.490, 0.570], loss: 9.397858, mean_absolute_error: 41.389439, mean_q: 52.958836
[F[K 137425/500000: episode: 1896, duration: 0.888s, episode steps: 59, steps per second: 66, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.678 [0.000, 4.000], mean observation: 0.503 [0.410, 0.590], loss: 8.157645, mean_absolute_error: 41.870007, mean_q: 53.642323
[F[K 137500/500000: episode: 1897, duration: 1.028s, episode steps: 75, steps per second: 73, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.040 [0.000, 4.000], mean observation: 0.525 [0.490, 0.640], loss: 7.528746, mean_absolute_error: 42.129616, mean_q: 54.100361
[F[K 137631/500000: episode: 1898, duration: 1.956s, episode steps: 131, steps per second: 67, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.206 [0.000, 4.000], mean observation: 0.483 [0.370, 0.510], loss: 8.542933, mean_absolute_error: 41.392231, mean_q: 52.964661
[F[K 137670/500000: episode: 1899, duration: 0.703s, episode steps: 39, steps per second: 55, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.564 [0.000, 4.000], mean observation: 0.512 [0.470, 0.640], loss: 7.239475, mean_absolute_error: 41.449387, mean_q: 53.241329
[F[K 137748/500000: episode: 1900, duration: 1.134s, episode steps: 78, steps per second: 69, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.397 [0.000, 4.000], mean observation: 0.514 [0.480, 0.630], loss: 8.427916, mean_absolute_error: 41.447422, mean_q: 53.129280
[F[K 137820/500000: episode: 1901, duration: 0.728s, episode steps: 72, steps per second: 99, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.917 [0.000, 4.000], mean observation: 0.482 [0.390, 0.530], loss: 7.586131, mean_absolute_error: 41.569782, mean_q: 53.272678
[F[K 137898/500000: episode: 1902, duration: 1.022s, episode steps: 78, steps per second: 76, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.506 [0.440, 0.620], loss: 8.135055, mean_absolute_error: 41.246857, mean_q: 52.842625
[F[K 138098/500000: episode: 1903, duration: 2.315s, episode steps: 200, steps per second: 86, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.105 [0.000, 4.000], mean observation: 0.500 [0.440, 0.550], loss: 7.664438, mean_absolute_error: 41.631176, mean_q: 53.288631
[F[K 138167/500000: episode: 1904, duration: 0.810s, episode steps: 69, steps per second: 85, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.502 [0.450, 0.550], loss: 8.557678, mean_absolute_error: 41.785343, mean_q: 53.483002
[F[K 138231/500000: episode: 1905, duration: 0.907s, episode steps: 64, steps per second: 71, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.483 [0.380, 0.530], loss: 8.508929, mean_absolute_error: 41.530582, mean_q: 53.232353
[F[K 138291/500000: episode: 1906, duration: 0.790s, episode steps: 60, steps per second: 76, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.950 [0.000, 4.000], mean observation: 0.502 [0.440, 0.550], loss: 9.285070, mean_absolute_error: 41.805115, mean_q: 53.534496
[F[K 138356/500000: episode: 1907, duration: 0.729s, episode steps: 65, steps per second: 89, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.138 [0.000, 4.000], mean observation: 0.502 [0.410, 0.600], loss: 7.269738, mean_absolute_error: 42.192108, mean_q: 54.090614
[F[K 138435/500000: episode: 1908, duration: 0.879s, episode steps: 79, steps per second: 90, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.013 [0.000, 4.000], mean observation: 0.488 [0.410, 0.530], loss: 9.463231, mean_absolute_error: 41.417767, mean_q: 52.939053
[F[K 138471/500000: episode: 1909, duration: 0.336s, episode steps: 36, steps per second: 107, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.278 [0.000, 3.000], mean observation: 0.502 [0.380, 0.650], loss: 8.558900, mean_absolute_error: 41.828373, mean_q: 53.518166
[F[K 138523/500000: episode: 1910, duration: 0.660s, episode steps: 52, steps per second: 79, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 0.481 [0.360, 0.530], loss: 9.214363, mean_absolute_error: 40.614750, mean_q: 51.934200
[F[K 138588/500000: episode: 1911, duration: 0.846s, episode steps: 65, steps per second: 77, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.969 [0.000, 4.000], mean observation: 0.479 [0.390, 0.520], loss: 8.243259, mean_absolute_error: 41.770153, mean_q: 53.507141
[F[K 138653/500000: episode: 1912, duration: 0.855s, episode steps: 65, steps per second: 76, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.092 [0.000, 4.000], mean observation: 0.479 [0.410, 0.520], loss: 7.594804, mean_absolute_error: 42.339722, mean_q: 54.284908
[F[K 138704/500000: episode: 1913, duration: 0.678s, episode steps: 51, steps per second: 75, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.961 [0.000, 4.000], mean observation: 0.494 [0.350, 0.590], loss: 8.604034, mean_absolute_error: 40.755585, mean_q: 52.297630
[F[K 138766/500000: episode: 1914, duration: 0.779s, episode steps: 62, steps per second: 80, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.919 [0.000, 4.000], mean observation: 0.520 [0.470, 0.610], loss: 8.051283, mean_absolute_error: 41.592945, mean_q: 53.323818
[F[K 138816/500000: episode: 1915, duration: 0.650s, episode steps: 50, steps per second: 77, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.880 [0.000, 4.000], mean observation: 0.501 [0.390, 0.610], loss: 8.083604, mean_absolute_error: 41.503036, mean_q: 53.271397
[F[K 138889/500000: episode: 1916, duration: 0.960s, episode steps: 73, steps per second: 76, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.516 [0.490, 0.570], loss: 8.612296, mean_absolute_error: 41.389576, mean_q: 53.079483
[F[K 139033/500000: episode: 1917, duration: 1.980s, episode steps: 144, steps per second: 73, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.097 [0.000, 4.000], mean observation: 0.483 [0.380, 0.510], loss: 7.977087, mean_absolute_error: 41.425545, mean_q: 52.963276
[F[K 139105/500000: episode: 1918, duration: 0.922s, episode steps: 72, steps per second: 78, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.513 [0.500, 0.580], loss: 7.664814, mean_absolute_error: 41.968376, mean_q: 53.808392
[F[K 139173/500000: episode: 1919, duration: 0.875s, episode steps: 68, steps per second: 78, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.912 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 8.847313, mean_absolute_error: 41.265923, mean_q: 52.718975
[F[K 139252/500000: episode: 1920, duration: 1.045s, episode steps: 79, steps per second: 76, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.532 [0.000, 4.000], mean observation: 0.483 [0.400, 0.530], loss: 8.478639, mean_absolute_error: 42.067799, mean_q: 53.800392
[F[K 139321/500000: episode: 1921, duration: 0.969s, episode steps: 69, steps per second: 71, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.971 [0.000, 4.000], mean observation: 0.503 [0.380, 0.620], loss: 8.739044, mean_absolute_error: 41.729713, mean_q: 53.403397
[F[K 139432/500000: episode: 1922, duration: 1.416s, episode steps: 111, steps per second: 78, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.892 [0.000, 4.000], mean observation: 0.492 [0.350, 0.570], loss: 10.209292, mean_absolute_error: 41.235161, mean_q: 52.726509
[F[K 139511/500000: episode: 1923, duration: 0.810s, episode steps: 79, steps per second: 97, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.165 [0.000, 4.000], mean observation: 0.496 [0.410, 0.540], loss: 8.452950, mean_absolute_error: 41.446571, mean_q: 53.200172
[F[K 139597/500000: episode: 1924, duration: 1.225s, episode steps: 86, steps per second: 70, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.506 [0.480, 0.540], loss: 8.546939, mean_absolute_error: 40.799324, mean_q: 52.136105
[F[K 139651/500000: episode: 1925, duration: 0.738s, episode steps: 54, steps per second: 73, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.259 [0.000, 4.000], mean observation: 0.511 [0.480, 0.590], loss: 7.375670, mean_absolute_error: 42.334446, mean_q: 54.221973
[F[K 139727/500000: episode: 1926, duration: 1.054s, episode steps: 76, steps per second: 72, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.276 [0.000, 4.000], mean observation: 0.496 [0.430, 0.530], loss: 7.818807, mean_absolute_error: 41.732655, mean_q: 53.520943
[F[K 139801/500000: episode: 1927, duration: 0.915s, episode steps: 74, steps per second: 81, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.162 [0.000, 4.000], mean observation: 0.506 [0.460, 0.600], loss: 8.377848, mean_absolute_error: 41.605175, mean_q: 53.251877
[F[K 139907/500000: episode: 1928, duration: 1.454s, episode steps: 106, steps per second: 73, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.283 [0.000, 4.000], mean observation: 0.505 [0.470, 0.530], loss: 9.667376, mean_absolute_error: 41.146648, mean_q: 52.618027
[F[K 139979/500000: episode: 1929, duration: 1.042s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.510 [0.490, 0.580], loss: 9.269962, mean_absolute_error: 41.391785, mean_q: 52.996475
[F[K 140039/500000: episode: 1930, duration: 0.885s, episode steps: 60, steps per second: 68, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.883 [0.000, 4.000], mean observation: 0.522 [0.500, 0.630], loss: 7.787738, mean_absolute_error: 41.880421, mean_q: 53.683582
[F[K 140100/500000: episode: 1931, duration: 0.747s, episode steps: 61, steps per second: 82, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.721 [0.000, 4.000], mean observation: 0.492 [0.350, 0.540], loss: 8.872903, mean_absolute_error: 41.354935, mean_q: 52.910038
[F[K 140187/500000: episode: 1932, duration: 1.201s, episode steps: 87, steps per second: 72, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [0.000, 4.000], mean observation: 0.513 [0.460, 0.640], loss: 8.098910, mean_absolute_error: 41.468681, mean_q: 53.129219
[F[K 140234/500000: episode: 1933, duration: 0.653s, episode steps: 47, steps per second: 72, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.766 [0.000, 4.000], mean observation: 0.493 [0.360, 0.560], loss: 7.590271, mean_absolute_error: 41.339790, mean_q: 52.997849
[F[K 140266/500000: episode: 1934, duration: 0.493s, episode steps: 32, steps per second: 65, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.312 [0.000, 3.000], mean observation: 0.517 [0.470, 0.650], loss: 6.821206, mean_absolute_error: 41.648857, mean_q: 53.445244
[F[K 140349/500000: episode: 1935, duration: 1.210s, episode steps: 83, steps per second: 69, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.145 [0.000, 4.000], mean observation: 0.490 [0.420, 0.530], loss: 8.208217, mean_absolute_error: 41.482857, mean_q: 53.259079
[F[K 140424/500000: episode: 1936, duration: 1.035s, episode steps: 75, steps per second: 72, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.293 [0.000, 4.000], mean observation: 0.504 [0.480, 0.550], loss: 8.950008, mean_absolute_error: 41.256447, mean_q: 52.913269
[F[K 140502/500000: episode: 1937, duration: 1.176s, episode steps: 78, steps per second: 66, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.949 [0.000, 4.000], mean observation: 0.518 [0.470, 0.640], loss: 8.173488, mean_absolute_error: 41.409397, mean_q: 53.129494
[F[K 140585/500000: episode: 1938, duration: 1.231s, episode steps: 83, steps per second: 67, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.205 [0.000, 4.000], mean observation: 0.500 [0.450, 0.540], loss: 9.004615, mean_absolute_error: 41.207981, mean_q: 52.752369
[F[K 140672/500000: episode: 1939, duration: 1.164s, episode steps: 87, steps per second: 75, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.069 [0.000, 4.000], mean observation: 0.493 [0.440, 0.520], loss: 7.813364, mean_absolute_error: 41.792721, mean_q: 53.441681
[F[K 140734/500000: episode: 1940, duration: 0.723s, episode steps: 62, steps per second: 86, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.565 [0.000, 4.000], mean observation: 0.503 [0.460, 0.560], loss: 9.580245, mean_absolute_error: 41.772377, mean_q: 53.580219
[F[K 140826/500000: episode: 1941, duration: 1.214s, episode steps: 92, steps per second: 76, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 0.510 [0.470, 0.590], loss: 9.373737, mean_absolute_error: 41.106903, mean_q: 52.606487
[F[K 140902/500000: episode: 1942, duration: 0.967s, episode steps: 76, steps per second: 79, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.501 [0.470, 0.550], loss: 8.339597, mean_absolute_error: 40.581139, mean_q: 51.914120
[F[K 140988/500000: episode: 1943, duration: 1.279s, episode steps: 86, steps per second: 67, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.953 [0.000, 4.000], mean observation: 0.493 [0.400, 0.530], loss: 7.885691, mean_absolute_error: 41.858196, mean_q: 53.619965
[F[K 141058/500000: episode: 1944, duration: 1.151s, episode steps: 70, steps per second: 61, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 0.490 [0.430, 0.530], loss: 8.802740, mean_absolute_error: 41.282467, mean_q: 52.794590
[F[K 141116/500000: episode: 1945, duration: 0.887s, episode steps: 58, steps per second: 65, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.017 [0.000, 4.000], mean observation: 0.497 [0.360, 0.580], loss: 7.317428, mean_absolute_error: 41.245960, mean_q: 53.059082
[F[K 141259/500000: episode: 1946, duration: 2.258s, episode steps: 143, steps per second: 63, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.063 [0.000, 4.000], mean observation: 0.497 [0.380, 0.600], loss: 7.910914, mean_absolute_error: 41.980579, mean_q: 53.779800
[F[K 141300/500000: episode: 1947, duration: 0.488s, episode steps: 41, steps per second: 84, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.854 [0.000, 4.000], mean observation: 0.520 [0.470, 0.610], loss: 8.217339, mean_absolute_error: 42.424911, mean_q: 54.337086
[F[K 141348/500000: episode: 1948, duration: 0.642s, episode steps: 48, steps per second: 75, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.729 [0.000, 4.000], mean observation: 0.504 [0.420, 0.620], loss: 9.360360, mean_absolute_error: 42.228481, mean_q: 54.018543
[F[K 141483/500000: episode: 1949, duration: 1.715s, episode steps: 135, steps per second: 79, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.556 [0.000, 4.000], mean observation: 0.487 [0.410, 0.530], loss: 8.528174, mean_absolute_error: 41.376041, mean_q: 52.956871
[F[K 141576/500000: episode: 1950, duration: 1.258s, episode steps: 93, steps per second: 74, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.194 [0.000, 4.000], mean observation: 0.482 [0.360, 0.530], loss: 8.358193, mean_absolute_error: 41.645897, mean_q: 53.357628
[F[K 141684/500000: episode: 1951, duration: 1.341s, episode steps: 108, steps per second: 81, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.935 [0.000, 4.000], mean observation: 0.502 [0.450, 0.540], loss: 8.053554, mean_absolute_error: 42.142902, mean_q: 53.928524
[F[K 141753/500000: episode: 1952, duration: 0.763s, episode steps: 69, steps per second: 90, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.508 [0.470, 0.610], loss: 8.222373, mean_absolute_error: 40.935204, mean_q: 52.432976
[F[K 141837/500000: episode: 1953, duration: 0.891s, episode steps: 84, steps per second: 94, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.536 [0.000, 4.000], mean observation: 0.502 [0.470, 0.540], loss: 8.394567, mean_absolute_error: 41.813843, mean_q: 53.505493
[F[K 141940/500000: episode: 1954, duration: 1.241s, episode steps: 103, steps per second: 83, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.477 [0.360, 0.520], loss: 8.428417, mean_absolute_error: 42.356682, mean_q: 54.265755
[F[K 142014/500000: episode: 1955, duration: 0.874s, episode steps: 74, steps per second: 85, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.480 [0.370, 0.520], loss: 8.002265, mean_absolute_error: 42.341656, mean_q: 54.124905
[F[K 142048/500000: episode: 1956, duration: 0.495s, episode steps: 34, steps per second: 69, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.588 [0.000, 3.000], mean observation: 0.517 [0.470, 0.650], loss: 9.200091, mean_absolute_error: 41.361996, mean_q: 52.949799
[F[K 142086/500000: episode: 1957, duration: 0.473s, episode steps: 38, steps per second: 80, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.763 [0.000, 4.000], mean observation: 0.512 [0.470, 0.630], loss: 7.725910, mean_absolute_error: 41.667370, mean_q: 53.295048
[F[K 142179/500000: episode: 1958, duration: 1.162s, episode steps: 93, steps per second: 80, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.742 [0.000, 4.000], mean observation: 0.497 [0.420, 0.530], loss: 7.977410, mean_absolute_error: 41.706787, mean_q: 53.308048
[F[K 142231/500000: episode: 1959, duration: 0.654s, episode steps: 52, steps per second: 80, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.923 [0.000, 4.000], mean observation: 0.503 [0.430, 0.580], loss: 10.264907, mean_absolute_error: 41.448666, mean_q: 53.055161
[F[K 142313/500000: episode: 1960, duration: 0.967s, episode steps: 82, steps per second: 85, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.744 [0.000, 4.000], mean observation: 0.484 [0.350, 0.510], loss: 7.777576, mean_absolute_error: 41.778145, mean_q: 53.541157
[F[K 142347/500000: episode: 1961, duration: 0.405s, episode steps: 34, steps per second: 84, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.647 [0.000, 4.000], mean observation: 0.521 [0.470, 0.640], loss: 8.836278, mean_absolute_error: 41.430019, mean_q: 53.073662
[F[K 142413/500000: episode: 1962, duration: 0.837s, episode steps: 66, steps per second: 79, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.894 [0.000, 4.000], mean observation: 0.508 [0.470, 0.570], loss: 8.250822, mean_absolute_error: 41.502113, mean_q: 53.131073
[F[K 142494/500000: episode: 1963, duration: 1.096s, episode steps: 81, steps per second: 74, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.383 [0.000, 4.000], mean observation: 0.495 [0.360, 0.600], loss: 8.027761, mean_absolute_error: 41.143169, mean_q: 52.680279
[F[K 142552/500000: episode: 1964, duration: 0.893s, episode steps: 58, steps per second: 65, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.879 [0.000, 4.000], mean observation: 0.501 [0.480, 0.530], loss: 8.737648, mean_absolute_error: 41.509678, mean_q: 53.169235
[F[K 142626/500000: episode: 1965, duration: 0.845s, episode steps: 74, steps per second: 88, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.878 [0.000, 4.000], mean observation: 0.486 [0.360, 0.530], loss: 8.963386, mean_absolute_error: 41.542461, mean_q: 53.203648
[F[K 142681/500000: episode: 1966, duration: 0.695s, episode steps: 55, steps per second: 79, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.018 [0.000, 4.000], mean observation: 0.499 [0.400, 0.620], loss: 8.657320, mean_absolute_error: 41.908222, mean_q: 53.675701
[F[K 142750/500000: episode: 1967, duration: 0.805s, episode steps: 69, steps per second: 86, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.913 [0.000, 4.000], mean observation: 0.494 [0.410, 0.550], loss: 8.814303, mean_absolute_error: 42.330902, mean_q: 54.158176
[F[K 142906/500000: episode: 1968, duration: 1.823s, episode steps: 156, steps per second: 86, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.173 [0.000, 4.000], mean observation: 0.508 [0.480, 0.580], loss: 9.136477, mean_absolute_error: 41.022400, mean_q: 52.450157
[F[K 142974/500000: episode: 1969, duration: 0.805s, episode steps: 68, steps per second: 85, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.514 [0.480, 0.590], loss: 7.904319, mean_absolute_error: 41.519997, mean_q: 53.145901
[F[K 143030/500000: episode: 1970, duration: 0.802s, episode steps: 56, steps per second: 70, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.499 [0.390, 0.640], loss: 8.780840, mean_absolute_error: 41.029816, mean_q: 52.437141
[F[K 143079/500000: episode: 1971, duration: 0.736s, episode steps: 49, steps per second: 67, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 8.069202, mean_absolute_error: 41.140549, mean_q: 52.744755
[F[K 143197/500000: episode: 1972, duration: 1.478s, episode steps: 118, steps per second: 80, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.280 [0.000, 4.000], mean observation: 0.478 [0.360, 0.530], loss: 8.486687, mean_absolute_error: 41.190197, mean_q: 52.774162
[F[K 143286/500000: episode: 1973, duration: 1.067s, episode steps: 89, steps per second: 83, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.112 [0.000, 4.000], mean observation: 0.488 [0.390, 0.520], loss: 8.294552, mean_absolute_error: 42.103046, mean_q: 53.975647
[F[K 143409/500000: episode: 1974, duration: 1.440s, episode steps: 123, steps per second: 85, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.593 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 7.576969, mean_absolute_error: 41.241444, mean_q: 52.781452
[F[K 143461/500000: episode: 1975, duration: 0.735s, episode steps: 52, steps per second: 71, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.135 [0.000, 4.000], mean observation: 0.499 [0.390, 0.610], loss: 8.300167, mean_absolute_error: 41.856544, mean_q: 53.598660
[F[K 143525/500000: episode: 1976, duration: 0.865s, episode steps: 64, steps per second: 74, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.359 [0.000, 4.000], mean observation: 0.521 [0.480, 0.640], loss: 9.220334, mean_absolute_error: 41.606457, mean_q: 53.236767
[F[K 143564/500000: episode: 1977, duration: 0.535s, episode steps: 39, steps per second: 73, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.513 [0.000, 4.000], mean observation: 0.506 [0.420, 0.640], loss: 9.060399, mean_absolute_error: 40.709095, mean_q: 52.081135
[F[K 143717/500000: episode: 1978, duration: 1.999s, episode steps: 153, steps per second: 77, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.157 [0.000, 4.000], mean observation: 0.491 [0.370, 0.550], loss: 9.168659, mean_absolute_error: 41.467587, mean_q: 53.167511
[F[K 143801/500000: episode: 1979, duration: 0.972s, episode steps: 84, steps per second: 86, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.499 [0.470, 0.550], loss: 8.598396, mean_absolute_error: 41.223072, mean_q: 52.817905
[F[K 143840/500000: episode: 1980, duration: 0.528s, episode steps: 39, steps per second: 74, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.564 [0.000, 4.000], mean observation: 0.525 [0.470, 0.640], loss: 8.651373, mean_absolute_error: 40.681290, mean_q: 52.088074
[F[K 143884/500000: episode: 1981, duration: 0.589s, episode steps: 44, steps per second: 75, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.977 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 6.899994, mean_absolute_error: 41.537540, mean_q: 53.276806
[F[K 143960/500000: episode: 1982, duration: 1.062s, episode steps: 76, steps per second: 72, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.934 [0.000, 4.000], mean observation: 0.495 [0.380, 0.590], loss: 8.845794, mean_absolute_error: 41.454750, mean_q: 53.070995
[F[K 144009/500000: episode: 1983, duration: 0.657s, episode steps: 49, steps per second: 75, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 8.206822, mean_absolute_error: 41.164669, mean_q: 52.802666
[F[K 144051/500000: episode: 1984, duration: 0.523s, episode steps: 42, steps per second: 80, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.786 [0.000, 4.000], mean observation: 0.510 [0.450, 0.640], loss: 9.066652, mean_absolute_error: 40.940819, mean_q: 52.302769
[F[K 144103/500000: episode: 1985, duration: 0.655s, episode steps: 52, steps per second: 79, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.981 [0.000, 4.000], mean observation: 0.512 [0.470, 0.610], loss: 8.762049, mean_absolute_error: 41.894192, mean_q: 53.772751
[F[K 144169/500000: episode: 1986, duration: 0.805s, episode steps: 66, steps per second: 82, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 0.482 [0.400, 0.530], loss: 8.739197, mean_absolute_error: 41.214840, mean_q: 52.679096
[F[K 144227/500000: episode: 1987, duration: 0.714s, episode steps: 58, steps per second: 81, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.897 [0.000, 4.000], mean observation: 0.503 [0.470, 0.530], loss: 8.916928, mean_absolute_error: 41.969051, mean_q: 53.753498
[F[K 144419/500000: episode: 1988, duration: 2.483s, episode steps: 192, steps per second: 77, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.476 [0.310, 0.530], loss: 8.750112, mean_absolute_error: 41.397198, mean_q: 53.071613
[F[K 144489/500000: episode: 1989, duration: 0.863s, episode steps: 70, steps per second: 81, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.514 [0.470, 0.660], loss: 8.322391, mean_absolute_error: 41.529976, mean_q: 53.069984
[F[K 144529/500000: episode: 1990, duration: 0.508s, episode steps: 40, steps per second: 79, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.550 [0.000, 4.000], mean observation: 0.518 [0.470, 0.610], loss: 8.547622, mean_absolute_error: 41.267857, mean_q: 52.884315
[F[K 144698/500000: episode: 1991, duration: 1.973s, episode steps: 169, steps per second: 86, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.959 [0.000, 4.000], mean observation: 0.522 [0.480, 0.660], loss: 7.156179, mean_absolute_error: 41.376614, mean_q: 53.061089
[F[K 144789/500000: episode: 1992, duration: 1.240s, episode steps: 91, steps per second: 73, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.110 [0.000, 4.000], mean observation: 0.489 [0.440, 0.530], loss: 8.283267, mean_absolute_error: 41.404205, mean_q: 53.030903
[F[K 144868/500000: episode: 1993, duration: 1.202s, episode steps: 79, steps per second: 66, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.367 [0.000, 4.000], mean observation: 0.499 [0.390, 0.600], loss: 7.701501, mean_absolute_error: 41.007225, mean_q: 52.577221
[F[K 144936/500000: episode: 1994, duration: 0.995s, episode steps: 68, steps per second: 68, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.956 [0.000, 4.000], mean observation: 0.493 [0.410, 0.530], loss: 7.314882, mean_absolute_error: 41.480042, mean_q: 53.128948
[F[K 145018/500000: episode: 1995, duration: 1.080s, episode steps: 82, steps per second: 76, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.510 [0.470, 0.590], loss: 8.293066, mean_absolute_error: 41.584126, mean_q: 53.222633
[F[K 145099/500000: episode: 1996, duration: 1.037s, episode steps: 81, steps per second: 78, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.519 [0.000, 4.000], mean observation: 0.495 [0.380, 0.560], loss: 8.188205, mean_absolute_error: 41.649799, mean_q: 53.183201
[F[K 145152/500000: episode: 1997, duration: 0.700s, episode steps: 53, steps per second: 76, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.501 [0.410, 0.630], loss: 8.808165, mean_absolute_error: 41.459290, mean_q: 53.115929
[F[K 145213/500000: episode: 1998, duration: 0.717s, episode steps: 61, steps per second: 85, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.705 [0.000, 4.000], mean observation: 0.505 [0.490, 0.530], loss: 9.301365, mean_absolute_error: 41.933723, mean_q: 53.766964
[F[K 145285/500000: episode: 1999, duration: 1.038s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 0.481 [0.360, 0.530], loss: 8.865175, mean_absolute_error: 40.750957, mean_q: 52.348946
[F[K 145365/500000: episode: 2000, duration: 1.057s, episode steps: 80, steps per second: 76, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.812 [0.000, 4.000], mean observation: 0.493 [0.430, 0.520], loss: 9.989326, mean_absolute_error: 41.416466, mean_q: 52.968163
[F[K 145433/500000: episode: 2001, duration: 0.854s, episode steps: 68, steps per second: 80, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.647 [0.000, 4.000], mean observation: 0.493 [0.410, 0.530], loss: 8.765041, mean_absolute_error: 40.814358, mean_q: 52.281322
[F[K 145505/500000: episode: 2002, duration: 1.085s, episode steps: 72, steps per second: 66, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.375 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 6.999996, mean_absolute_error: 41.300945, mean_q: 52.975166
[F[K 145556/500000: episode: 2003, duration: 0.762s, episode steps: 51, steps per second: 67, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.521 [0.470, 0.650], loss: 8.363394, mean_absolute_error: 40.948654, mean_q: 52.534027
[F[K 145627/500000: episode: 2004, duration: 0.901s, episode steps: 71, steps per second: 79, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.423 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 8.236719, mean_absolute_error: 41.677906, mean_q: 53.443058
[F[K 145827/500000: episode: 2005, duration: 2.802s, episode steps: 200, steps per second: 71, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.165 [0.000, 4.000], mean observation: 0.510 [0.430, 0.650], loss: 8.037267, mean_absolute_error: 41.326344, mean_q: 52.945610
[F[K 145889/500000: episode: 2006, duration: 0.902s, episode steps: 62, steps per second: 69, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.210 [0.000, 4.000], mean observation: 0.505 [0.440, 0.580], loss: 8.393196, mean_absolute_error: 42.481567, mean_q: 54.393879
[F[K 145949/500000: episode: 2007, duration: 0.749s, episode steps: 60, steps per second: 80, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.520 [0.470, 0.610], loss: 8.602231, mean_absolute_error: 41.376072, mean_q: 52.990208
[F[K 146026/500000: episode: 2008, duration: 1.080s, episode steps: 77, steps per second: 71, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.065 [0.000, 4.000], mean observation: 0.508 [0.490, 0.540], loss: 8.579921, mean_absolute_error: 41.162167, mean_q: 52.716766
[F[K 146107/500000: episode: 2009, duration: 1.260s, episode steps: 81, steps per second: 64, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.049 [0.000, 4.000], mean observation: 0.498 [0.360, 0.620], loss: 8.270228, mean_absolute_error: 42.003452, mean_q: 53.856174
[F[K 146140/500000: episode: 2010, duration: 0.524s, episode steps: 33, steps per second: 63, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.636 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 7.688329, mean_absolute_error: 40.242290, mean_q: 51.663967
[F[K 146181/500000: episode: 2011, duration: 0.639s, episode steps: 41, steps per second: 64, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.390 [0.000, 3.000], mean observation: 0.508 [0.440, 0.640], loss: 8.947946, mean_absolute_error: 41.711582, mean_q: 53.310093
[F[K 146240/500000: episode: 2012, duration: 0.924s, episode steps: 59, steps per second: 64, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.153 [0.000, 4.000], mean observation: 0.517 [0.470, 0.610], loss: 8.131364, mean_absolute_error: 41.650600, mean_q: 53.346844
[F[K 146304/500000: episode: 2013, duration: 0.967s, episode steps: 64, steps per second: 66, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.511 [0.470, 0.620], loss: 8.848150, mean_absolute_error: 40.949974, mean_q: 52.481827
[F[K 146375/500000: episode: 2014, duration: 1.069s, episode steps: 71, steps per second: 66, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.169 [0.000, 4.000], mean observation: 0.492 [0.450, 0.520], loss: 8.034300, mean_absolute_error: 42.078983, mean_q: 53.758286
[F[K 146442/500000: episode: 2015, duration: 0.953s, episode steps: 67, steps per second: 70, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.119 [0.000, 4.000], mean observation: 0.509 [0.490, 0.560], loss: 7.320282, mean_absolute_error: 41.661644, mean_q: 53.377602
[F[K 146633/500000: episode: 2016, duration: 2.622s, episode steps: 191, steps per second: 73, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.503 [0.480, 0.550], loss: 8.032109, mean_absolute_error: 41.749008, mean_q: 53.462276
[F[K 146696/500000: episode: 2017, duration: 0.910s, episode steps: 63, steps per second: 69, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.238 [0.000, 4.000], mean observation: 0.490 [0.370, 0.530], loss: 7.597083, mean_absolute_error: 42.186115, mean_q: 54.085644
[F[K 146772/500000: episode: 2018, duration: 0.972s, episode steps: 76, steps per second: 78, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.855 [0.000, 4.000], mean observation: 0.486 [0.370, 0.510], loss: 7.931751, mean_absolute_error: 41.679577, mean_q: 53.417976
[F[K 146848/500000: episode: 2019, duration: 0.916s, episode steps: 76, steps per second: 83, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.211 [0.000, 4.000], mean observation: 0.491 [0.440, 0.530], loss: 8.545525, mean_absolute_error: 41.679230, mean_q: 53.405701
[F[K 146911/500000: episode: 2020, duration: 0.852s, episode steps: 63, steps per second: 74, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.498 [0.360, 0.610], loss: 7.976974, mean_absolute_error: 41.380859, mean_q: 53.075321
[F[K 146967/500000: episode: 2021, duration: 0.714s, episode steps: 56, steps per second: 78, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.714 [0.000, 4.000], mean observation: 0.507 [0.440, 0.620], loss: 7.629949, mean_absolute_error: 41.556789, mean_q: 53.296288
[F[K 147062/500000: episode: 2022, duration: 1.328s, episode steps: 95, steps per second: 72, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.053 [0.000, 4.000], mean observation: 0.501 [0.400, 0.630], loss: 8.501595, mean_absolute_error: 42.077183, mean_q: 53.946232
[F[K 147195/500000: episode: 2023, duration: 1.495s, episode steps: 133, steps per second: 89, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 8.585987, mean_absolute_error: 41.215168, mean_q: 52.802067
[F[K 147300/500000: episode: 2024, duration: 1.213s, episode steps: 105, steps per second: 87, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.181 [0.000, 4.000], mean observation: 0.525 [0.490, 0.670], loss: 7.750958, mean_absolute_error: 41.582394, mean_q: 53.173862
[F[K 147370/500000: episode: 2025, duration: 1.026s, episode steps: 70, steps per second: 68, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.500 [0.450, 0.580], loss: 9.860752, mean_absolute_error: 41.770763, mean_q: 53.416260
[F[K 147436/500000: episode: 2026, duration: 0.906s, episode steps: 66, steps per second: 73, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.545 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 8.368556, mean_absolute_error: 41.372330, mean_q: 53.002144
[F[K 147473/500000: episode: 2027, duration: 0.554s, episode steps: 37, steps per second: 67, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.541 [0.000, 3.000], mean observation: 0.527 [0.480, 0.650], loss: 8.419587, mean_absolute_error: 41.087730, mean_q: 52.664623
[F[K 147539/500000: episode: 2028, duration: 0.937s, episode steps: 66, steps per second: 70, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.773 [0.000, 4.000], mean observation: 0.498 [0.400, 0.540], loss: 8.351226, mean_absolute_error: 41.969555, mean_q: 53.681339
[F[K 147623/500000: episode: 2029, duration: 1.182s, episode steps: 84, steps per second: 71, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.976 [0.000, 4.000], mean observation: 0.498 [0.470, 0.540], loss: 8.518616, mean_absolute_error: 41.837360, mean_q: 53.502075
[F[K 147749/500000: episode: 2030, duration: 1.592s, episode steps: 126, steps per second: 79, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.159 [0.000, 4.000], mean observation: 0.493 [0.370, 0.570], loss: 8.556974, mean_absolute_error: 41.563057, mean_q: 53.162186
[F[K 147802/500000: episode: 2031, duration: 0.754s, episode steps: 53, steps per second: 70, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.623 [0.000, 4.000], mean observation: 0.519 [0.490, 0.580], loss: 9.579048, mean_absolute_error: 41.994656, mean_q: 53.777561
[F[K 147876/500000: episode: 2032, duration: 1.011s, episode steps: 74, steps per second: 73, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.973 [0.000, 4.000], mean observation: 0.504 [0.420, 0.650], loss: 8.546167, mean_absolute_error: 41.408783, mean_q: 52.911510
[F[K 147972/500000: episode: 2033, duration: 1.209s, episode steps: 96, steps per second: 79, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.073 [0.000, 4.000], mean observation: 0.492 [0.350, 0.590], loss: 9.213881, mean_absolute_error: 41.139088, mean_q: 52.669689
[F[K 148070/500000: episode: 2034, duration: 1.244s, episode steps: 98, steps per second: 79, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.163 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 9.317945, mean_absolute_error: 41.799694, mean_q: 53.411053
[F[K 148114/500000: episode: 2035, duration: 0.730s, episode steps: 44, steps per second: 60, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.682 [0.000, 4.000], mean observation: 0.521 [0.470, 0.640], loss: 8.442669, mean_absolute_error: 40.892265, mean_q: 52.402294
[F[K 148187/500000: episode: 2036, duration: 0.990s, episode steps: 73, steps per second: 74, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.493 [0.000, 4.000], mean observation: 0.505 [0.420, 0.600], loss: 7.938762, mean_absolute_error: 41.257050, mean_q: 53.016613
[F[K 148243/500000: episode: 2037, duration: 0.820s, episode steps: 56, steps per second: 68, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.929 [0.000, 4.000], mean observation: 0.520 [0.470, 0.610], loss: 9.630409, mean_absolute_error: 41.688934, mean_q: 53.247517
[F[K 148308/500000: episode: 2038, duration: 0.738s, episode steps: 65, steps per second: 88, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.677 [0.000, 4.000], mean observation: 0.523 [0.490, 0.640], loss: 9.139300, mean_absolute_error: 41.649677, mean_q: 53.275517
[F[K 148366/500000: episode: 2039, duration: 0.798s, episode steps: 58, steps per second: 73, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.476 [0.370, 0.530], loss: 8.225527, mean_absolute_error: 41.383381, mean_q: 53.000774
[F[K 148422/500000: episode: 2040, duration: 0.836s, episode steps: 56, steps per second: 67, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.946 [0.000, 4.000], mean observation: 0.492 [0.440, 0.530], loss: 7.638146, mean_absolute_error: 41.421352, mean_q: 53.056263
[F[K 148475/500000: episode: 2041, duration: 0.720s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.487 [0.360, 0.550], loss: 8.460271, mean_absolute_error: 41.782124, mean_q: 53.560791
[F[K 148528/500000: episode: 2042, duration: 0.687s, episode steps: 53, steps per second: 77, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.830 [0.000, 4.000], mean observation: 0.517 [0.480, 0.590], loss: 9.730892, mean_absolute_error: 41.215443, mean_q: 52.851128
[F[K 148645/500000: episode: 2043, duration: 1.664s, episode steps: 117, steps per second: 70, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.547 [0.000, 4.000], mean observation: 0.498 [0.390, 0.600], loss: 8.164239, mean_absolute_error: 41.540607, mean_q: 53.178677
[F[K 148746/500000: episode: 2044, duration: 1.587s, episode steps: 101, steps per second: 64, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.990 [0.000, 4.000], mean observation: 0.484 [0.360, 0.520], loss: 7.797394, mean_absolute_error: 41.576805, mean_q: 53.251133
[F[K 148852/500000: episode: 2045, duration: 1.578s, episode steps: 106, steps per second: 67, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.877 [0.000, 4.000], mean observation: 0.518 [0.470, 0.670], loss: 8.194517, mean_absolute_error: 40.998173, mean_q: 52.484478
[F[K 148916/500000: episode: 2046, duration: 0.976s, episode steps: 64, steps per second: 66, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.516 [0.490, 0.570], loss: 8.771220, mean_absolute_error: 41.704559, mean_q: 53.301147
[F[K 148962/500000: episode: 2047, duration: 0.732s, episode steps: 46, steps per second: 63, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.370 [0.000, 4.000], mean observation: 0.519 [0.470, 0.630], loss: 8.271673, mean_absolute_error: 40.871140, mean_q: 52.531761
[F[K 149036/500000: episode: 2048, duration: 1.254s, episode steps: 74, steps per second: 59, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.838 [0.000, 4.000], mean observation: 0.499 [0.390, 0.580], loss: 10.030661, mean_absolute_error: 41.345142, mean_q: 52.872063
[F[K 149097/500000: episode: 2049, duration: 1.002s, episode steps: 61, steps per second: 61, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.639 [0.000, 4.000], mean observation: 0.505 [0.480, 0.560], loss: 9.953755, mean_absolute_error: 41.897766, mean_q: 53.441841
[F[K 149163/500000: episode: 2050, duration: 0.859s, episode steps: 66, steps per second: 77, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.015 [0.000, 4.000], mean observation: 0.495 [0.350, 0.580], loss: 7.087844, mean_absolute_error: 41.562099, mean_q: 53.329147
[F[K 149238/500000: episode: 2051, duration: 1.117s, episode steps: 75, steps per second: 67, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.987 [0.000, 4.000], mean observation: 0.477 [0.350, 0.520], loss: 8.221080, mean_absolute_error: 41.480499, mean_q: 53.104103
[F[K 149314/500000: episode: 2052, duration: 1.102s, episode steps: 76, steps per second: 69, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.053 [0.000, 4.000], mean observation: 0.496 [0.450, 0.530], loss: 7.520492, mean_absolute_error: 40.990627, mean_q: 52.553673
[F[K 149400/500000: episode: 2053, duration: 1.168s, episode steps: 86, steps per second: 74, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.012 [0.000, 4.000], mean observation: 0.495 [0.410, 0.530], loss: 8.852946, mean_absolute_error: 41.135536, mean_q: 52.648148
[F[K 149476/500000: episode: 2054, duration: 0.996s, episode steps: 76, steps per second: 76, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.961 [0.000, 4.000], mean observation: 0.476 [0.380, 0.520], loss: 8.971230, mean_absolute_error: 41.176243, mean_q: 52.692554
[F[K 149541/500000: episode: 2055, duration: 0.863s, episode steps: 65, steps per second: 75, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.092 [0.000, 4.000], mean observation: 0.503 [0.430, 0.600], loss: 7.766525, mean_absolute_error: 41.577961, mean_q: 53.226986
[F[K 149615/500000: episode: 2056, duration: 0.762s, episode steps: 74, steps per second: 97, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.892 [0.000, 4.000], mean observation: 0.517 [0.470, 0.590], loss: 8.308579, mean_absolute_error: 41.400768, mean_q: 53.144424
[F[K 149689/500000: episode: 2057, duration: 0.851s, episode steps: 74, steps per second: 87, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.491 [0.360, 0.570], loss: 8.164519, mean_absolute_error: 41.249939, mean_q: 52.836239
[F[K 149767/500000: episode: 2058, duration: 0.944s, episode steps: 78, steps per second: 83, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.872 [0.000, 4.000], mean observation: 0.493 [0.380, 0.570], loss: 8.361322, mean_absolute_error: 41.741257, mean_q: 53.431759
[F[K 149834/500000: episode: 2059, duration: 0.710s, episode steps: 67, steps per second: 94, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.104 [0.000, 4.000], mean observation: 0.499 [0.380, 0.610], loss: 9.356841, mean_absolute_error: 41.925426, mean_q: 53.529755
[F[K 149902/500000: episode: 2060, duration: 0.692s, episode steps: 68, steps per second: 98, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.971 [0.000, 4.000], mean observation: 0.514 [0.480, 0.610], loss: 8.239332, mean_absolute_error: 41.548225, mean_q: 53.217583
[F[K 149966/500000: episode: 2061, duration: 0.734s, episode steps: 64, steps per second: 87, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.297 [0.000, 4.000], mean observation: 0.491 [0.430, 0.530], loss: 9.007792, mean_absolute_error: 41.810780, mean_q: 53.502621
[F[K 150049/500000: episode: 2062, duration: 0.929s, episode steps: 83, steps per second: 89, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.133 [0.000, 4.000], mean observation: 0.491 [0.430, 0.530], loss: 8.782160, mean_absolute_error: 41.583817, mean_q: 53.214542
[F[K 150152/500000: episode: 2063, duration: 1.237s, episode steps: 103, steps per second: 83, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.165 [0.000, 4.000], mean observation: 0.488 [0.340, 0.550], loss: 8.657662, mean_absolute_error: 41.381519, mean_q: 52.853596
[F[K 150234/500000: episode: 2064, duration: 1.065s, episode steps: 82, steps per second: 77, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.146 [0.000, 4.000], mean observation: 0.502 [0.470, 0.530], loss: 8.752098, mean_absolute_error: 41.525501, mean_q: 53.144131
[F[K 150297/500000: episode: 2065, duration: 0.851s, episode steps: 63, steps per second: 74, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.206 [0.000, 4.000], mean observation: 0.495 [0.410, 0.530], loss: 7.955888, mean_absolute_error: 41.329048, mean_q: 52.927059
[F[K 150358/500000: episode: 2066, duration: 0.739s, episode steps: 61, steps per second: 83, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.918 [0.000, 4.000], mean observation: 0.501 [0.460, 0.530], loss: 8.681451, mean_absolute_error: 41.750313, mean_q: 53.408062
[F[K 150412/500000: episode: 2067, duration: 0.557s, episode steps: 54, steps per second: 97, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.222 [0.000, 4.000], mean observation: 0.493 [0.360, 0.560], loss: 8.268585, mean_absolute_error: 41.602024, mean_q: 53.378010
[F[K 150552/500000: episode: 2068, duration: 1.508s, episode steps: 140, steps per second: 93, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.879 [0.000, 4.000], mean observation: 0.480 [0.390, 0.530], loss: 8.670157, mean_absolute_error: 41.509026, mean_q: 53.153774
[F[K 150649/500000: episode: 2069, duration: 1.156s, episode steps: 97, steps per second: 84, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.113 [0.000, 4.000], mean observation: 0.491 [0.420, 0.530], loss: 7.282763, mean_absolute_error: 41.826218, mean_q: 53.689358
[F[K 150770/500000: episode: 2070, duration: 1.425s, episode steps: 121, steps per second: 85, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.165 [0.000, 4.000], mean observation: 0.492 [0.430, 0.530], loss: 8.302927, mean_absolute_error: 41.462734, mean_q: 53.081211
[F[K 150809/500000: episode: 2071, duration: 0.553s, episode steps: 39, steps per second: 70, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.692 [0.000, 4.000], mean observation: 0.507 [0.420, 0.640], loss: 8.100450, mean_absolute_error: 40.692440, mean_q: 52.246468
[F[K 150870/500000: episode: 2072, duration: 0.731s, episode steps: 61, steps per second: 83, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.951 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 8.878407, mean_absolute_error: 41.281872, mean_q: 52.893440
[F[K 150971/500000: episode: 2073, duration: 1.320s, episode steps: 101, steps per second: 76, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.020 [0.000, 4.000], mean observation: 0.511 [0.470, 0.580], loss: 7.374600, mean_absolute_error: 41.821693, mean_q: 53.579803
[F[K 151049/500000: episode: 2074, duration: 0.961s, episode steps: 78, steps per second: 81, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.462 [0.000, 4.000], mean observation: 0.504 [0.410, 0.640], loss: 8.775535, mean_absolute_error: 42.205414, mean_q: 53.917244
[F[K 151124/500000: episode: 2075, duration: 0.901s, episode steps: 75, steps per second: 83, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.293 [0.000, 4.000], mean observation: 0.512 [0.480, 0.590], loss: 9.389907, mean_absolute_error: 42.025997, mean_q: 53.653450
[F[K 151236/500000: episode: 2076, duration: 1.654s, episode steps: 112, steps per second: 68, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.268 [0.000, 4.000], mean observation: 0.504 [0.480, 0.540], loss: 7.969803, mean_absolute_error: 41.588310, mean_q: 53.380157
[F[K 151297/500000: episode: 2077, duration: 0.716s, episode steps: 61, steps per second: 85, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.262 [0.000, 4.000], mean observation: 0.480 [0.380, 0.530], loss: 8.578466, mean_absolute_error: 41.642593, mean_q: 53.425217
[F[K 151367/500000: episode: 2078, duration: 0.915s, episode steps: 70, steps per second: 77, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.505 [0.470, 0.580], loss: 8.060318, mean_absolute_error: 41.995129, mean_q: 53.799835
[F[K 151480/500000: episode: 2079, duration: 1.393s, episode steps: 113, steps per second: 81, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.484 [0.370, 0.530], loss: 9.175633, mean_absolute_error: 40.945507, mean_q: 52.349648
[F[K 151548/500000: episode: 2080, duration: 0.941s, episode steps: 68, steps per second: 72, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.868 [0.000, 4.000], mean observation: 0.510 [0.470, 0.580], loss: 8.020616, mean_absolute_error: 41.836094, mean_q: 53.661804
[F[K 151590/500000: episode: 2081, duration: 0.538s, episode steps: 42, steps per second: 78, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.690 [0.000, 4.000], mean observation: 0.518 [0.470, 0.650], loss: 10.244908, mean_absolute_error: 41.492817, mean_q: 53.207310
[F[K 151646/500000: episode: 2082, duration: 0.734s, episode steps: 56, steps per second: 76, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.495 [0.420, 0.530], loss: 7.555847, mean_absolute_error: 41.638958, mean_q: 53.361530
[F[K 151732/500000: episode: 2083, duration: 1.181s, episode steps: 86, steps per second: 73, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.505 [0.430, 0.640], loss: 10.216597, mean_absolute_error: 41.593006, mean_q: 53.181515
[F[K 151792/500000: episode: 2084, duration: 0.883s, episode steps: 60, steps per second: 68, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.700 [0.000, 4.000], mean observation: 0.499 [0.350, 0.600], loss: 7.789213, mean_absolute_error: 41.220707, mean_q: 52.860806
[F[K 151856/500000: episode: 2085, duration: 0.849s, episode steps: 64, steps per second: 75, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.891 [0.000, 4.000], mean observation: 0.499 [0.370, 0.590], loss: 8.511509, mean_absolute_error: 41.666504, mean_q: 53.373848
[F[K 151922/500000: episode: 2086, duration: 0.785s, episode steps: 66, steps per second: 84, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.970 [0.000, 4.000], mean observation: 0.484 [0.380, 0.530], loss: 8.031174, mean_absolute_error: 41.172932, mean_q: 52.889019
[F[K 151965/500000: episode: 2087, duration: 0.493s, episode steps: 43, steps per second: 87, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.465 [0.000, 4.000], mean observation: 0.516 [0.470, 0.620], loss: 9.660169, mean_absolute_error: 41.533310, mean_q: 53.078976
[F[K 152050/500000: episode: 2088, duration: 1.086s, episode steps: 85, steps per second: 78, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.035 [0.000, 4.000], mean observation: 0.479 [0.360, 0.520], loss: 8.442590, mean_absolute_error: 40.897167, mean_q: 52.409477
[F[K 152128/500000: episode: 2089, duration: 0.964s, episode steps: 78, steps per second: 81, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.923 [0.000, 4.000], mean observation: 0.513 [0.470, 0.620], loss: 7.984359, mean_absolute_error: 41.627625, mean_q: 53.290138
[F[K 152189/500000: episode: 2090, duration: 0.714s, episode steps: 61, steps per second: 85, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.918 [0.000, 4.000], mean observation: 0.472 [0.350, 0.510], loss: 6.975232, mean_absolute_error: 41.251038, mean_q: 52.826515
[F[K 152255/500000: episode: 2091, duration: 0.814s, episode steps: 66, steps per second: 81, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.030 [0.000, 4.000], mean observation: 0.511 [0.470, 0.590], loss: 9.754028, mean_absolute_error: 41.039974, mean_q: 52.567383
[F[K 152311/500000: episode: 2092, duration: 0.761s, episode steps: 56, steps per second: 74, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.982 [0.000, 4.000], mean observation: 0.513 [0.500, 0.570], loss: 8.277227, mean_absolute_error: 41.447178, mean_q: 53.063679
[F[K 152479/500000: episode: 2093, duration: 2.003s, episode steps: 168, steps per second: 84, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.196 [0.000, 4.000], mean observation: 0.503 [0.430, 0.610], loss: 9.327127, mean_absolute_error: 41.070736, mean_q: 52.540451
[F[K 152541/500000: episode: 2094, duration: 0.824s, episode steps: 62, steps per second: 75, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.758 [0.000, 4.000], mean observation: 0.499 [0.450, 0.530], loss: 8.869646, mean_absolute_error: 41.437222, mean_q: 53.034786
[F[K 152595/500000: episode: 2095, duration: 0.667s, episode steps: 54, steps per second: 81, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 8.057597, mean_absolute_error: 41.416431, mean_q: 53.113819
[F[K 152671/500000: episode: 2096, duration: 1.002s, episode steps: 76, steps per second: 76, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.618 [0.000, 4.000], mean observation: 0.515 [0.480, 0.650], loss: 9.173291, mean_absolute_error: 41.641434, mean_q: 53.179413
[F[K 152777/500000: episode: 2097, duration: 1.308s, episode steps: 106, steps per second: 81, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.538 [0.000, 4.000], mean observation: 0.490 [0.410, 0.530], loss: 8.508250, mean_absolute_error: 40.863274, mean_q: 52.319489
[F[K 152880/500000: episode: 2098, duration: 1.074s, episode steps: 103, steps per second: 96, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.282 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 9.022935, mean_absolute_error: 41.604881, mean_q: 53.282967
[F[K 152937/500000: episode: 2099, duration: 0.599s, episode steps: 57, steps per second: 95, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.614 [0.000, 4.000], mean observation: 0.501 [0.400, 0.600], loss: 8.064797, mean_absolute_error: 41.957623, mean_q: 53.660027
[F[K 153000/500000: episode: 2100, duration: 0.825s, episode steps: 63, steps per second: 76, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.730 [0.000, 4.000], mean observation: 0.515 [0.470, 0.600], loss: 7.426431, mean_absolute_error: 41.216965, mean_q: 52.826759
[F[K 153059/500000: episode: 2101, duration: 0.807s, episode steps: 59, steps per second: 73, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.492 [0.000, 4.000], mean observation: 0.484 [0.410, 0.530], loss: 8.196105, mean_absolute_error: 41.205242, mean_q: 52.812706
[F[K 153198/500000: episode: 2102, duration: 1.813s, episode steps: 139, steps per second: 77, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.065 [0.000, 4.000], mean observation: 0.513 [0.480, 0.600], loss: 8.784530, mean_absolute_error: 41.380676, mean_q: 52.997353
[F[K 153298/500000: episode: 2103, duration: 1.226s, episode steps: 100, steps per second: 82, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.040 [0.000, 4.000], mean observation: 0.502 [0.420, 0.610], loss: 8.304571, mean_absolute_error: 41.162495, mean_q: 52.737587
[F[K 153363/500000: episode: 2104, duration: 0.813s, episode steps: 65, steps per second: 80, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.769 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 7.700127, mean_absolute_error: 42.353077, mean_q: 54.081837
[F[K 153435/500000: episode: 2105, duration: 0.765s, episode steps: 72, steps per second: 94, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.181 [0.000, 4.000], mean observation: 0.492 [0.450, 0.520], loss: 7.310451, mean_absolute_error: 40.822872, mean_q: 52.222397
[F[K 153516/500000: episode: 2106, duration: 0.973s, episode steps: 81, steps per second: 83, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.827 [0.000, 4.000], mean observation: 0.509 [0.470, 0.560], loss: 8.130546, mean_absolute_error: 41.834984, mean_q: 53.543842
[F[K 153608/500000: episode: 2107, duration: 1.160s, episode steps: 92, steps per second: 79, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.512 [0.480, 0.560], loss: 7.918136, mean_absolute_error: 41.742821, mean_q: 53.428558
[F[K 153658/500000: episode: 2108, duration: 0.596s, episode steps: 50, steps per second: 84, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 0.501 [0.360, 0.640], loss: 8.659695, mean_absolute_error: 41.304008, mean_q: 52.906906
[F[K 153770/500000: episode: 2109, duration: 1.265s, episode steps: 112, steps per second: 89, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.991 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 7.289374, mean_absolute_error: 41.813171, mean_q: 53.482967
[F[K 153844/500000: episode: 2110, duration: 0.955s, episode steps: 74, steps per second: 77, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.230 [0.000, 4.000], mean observation: 0.495 [0.360, 0.600], loss: 8.458989, mean_absolute_error: 41.499989, mean_q: 53.096199
[F[K 153888/500000: episode: 2111, duration: 0.500s, episode steps: 44, steps per second: 88, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.659 [0.000, 4.000], mean observation: 0.509 [0.450, 0.640], loss: 9.704975, mean_absolute_error: 41.355824, mean_q: 52.989967
[F[K 153929/500000: episode: 2112, duration: 0.477s, episode steps: 41, steps per second: 86, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.073 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 10.507002, mean_absolute_error: 40.804623, mean_q: 52.099079
[F[K 154013/500000: episode: 2113, duration: 0.983s, episode steps: 84, steps per second: 85, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 0.497 [0.440, 0.530], loss: 7.413215, mean_absolute_error: 41.220467, mean_q: 52.846504
[F[K 154080/500000: episode: 2114, duration: 0.824s, episode steps: 67, steps per second: 81, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.060 [0.000, 4.000], mean observation: 0.521 [0.470, 0.610], loss: 8.072445, mean_absolute_error: 41.302803, mean_q: 52.799301
[F[K 154185/500000: episode: 2115, duration: 1.029s, episode steps: 105, steps per second: 102, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.324 [0.000, 4.000], mean observation: 0.482 [0.360, 0.530], loss: 8.335868, mean_absolute_error: 41.546379, mean_q: 53.201942
[F[K 154218/500000: episode: 2116, duration: 0.396s, episode steps: 33, steps per second: 83, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.818 [0.000, 4.000], mean observation: 0.518 [0.470, 0.620], loss: 9.722050, mean_absolute_error: 41.694065, mean_q: 53.295689
[F[K 154310/500000: episode: 2117, duration: 1.132s, episode steps: 92, steps per second: 81, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.022 [0.000, 4.000], mean observation: 0.498 [0.450, 0.530], loss: 9.043175, mean_absolute_error: 41.842384, mean_q: 53.452808
[F[K 154361/500000: episode: 2118, duration: 0.784s, episode steps: 51, steps per second: 65, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.255 [0.000, 4.000], mean observation: 0.485 [0.370, 0.530], loss: 8.877323, mean_absolute_error: 42.182693, mean_q: 53.992809
[F[K 154415/500000: episode: 2119, duration: 0.758s, episode steps: 54, steps per second: 71, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.074 [0.000, 4.000], mean observation: 0.479 [0.380, 0.510], loss: 8.736857, mean_absolute_error: 41.585098, mean_q: 53.129005
[F[K 154494/500000: episode: 2120, duration: 1.003s, episode steps: 79, steps per second: 79, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.835 [0.000, 4.000], mean observation: 0.506 [0.480, 0.550], loss: 8.083055, mean_absolute_error: 41.719753, mean_q: 53.387764
[F[K 154568/500000: episode: 2121, duration: 1.032s, episode steps: 74, steps per second: 72, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.446 [0.000, 4.000], mean observation: 0.509 [0.480, 0.600], loss: 10.064830, mean_absolute_error: 41.072948, mean_q: 52.567970
[F[K 154697/500000: episode: 2122, duration: 1.546s, episode steps: 129, steps per second: 83, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.109 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 8.694397, mean_absolute_error: 41.582127, mean_q: 53.194405
[F[K 154797/500000: episode: 2123, duration: 1.037s, episode steps: 100, steps per second: 96, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.180 [0.000, 4.000], mean observation: 0.501 [0.470, 0.570], loss: 8.279369, mean_absolute_error: 40.715130, mean_q: 52.123043
[F[K 154837/500000: episode: 2124, duration: 0.593s, episode steps: 40, steps per second: 67, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.775 [0.000, 4.000], mean observation: 0.525 [0.470, 0.650], loss: 9.400290, mean_absolute_error: 41.081234, mean_q: 52.630211
[F[K 154913/500000: episode: 2125, duration: 1.102s, episode steps: 76, steps per second: 69, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.368 [0.000, 4.000], mean observation: 0.478 [0.380, 0.530], loss: 7.763632, mean_absolute_error: 41.630718, mean_q: 53.419037
[F[K 154980/500000: episode: 2126, duration: 0.931s, episode steps: 67, steps per second: 72, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.433 [0.000, 4.000], mean observation: 0.500 [0.360, 0.630], loss: 9.448896, mean_absolute_error: 41.073566, mean_q: 52.498192
[F[K 155017/500000: episode: 2127, duration: 0.476s, episode steps: 37, steps per second: 78, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.811 [0.000, 4.000], mean observation: 0.526 [0.470, 0.640], loss: 9.162715, mean_absolute_error: 40.756573, mean_q: 52.269405
[F[K 155106/500000: episode: 2128, duration: 1.160s, episode steps: 89, steps per second: 77, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.157 [0.000, 4.000], mean observation: 0.505 [0.430, 0.600], loss: 8.527183, mean_absolute_error: 41.579128, mean_q: 53.281750
[F[K 155185/500000: episode: 2129, duration: 1.005s, episode steps: 79, steps per second: 79, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.633 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 8.701399, mean_absolute_error: 41.915699, mean_q: 53.669147
[F[K 155252/500000: episode: 2130, duration: 0.961s, episode steps: 67, steps per second: 70, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.497 [0.450, 0.530], loss: 8.901602, mean_absolute_error: 41.935677, mean_q: 53.657898
[F[K 155343/500000: episode: 2131, duration: 1.257s, episode steps: 91, steps per second: 72, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.198 [0.000, 4.000], mean observation: 0.490 [0.420, 0.530], loss: 8.194762, mean_absolute_error: 41.679955, mean_q: 53.320179
[F[K 155405/500000: episode: 2132, duration: 0.985s, episode steps: 62, steps per second: 63, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.968 [0.000, 4.000], mean observation: 0.492 [0.450, 0.510], loss: 8.806932, mean_absolute_error: 41.106899, mean_q: 52.613575
[F[K 155605/500000: episode: 2133, duration: 2.594s, episode steps: 200, steps per second: 77, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.700 [0.000, 4.000], mean observation: 0.502 [0.470, 0.550], loss: 8.143020, mean_absolute_error: 41.491817, mean_q: 53.116562
[F[K 155684/500000: episode: 2134, duration: 1.148s, episode steps: 79, steps per second: 69, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.241 [0.000, 4.000], mean observation: 0.495 [0.400, 0.530], loss: 8.414574, mean_absolute_error: 40.940430, mean_q: 52.522831
[F[K 155789/500000: episode: 2135, duration: 1.441s, episode steps: 105, steps per second: 73, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.562 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 8.371946, mean_absolute_error: 41.728859, mean_q: 53.551945
[F[K 155849/500000: episode: 2136, duration: 0.803s, episode steps: 60, steps per second: 75, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.583 [0.000, 4.000], mean observation: 0.502 [0.460, 0.530], loss: 9.369900, mean_absolute_error: 41.843567, mean_q: 53.368607
[F[K 155927/500000: episode: 2137, duration: 1.075s, episode steps: 78, steps per second: 73, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.064 [0.000, 4.000], mean observation: 0.480 [0.370, 0.520], loss: 8.624505, mean_absolute_error: 42.167488, mean_q: 53.977669
[F[K 155988/500000: episode: 2138, duration: 0.822s, episode steps: 61, steps per second: 74, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.311 [0.000, 4.000], mean observation: 0.504 [0.430, 0.580], loss: 7.947811, mean_absolute_error: 41.991791, mean_q: 53.734909
[F[K 156062/500000: episode: 2139, duration: 1.017s, episode steps: 74, steps per second: 73, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.838 [0.000, 4.000], mean observation: 0.506 [0.450, 0.610], loss: 7.458583, mean_absolute_error: 41.665966, mean_q: 53.445213
[F[K 156122/500000: episode: 2140, duration: 0.848s, episode steps: 60, steps per second: 71, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.383 [0.000, 4.000], mean observation: 0.516 [0.500, 0.590], loss: 8.511380, mean_absolute_error: 40.890984, mean_q: 52.299149
[F[K 156186/500000: episode: 2141, duration: 0.828s, episode steps: 64, steps per second: 77, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.520 [0.480, 0.600], loss: 9.426990, mean_absolute_error: 42.392128, mean_q: 54.175041
[F[K 156248/500000: episode: 2142, duration: 0.923s, episode steps: 62, steps per second: 67, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.242 [0.000, 4.000], mean observation: 0.495 [0.440, 0.530], loss: 8.645094, mean_absolute_error: 40.968842, mean_q: 52.534492
[F[K 156301/500000: episode: 2143, duration: 0.780s, episode steps: 53, steps per second: 68, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.509 [0.000, 4.000], mean observation: 0.490 [0.370, 0.530], loss: 7.769809, mean_absolute_error: 42.027447, mean_q: 53.807388
[F[K 156371/500000: episode: 2144, duration: 0.989s, episode steps: 70, steps per second: 71, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.506 [0.480, 0.570], loss: 9.191214, mean_absolute_error: 41.425877, mean_q: 53.007538
[F[K 156414/500000: episode: 2145, duration: 0.604s, episode steps: 43, steps per second: 71, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.510 [0.480, 0.610], loss: 8.690201, mean_absolute_error: 41.612438, mean_q: 53.354408
[F[K 156452/500000: episode: 2146, duration: 0.531s, episode steps: 38, steps per second: 72, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.512 [0.470, 0.620], loss: 8.863838, mean_absolute_error: 40.791473, mean_q: 52.231419
[F[K 156520/500000: episode: 2147, duration: 0.886s, episode steps: 68, steps per second: 77, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.868 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 8.557716, mean_absolute_error: 41.507401, mean_q: 53.217712
[F[K 156571/500000: episode: 2148, duration: 0.635s, episode steps: 51, steps per second: 80, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.500 [0.380, 0.590], loss: 8.888372, mean_absolute_error: 41.811256, mean_q: 53.474541
[F[K 156641/500000: episode: 2149, duration: 0.959s, episode steps: 70, steps per second: 73, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.497 [0.380, 0.600], loss: 9.992234, mean_absolute_error: 41.144775, mean_q: 52.664410
[F[K 156733/500000: episode: 2150, duration: 1.230s, episode steps: 92, steps per second: 75, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.663 [0.000, 4.000], mean observation: 0.499 [0.410, 0.560], loss: 8.133424, mean_absolute_error: 41.777077, mean_q: 53.604141
[F[K 156793/500000: episode: 2151, duration: 0.892s, episode steps: 60, steps per second: 67, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.503 [0.360, 0.640], loss: 8.412580, mean_absolute_error: 40.754704, mean_q: 52.112278
[F[K 156846/500000: episode: 2152, duration: 0.673s, episode steps: 53, steps per second: 79, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.566 [0.000, 4.000], mean observation: 0.510 [0.460, 0.620], loss: 8.416311, mean_absolute_error: 41.493111, mean_q: 53.028763
[F[K 156952/500000: episode: 2153, duration: 1.548s, episode steps: 106, steps per second: 68, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.510 [0.480, 0.590], loss: 9.589589, mean_absolute_error: 41.407085, mean_q: 53.002842
[F[K 157013/500000: episode: 2154, duration: 0.880s, episode steps: 61, steps per second: 69, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.967 [0.000, 4.000], mean observation: 0.495 [0.450, 0.520], loss: 7.512695, mean_absolute_error: 41.752480, mean_q: 53.513302
[F[K 157089/500000: episode: 2155, duration: 1.001s, episode steps: 76, steps per second: 76, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.974 [0.000, 4.000], mean observation: 0.515 [0.490, 0.600], loss: 8.677880, mean_absolute_error: 41.460548, mean_q: 53.060482
[F[K 157158/500000: episode: 2156, duration: 1.017s, episode steps: 69, steps per second: 68, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.246 [0.000, 4.000], mean observation: 0.491 [0.450, 0.520], loss: 8.452068, mean_absolute_error: 40.809933, mean_q: 52.270538
[F[K 157199/500000: episode: 2157, duration: 0.594s, episode steps: 41, steps per second: 69, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.854 [0.000, 4.000], mean observation: 0.507 [0.430, 0.650], loss: 7.304753, mean_absolute_error: 41.485474, mean_q: 53.240105
[F[K 157283/500000: episode: 2158, duration: 1.175s, episode steps: 84, steps per second: 71, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.714 [0.000, 4.000], mean observation: 0.480 [0.380, 0.520], loss: 9.429572, mean_absolute_error: 41.015175, mean_q: 52.609455
[F[K 157478/500000: episode: 2159, duration: 2.495s, episode steps: 195, steps per second: 78, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.067 [0.000, 4.000], mean observation: 0.482 [0.300, 0.520], loss: 8.665528, mean_absolute_error: 41.430756, mean_q: 52.984390
[F[K 157589/500000: episode: 2160, duration: 1.325s, episode steps: 111, steps per second: 84, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.784 [0.000, 4.000], mean observation: 0.475 [0.340, 0.530], loss: 8.133930, mean_absolute_error: 41.707951, mean_q: 53.403030
[F[K 157669/500000: episode: 2161, duration: 1.111s, episode steps: 80, steps per second: 72, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.700 [0.000, 4.000], mean observation: 0.492 [0.380, 0.530], loss: 8.939955, mean_absolute_error: 41.625740, mean_q: 53.315620
[F[K 157738/500000: episode: 2162, duration: 0.959s, episode steps: 69, steps per second: 72, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.870 [0.000, 4.000], mean observation: 0.497 [0.350, 0.580], loss: 8.646642, mean_absolute_error: 41.146557, mean_q: 52.736458
[F[K 157829/500000: episode: 2163, duration: 1.344s, episode steps: 91, steps per second: 68, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.440 [0.000, 4.000], mean observation: 0.491 [0.360, 0.560], loss: 8.179022, mean_absolute_error: 41.768490, mean_q: 53.502293
[F[K 157885/500000: episode: 2164, duration: 0.791s, episode steps: 56, steps per second: 71, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.607 [0.000, 4.000], mean observation: 0.488 [0.430, 0.530], loss: 8.174299, mean_absolute_error: 40.743084, mean_q: 52.167400
[F[K 157949/500000: episode: 2165, duration: 0.945s, episode steps: 64, steps per second: 68, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.766 [0.000, 4.000], mean observation: 0.501 [0.450, 0.530], loss: 9.365651, mean_absolute_error: 41.587723, mean_q: 53.176846
[F[K 158138/500000: episode: 2166, duration: 2.628s, episode steps: 189, steps per second: 72, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 0.472 [0.320, 0.530], loss: 7.911111, mean_absolute_error: 41.185143, mean_q: 52.714916
[F[K 158223/500000: episode: 2167, duration: 1.119s, episode steps: 85, steps per second: 76, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.481 [0.350, 0.530], loss: 8.517437, mean_absolute_error: 40.901237, mean_q: 52.489506
[F[K 158308/500000: episode: 2168, duration: 1.233s, episode steps: 85, steps per second: 69, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.502 [0.420, 0.570], loss: 9.009393, mean_absolute_error: 41.589363, mean_q: 53.221836
[F[K 158377/500000: episode: 2169, duration: 0.780s, episode steps: 69, steps per second: 88, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.855 [0.000, 4.000], mean observation: 0.511 [0.490, 0.590], loss: 8.795825, mean_absolute_error: 42.170403, mean_q: 54.012272
[F[K 158455/500000: episode: 2170, duration: 1.076s, episode steps: 78, steps per second: 72, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.564 [0.000, 4.000], mean observation: 0.522 [0.500, 0.640], loss: 8.193296, mean_absolute_error: 41.597179, mean_q: 53.214649
[F[K 158534/500000: episode: 2171, duration: 0.939s, episode steps: 79, steps per second: 84, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.696 [0.000, 4.000], mean observation: 0.480 [0.330, 0.530], loss: 7.589100, mean_absolute_error: 41.402122, mean_q: 52.975376
[F[K 158587/500000: episode: 2172, duration: 0.718s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.774 [0.000, 4.000], mean observation: 0.480 [0.370, 0.530], loss: 7.515861, mean_absolute_error: 42.673470, mean_q: 54.613911
[F[K 158677/500000: episode: 2173, duration: 1.029s, episode steps: 90, steps per second: 87, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.989 [0.000, 4.000], mean observation: 0.487 [0.410, 0.530], loss: 8.209467, mean_absolute_error: 41.412151, mean_q: 53.048569
[F[K 158769/500000: episode: 2174, duration: 1.275s, episode steps: 92, steps per second: 72, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.413 [0.000, 4.000], mean observation: 0.508 [0.470, 0.570], loss: 8.230996, mean_absolute_error: 41.564583, mean_q: 53.169449
[F[K 158830/500000: episode: 2175, duration: 0.826s, episode steps: 61, steps per second: 74, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.033 [0.000, 4.000], mean observation: 0.481 [0.390, 0.500], loss: 10.376182, mean_absolute_error: 41.288891, mean_q: 52.789406
[F[K 158899/500000: episode: 2176, duration: 0.823s, episode steps: 69, steps per second: 84, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.072 [0.000, 4.000], mean observation: 0.513 [0.470, 0.650], loss: 9.377851, mean_absolute_error: 41.467815, mean_q: 53.088680
[F[K 158963/500000: episode: 2177, duration: 0.912s, episode steps: 64, steps per second: 70, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.487 [0.350, 0.530], loss: 8.172354, mean_absolute_error: 41.418266, mean_q: 53.079468
[F[K 159017/500000: episode: 2178, duration: 0.761s, episode steps: 54, steps per second: 71, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.741 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 9.135661, mean_absolute_error: 40.405838, mean_q: 51.659698
[F[K 159091/500000: episode: 2179, duration: 1.023s, episode steps: 74, steps per second: 72, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.865 [0.000, 4.000], mean observation: 0.499 [0.470, 0.530], loss: 8.472965, mean_absolute_error: 41.014454, mean_q: 52.553150
[F[K 159173/500000: episode: 2180, duration: 1.082s, episode steps: 82, steps per second: 76, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.841 [0.000, 4.000], mean observation: 0.488 [0.430, 0.530], loss: 8.960482, mean_absolute_error: 41.251419, mean_q: 52.801609
[F[K 159281/500000: episode: 2181, duration: 1.491s, episode steps: 108, steps per second: 72, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 0.497 [0.360, 0.580], loss: 8.364037, mean_absolute_error: 41.442173, mean_q: 53.041965
[F[K 159328/500000: episode: 2182, duration: 0.846s, episode steps: 47, steps per second: 56, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.507 [0.440, 0.630], loss: 8.137796, mean_absolute_error: 41.685944, mean_q: 53.375793
[F[K 159401/500000: episode: 2183, duration: 1.172s, episode steps: 73, steps per second: 62, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.487 [0.380, 0.530], loss: 8.082342, mean_absolute_error: 41.390884, mean_q: 53.004429
[F[K 159502/500000: episode: 2184, duration: 1.715s, episode steps: 101, steps per second: 59, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.881 [0.000, 4.000], mean observation: 0.517 [0.470, 0.640], loss: 7.675117, mean_absolute_error: 41.253857, mean_q: 52.871380
[F[K 159560/500000: episode: 2185, duration: 1.011s, episode steps: 58, steps per second: 57, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.759 [0.000, 4.000], mean observation: 0.499 [0.410, 0.550], loss: 7.821778, mean_absolute_error: 41.372250, mean_q: 53.029320
[F[K 159698/500000: episode: 2186, duration: 2.340s, episode steps: 138, steps per second: 59, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.507 [0.000, 4.000], mean observation: 0.506 [0.360, 0.670], loss: 8.967525, mean_absolute_error: 41.751240, mean_q: 53.411392
[F[K 159761/500000: episode: 2187, duration: 1.050s, episode steps: 63, steps per second: 60, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.079 [0.000, 4.000], mean observation: 0.493 [0.470, 0.520], loss: 9.937255, mean_absolute_error: 41.086643, mean_q: 52.513893
[F[K 159830/500000: episode: 2188, duration: 1.054s, episode steps: 69, steps per second: 65, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.488 [0.350, 0.530], loss: 8.891889, mean_absolute_error: 41.316303, mean_q: 52.956963
[F[K 159902/500000: episode: 2189, duration: 1.047s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.653 [0.000, 4.000], mean observation: 0.504 [0.440, 0.600], loss: 7.760462, mean_absolute_error: 41.147259, mean_q: 52.708061
[F[K 159974/500000: episode: 2190, duration: 1.009s, episode steps: 72, steps per second: 71, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.097 [0.000, 4.000], mean observation: 0.491 [0.380, 0.530], loss: 8.210248, mean_absolute_error: 41.547913, mean_q: 53.255871
[F[K 160046/500000: episode: 2191, duration: 1.092s, episode steps: 72, steps per second: 66, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.861 [0.000, 4.000], mean observation: 0.499 [0.390, 0.640], loss: 7.492162, mean_absolute_error: 40.394131, mean_q: 51.640862
[F[K 160091/500000: episode: 2192, duration: 0.658s, episode steps: 45, steps per second: 68, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.518 [0.470, 0.640], loss: 7.497700, mean_absolute_error: 41.416660, mean_q: 53.111946
[F[K 160241/500000: episode: 2193, duration: 2.116s, episode steps: 150, steps per second: 71, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.313 [0.000, 4.000], mean observation: 0.486 [0.410, 0.510], loss: 8.640688, mean_absolute_error: 41.339405, mean_q: 52.945301
[F[K 160298/500000: episode: 2194, duration: 0.904s, episode steps: 57, steps per second: 63, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.484 [0.360, 0.510], loss: 10.016570, mean_absolute_error: 41.537903, mean_q: 53.084354
[F[K 160361/500000: episode: 2195, duration: 0.854s, episode steps: 63, steps per second: 74, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.079 [0.000, 4.000], mean observation: 0.498 [0.460, 0.530], loss: 8.506330, mean_absolute_error: 41.573627, mean_q: 53.264538
[F[K 160484/500000: episode: 2196, duration: 1.858s, episode steps: 123, steps per second: 66, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.195 [0.000, 4.000], mean observation: 0.507 [0.460, 0.570], loss: 8.042496, mean_absolute_error: 41.928425, mean_q: 53.704060
[F[K 160553/500000: episode: 2197, duration: 0.894s, episode steps: 69, steps per second: 77, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.116 [0.000, 4.000], mean observation: 0.506 [0.480, 0.560], loss: 8.456346, mean_absolute_error: 40.953533, mean_q: 52.473656
[F[K 160624/500000: episode: 2198, duration: 1.041s, episode steps: 71, steps per second: 68, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.492 [0.400, 0.550], loss: 7.631700, mean_absolute_error: 41.701439, mean_q: 53.381329
[F[K 160682/500000: episode: 2199, duration: 0.814s, episode steps: 58, steps per second: 71, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.069 [0.000, 4.000], mean observation: 0.507 [0.460, 0.650], loss: 8.817107, mean_absolute_error: 41.995628, mean_q: 53.770214
[F[K 160748/500000: episode: 2200, duration: 0.898s, episode steps: 66, steps per second: 73, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.076 [0.000, 4.000], mean observation: 0.497 [0.410, 0.580], loss: 8.224529, mean_absolute_error: 41.260529, mean_q: 52.692795
[F[K 160816/500000: episode: 2201, duration: 0.953s, episode steps: 68, steps per second: 71, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.508 [0.470, 0.570], loss: 8.512642, mean_absolute_error: 41.882332, mean_q: 53.611816
[F[K 160921/500000: episode: 2202, duration: 1.614s, episode steps: 105, steps per second: 65, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.448 [0.000, 4.000], mean observation: 0.476 [0.370, 0.530], loss: 8.976965, mean_absolute_error: 41.160839, mean_q: 52.739223
[F[K 160994/500000: episode: 2203, duration: 1.001s, episode steps: 73, steps per second: 73, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.945 [0.000, 4.000], mean observation: 0.504 [0.380, 0.630], loss: 8.947523, mean_absolute_error: 41.500828, mean_q: 53.065212
[F[K 161134/500000: episode: 2204, duration: 2.222s, episode steps: 140, steps per second: 63, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.511 [0.410, 0.680], loss: 7.115079, mean_absolute_error: 41.457905, mean_q: 53.128063
[F[K 161229/500000: episode: 2205, duration: 1.413s, episode steps: 95, steps per second: 67, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.989 [0.000, 4.000], mean observation: 0.494 [0.420, 0.540], loss: 9.379926, mean_absolute_error: 42.069756, mean_q: 53.820869
[F[K 161358/500000: episode: 2206, duration: 1.926s, episode steps: 129, steps per second: 67, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.163 [0.000, 4.000], mean observation: 0.500 [0.430, 0.580], loss: 9.501839, mean_absolute_error: 41.381100, mean_q: 52.854370
[F[K 161438/500000: episode: 2207, duration: 1.083s, episode steps: 80, steps per second: 74, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.514 [0.490, 0.590], loss: 8.093530, mean_absolute_error: 41.144276, mean_q: 52.575844
[F[K 161551/500000: episode: 2208, duration: 1.355s, episode steps: 113, steps per second: 83, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.044 [0.000, 4.000], mean observation: 0.511 [0.430, 0.650], loss: 9.085373, mean_absolute_error: 41.539257, mean_q: 53.094250
[F[K 161671/500000: episode: 2209, duration: 1.729s, episode steps: 120, steps per second: 69, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.525 [0.000, 4.000], mean observation: 0.482 [0.340, 0.530], loss: 8.148719, mean_absolute_error: 41.356770, mean_q: 52.961872
[F[K 161798/500000: episode: 2210, duration: 1.867s, episode steps: 127, steps per second: 68, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.496 [0.000, 4.000], mean observation: 0.504 [0.460, 0.580], loss: 9.262143, mean_absolute_error: 41.316872, mean_q: 52.788319
[F[K 161887/500000: episode: 2211, duration: 1.153s, episode steps: 89, steps per second: 77, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.753 [0.000, 4.000], mean observation: 0.474 [0.360, 0.510], loss: 8.805094, mean_absolute_error: 41.953911, mean_q: 53.634354
[F[K 161965/500000: episode: 2212, duration: 1.183s, episode steps: 78, steps per second: 66, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.449 [0.000, 4.000], mean observation: 0.502 [0.460, 0.570], loss: 8.246926, mean_absolute_error: 41.641300, mean_q: 53.326042
[F[K 162033/500000: episode: 2213, duration: 1.062s, episode steps: 68, steps per second: 64, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.985 [0.000, 4.000], mean observation: 0.489 [0.440, 0.520], loss: 8.112363, mean_absolute_error: 41.302559, mean_q: 52.886532
[F[K 162152/500000: episode: 2214, duration: 1.670s, episode steps: 119, steps per second: 71, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.849 [0.000, 4.000], mean observation: 0.499 [0.390, 0.610], loss: 8.590022, mean_absolute_error: 41.760937, mean_q: 53.407310
[F[K 162237/500000: episode: 2215, duration: 1.217s, episode steps: 85, steps per second: 70, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.035 [0.000, 4.000], mean observation: 0.499 [0.420, 0.560], loss: 9.342710, mean_absolute_error: 41.121048, mean_q: 52.614471
[F[K 162315/500000: episode: 2216, duration: 1.169s, episode steps: 78, steps per second: 67, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.064 [0.000, 4.000], mean observation: 0.502 [0.410, 0.640], loss: 9.452301, mean_absolute_error: 41.778687, mean_q: 53.484558
[F[K 162415/500000: episode: 2217, duration: 1.384s, episode steps: 100, steps per second: 72, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.710 [0.000, 4.000], mean observation: 0.505 [0.390, 0.640], loss: 9.751448, mean_absolute_error: 41.028152, mean_q: 52.577328
[F[K 162534/500000: episode: 2218, duration: 1.758s, episode steps: 119, steps per second: 68, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.924 [0.000, 4.000], mean observation: 0.508 [0.480, 0.560], loss: 10.286991, mean_absolute_error: 41.346149, mean_q: 52.859318
[F[K 162590/500000: episode: 2219, duration: 0.801s, episode steps: 56, steps per second: 70, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.929 [0.000, 4.000], mean observation: 0.477 [0.390, 0.510], loss: 7.340168, mean_absolute_error: 41.395027, mean_q: 53.105835
[F[K 162653/500000: episode: 2220, duration: 0.925s, episode steps: 63, steps per second: 68, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.968 [0.000, 4.000], mean observation: 0.477 [0.360, 0.520], loss: 9.158072, mean_absolute_error: 41.603645, mean_q: 53.340607
[F[K 162734/500000: episode: 2221, duration: 1.229s, episode steps: 81, steps per second: 66, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.444 [0.000, 4.000], mean observation: 0.521 [0.470, 0.650], loss: 9.229846, mean_absolute_error: 40.876244, mean_q: 52.309277
[F[K 162791/500000: episode: 2222, duration: 0.784s, episode steps: 57, steps per second: 73, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.719 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 8.726661, mean_absolute_error: 41.887501, mean_q: 53.584694
[F[K 162876/500000: episode: 2223, duration: 1.410s, episode steps: 85, steps per second: 60, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.505 [0.440, 0.590], loss: 9.894215, mean_absolute_error: 41.076557, mean_q: 52.681648
[F[K 162968/500000: episode: 2224, duration: 1.350s, episode steps: 92, steps per second: 68, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.496 [0.350, 0.590], loss: 8.239663, mean_absolute_error: 40.949608, mean_q: 52.462330
[F[K 163097/500000: episode: 2225, duration: 1.829s, episode steps: 129, steps per second: 71, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.140 [0.000, 4.000], mean observation: 0.515 [0.480, 0.630], loss: 8.516909, mean_absolute_error: 41.494656, mean_q: 53.157375
[F[K 163257/500000: episode: 2226, duration: 2.396s, episode steps: 160, steps per second: 67, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.169 [0.000, 4.000], mean observation: 0.502 [0.470, 0.570], loss: 9.030492, mean_absolute_error: 41.338539, mean_q: 52.921864
[F[K 163328/500000: episode: 2227, duration: 0.966s, episode steps: 71, steps per second: 74, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.676 [0.000, 4.000], mean observation: 0.519 [0.470, 0.600], loss: 8.258493, mean_absolute_error: 41.160652, mean_q: 52.645248
[F[K 163401/500000: episode: 2228, duration: 0.964s, episode steps: 73, steps per second: 76, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.123 [0.000, 4.000], mean observation: 0.508 [0.450, 0.630], loss: 7.830501, mean_absolute_error: 40.507584, mean_q: 51.881470
[F[K 163583/500000: episode: 2229, duration: 2.579s, episode steps: 182, steps per second: 71, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.824 [0.000, 4.000], mean observation: 0.518 [0.440, 0.720], loss: 8.145167, mean_absolute_error: 41.521114, mean_q: 53.140614
[F[K 163657/500000: episode: 2230, duration: 1.030s, episode steps: 74, steps per second: 72, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.473 [0.000, 4.000], mean observation: 0.484 [0.410, 0.530], loss: 8.375508, mean_absolute_error: 41.392567, mean_q: 52.980793
[F[K 163732/500000: episode: 2231, duration: 0.888s, episode steps: 75, steps per second: 84, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.800 [0.000, 4.000], mean observation: 0.485 [0.340, 0.530], loss: 8.016618, mean_absolute_error: 41.361717, mean_q: 52.949635
[F[K 163794/500000: episode: 2232, duration: 0.659s, episode steps: 62, steps per second: 94, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.484 [0.000, 4.000], mean observation: 0.501 [0.450, 0.570], loss: 7.990351, mean_absolute_error: 42.359497, mean_q: 54.235832
[F[K 163860/500000: episode: 2233, duration: 0.885s, episode steps: 66, steps per second: 75, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.493 [0.400, 0.530], loss: 7.990585, mean_absolute_error: 41.601315, mean_q: 53.182148
[F[K 163922/500000: episode: 2234, duration: 0.667s, episode steps: 62, steps per second: 93, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.823 [0.000, 4.000], mean observation: 0.490 [0.370, 0.560], loss: 10.249383, mean_absolute_error: 41.368626, mean_q: 52.927822
[F[K 163966/500000: episode: 2235, duration: 0.574s, episode steps: 44, steps per second: 77, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.795 [0.000, 4.000], mean observation: 0.513 [0.460, 0.650], loss: 7.881677, mean_absolute_error: 41.318958, mean_q: 52.699413
[F[K 164084/500000: episode: 2236, duration: 1.409s, episode steps: 118, steps per second: 84, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.407 [0.000, 4.000], mean observation: 0.491 [0.380, 0.550], loss: 7.971524, mean_absolute_error: 41.489590, mean_q: 53.159367
[F[K 164215/500000: episode: 2237, duration: 1.639s, episode steps: 131, steps per second: 80, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.282 [0.000, 4.000], mean observation: 0.493 [0.380, 0.550], loss: 8.081565, mean_absolute_error: 41.481907, mean_q: 53.153221
[F[K 164351/500000: episode: 2238, duration: 1.831s, episode steps: 136, steps per second: 74, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.956 [0.000, 4.000], mean observation: 0.501 [0.460, 0.530], loss: 8.067940, mean_absolute_error: 41.398491, mean_q: 53.019642
[F[K 164408/500000: episode: 2239, duration: 0.805s, episode steps: 57, steps per second: 71, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.737 [0.000, 4.000], mean observation: 0.488 [0.410, 0.530], loss: 8.922061, mean_absolute_error: 41.213829, mean_q: 52.855839
[F[K 164480/500000: episode: 2240, duration: 0.879s, episode steps: 72, steps per second: 82, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 8.438881, mean_absolute_error: 41.777206, mean_q: 53.566933
[F[K 164538/500000: episode: 2241, duration: 0.851s, episode steps: 58, steps per second: 68, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 9.169703, mean_absolute_error: 41.753216, mean_q: 53.395576
[F[K 164612/500000: episode: 2242, duration: 1.067s, episode steps: 74, steps per second: 69, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.514 [0.000, 4.000], mean observation: 0.492 [0.390, 0.530], loss: 9.001101, mean_absolute_error: 41.560371, mean_q: 53.236233
[F[K 164664/500000: episode: 2243, duration: 0.730s, episode steps: 52, steps per second: 71, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.846 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 8.042088, mean_absolute_error: 41.308151, mean_q: 52.979290
[F[K 164739/500000: episode: 2244, duration: 0.986s, episode steps: 75, steps per second: 76, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.504 [0.420, 0.600], loss: 8.982499, mean_absolute_error: 41.515678, mean_q: 53.220119
[F[K 164812/500000: episode: 2245, duration: 1.009s, episode steps: 73, steps per second: 72, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.192 [0.000, 4.000], mean observation: 0.517 [0.490, 0.590], loss: 8.589618, mean_absolute_error: 41.885429, mean_q: 53.579571
[F[K 164882/500000: episode: 2246, duration: 1.043s, episode steps: 70, steps per second: 67, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.490 [0.380, 0.550], loss: 8.275674, mean_absolute_error: 41.071789, mean_q: 52.537502
[F[K 164967/500000: episode: 2247, duration: 0.960s, episode steps: 85, steps per second: 89, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.871 [0.000, 4.000], mean observation: 0.507 [0.470, 0.560], loss: 8.594040, mean_absolute_error: 41.817047, mean_q: 53.509399
[F[K 165053/500000: episode: 2248, duration: 1.324s, episode steps: 86, steps per second: 65, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.395 [0.000, 4.000], mean observation: 0.514 [0.470, 0.610], loss: 8.625362, mean_absolute_error: 41.396183, mean_q: 52.984447
[F[K 165155/500000: episode: 2249, duration: 1.513s, episode steps: 102, steps per second: 67, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.157 [0.000, 4.000], mean observation: 0.499 [0.370, 0.610], loss: 8.853417, mean_absolute_error: 41.216198, mean_q: 52.766171
[F[K 165223/500000: episode: 2250, duration: 0.827s, episode steps: 68, steps per second: 82, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.485 [0.000, 4.000], mean observation: 0.497 [0.380, 0.590], loss: 7.105156, mean_absolute_error: 41.763256, mean_q: 53.619324
[F[K 165308/500000: episode: 2251, duration: 1.260s, episode steps: 85, steps per second: 67, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.294 [0.000, 4.000], mean observation: 0.494 [0.380, 0.560], loss: 7.355542, mean_absolute_error: 41.060738, mean_q: 52.625443
[F[K 165354/500000: episode: 2252, duration: 0.646s, episode steps: 46, steps per second: 71, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.783 [0.000, 4.000], mean observation: 0.511 [0.470, 0.620], loss: 8.460334, mean_absolute_error: 41.779430, mean_q: 53.381985
[F[K 165425/500000: episode: 2253, duration: 1.039s, episode steps: 71, steps per second: 68, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.408 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 9.407981, mean_absolute_error: 40.693970, mean_q: 52.148979
[F[K 165483/500000: episode: 2254, duration: 0.932s, episode steps: 58, steps per second: 62, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.948 [0.000, 4.000], mean observation: 0.518 [0.490, 0.580], loss: 8.626863, mean_absolute_error: 40.263889, mean_q: 51.567394
[F[K 165540/500000: episode: 2255, duration: 0.846s, episode steps: 57, steps per second: 67, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.469 [0.350, 0.510], loss: 7.431728, mean_absolute_error: 41.457790, mean_q: 53.158981
[F[K 165684/500000: episode: 2256, duration: 1.854s, episode steps: 144, steps per second: 78, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.576 [0.000, 4.000], mean observation: 0.475 [0.290, 0.510], loss: 8.573581, mean_absolute_error: 41.340912, mean_q: 52.909912
[F[K 165758/500000: episode: 2257, duration: 1.049s, episode steps: 74, steps per second: 71, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.041 [0.000, 4.000], mean observation: 0.504 [0.380, 0.640], loss: 9.523864, mean_absolute_error: 41.269661, mean_q: 52.753448
[F[K 165802/500000: episode: 2258, duration: 0.687s, episode steps: 44, steps per second: 64, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.773 [0.000, 4.000], mean observation: 0.504 [0.390, 0.650], loss: 7.629967, mean_absolute_error: 40.839504, mean_q: 52.201950
[F[K 165873/500000: episode: 2259, duration: 1.103s, episode steps: 71, steps per second: 64, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.873 [0.000, 4.000], mean observation: 0.508 [0.490, 0.580], loss: 8.004560, mean_absolute_error: 41.268047, mean_q: 52.834248
[F[K 165965/500000: episode: 2260, duration: 1.314s, episode steps: 92, steps per second: 70, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.011 [0.000, 4.000], mean observation: 0.495 [0.390, 0.580], loss: 9.341721, mean_absolute_error: 41.492165, mean_q: 53.188992
[F[K 166098/500000: episode: 2261, duration: 1.727s, episode steps: 133, steps per second: 77, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.955 [0.000, 4.000], mean observation: 0.504 [0.400, 0.640], loss: 7.634164, mean_absolute_error: 41.426773, mean_q: 53.092777
[F[K 166166/500000: episode: 2262, duration: 0.977s, episode steps: 68, steps per second: 70, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.495 [0.350, 0.580], loss: 8.859934, mean_absolute_error: 41.619915, mean_q: 53.318905
[F[K 166245/500000: episode: 2263, duration: 1.160s, episode steps: 79, steps per second: 68, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.025 [0.000, 4.000], mean observation: 0.484 [0.350, 0.530], loss: 7.931209, mean_absolute_error: 41.460388, mean_q: 53.147030
[F[K 166286/500000: episode: 2264, duration: 0.654s, episode steps: 41, steps per second: 63, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.902 [0.000, 4.000], mean observation: 0.510 [0.430, 0.650], loss: 9.490746, mean_absolute_error: 41.261608, mean_q: 52.801003
[F[K 166361/500000: episode: 2265, duration: 1.060s, episode steps: 75, steps per second: 71, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.973 [0.000, 4.000], mean observation: 0.506 [0.460, 0.570], loss: 8.533683, mean_absolute_error: 41.500805, mean_q: 53.213310
[F[K 166561/500000: episode: 2266, duration: 3.117s, episode steps: 200, steps per second: 64, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.479 [0.320, 0.550], loss: 8.253736, mean_absolute_error: 41.681339, mean_q: 53.414795
[F[K 166631/500000: episode: 2267, duration: 1.059s, episode steps: 70, steps per second: 66, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.506 [0.400, 0.630], loss: 7.981504, mean_absolute_error: 41.089943, mean_q: 52.564846
[F[K 166718/500000: episode: 2268, duration: 1.487s, episode steps: 87, steps per second: 58, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.678 [0.000, 4.000], mean observation: 0.487 [0.390, 0.520], loss: 8.772326, mean_absolute_error: 40.585346, mean_q: 51.993275
[F[K 166785/500000: episode: 2269, duration: 1.035s, episode steps: 67, steps per second: 65, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 0.519 [0.470, 0.640], loss: 9.103986, mean_absolute_error: 41.080544, mean_q: 52.534115
[F[K 166879/500000: episode: 2270, duration: 1.649s, episode steps: 94, steps per second: 57, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.277 [0.000, 4.000], mean observation: 0.500 [0.460, 0.550], loss: 9.093185, mean_absolute_error: 41.441105, mean_q: 53.072411
[F[K 166959/500000: episode: 2271, duration: 1.416s, episode steps: 80, steps per second: 56, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.503 [0.470, 0.530], loss: 8.246737, mean_absolute_error: 40.784935, mean_q: 52.204052
[F[K 167006/500000: episode: 2272, duration: 0.850s, episode steps: 47, steps per second: 55, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.489 [0.000, 4.000], mean observation: 0.522 [0.470, 0.630], loss: 8.684778, mean_absolute_error: 41.474800, mean_q: 53.074718
[F[K 167206/500000: episode: 2273, duration: 3.167s, episode steps: 200, steps per second: 63, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.225 [0.000, 4.000], mean observation: 0.494 [0.390, 0.570], loss: 8.184498, mean_absolute_error: 41.168018, mean_q: 52.816227
[F[K 167239/500000: episode: 2274, duration: 0.432s, episode steps: 33, steps per second: 76, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.576 [0.000, 4.000], mean observation: 0.507 [0.430, 0.640], loss: 7.719768, mean_absolute_error: 41.443817, mean_q: 53.299675
[F[K 167369/500000: episode: 2275, duration: 1.840s, episode steps: 130, steps per second: 71, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.338 [0.000, 4.000], mean observation: 0.485 [0.400, 0.530], loss: 8.626002, mean_absolute_error: 41.149197, mean_q: 52.742119
[F[K 167415/500000: episode: 2276, duration: 0.520s, episode steps: 46, steps per second: 88, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.630 [0.000, 4.000], mean observation: 0.528 [0.470, 0.640], loss: 9.310529, mean_absolute_error: 41.326622, mean_q: 52.796864
[F[K 167469/500000: episode: 2277, duration: 0.666s, episode steps: 54, steps per second: 81, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.056 [0.000, 4.000], mean observation: 0.494 [0.470, 0.510], loss: 8.398356, mean_absolute_error: 41.114769, mean_q: 52.735157
[F[K 167547/500000: episode: 2278, duration: 1.019s, episode steps: 78, steps per second: 77, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.949 [0.000, 4.000], mean observation: 0.502 [0.380, 0.620], loss: 8.632396, mean_absolute_error: 40.824333, mean_q: 52.327496
[F[K 167633/500000: episode: 2279, duration: 1.110s, episode steps: 86, steps per second: 77, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.919 [0.000, 4.000], mean observation: 0.491 [0.390, 0.530], loss: 8.951949, mean_absolute_error: 41.260899, mean_q: 52.850433
[F[K 167822/500000: episode: 2280, duration: 2.621s, episode steps: 189, steps per second: 72, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.254 [0.000, 4.000], mean observation: 0.490 [0.420, 0.530], loss: 8.679040, mean_absolute_error: 41.789139, mean_q: 53.583866
[F[K 167886/500000: episode: 2281, duration: 0.885s, episode steps: 64, steps per second: 72, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.508 [0.490, 0.550], loss: 9.521443, mean_absolute_error: 41.032814, mean_q: 52.577461
[F[K 167967/500000: episode: 2282, duration: 1.158s, episode steps: 81, steps per second: 70, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.296 [0.000, 4.000], mean observation: 0.504 [0.430, 0.610], loss: 8.491384, mean_absolute_error: 41.760174, mean_q: 53.522038
[F[K 168042/500000: episode: 2283, duration: 1.039s, episode steps: 75, steps per second: 72, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.840 [0.000, 4.000], mean observation: 0.510 [0.490, 0.550], loss: 8.058411, mean_absolute_error: 41.640755, mean_q: 53.397705
[F[K 168106/500000: episode: 2284, duration: 0.956s, episode steps: 64, steps per second: 67, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.281 [0.000, 4.000], mean observation: 0.501 [0.380, 0.640], loss: 8.467829, mean_absolute_error: 41.749027, mean_q: 53.431969
[F[K 168145/500000: episode: 2285, duration: 0.526s, episode steps: 39, steps per second: 74, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.897 [0.000, 4.000], mean observation: 0.498 [0.350, 0.620], loss: 7.105772, mean_absolute_error: 41.713730, mean_q: 53.532497
[F[K 168254/500000: episode: 2286, duration: 1.291s, episode steps: 109, steps per second: 84, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.817 [0.000, 4.000], mean observation: 0.496 [0.380, 0.570], loss: 9.042719, mean_absolute_error: 41.450844, mean_q: 53.098373
[F[K 168342/500000: episode: 2287, duration: 1.086s, episode steps: 88, steps per second: 81, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.504 [0.400, 0.640], loss: 9.557409, mean_absolute_error: 41.905300, mean_q: 53.721634
[F[K 168407/500000: episode: 2288, duration: 0.689s, episode steps: 65, steps per second: 94, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.246 [0.000, 4.000], mean observation: 0.480 [0.390, 0.500], loss: 8.879851, mean_absolute_error: 41.935246, mean_q: 53.739502
[F[K 168520/500000: episode: 2289, duration: 1.557s, episode steps: 113, steps per second: 73, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.407 [0.000, 4.000], mean observation: 0.507 [0.490, 0.570], loss: 7.899909, mean_absolute_error: 41.437893, mean_q: 53.079285
[F[K 168720/500000: episode: 2290, duration: 2.647s, episode steps: 200, steps per second: 76, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.195 [0.000, 4.000], mean observation: 0.498 [0.370, 0.620], loss: 7.938880, mean_absolute_error: 41.433300, mean_q: 53.059620
[F[K 168789/500000: episode: 2291, duration: 0.933s, episode steps: 69, steps per second: 74, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.464 [0.000, 4.000], mean observation: 0.484 [0.390, 0.530], loss: 7.693985, mean_absolute_error: 41.296860, mean_q: 52.853680
[F[K 168859/500000: episode: 2292, duration: 0.905s, episode steps: 70, steps per second: 77, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.814 [0.000, 4.000], mean observation: 0.509 [0.470, 0.590], loss: 7.081999, mean_absolute_error: 40.882187, mean_q: 52.461235
[F[K 168930/500000: episode: 2293, duration: 0.970s, episode steps: 71, steps per second: 73, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.901 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 9.972232, mean_absolute_error: 41.313740, mean_q: 52.818386
[F[K 169066/500000: episode: 2294, duration: 1.778s, episode steps: 136, steps per second: 76, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.088 [0.000, 4.000], mean observation: 0.494 [0.390, 0.540], loss: 7.489718, mean_absolute_error: 41.517117, mean_q: 53.225342
[F[K 169111/500000: episode: 2295, duration: 0.622s, episode steps: 45, steps per second: 72, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.644 [0.000, 3.000], mean observation: 0.513 [0.470, 0.620], loss: 7.880455, mean_absolute_error: 41.335117, mean_q: 52.951828
[F[K 169186/500000: episode: 2296, duration: 0.986s, episode steps: 75, steps per second: 76, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.187 [0.000, 4.000], mean observation: 0.479 [0.360, 0.530], loss: 8.306047, mean_absolute_error: 41.273140, mean_q: 52.885864
[F[K 169267/500000: episode: 2297, duration: 1.058s, episode steps: 81, steps per second: 77, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.901 [0.000, 3.000], mean observation: 0.496 [0.460, 0.520], loss: 9.714849, mean_absolute_error: 41.796738, mean_q: 53.512833
[F[K 169367/500000: episode: 2298, duration: 1.381s, episode steps: 100, steps per second: 72, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.491 [0.340, 0.560], loss: 8.951978, mean_absolute_error: 41.067139, mean_q: 52.677799
[F[K 169463/500000: episode: 2299, duration: 1.369s, episode steps: 96, steps per second: 70, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.031 [0.000, 4.000], mean observation: 0.504 [0.420, 0.610], loss: 8.024685, mean_absolute_error: 41.936871, mean_q: 53.745388
[F[K 169511/500000: episode: 2300, duration: 0.749s, episode steps: 48, steps per second: 64, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.979 [0.000, 4.000], mean observation: 0.523 [0.470, 0.650], loss: 9.575706, mean_absolute_error: 41.337513, mean_q: 52.926228
[F[K 169597/500000: episode: 2301, duration: 1.209s, episode steps: 86, steps per second: 71, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.384 [0.000, 4.000], mean observation: 0.479 [0.360, 0.530], loss: 8.026964, mean_absolute_error: 41.624523, mean_q: 53.340084
[F[K 169753/500000: episode: 2302, duration: 2.004s, episode steps: 156, steps per second: 78, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.821 [0.000, 4.000], mean observation: 0.505 [0.450, 0.590], loss: 8.002048, mean_absolute_error: 41.518517, mean_q: 53.103691
[F[K 169796/500000: episode: 2303, duration: 0.541s, episode steps: 43, steps per second: 79, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.837 [0.000, 4.000], mean observation: 0.519 [0.470, 0.640], loss: 7.846094, mean_absolute_error: 41.501705, mean_q: 53.018795
[F[K 169847/500000: episode: 2304, duration: 0.701s, episode steps: 51, steps per second: 73, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.824 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 8.265279, mean_absolute_error: 41.114258, mean_q: 52.709976
[F[K 169923/500000: episode: 2305, duration: 1.143s, episode steps: 76, steps per second: 66, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.829 [0.000, 4.000], mean observation: 0.497 [0.410, 0.580], loss: 8.535897, mean_absolute_error: 41.235710, mean_q: 52.899487
[F[K 170005/500000: episode: 2306, duration: 1.050s, episode steps: 82, steps per second: 78, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.963 [0.000, 4.000], mean observation: 0.504 [0.420, 0.590], loss: 8.692476, mean_absolute_error: 40.817902, mean_q: 52.300678
[F[K 170088/500000: episode: 2307, duration: 1.120s, episode steps: 83, steps per second: 74, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.193 [0.000, 4.000], mean observation: 0.507 [0.470, 0.580], loss: 7.726118, mean_absolute_error: 41.793488, mean_q: 53.448044
[F[K 170226/500000: episode: 2308, duration: 1.733s, episode steps: 138, steps per second: 80, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.783 [0.000, 4.000], mean observation: 0.497 [0.420, 0.550], loss: 7.999492, mean_absolute_error: 40.934593, mean_q: 52.488495
[F[K 170404/500000: episode: 2309, duration: 2.586s, episode steps: 178, steps per second: 69, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.062 [0.000, 4.000], mean observation: 0.505 [0.450, 0.580], loss: 8.677587, mean_absolute_error: 41.398506, mean_q: 52.986725
[F[K 170487/500000: episode: 2310, duration: 1.071s, episode steps: 83, steps per second: 77, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.892 [0.000, 4.000], mean observation: 0.488 [0.390, 0.530], loss: 8.384724, mean_absolute_error: 41.768394, mean_q: 53.509212
[F[K 170548/500000: episode: 2311, duration: 0.810s, episode steps: 61, steps per second: 75, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.489 [0.390, 0.520], loss: 8.293695, mean_absolute_error: 41.191013, mean_q: 52.729282
[F[K 170622/500000: episode: 2312, duration: 1.111s, episode steps: 74, steps per second: 67, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.041 [0.000, 4.000], mean observation: 0.505 [0.450, 0.600], loss: 8.423810, mean_absolute_error: 41.615902, mean_q: 53.373550
[F[K 170760/500000: episode: 2313, duration: 2.105s, episode steps: 138, steps per second: 66, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.072 [0.000, 4.000], mean observation: 0.488 [0.410, 0.550], loss: 8.070184, mean_absolute_error: 41.401989, mean_q: 53.054222
[F[K 170797/500000: episode: 2314, duration: 0.542s, episode steps: 37, steps per second: 68, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.533 [0.500, 0.650], loss: 9.462369, mean_absolute_error: 41.645031, mean_q: 53.265274
[F[K 170874/500000: episode: 2315, duration: 0.949s, episode steps: 77, steps per second: 81, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.182 [0.000, 4.000], mean observation: 0.517 [0.470, 0.590], loss: 8.208302, mean_absolute_error: 41.508202, mean_q: 53.115150
[F[K 170972/500000: episode: 2316, duration: 1.284s, episode steps: 98, steps per second: 76, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.684 [0.000, 4.000], mean observation: 0.487 [0.410, 0.530], loss: 7.666832, mean_absolute_error: 40.932526, mean_q: 52.476238
[F[K 171062/500000: episode: 2317, duration: 1.185s, episode steps: 90, steps per second: 76, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.700 [0.000, 4.000], mean observation: 0.502 [0.470, 0.530], loss: 8.512903, mean_absolute_error: 41.271751, mean_q: 52.810741
[F[K 171101/500000: episode: 2318, duration: 0.572s, episode steps: 39, steps per second: 68, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.769 [0.000, 4.000], mean observation: 0.504 [0.400, 0.630], loss: 8.598526, mean_absolute_error: 40.031624, mean_q: 51.324310
[F[K 171228/500000: episode: 2319, duration: 1.763s, episode steps: 127, steps per second: 72, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.764 [0.000, 4.000], mean observation: 0.508 [0.430, 0.630], loss: 9.331525, mean_absolute_error: 40.869423, mean_q: 52.339340
[F[K 171326/500000: episode: 2320, duration: 1.406s, episode steps: 98, steps per second: 70, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.495 [0.450, 0.520], loss: 9.126800, mean_absolute_error: 41.073807, mean_q: 52.560150
[F[K 171382/500000: episode: 2321, duration: 0.755s, episode steps: 56, steps per second: 74, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.089 [0.000, 4.000], mean observation: 0.485 [0.400, 0.530], loss: 6.538280, mean_absolute_error: 40.843090, mean_q: 52.364483
[F[K 171433/500000: episode: 2322, duration: 0.787s, episode steps: 51, steps per second: 65, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.196 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 10.346857, mean_absolute_error: 41.038563, mean_q: 52.340141
[F[K 171503/500000: episode: 2323, duration: 0.970s, episode steps: 70, steps per second: 72, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.492 [0.420, 0.530], loss: 7.979597, mean_absolute_error: 41.327496, mean_q: 52.950848
[F[K 171599/500000: episode: 2324, duration: 1.191s, episode steps: 96, steps per second: 81, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 7.626084, mean_absolute_error: 41.201092, mean_q: 52.747150
[F[K 171662/500000: episode: 2325, duration: 0.943s, episode steps: 63, steps per second: 67, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.937 [0.000, 4.000], mean observation: 0.505 [0.460, 0.620], loss: 8.574594, mean_absolute_error: 41.588280, mean_q: 53.095818
[F[K 171726/500000: episode: 2326, duration: 0.830s, episode steps: 64, steps per second: 77, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.578 [0.000, 4.000], mean observation: 0.511 [0.480, 0.610], loss: 7.500545, mean_absolute_error: 41.708668, mean_q: 53.262009
[F[K 171811/500000: episode: 2327, duration: 1.141s, episode steps: 85, steps per second: 74, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.859 [0.000, 4.000], mean observation: 0.506 [0.470, 0.560], loss: 8.912498, mean_absolute_error: 41.268570, mean_q: 52.811409
[F[K 171890/500000: episode: 2328, duration: 1.188s, episode steps: 79, steps per second: 67, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.127 [0.000, 4.000], mean observation: 0.506 [0.470, 0.580], loss: 7.170177, mean_absolute_error: 41.039093, mean_q: 52.531700
[F[K 171961/500000: episode: 2329, duration: 1.141s, episode steps: 71, steps per second: 62, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.930 [0.000, 4.000], mean observation: 0.494 [0.380, 0.580], loss: 7.583872, mean_absolute_error: 41.510517, mean_q: 53.114643
[F[K 172011/500000: episode: 2330, duration: 0.741s, episode steps: 50, steps per second: 67, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.340 [0.000, 4.000], mean observation: 0.492 [0.370, 0.550], loss: 8.411665, mean_absolute_error: 41.753365, mean_q: 53.423859
[F[K 172065/500000: episode: 2331, duration: 0.708s, episode steps: 54, steps per second: 76, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.093 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 7.589200, mean_absolute_error: 42.076202, mean_q: 53.933918
[F[K 172114/500000: episode: 2332, duration: 0.776s, episode steps: 49, steps per second: 63, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.513 [0.470, 0.620], loss: 10.386530, mean_absolute_error: 41.158558, mean_q: 52.750359
[F[K 172193/500000: episode: 2333, duration: 1.128s, episode steps: 79, steps per second: 70, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.512 [0.480, 0.610], loss: 9.025461, mean_absolute_error: 41.169193, mean_q: 52.750877
[F[K 172286/500000: episode: 2334, duration: 1.317s, episode steps: 93, steps per second: 71, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 0.505 [0.480, 0.530], loss: 10.142602, mean_absolute_error: 41.012733, mean_q: 52.440392
[F[K 172357/500000: episode: 2335, duration: 1.123s, episode steps: 71, steps per second: 63, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.803 [0.000, 4.000], mean observation: 0.492 [0.370, 0.580], loss: 9.029725, mean_absolute_error: 41.437767, mean_q: 53.098221
[F[K 172401/500000: episode: 2336, duration: 0.645s, episode steps: 44, steps per second: 68, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.523 [0.000, 4.000], mean observation: 0.511 [0.460, 0.640], loss: 8.669160, mean_absolute_error: 41.480057, mean_q: 53.147083
[F[K 172526/500000: episode: 2337, duration: 1.823s, episode steps: 125, steps per second: 69, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.160 [0.000, 4.000], mean observation: 0.499 [0.450, 0.530], loss: 8.124763, mean_absolute_error: 42.001553, mean_q: 53.799549
[F[K 172660/500000: episode: 2338, duration: 2.024s, episode steps: 134, steps per second: 66, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.090 [0.000, 4.000], mean observation: 0.485 [0.370, 0.530], loss: 8.189773, mean_absolute_error: 41.739925, mean_q: 53.354126
[F[K 172746/500000: episode: 2339, duration: 1.184s, episode steps: 86, steps per second: 73, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.860 [0.000, 4.000], mean observation: 0.503 [0.440, 0.580], loss: 8.762167, mean_absolute_error: 41.422501, mean_q: 53.044991
[F[K 172834/500000: episode: 2340, duration: 1.281s, episode steps: 88, steps per second: 69, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.841 [0.000, 4.000], mean observation: 0.503 [0.400, 0.650], loss: 8.381403, mean_absolute_error: 41.931156, mean_q: 53.658035
[F[K 172885/500000: episode: 2341, duration: 0.705s, episode steps: 51, steps per second: 72, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.627 [0.000, 4.000], mean observation: 0.505 [0.400, 0.640], loss: 9.574329, mean_absolute_error: 41.389854, mean_q: 52.898838
[F[K 172964/500000: episode: 2342, duration: 1.269s, episode steps: 79, steps per second: 62, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.278 [0.000, 4.000], mean observation: 0.504 [0.480, 0.530], loss: 8.003831, mean_absolute_error: 41.457481, mean_q: 53.072243
[F[K 173057/500000: episode: 2343, duration: 1.318s, episode steps: 93, steps per second: 71, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.280 [0.000, 4.000], mean observation: 0.488 [0.410, 0.530], loss: 7.865210, mean_absolute_error: 41.387825, mean_q: 53.089436
[F[K 173135/500000: episode: 2344, duration: 1.265s, episode steps: 78, steps per second: 62, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.987 [0.000, 4.000], mean observation: 0.504 [0.430, 0.580], loss: 9.616353, mean_absolute_error: 41.179771, mean_q: 52.702942
[F[K 173258/500000: episode: 2345, duration: 1.887s, episode steps: 123, steps per second: 65, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.146 [0.000, 4.000], mean observation: 0.510 [0.450, 0.610], loss: 7.935568, mean_absolute_error: 41.555485, mean_q: 53.250774
[F[K 173334/500000: episode: 2346, duration: 1.191s, episode steps: 76, steps per second: 64, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.908 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 7.788161, mean_absolute_error: 41.225903, mean_q: 52.907192
[F[K 173376/500000: episode: 2347, duration: 0.747s, episode steps: 42, steps per second: 56, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.905 [0.000, 4.000], mean observation: 0.527 [0.490, 0.640], loss: 8.855229, mean_absolute_error: 41.651344, mean_q: 53.445549
[F[K 173444/500000: episode: 2348, duration: 1.061s, episode steps: 68, steps per second: 64, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.059 [0.000, 4.000], mean observation: 0.481 [0.390, 0.520], loss: 10.169842, mean_absolute_error: 41.057945, mean_q: 52.424820
[F[K 173582/500000: episode: 2349, duration: 2.324s, episode steps: 138, steps per second: 59, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.312 [0.000, 4.000], mean observation: 0.478 [0.350, 0.510], loss: 8.693720, mean_absolute_error: 41.598724, mean_q: 53.218353
[F[K 173664/500000: episode: 2350, duration: 1.309s, episode steps: 82, steps per second: 63, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.610 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 8.723384, mean_absolute_error: 41.445229, mean_q: 53.098057
[F[K 173765/500000: episode: 2351, duration: 1.490s, episode steps: 101, steps per second: 68, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.366 [0.000, 4.000], mean observation: 0.486 [0.330, 0.540], loss: 8.352083, mean_absolute_error: 41.363132, mean_q: 52.928867
[F[K 173844/500000: episode: 2352, duration: 1.287s, episode steps: 79, steps per second: 61, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.962 [0.000, 4.000], mean observation: 0.504 [0.480, 0.560], loss: 7.398279, mean_absolute_error: 41.292938, mean_q: 52.870575
[F[K 173880/500000: episode: 2353, duration: 0.585s, episode steps: 36, steps per second: 62, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.556 [0.000, 4.000], mean observation: 0.513 [0.470, 0.640], loss: 8.675259, mean_absolute_error: 41.712563, mean_q: 53.431103
[F[K 173919/500000: episode: 2354, duration: 0.704s, episode steps: 39, steps per second: 55, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.590 [0.000, 4.000], mean observation: 0.526 [0.470, 0.650], loss: 10.336925, mean_absolute_error: 41.055298, mean_q: 52.484226
[F[K 174015/500000: episode: 2355, duration: 1.406s, episode steps: 96, steps per second: 68, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.521 [0.000, 4.000], mean observation: 0.471 [0.340, 0.530], loss: 7.815342, mean_absolute_error: 41.227566, mean_q: 52.848042
[F[K 174086/500000: episode: 2356, duration: 1.044s, episode steps: 71, steps per second: 68, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.296 [0.000, 4.000], mean observation: 0.499 [0.360, 0.620], loss: 8.284009, mean_absolute_error: 41.198154, mean_q: 52.671085
[F[K 174153/500000: episode: 2357, duration: 0.945s, episode steps: 67, steps per second: 71, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.866 [0.000, 4.000], mean observation: 0.522 [0.480, 0.660], loss: 8.174404, mean_absolute_error: 41.790134, mean_q: 53.470196
[F[K 174218/500000: episode: 2358, duration: 1.059s, episode steps: 65, steps per second: 61, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.815 [0.000, 4.000], mean observation: 0.513 [0.490, 0.600], loss: 8.949167, mean_absolute_error: 40.194645, mean_q: 51.544327
[F[K 174267/500000: episode: 2359, duration: 0.820s, episode steps: 49, steps per second: 60, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.519 [0.470, 0.600], loss: 8.260092, mean_absolute_error: 41.431717, mean_q: 52.990971
[F[K 174359/500000: episode: 2360, duration: 1.448s, episode steps: 92, steps per second: 64, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.891 [0.000, 4.000], mean observation: 0.491 [0.380, 0.540], loss: 7.583962, mean_absolute_error: 41.239697, mean_q: 52.858154
[F[K 174421/500000: episode: 2361, duration: 1.063s, episode steps: 62, steps per second: 58, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.806 [0.000, 4.000], mean observation: 0.501 [0.430, 0.610], loss: 8.595332, mean_absolute_error: 40.622845, mean_q: 51.951420
[F[K 174478/500000: episode: 2362, duration: 0.901s, episode steps: 57, steps per second: 63, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.895 [0.000, 4.000], mean observation: 0.505 [0.480, 0.530], loss: 7.519264, mean_absolute_error: 41.215096, mean_q: 52.699894
[F[K 174540/500000: episode: 2363, duration: 0.992s, episode steps: 62, steps per second: 63, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.871 [0.000, 4.000], mean observation: 0.473 [0.370, 0.510], loss: 9.672995, mean_absolute_error: 41.633026, mean_q: 53.194019
[F[K 174609/500000: episode: 2364, duration: 1.209s, episode steps: 69, steps per second: 57, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.855 [0.000, 4.000], mean observation: 0.501 [0.460, 0.580], loss: 8.445205, mean_absolute_error: 40.812721, mean_q: 52.197140
[F[K 174684/500000: episode: 2365, duration: 1.342s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.813 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 7.610952, mean_absolute_error: 41.473640, mean_q: 53.091473
[F[K 174790/500000: episode: 2366, duration: 1.614s, episode steps: 106, steps per second: 66, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.368 [0.000, 4.000], mean observation: 0.484 [0.370, 0.530], loss: 7.576758, mean_absolute_error: 41.747780, mean_q: 53.442722
[F[K 174837/500000: episode: 2367, duration: 0.719s, episode steps: 47, steps per second: 65, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.681 [0.000, 4.000], mean observation: 0.502 [0.370, 0.650], loss: 9.368521, mean_absolute_error: 41.627167, mean_q: 53.347572
[F[K 174958/500000: episode: 2368, duration: 1.981s, episode steps: 121, steps per second: 61, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.488 [0.000, 4.000], mean observation: 0.494 [0.370, 0.600], loss: 9.274948, mean_absolute_error: 40.953220, mean_q: 52.366623
[F[K 175014/500000: episode: 2369, duration: 0.921s, episode steps: 56, steps per second: 61, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.508 [0.470, 0.590], loss: 8.152511, mean_absolute_error: 40.504406, mean_q: 51.864815
[F[K 175092/500000: episode: 2370, duration: 1.274s, episode steps: 78, steps per second: 61, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.346 [0.000, 4.000], mean observation: 0.494 [0.340, 0.570], loss: 8.437084, mean_absolute_error: 40.312412, mean_q: 51.567341
[F[K 175158/500000: episode: 2371, duration: 1.090s, episode steps: 66, steps per second: 61, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.970 [0.000, 4.000], mean observation: 0.497 [0.360, 0.610], loss: 8.625905, mean_absolute_error: 41.072407, mean_q: 52.517372
[F[K 175237/500000: episode: 2372, duration: 1.310s, episode steps: 79, steps per second: 60, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.266 [0.000, 4.000], mean observation: 0.504 [0.440, 0.590], loss: 9.870232, mean_absolute_error: 41.135170, mean_q: 52.575314
[F[K 175333/500000: episode: 2373, duration: 1.540s, episode steps: 96, steps per second: 62, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.010 [0.000, 4.000], mean observation: 0.512 [0.490, 0.560], loss: 7.839655, mean_absolute_error: 40.993900, mean_q: 52.567860
[F[K 175403/500000: episode: 2374, duration: 1.096s, episode steps: 70, steps per second: 64, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.514 [0.000, 4.000], mean observation: 0.484 [0.400, 0.530], loss: 7.507329, mean_absolute_error: 41.137871, mean_q: 52.677826
[F[K 175469/500000: episode: 2375, duration: 1.059s, episode steps: 66, steps per second: 62, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 0.486 [0.370, 0.530], loss: 7.504762, mean_absolute_error: 40.721527, mean_q: 52.049545
[F[K 175532/500000: episode: 2376, duration: 1.038s, episode steps: 63, steps per second: 61, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.510 [0.470, 0.640], loss: 9.545755, mean_absolute_error: 40.740528, mean_q: 52.024799
[F[K 175584/500000: episode: 2377, duration: 0.901s, episode steps: 52, steps per second: 58, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.492 [0.430, 0.530], loss: 8.432181, mean_absolute_error: 40.616432, mean_q: 51.999352
[F[K 175644/500000: episode: 2378, duration: 1.008s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.150 [0.000, 4.000], mean observation: 0.496 [0.350, 0.600], loss: 7.852185, mean_absolute_error: 41.704468, mean_q: 53.429668
[F[K 175734/500000: episode: 2379, duration: 1.544s, episode steps: 90, steps per second: 58, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.491 [0.380, 0.530], loss: 7.573568, mean_absolute_error: 41.564968, mean_q: 53.261536
[F[K 175878/500000: episode: 2380, duration: 2.384s, episode steps: 144, steps per second: 60, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.028 [0.000, 4.000], mean observation: 0.506 [0.470, 0.570], loss: 8.275208, mean_absolute_error: 41.154079, mean_q: 52.655563
[F[K 175974/500000: episode: 2381, duration: 1.378s, episode steps: 96, steps per second: 70, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.750 [0.000, 4.000], mean observation: 0.499 [0.440, 0.570], loss: 8.430158, mean_absolute_error: 41.059612, mean_q: 52.601227
[F[K 176042/500000: episode: 2382, duration: 1.230s, episode steps: 68, steps per second: 55, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.926 [0.000, 4.000], mean observation: 0.499 [0.360, 0.600], loss: 7.691968, mean_absolute_error: 41.251129, mean_q: 52.832134
[F[K 176094/500000: episode: 2383, duration: 0.839s, episode steps: 52, steps per second: 62, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.615 [0.000, 4.000], mean observation: 0.522 [0.470, 0.630], loss: 7.979650, mean_absolute_error: 40.388680, mean_q: 51.830208
[F[K 176150/500000: episode: 2384, duration: 0.937s, episode steps: 56, steps per second: 60, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 0.502 [0.390, 0.630], loss: 9.873904, mean_absolute_error: 40.695347, mean_q: 52.008049
[F[K 176255/500000: episode: 2385, duration: 1.538s, episode steps: 105, steps per second: 68, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.905 [0.000, 4.000], mean observation: 0.506 [0.450, 0.600], loss: 8.877231, mean_absolute_error: 40.788334, mean_q: 52.222855
[F[K 176322/500000: episode: 2386, duration: 0.958s, episode steps: 67, steps per second: 70, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.731 [0.000, 4.000], mean observation: 0.498 [0.430, 0.580], loss: 7.724226, mean_absolute_error: 41.319401, mean_q: 52.839729
[F[K 176392/500000: episode: 2387, duration: 0.956s, episode steps: 70, steps per second: 73, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.114 [0.000, 4.000], mean observation: 0.498 [0.390, 0.600], loss: 7.806192, mean_absolute_error: 40.713306, mean_q: 51.999115
[F[K 176447/500000: episode: 2388, duration: 0.710s, episode steps: 55, steps per second: 77, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.164 [0.000, 4.000], mean observation: 0.492 [0.420, 0.530], loss: 10.849207, mean_absolute_error: 41.038979, mean_q: 52.462753
[F[K 176497/500000: episode: 2389, duration: 0.753s, episode steps: 50, steps per second: 66, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.220 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 9.658956, mean_absolute_error: 40.598389, mean_q: 51.983116
[F[K 176569/500000: episode: 2390, duration: 1.046s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.139 [0.000, 4.000], mean observation: 0.494 [0.360, 0.590], loss: 8.565089, mean_absolute_error: 40.277184, mean_q: 51.598049
[F[K 176758/500000: episode: 2391, duration: 2.673s, episode steps: 189, steps per second: 71, episode reward: 189.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.085 [0.000, 4.000], mean observation: 0.475 [0.360, 0.520], loss: 8.093117, mean_absolute_error: 40.880817, mean_q: 52.297443
[F[K 176904/500000: episode: 2392, duration: 2.049s, episode steps: 146, steps per second: 71, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.521 [0.000, 4.000], mean observation: 0.487 [0.420, 0.530], loss: 8.750257, mean_absolute_error: 41.238602, mean_q: 52.827770
[F[K 176980/500000: episode: 2393, duration: 0.979s, episode steps: 76, steps per second: 78, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.395 [0.000, 4.000], mean observation: 0.486 [0.370, 0.530], loss: 7.792977, mean_absolute_error: 41.330414, mean_q: 52.842236
[F[K 177033/500000: episode: 2394, duration: 0.868s, episode steps: 53, steps per second: 61, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.830 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 10.017312, mean_absolute_error: 41.010918, mean_q: 52.356926
[F[K 177199/500000: episode: 2395, duration: 2.729s, episode steps: 166, steps per second: 61, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.476 [0.000, 4.000], mean observation: 0.475 [0.320, 0.530], loss: 7.925499, mean_absolute_error: 40.909122, mean_q: 52.385349
[F[K 177271/500000: episode: 2396, duration: 1.138s, episode steps: 72, steps per second: 63, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.056 [0.000, 4.000], mean observation: 0.505 [0.410, 0.620], loss: 8.882757, mean_absolute_error: 40.640831, mean_q: 51.974041
[F[K 177379/500000: episode: 2397, duration: 1.885s, episode steps: 108, steps per second: 57, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 0.502 [0.460, 0.540], loss: 7.948658, mean_absolute_error: 41.093723, mean_q: 52.620003
[F[K 177463/500000: episode: 2398, duration: 1.132s, episode steps: 84, steps per second: 74, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.702 [0.000, 4.000], mean observation: 0.487 [0.380, 0.510], loss: 8.430544, mean_absolute_error: 41.385281, mean_q: 52.915237
[F[K 177574/500000: episode: 2399, duration: 1.498s, episode steps: 111, steps per second: 74, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.973 [0.000, 4.000], mean observation: 0.488 [0.390, 0.520], loss: 7.617980, mean_absolute_error: 40.851879, mean_q: 52.317394
[F[K 177774/500000: episode: 2400, duration: 2.668s, episode steps: 200, steps per second: 75, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.345 [0.000, 4.000], mean observation: 0.494 [0.390, 0.570], loss: 8.546852, mean_absolute_error: 41.014870, mean_q: 52.478706
[F[K 177868/500000: episode: 2401, duration: 1.336s, episode steps: 94, steps per second: 70, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.202 [0.000, 4.000], mean observation: 0.486 [0.340, 0.530], loss: 9.692160, mean_absolute_error: 40.382442, mean_q: 51.650997
[F[K 177938/500000: episode: 2402, duration: 0.961s, episode steps: 70, steps per second: 73, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.501 [0.480, 0.530], loss: 9.208555, mean_absolute_error: 41.013073, mean_q: 52.518616
[F[K 178020/500000: episode: 2403, duration: 0.906s, episode steps: 82, steps per second: 90, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.511 [0.480, 0.630], loss: 8.346938, mean_absolute_error: 41.199512, mean_q: 52.742847
[F[K 178075/500000: episode: 2404, duration: 0.677s, episode steps: 55, steps per second: 81, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.018 [0.000, 4.000], mean observation: 0.481 [0.350, 0.510], loss: 8.334243, mean_absolute_error: 41.268997, mean_q: 52.765457
[F[K 178146/500000: episode: 2405, duration: 0.988s, episode steps: 71, steps per second: 72, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.028 [0.000, 4.000], mean observation: 0.501 [0.430, 0.550], loss: 8.096196, mean_absolute_error: 40.546925, mean_q: 51.923775
[F[K 178237/500000: episode: 2406, duration: 1.134s, episode steps: 91, steps per second: 80, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.879 [0.000, 4.000], mean observation: 0.516 [0.490, 0.620], loss: 7.985517, mean_absolute_error: 40.912601, mean_q: 52.328796
[F[K 178284/500000: episode: 2407, duration: 0.568s, episode steps: 47, steps per second: 83, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.681 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 8.464414, mean_absolute_error: 41.368793, mean_q: 52.871960
[F[K 178346/500000: episode: 2408, duration: 0.701s, episode steps: 62, steps per second: 88, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 0.492 [0.380, 0.570], loss: 8.992009, mean_absolute_error: 40.490761, mean_q: 51.795746
[F[K 178407/500000: episode: 2409, duration: 0.724s, episode steps: 61, steps per second: 84, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.393 [0.000, 4.000], mean observation: 0.497 [0.380, 0.590], loss: 10.206996, mean_absolute_error: 40.841244, mean_q: 52.204483
[F[K 178467/500000: episode: 2410, duration: 0.751s, episode steps: 60, steps per second: 80, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.512 [0.470, 0.590], loss: 11.730988, mean_absolute_error: 40.920410, mean_q: 52.293064
[F[K 178648/500000: episode: 2411, duration: 2.121s, episode steps: 181, steps per second: 85, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.177 [0.000, 4.000], mean observation: 0.520 [0.450, 0.720], loss: 8.492280, mean_absolute_error: 40.425468, mean_q: 51.795990
[F[K 178708/500000: episode: 2412, duration: 0.693s, episode steps: 60, steps per second: 87, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.503 [0.400, 0.620], loss: 7.307395, mean_absolute_error: 41.755722, mean_q: 53.214455
[F[K 178747/500000: episode: 2413, duration: 0.484s, episode steps: 39, steps per second: 81, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.308 [0.000, 3.000], mean observation: 0.510 [0.470, 0.630], loss: 10.459908, mean_absolute_error: 41.611195, mean_q: 53.183167
[F[K 178826/500000: episode: 2414, duration: 0.870s, episode steps: 79, steps per second: 91, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.304 [0.000, 4.000], mean observation: 0.489 [0.380, 0.540], loss: 9.514559, mean_absolute_error: 40.629955, mean_q: 51.908894
[F[K 178932/500000: episode: 2415, duration: 1.188s, episode steps: 106, steps per second: 89, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.623 [0.000, 4.000], mean observation: 0.499 [0.440, 0.580], loss: 10.052980, mean_absolute_error: 40.512779, mean_q: 51.836853
[F[K 178988/500000: episode: 2416, duration: 0.685s, episode steps: 56, steps per second: 82, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.484 [0.420, 0.530], loss: 8.011279, mean_absolute_error: 40.581120, mean_q: 51.868805
[F[K 179051/500000: episode: 2417, duration: 0.787s, episode steps: 63, steps per second: 80, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.254 [0.000, 4.000], mean observation: 0.504 [0.450, 0.560], loss: 7.986241, mean_absolute_error: 40.199326, mean_q: 51.589130
[F[K 179090/500000: episode: 2418, duration: 0.504s, episode steps: 39, steps per second: 77, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.590 [0.000, 4.000], mean observation: 0.528 [0.470, 0.650], loss: 10.011959, mean_absolute_error: 41.199867, mean_q: 52.673859
[F[K 179127/500000: episode: 2419, duration: 0.414s, episode steps: 37, steps per second: 89, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.135 [0.000, 3.000], mean observation: 0.501 [0.370, 0.650], loss: 8.559476, mean_absolute_error: 40.595570, mean_q: 51.991253
[F[K 179235/500000: episode: 2420, duration: 1.338s, episode steps: 108, steps per second: 81, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.102 [0.000, 4.000], mean observation: 0.497 [0.370, 0.570], loss: 9.693406, mean_absolute_error: 40.461647, mean_q: 51.659214
[F[K 179358/500000: episode: 2421, duration: 1.596s, episode steps: 123, steps per second: 77, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.488 [0.000, 4.000], mean observation: 0.492 [0.430, 0.530], loss: 8.904024, mean_absolute_error: 40.707691, mean_q: 52.071156
[F[K 179427/500000: episode: 2422, duration: 0.799s, episode steps: 69, steps per second: 86, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.493 [0.440, 0.510], loss: 8.816180, mean_absolute_error: 40.782787, mean_q: 52.015148
[F[K 179473/500000: episode: 2423, duration: 0.572s, episode steps: 46, steps per second: 80, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.502 [0.370, 0.650], loss: 8.331417, mean_absolute_error: 40.428535, mean_q: 51.687164
[F[K 179532/500000: episode: 2424, duration: 0.766s, episode steps: 59, steps per second: 77, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.511 [0.470, 0.590], loss: 7.860331, mean_absolute_error: 40.345402, mean_q: 51.677471
[F[K 179581/500000: episode: 2425, duration: 0.612s, episode steps: 49, steps per second: 80, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.959 [0.000, 4.000], mean observation: 0.501 [0.390, 0.620], loss: 9.584538, mean_absolute_error: 40.045422, mean_q: 51.227631
[F[K 179649/500000: episode: 2426, duration: 0.806s, episode steps: 68, steps per second: 84, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.484 [0.400, 0.520], loss: 9.476779, mean_absolute_error: 40.505527, mean_q: 51.689007
[F[K 179720/500000: episode: 2427, duration: 0.781s, episode steps: 71, steps per second: 91, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.310 [0.000, 4.000], mean observation: 0.504 [0.440, 0.580], loss: 8.261672, mean_absolute_error: 40.257053, mean_q: 51.624893
[F[K 179769/500000: episode: 2428, duration: 0.550s, episode steps: 49, steps per second: 89, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.265 [0.000, 4.000], mean observation: 0.503 [0.410, 0.620], loss: 9.336790, mean_absolute_error: 40.323788, mean_q: 51.618896
[F[K 179839/500000: episode: 2429, duration: 0.926s, episode steps: 70, steps per second: 76, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.871 [0.000, 4.000], mean observation: 0.484 [0.430, 0.510], loss: 8.064240, mean_absolute_error: 41.274879, mean_q: 52.784523
[F[K 179900/500000: episode: 2430, duration: 0.809s, episode steps: 61, steps per second: 75, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.803 [0.000, 4.000], mean observation: 0.500 [0.430, 0.590], loss: 9.240228, mean_absolute_error: 40.906013, mean_q: 52.299999
[F[K 179976/500000: episode: 2431, duration: 0.952s, episode steps: 76, steps per second: 80, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.289 [0.000, 4.000], mean observation: 0.497 [0.460, 0.530], loss: 8.723201, mean_absolute_error: 40.345459, mean_q: 51.580227
[F[K 180058/500000: episode: 2432, duration: 0.992s, episode steps: 82, steps per second: 83, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.927 [0.000, 4.000], mean observation: 0.507 [0.480, 0.580], loss: 8.083627, mean_absolute_error: 40.483799, mean_q: 51.785538
[F[K 180187/500000: episode: 2433, duration: 1.605s, episode steps: 129, steps per second: 80, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.487 [0.360, 0.540], loss: 7.570776, mean_absolute_error: 40.753990, mean_q: 52.199028
[F[K 180265/500000: episode: 2434, duration: 0.918s, episode steps: 78, steps per second: 85, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.885 [0.000, 4.000], mean observation: 0.502 [0.470, 0.530], loss: 7.510574, mean_absolute_error: 41.281143, mean_q: 52.843399
[F[K 180370/500000: episode: 2435, duration: 1.300s, episode steps: 105, steps per second: 81, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.295 [0.000, 4.000], mean observation: 0.504 [0.460, 0.580], loss: 7.171533, mean_absolute_error: 41.105701, mean_q: 52.659779
[F[K 180444/500000: episode: 2436, duration: 0.933s, episode steps: 74, steps per second: 79, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.189 [0.000, 4.000], mean observation: 0.499 [0.470, 0.520], loss: 8.102280, mean_absolute_error: 40.662701, mean_q: 51.945469
[F[K 180517/500000: episode: 2437, duration: 0.911s, episode steps: 73, steps per second: 80, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.137 [0.000, 4.000], mean observation: 0.489 [0.420, 0.520], loss: 7.865562, mean_absolute_error: 40.590733, mean_q: 52.055080
[F[K 180597/500000: episode: 2438, duration: 0.968s, episode steps: 80, steps per second: 83, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.950 [0.000, 4.000], mean observation: 0.505 [0.470, 0.580], loss: 8.840345, mean_absolute_error: 40.426369, mean_q: 51.648243
[F[K 180751/500000: episode: 2439, duration: 2.006s, episode steps: 154, steps per second: 77, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 0.476 [0.320, 0.530], loss: 7.777019, mean_absolute_error: 40.598305, mean_q: 51.937817
[F[K 180829/500000: episode: 2440, duration: 0.995s, episode steps: 78, steps per second: 78, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.962 [0.000, 4.000], mean observation: 0.500 [0.430, 0.600], loss: 8.829263, mean_absolute_error: 40.398502, mean_q: 51.811073
[F[K 180903/500000: episode: 2441, duration: 0.940s, episode steps: 74, steps per second: 79, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 0.485 [0.360, 0.530], loss: 7.973568, mean_absolute_error: 40.545540, mean_q: 51.772881
[F[K 180976/500000: episode: 2442, duration: 0.881s, episode steps: 73, steps per second: 83, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.247 [0.000, 4.000], mean observation: 0.512 [0.470, 0.580], loss: 8.797250, mean_absolute_error: 40.554302, mean_q: 51.808475
[F[K 181161/500000: episode: 2443, duration: 2.009s, episode steps: 185, steps per second: 92, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.670 [0.000, 4.000], mean observation: 0.485 [0.300, 0.540], loss: 8.234018, mean_absolute_error: 41.257828, mean_q: 52.797222
[F[K 181300/500000: episode: 2444, duration: 1.712s, episode steps: 139, steps per second: 81, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.022 [0.000, 4.000], mean observation: 0.483 [0.400, 0.530], loss: 8.610745, mean_absolute_error: 40.229744, mean_q: 51.495052
[F[K 181363/500000: episode: 2445, duration: 0.719s, episode steps: 63, steps per second: 88, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.841 [0.000, 4.000], mean observation: 0.522 [0.470, 0.620], loss: 7.666654, mean_absolute_error: 40.565327, mean_q: 51.896580
[F[K 181440/500000: episode: 2446, duration: 0.924s, episode steps: 77, steps per second: 83, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.883 [0.000, 4.000], mean observation: 0.510 [0.470, 0.630], loss: 8.339278, mean_absolute_error: 40.946156, mean_q: 52.360905
[F[K 181624/500000: episode: 2447, duration: 2.200s, episode steps: 184, steps per second: 84, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.223 [0.000, 4.000], mean observation: 0.514 [0.470, 0.600], loss: 7.985267, mean_absolute_error: 40.915161, mean_q: 52.396191
[F[K 181664/500000: episode: 2448, duration: 0.564s, episode steps: 40, steps per second: 71, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.650 [0.000, 4.000], mean observation: 0.517 [0.470, 0.640], loss: 7.810039, mean_absolute_error: 41.654930, mean_q: 53.251972
[F[K 181724/500000: episode: 2449, duration: 0.885s, episode steps: 60, steps per second: 68, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.050 [0.000, 4.000], mean observation: 0.503 [0.470, 0.560], loss: 8.659973, mean_absolute_error: 41.168159, mean_q: 52.741753
[F[K 181797/500000: episode: 2450, duration: 0.877s, episode steps: 73, steps per second: 83, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.516 [0.480, 0.620], loss: 7.988382, mean_absolute_error: 41.182137, mean_q: 52.859722
[F[K 181915/500000: episode: 2451, duration: 1.177s, episode steps: 118, steps per second: 100, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.515 [0.450, 0.650], loss: 8.733697, mean_absolute_error: 40.896500, mean_q: 52.282555
[F[K 181976/500000: episode: 2452, duration: 0.617s, episode steps: 61, steps per second: 99, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.361 [0.000, 4.000], mean observation: 0.497 [0.470, 0.530], loss: 9.467802, mean_absolute_error: 41.051994, mean_q: 52.616837
[F[K 182029/500000: episode: 2453, duration: 0.571s, episode steps: 53, steps per second: 93, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.660 [0.000, 4.000], mean observation: 0.517 [0.470, 0.610], loss: 8.491699, mean_absolute_error: 40.911385, mean_q: 52.242023
[F[K 182122/500000: episode: 2454, duration: 0.950s, episode steps: 93, steps per second: 98, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.032 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 7.147831, mean_absolute_error: 41.297752, mean_q: 52.787777
[F[K 182168/500000: episode: 2455, duration: 0.527s, episode steps: 46, steps per second: 87, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.152 [0.000, 4.000], mean observation: 0.513 [0.480, 0.630], loss: 9.881732, mean_absolute_error: 40.843273, mean_q: 52.237564
[F[K 182261/500000: episode: 2456, duration: 0.993s, episode steps: 93, steps per second: 94, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.634 [0.000, 4.000], mean observation: 0.492 [0.400, 0.540], loss: 8.064757, mean_absolute_error: 40.905899, mean_q: 52.351414
[F[K 182303/500000: episode: 2457, duration: 0.419s, episode steps: 42, steps per second: 100, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.762 [0.000, 4.000], mean observation: 0.512 [0.470, 0.640], loss: 7.703712, mean_absolute_error: 40.645100, mean_q: 52.072578
[F[K 182386/500000: episode: 2458, duration: 0.938s, episode steps: 83, steps per second: 89, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.916 [0.000, 4.000], mean observation: 0.498 [0.470, 0.530], loss: 7.891119, mean_absolute_error: 41.191898, mean_q: 52.673950
[F[K 182450/500000: episode: 2459, duration: 0.664s, episode steps: 64, steps per second: 96, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.501 [0.430, 0.600], loss: 8.470236, mean_absolute_error: 41.078217, mean_q: 52.548500
[F[K 182578/500000: episode: 2460, duration: 1.442s, episode steps: 128, steps per second: 89, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.133 [0.000, 4.000], mean observation: 0.479 [0.380, 0.530], loss: 7.641470, mean_absolute_error: 41.393539, mean_q: 52.963097
[F[K 182645/500000: episode: 2461, duration: 0.809s, episode steps: 67, steps per second: 83, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.821 [0.000, 4.000], mean observation: 0.500 [0.410, 0.560], loss: 8.031307, mean_absolute_error: 40.798374, mean_q: 52.222939
[F[K 182708/500000: episode: 2462, duration: 0.660s, episode steps: 63, steps per second: 95, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.714 [0.000, 4.000], mean observation: 0.500 [0.440, 0.550], loss: 8.598155, mean_absolute_error: 40.840157, mean_q: 52.365540
[F[K 182754/500000: episode: 2463, duration: 0.507s, episode steps: 46, steps per second: 91, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.783 [0.000, 4.000], mean observation: 0.490 [0.360, 0.530], loss: 8.496398, mean_absolute_error: 40.372257, mean_q: 51.608723
[F[K 182815/500000: episode: 2464, duration: 0.638s, episode steps: 61, steps per second: 96, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.115 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 8.796779, mean_absolute_error: 41.138393, mean_q: 52.663883
[F[K 182925/500000: episode: 2465, duration: 1.030s, episode steps: 110, steps per second: 107, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.900 [0.000, 4.000], mean observation: 0.504 [0.470, 0.590], loss: 7.813981, mean_absolute_error: 40.631641, mean_q: 52.059853
[F[K 182971/500000: episode: 2466, duration: 0.444s, episode steps: 46, steps per second: 104, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.543 [0.000, 4.000], mean observation: 0.514 [0.470, 0.610], loss: 8.534036, mean_absolute_error: 40.865654, mean_q: 52.401749
[F[K 183029/500000: episode: 2467, duration: 0.605s, episode steps: 58, steps per second: 96, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 8.819335, mean_absolute_error: 40.916302, mean_q: 52.289024
[F[K 183219/500000: episode: 2468, duration: 2.077s, episode steps: 190, steps per second: 91, episode reward: 190.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.498 [0.460, 0.540], loss: 7.369791, mean_absolute_error: 41.496891, mean_q: 53.242775
[F[K 183281/500000: episode: 2469, duration: 0.693s, episode steps: 62, steps per second: 90, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.726 [0.000, 4.000], mean observation: 0.495 [0.350, 0.610], loss: 8.706544, mean_absolute_error: 41.222454, mean_q: 52.694847
[F[K 183409/500000: episode: 2470, duration: 1.405s, episode steps: 128, steps per second: 91, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.969 [0.000, 4.000], mean observation: 0.497 [0.400, 0.590], loss: 7.403764, mean_absolute_error: 41.246017, mean_q: 52.930557
[F[K 183480/500000: episode: 2471, duration: 0.793s, episode steps: 71, steps per second: 90, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.492 [0.430, 0.530], loss: 7.910064, mean_absolute_error: 40.962784, mean_q: 52.516617
[F[K 183556/500000: episode: 2472, duration: 0.875s, episode steps: 76, steps per second: 87, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.421 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 8.524049, mean_absolute_error: 41.565735, mean_q: 53.119740
[F[K 183616/500000: episode: 2473, duration: 0.708s, episode steps: 60, steps per second: 85, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.283 [0.000, 4.000], mean observation: 0.505 [0.440, 0.650], loss: 7.476314, mean_absolute_error: 40.747738, mean_q: 52.135460
[F[K 183662/500000: episode: 2474, duration: 0.590s, episode steps: 46, steps per second: 78, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.804 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 6.996237, mean_absolute_error: 41.224842, mean_q: 52.933891
[F[K 183738/500000: episode: 2475, duration: 1.078s, episode steps: 76, steps per second: 70, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 0.509 [0.480, 0.600], loss: 7.604272, mean_absolute_error: 41.651188, mean_q: 53.408127
[F[K 183810/500000: episode: 2476, duration: 0.876s, episode steps: 72, steps per second: 82, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 0.511 [0.470, 0.580], loss: 8.730585, mean_absolute_error: 41.443359, mean_q: 53.117718
[F[K 183876/500000: episode: 2477, duration: 0.841s, episode steps: 66, steps per second: 78, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.879 [0.000, 4.000], mean observation: 0.509 [0.470, 0.640], loss: 8.143330, mean_absolute_error: 41.365608, mean_q: 53.029118
[F[K 184028/500000: episode: 2478, duration: 2.028s, episode steps: 152, steps per second: 75, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 0.488 [0.380, 0.520], loss: 8.281470, mean_absolute_error: 41.505898, mean_q: 53.111500
[F[K 184096/500000: episode: 2479, duration: 0.865s, episode steps: 68, steps per second: 79, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.779 [0.000, 4.000], mean observation: 0.509 [0.470, 0.620], loss: 7.572553, mean_absolute_error: 40.989594, mean_q: 52.441883
[F[K 184164/500000: episode: 2480, duration: 0.870s, episode steps: 68, steps per second: 78, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.706 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 8.104704, mean_absolute_error: 41.506874, mean_q: 53.054710
[F[K 184253/500000: episode: 2481, duration: 1.198s, episode steps: 89, steps per second: 74, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.022 [0.000, 4.000], mean observation: 0.515 [0.460, 0.610], loss: 7.752852, mean_absolute_error: 40.798931, mean_q: 52.250542
[F[K 184330/500000: episode: 2482, duration: 0.951s, episode steps: 77, steps per second: 81, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.502 [0.390, 0.600], loss: 10.000830, mean_absolute_error: 41.453884, mean_q: 52.938160
[F[K 184403/500000: episode: 2483, duration: 1.034s, episode steps: 73, steps per second: 71, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 8.241086, mean_absolute_error: 41.042690, mean_q: 52.511543
[F[K 184481/500000: episode: 2484, duration: 1.037s, episode steps: 78, steps per second: 75, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.769 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 8.212849, mean_absolute_error: 40.962299, mean_q: 52.541523
[F[K 184563/500000: episode: 2485, duration: 0.901s, episode steps: 82, steps per second: 91, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.487 [0.400, 0.520], loss: 8.987832, mean_absolute_error: 40.772228, mean_q: 52.157810
[F[K 184599/500000: episode: 2486, duration: 0.529s, episode steps: 36, steps per second: 68, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.861 [0.000, 4.000], mean observation: 0.527 [0.470, 0.640], loss: 9.184019, mean_absolute_error: 40.844940, mean_q: 52.241104
[F[K 184666/500000: episode: 2487, duration: 0.965s, episode steps: 67, steps per second: 69, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.701 [0.000, 4.000], mean observation: 0.512 [0.460, 0.600], loss: 9.418463, mean_absolute_error: 40.428181, mean_q: 51.789238
[F[K 184768/500000: episode: 2488, duration: 1.358s, episode steps: 102, steps per second: 75, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.265 [0.000, 4.000], mean observation: 0.503 [0.470, 0.530], loss: 8.939099, mean_absolute_error: 40.650158, mean_q: 51.973728
[F[K 184814/500000: episode: 2489, duration: 0.602s, episode steps: 46, steps per second: 76, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.413 [0.000, 4.000], mean observation: 0.502 [0.370, 0.650], loss: 7.396143, mean_absolute_error: 41.368874, mean_q: 52.952145
[F[K 184892/500000: episode: 2490, duration: 0.986s, episode steps: 78, steps per second: 79, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.231 [0.000, 4.000], mean observation: 0.492 [0.400, 0.530], loss: 7.727681, mean_absolute_error: 40.636627, mean_q: 52.012470
[F[K 184971/500000: episode: 2491, duration: 0.981s, episode steps: 79, steps per second: 81, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.975 [0.000, 4.000], mean observation: 0.482 [0.410, 0.520], loss: 9.240448, mean_absolute_error: 41.469425, mean_q: 53.049404
[F[K 185040/500000: episode: 2492, duration: 0.827s, episode steps: 69, steps per second: 83, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.275 [0.000, 4.000], mean observation: 0.493 [0.400, 0.530], loss: 7.771804, mean_absolute_error: 41.222042, mean_q: 52.820618
[F[K 185085/500000: episode: 2493, duration: 0.600s, episode steps: 45, steps per second: 75, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.311 [0.000, 4.000], mean observation: 0.512 [0.470, 0.640], loss: 9.230882, mean_absolute_error: 40.787216, mean_q: 52.290928
[F[K 185163/500000: episode: 2494, duration: 1.002s, episode steps: 78, steps per second: 78, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.910 [0.000, 4.000], mean observation: 0.492 [0.360, 0.580], loss: 7.130670, mean_absolute_error: 40.882210, mean_q: 52.472752
[F[K 185283/500000: episode: 2495, duration: 1.697s, episode steps: 120, steps per second: 71, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.467 [0.000, 4.000], mean observation: 0.481 [0.340, 0.530], loss: 8.944505, mean_absolute_error: 40.860313, mean_q: 52.359264
[F[K 185328/500000: episode: 2496, duration: 0.568s, episode steps: 45, steps per second: 79, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.512 [0.450, 0.660], loss: 7.234942, mean_absolute_error: 40.366837, mean_q: 51.826130
[F[K 185386/500000: episode: 2497, duration: 0.805s, episode steps: 58, steps per second: 72, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.776 [0.000, 4.000], mean observation: 0.485 [0.400, 0.530], loss: 7.526001, mean_absolute_error: 41.108139, mean_q: 52.710281
[F[K 185468/500000: episode: 2498, duration: 0.986s, episode steps: 82, steps per second: 83, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.495 [0.410, 0.530], loss: 8.143366, mean_absolute_error: 41.187923, mean_q: 52.674145
[F[K 185528/500000: episode: 2499, duration: 0.767s, episode steps: 60, steps per second: 78, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.367 [0.000, 4.000], mean observation: 0.505 [0.390, 0.630], loss: 8.740985, mean_absolute_error: 40.718666, mean_q: 51.991581
[F[K 185666/500000: episode: 2500, duration: 1.819s, episode steps: 138, steps per second: 76, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.509 [0.470, 0.600], loss: 8.467016, mean_absolute_error: 40.293629, mean_q: 51.656803
[F[K 185750/500000: episode: 2501, duration: 1.174s, episode steps: 84, steps per second: 72, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.905 [0.000, 4.000], mean observation: 0.492 [0.400, 0.540], loss: 8.053253, mean_absolute_error: 40.827072, mean_q: 52.184139
[F[K 185840/500000: episode: 2502, duration: 1.141s, episode steps: 90, steps per second: 79, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.900 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 7.190773, mean_absolute_error: 41.402512, mean_q: 53.053272
[F[K 185947/500000: episode: 2503, duration: 1.431s, episode steps: 107, steps per second: 75, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.308 [0.000, 4.000], mean observation: 0.498 [0.440, 0.540], loss: 7.622308, mean_absolute_error: 41.132217, mean_q: 52.650276
[F[K 186023/500000: episode: 2504, duration: 1.021s, episode steps: 76, steps per second: 74, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.671 [0.000, 4.000], mean observation: 0.483 [0.350, 0.520], loss: 7.822732, mean_absolute_error: 41.079449, mean_q: 52.578564
[F[K 186101/500000: episode: 2505, duration: 0.959s, episode steps: 78, steps per second: 81, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.496 [0.420, 0.530], loss: 8.408053, mean_absolute_error: 41.079182, mean_q: 52.553104
[F[K 186185/500000: episode: 2506, duration: 1.135s, episode steps: 84, steps per second: 74, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.107 [0.000, 4.000], mean observation: 0.502 [0.430, 0.570], loss: 7.522185, mean_absolute_error: 40.343586, mean_q: 51.651825
[F[K 186231/500000: episode: 2507, duration: 0.617s, episode steps: 46, steps per second: 75, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.913 [0.000, 4.000], mean observation: 0.527 [0.470, 0.630], loss: 8.057624, mean_absolute_error: 40.730045, mean_q: 52.108086
[F[K 186284/500000: episode: 2508, duration: 0.749s, episode steps: 53, steps per second: 71, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.585 [0.000, 4.000], mean observation: 0.517 [0.480, 0.630], loss: 8.293285, mean_absolute_error: 40.265366, mean_q: 51.632198
[F[K 186354/500000: episode: 2509, duration: 0.968s, episode steps: 70, steps per second: 72, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.493 [0.410, 0.550], loss: 7.897696, mean_absolute_error: 40.686596, mean_q: 52.115440
[F[K 186422/500000: episode: 2510, duration: 1.000s, episode steps: 68, steps per second: 68, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.353 [0.000, 4.000], mean observation: 0.525 [0.500, 0.650], loss: 9.620888, mean_absolute_error: 40.927082, mean_q: 52.402958
[F[K 186499/500000: episode: 2511, duration: 1.094s, episode steps: 77, steps per second: 70, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.468 [0.000, 4.000], mean observation: 0.488 [0.380, 0.530], loss: 7.279166, mean_absolute_error: 40.884308, mean_q: 52.410847
[F[K 186572/500000: episode: 2512, duration: 1.024s, episode steps: 73, steps per second: 71, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.301 [0.000, 4.000], mean observation: 0.498 [0.360, 0.620], loss: 8.697423, mean_absolute_error: 40.695923, mean_q: 52.149723
[F[K 186632/500000: episode: 2513, duration: 0.883s, episode steps: 60, steps per second: 68, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.508 [0.470, 0.590], loss: 8.759848, mean_absolute_error: 40.324284, mean_q: 51.663986
[F[K 186726/500000: episode: 2514, duration: 1.335s, episode steps: 94, steps per second: 70, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.128 [0.000, 4.000], mean observation: 0.505 [0.480, 0.550], loss: 8.678901, mean_absolute_error: 40.787441, mean_q: 52.374588
[F[K 186787/500000: episode: 2515, duration: 0.966s, episode steps: 61, steps per second: 63, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.590 [0.000, 4.000], mean observation: 0.487 [0.390, 0.510], loss: 7.239463, mean_absolute_error: 41.332386, mean_q: 52.953270
[F[K 186881/500000: episode: 2516, duration: 1.497s, episode steps: 94, steps per second: 63, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.202 [0.000, 4.000], mean observation: 0.496 [0.430, 0.530], loss: 7.479187, mean_absolute_error: 41.024822, mean_q: 52.510788
[F[K 186968/500000: episode: 2517, duration: 1.356s, episode steps: 87, steps per second: 64, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.437 [0.000, 4.000], mean observation: 0.491 [0.400, 0.530], loss: 7.652780, mean_absolute_error: 41.099815, mean_q: 52.669968
[F[K 187024/500000: episode: 2518, duration: 0.856s, episode steps: 56, steps per second: 65, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.929 [0.000, 4.000], mean observation: 0.498 [0.370, 0.610], loss: 9.709790, mean_absolute_error: 41.055973, mean_q: 52.608803
[F[K 187070/500000: episode: 2519, duration: 0.678s, episode steps: 46, steps per second: 68, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.522 [0.000, 4.000], mean observation: 0.504 [0.430, 0.620], loss: 11.202836, mean_absolute_error: 39.968216, mean_q: 51.147816
[F[K 187124/500000: episode: 2520, duration: 0.931s, episode steps: 54, steps per second: 58, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.515 [0.470, 0.620], loss: 7.686280, mean_absolute_error: 41.200680, mean_q: 52.895962
[F[K 187179/500000: episode: 2521, duration: 0.990s, episode steps: 55, steps per second: 56, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.818 [0.000, 4.000], mean observation: 0.513 [0.460, 0.630], loss: 8.307503, mean_absolute_error: 40.750088, mean_q: 52.268826
[F[K 187227/500000: episode: 2522, duration: 0.789s, episode steps: 48, steps per second: 61, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.526 [0.480, 0.640], loss: 7.715324, mean_absolute_error: 41.056137, mean_q: 52.659546
[F[K 187289/500000: episode: 2523, duration: 0.953s, episode steps: 62, steps per second: 65, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.129 [0.000, 4.000], mean observation: 0.523 [0.490, 0.650], loss: 7.773192, mean_absolute_error: 40.143856, mean_q: 51.554573
[F[K 187342/500000: episode: 2524, duration: 0.883s, episode steps: 53, steps per second: 60, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.283 [0.000, 4.000], mean observation: 0.488 [0.380, 0.530], loss: 7.201347, mean_absolute_error: 40.439091, mean_q: 51.876793
[F[K 187434/500000: episode: 2525, duration: 1.188s, episode steps: 92, steps per second: 77, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.337 [0.000, 4.000], mean observation: 0.504 [0.470, 0.540], loss: 7.199215, mean_absolute_error: 41.256001, mean_q: 52.903347
[F[K 187493/500000: episode: 2526, duration: 0.868s, episode steps: 59, steps per second: 68, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.576 [0.000, 4.000], mean observation: 0.505 [0.480, 0.540], loss: 12.025003, mean_absolute_error: 39.868076, mean_q: 51.100098
[F[K 187566/500000: episode: 2527, duration: 1.025s, episode steps: 73, steps per second: 71, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.603 [0.000, 4.000], mean observation: 0.485 [0.350, 0.530], loss: 8.377805, mean_absolute_error: 40.723259, mean_q: 52.298065
[F[K 187641/500000: episode: 2528, duration: 1.194s, episode steps: 75, steps per second: 63, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.427 [0.000, 4.000], mean observation: 0.496 [0.470, 0.530], loss: 8.671007, mean_absolute_error: 40.369144, mean_q: 51.828762
[F[K 187716/500000: episode: 2529, duration: 1.171s, episode steps: 75, steps per second: 64, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.147 [0.000, 4.000], mean observation: 0.499 [0.430, 0.540], loss: 7.912752, mean_absolute_error: 40.782753, mean_q: 52.271183
[F[K 187779/500000: episode: 2530, duration: 0.865s, episode steps: 63, steps per second: 73, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.937 [0.000, 4.000], mean observation: 0.505 [0.440, 0.600], loss: 9.161911, mean_absolute_error: 40.670952, mean_q: 52.086002
[F[K 187850/500000: episode: 2531, duration: 1.145s, episode steps: 71, steps per second: 62, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.592 [0.000, 4.000], mean observation: 0.490 [0.370, 0.530], loss: 7.343742, mean_absolute_error: 40.674892, mean_q: 52.163673
[F[K 187940/500000: episode: 2532, duration: 1.368s, episode steps: 90, steps per second: 66, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.056 [0.000, 4.000], mean observation: 0.522 [0.490, 0.630], loss: 8.251836, mean_absolute_error: 40.041080, mean_q: 51.241245
[F[K 188023/500000: episode: 2533, duration: 1.234s, episode steps: 83, steps per second: 67, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.325 [0.000, 4.000], mean observation: 0.487 [0.380, 0.520], loss: 8.526202, mean_absolute_error: 40.099884, mean_q: 51.454189
[F[K 188126/500000: episode: 2534, duration: 1.357s, episode steps: 103, steps per second: 76, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.961 [0.000, 4.000], mean observation: 0.492 [0.390, 0.530], loss: 7.864258, mean_absolute_error: 41.487640, mean_q: 53.190880
[F[K 188206/500000: episode: 2535, duration: 1.230s, episode steps: 80, steps per second: 65, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.474 [0.370, 0.510], loss: 7.896029, mean_absolute_error: 41.054527, mean_q: 52.607441
[F[K 188279/500000: episode: 2536, duration: 1.223s, episode steps: 73, steps per second: 60, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.753 [0.000, 4.000], mean observation: 0.498 [0.380, 0.620], loss: 8.569139, mean_absolute_error: 40.546654, mean_q: 51.965034
[F[K 188368/500000: episode: 2537, duration: 1.434s, episode steps: 89, steps per second: 62, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.742 [0.000, 4.000], mean observation: 0.481 [0.360, 0.530], loss: 8.432452, mean_absolute_error: 40.043495, mean_q: 51.345261
[F[K 188508/500000: episode: 2538, duration: 2.270s, episode steps: 140, steps per second: 62, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.300 [0.000, 4.000], mean observation: 0.514 [0.470, 0.590], loss: 7.421631, mean_absolute_error: 41.045998, mean_q: 52.664497
[F[K 188569/500000: episode: 2539, duration: 0.737s, episode steps: 61, steps per second: 83, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.934 [0.000, 4.000], mean observation: 0.514 [0.480, 0.620], loss: 8.763299, mean_absolute_error: 41.272469, mean_q: 52.931957
[F[K 188641/500000: episode: 2540, duration: 1.141s, episode steps: 72, steps per second: 63, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.501 [0.460, 0.570], loss: 8.322521, mean_absolute_error: 40.204762, mean_q: 51.528030
[F[K 188769/500000: episode: 2541, duration: 2.154s, episode steps: 128, steps per second: 59, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.008 [0.000, 4.000], mean observation: 0.495 [0.450, 0.540], loss: 8.039145, mean_absolute_error: 40.605171, mean_q: 52.060402
[F[K 188821/500000: episode: 2542, duration: 0.866s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.673 [0.000, 4.000], mean observation: 0.527 [0.480, 0.650], loss: 9.197788, mean_absolute_error: 40.256950, mean_q: 51.699905
[F[K 188888/500000: episode: 2543, duration: 1.090s, episode steps: 67, steps per second: 61, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.090 [0.000, 4.000], mean observation: 0.505 [0.470, 0.600], loss: 7.910015, mean_absolute_error: 40.301582, mean_q: 51.609077
[F[K 188964/500000: episode: 2544, duration: 0.990s, episode steps: 76, steps per second: 77, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.513 [0.000, 4.000], mean observation: 0.490 [0.430, 0.530], loss: 7.882623, mean_absolute_error: 40.519508, mean_q: 51.984264
[F[K 189038/500000: episode: 2545, duration: 1.181s, episode steps: 74, steps per second: 63, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.041 [0.000, 4.000], mean observation: 0.491 [0.380, 0.530], loss: 8.252013, mean_absolute_error: 41.249226, mean_q: 52.860550
[F[K 189111/500000: episode: 2546, duration: 1.138s, episode steps: 73, steps per second: 64, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.507 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 9.051673, mean_absolute_error: 40.177204, mean_q: 51.531544
[F[K 189179/500000: episode: 2547, duration: 1.048s, episode steps: 68, steps per second: 65, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.794 [0.000, 4.000], mean observation: 0.512 [0.440, 0.650], loss: 7.123046, mean_absolute_error: 40.741020, mean_q: 52.347969
[F[K 189283/500000: episode: 2548, duration: 1.519s, episode steps: 104, steps per second: 68, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.510 [0.470, 0.580], loss: 8.047170, mean_absolute_error: 40.618168, mean_q: 52.122524
[F[K 189373/500000: episode: 2549, duration: 1.213s, episode steps: 90, steps per second: 74, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.144 [0.000, 4.000], mean observation: 0.506 [0.440, 0.650], loss: 8.207856, mean_absolute_error: 41.013027, mean_q: 52.520226
[F[K 189416/500000: episode: 2550, duration: 0.599s, episode steps: 43, steps per second: 72, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.419 [0.000, 3.000], mean observation: 0.516 [0.470, 0.630], loss: 9.460065, mean_absolute_error: 40.083069, mean_q: 51.361267
[F[K 189491/500000: episode: 2551, duration: 1.328s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.512 [0.470, 0.620], loss: 7.891591, mean_absolute_error: 40.418694, mean_q: 51.922050
[F[K 189562/500000: episode: 2552, duration: 1.220s, episode steps: 71, steps per second: 58, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.845 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 7.836955, mean_absolute_error: 41.074146, mean_q: 52.586010
[F[K 189635/500000: episode: 2553, duration: 0.985s, episode steps: 73, steps per second: 74, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.973 [0.000, 4.000], mean observation: 0.489 [0.400, 0.530], loss: 8.504560, mean_absolute_error: 40.918926, mean_q: 52.492096
[F[K 189707/500000: episode: 2554, duration: 1.246s, episode steps: 72, steps per second: 58, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.222 [0.000, 4.000], mean observation: 0.483 [0.380, 0.520], loss: 7.852773, mean_absolute_error: 40.984062, mean_q: 52.670975
[F[K 189770/500000: episode: 2555, duration: 1.174s, episode steps: 63, steps per second: 54, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.540 [0.000, 4.000], mean observation: 0.485 [0.400, 0.530], loss: 8.488868, mean_absolute_error: 40.203220, mean_q: 51.547409
[F[K 189862/500000: episode: 2556, duration: 1.314s, episode steps: 92, steps per second: 70, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.924 [0.000, 4.000], mean observation: 0.493 [0.410, 0.520], loss: 8.548155, mean_absolute_error: 40.369534, mean_q: 51.812592
[F[K 189945/500000: episode: 2557, duration: 1.182s, episode steps: 83, steps per second: 70, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.096 [0.000, 4.000], mean observation: 0.494 [0.370, 0.550], loss: 7.707967, mean_absolute_error: 41.027210, mean_q: 52.627724
[F[K 190007/500000: episode: 2558, duration: 0.808s, episode steps: 62, steps per second: 77, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.161 [0.000, 4.000], mean observation: 0.518 [0.490, 0.600], loss: 8.547723, mean_absolute_error: 40.532391, mean_q: 51.906841
[F[K 190114/500000: episode: 2559, duration: 1.373s, episode steps: 107, steps per second: 78, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.897 [0.000, 4.000], mean observation: 0.518 [0.480, 0.620], loss: 8.901801, mean_absolute_error: 40.516064, mean_q: 51.938915
[F[K 190190/500000: episode: 2560, duration: 1.040s, episode steps: 76, steps per second: 73, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.724 [0.000, 4.000], mean observation: 0.508 [0.460, 0.610], loss: 8.396454, mean_absolute_error: 41.046268, mean_q: 52.696484
[F[K 190286/500000: episode: 2561, duration: 1.238s, episode steps: 96, steps per second: 78, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.503 [0.380, 0.620], loss: 10.040479, mean_absolute_error: 40.644001, mean_q: 52.126575
[F[K 190381/500000: episode: 2562, duration: 1.329s, episode steps: 95, steps per second: 71, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.221 [0.000, 4.000], mean observation: 0.501 [0.390, 0.630], loss: 8.157791, mean_absolute_error: 40.438087, mean_q: 51.892876
[F[K 190463/500000: episode: 2563, duration: 1.138s, episode steps: 82, steps per second: 72, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.463 [0.000, 4.000], mean observation: 0.511 [0.480, 0.570], loss: 8.557646, mean_absolute_error: 40.490318, mean_q: 51.939121
[F[K 190548/500000: episode: 2564, duration: 1.062s, episode steps: 85, steps per second: 80, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.271 [0.000, 4.000], mean observation: 0.492 [0.350, 0.580], loss: 7.540471, mean_absolute_error: 40.865273, mean_q: 52.460995
[F[K 190634/500000: episode: 2565, duration: 1.075s, episode steps: 86, steps per second: 80, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.023 [0.000, 4.000], mean observation: 0.509 [0.470, 0.570], loss: 8.524026, mean_absolute_error: 40.619286, mean_q: 52.067444
[F[K 190745/500000: episode: 2566, duration: 1.475s, episode steps: 111, steps per second: 75, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.189 [0.000, 4.000], mean observation: 0.491 [0.380, 0.540], loss: 8.104589, mean_absolute_error: 40.579929, mean_q: 52.059036
[F[K 190796/500000: episode: 2567, duration: 0.606s, episode steps: 51, steps per second: 84, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.275 [0.000, 4.000], mean observation: 0.500 [0.390, 0.610], loss: 8.614708, mean_absolute_error: 41.022930, mean_q: 52.599457
[F[K 190846/500000: episode: 2568, duration: 0.531s, episode steps: 50, steps per second: 94, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.220 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 8.402334, mean_absolute_error: 40.702354, mean_q: 52.154816
[F[K 190905/500000: episode: 2569, duration: 0.751s, episode steps: 59, steps per second: 79, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.373 [0.000, 4.000], mean observation: 0.512 [0.450, 0.640], loss: 7.770163, mean_absolute_error: 40.456367, mean_q: 51.920357
[F[K 190977/500000: episode: 2570, duration: 1.104s, episode steps: 72, steps per second: 65, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.483 [0.380, 0.530], loss: 8.051008, mean_absolute_error: 40.690483, mean_q: 52.307491
[F[K 191063/500000: episode: 2571, duration: 1.116s, episode steps: 86, steps per second: 77, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.895 [0.000, 4.000], mean observation: 0.503 [0.400, 0.640], loss: 7.921335, mean_absolute_error: 40.483334, mean_q: 51.943802
[F[K 191109/500000: episode: 2572, duration: 0.554s, episode steps: 46, steps per second: 83, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.517 [0.490, 0.640], loss: 8.134727, mean_absolute_error: 40.697559, mean_q: 52.249001
[F[K 191182/500000: episode: 2573, duration: 0.981s, episode steps: 73, steps per second: 74, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.836 [0.000, 4.000], mean observation: 0.479 [0.360, 0.510], loss: 8.934570, mean_absolute_error: 40.294483, mean_q: 51.759747
[F[K 191251/500000: episode: 2574, duration: 0.883s, episode steps: 69, steps per second: 78, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.565 [0.000, 4.000], mean observation: 0.487 [0.340, 0.540], loss: 8.068302, mean_absolute_error: 40.534496, mean_q: 52.007896
[F[K 191295/500000: episode: 2575, duration: 0.565s, episode steps: 44, steps per second: 78, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.591 [0.000, 4.000], mean observation: 0.519 [0.470, 0.630], loss: 8.770841, mean_absolute_error: 40.682213, mean_q: 52.232410
[F[K 191352/500000: episode: 2576, duration: 0.812s, episode steps: 57, steps per second: 70, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.579 [0.000, 4.000], mean observation: 0.499 [0.430, 0.550], loss: 8.802348, mean_absolute_error: 40.258965, mean_q: 51.691860
[F[K 191405/500000: episode: 2577, duration: 0.707s, episode steps: 53, steps per second: 75, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.075 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 10.379879, mean_absolute_error: 39.881519, mean_q: 51.003216
[F[K 191529/500000: episode: 2578, duration: 1.749s, episode steps: 124, steps per second: 71, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.371 [0.000, 4.000], mean observation: 0.491 [0.380, 0.530], loss: 8.538774, mean_absolute_error: 40.322113, mean_q: 51.647163
[F[K 191584/500000: episode: 2579, duration: 0.766s, episode steps: 55, steps per second: 72, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.418 [0.000, 4.000], mean observation: 0.483 [0.370, 0.530], loss: 9.909999, mean_absolute_error: 39.899578, mean_q: 51.144241
[F[K 191652/500000: episode: 2580, duration: 1.030s, episode steps: 68, steps per second: 66, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.838 [0.000, 4.000], mean observation: 0.508 [0.500, 0.530], loss: 8.176659, mean_absolute_error: 39.768570, mean_q: 50.924591
[F[K 191738/500000: episode: 2581, duration: 1.177s, episode steps: 86, steps per second: 73, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.081 [0.000, 4.000], mean observation: 0.473 [0.360, 0.520], loss: 8.246522, mean_absolute_error: 40.687771, mean_q: 52.132076
[F[K 191793/500000: episode: 2582, duration: 0.715s, episode steps: 55, steps per second: 77, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.418 [0.000, 4.000], mean observation: 0.477 [0.380, 0.530], loss: 10.863486, mean_absolute_error: 39.581806, mean_q: 50.663074
[F[K 191903/500000: episode: 2583, duration: 1.550s, episode steps: 110, steps per second: 71, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.527 [0.000, 4.000], mean observation: 0.479 [0.360, 0.530], loss: 8.108516, mean_absolute_error: 40.716740, mean_q: 52.230118
[F[K 191943/500000: episode: 2584, duration: 0.510s, episode steps: 40, steps per second: 78, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.175 [0.000, 3.000], mean observation: 0.507 [0.420, 0.640], loss: 7.656840, mean_absolute_error: 40.325420, mean_q: 51.823772
[F[K 191981/500000: episode: 2585, duration: 0.498s, episode steps: 38, steps per second: 76, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.447 [0.000, 4.000], mean observation: 0.499 [0.360, 0.650], loss: 8.999044, mean_absolute_error: 40.568737, mean_q: 52.024559
[F[K 192080/500000: episode: 2586, duration: 1.364s, episode steps: 99, steps per second: 73, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.511 [0.460, 0.620], loss: 7.949721, mean_absolute_error: 40.852493, mean_q: 52.346054
[F[K 192150/500000: episode: 2587, duration: 0.985s, episode steps: 70, steps per second: 71, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.757 [0.000, 4.000], mean observation: 0.506 [0.470, 0.600], loss: 7.644857, mean_absolute_error: 40.321918, mean_q: 51.797466
[F[K 192207/500000: episode: 2588, duration: 0.712s, episode steps: 57, steps per second: 80, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.456 [0.000, 4.000], mean observation: 0.516 [0.470, 0.620], loss: 9.527277, mean_absolute_error: 39.901230, mean_q: 51.276131
[F[K 192286/500000: episode: 2589, duration: 0.804s, episode steps: 79, steps per second: 98, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.734 [0.000, 4.000], mean observation: 0.490 [0.430, 0.530], loss: 7.545464, mean_absolute_error: 40.232834, mean_q: 51.696751
[F[K 192382/500000: episode: 2590, duration: 1.348s, episode steps: 96, steps per second: 71, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.990 [0.000, 4.000], mean observation: 0.500 [0.370, 0.630], loss: 8.348834, mean_absolute_error: 40.597301, mean_q: 52.082317
[F[K 192459/500000: episode: 2591, duration: 1.127s, episode steps: 77, steps per second: 68, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.922 [0.000, 4.000], mean observation: 0.493 [0.390, 0.530], loss: 8.038558, mean_absolute_error: 40.198215, mean_q: 51.586697
[F[K 192512/500000: episode: 2592, duration: 0.717s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.521 [0.470, 0.610], loss: 9.412866, mean_absolute_error: 40.317730, mean_q: 51.627293
[F[K 192569/500000: episode: 2593, duration: 0.745s, episode steps: 57, steps per second: 76, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.053 [0.000, 4.000], mean observation: 0.507 [0.470, 0.560], loss: 7.770317, mean_absolute_error: 40.048801, mean_q: 51.396309
[F[K 192650/500000: episode: 2594, duration: 1.029s, episode steps: 81, steps per second: 79, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.185 [0.000, 4.000], mean observation: 0.487 [0.410, 0.510], loss: 7.931262, mean_absolute_error: 40.136452, mean_q: 51.344269
[F[K 192715/500000: episode: 2595, duration: 0.810s, episode steps: 65, steps per second: 80, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.123 [0.000, 4.000], mean observation: 0.512 [0.470, 0.650], loss: 9.036278, mean_absolute_error: 40.007862, mean_q: 51.326824
[F[K 192785/500000: episode: 2596, duration: 0.969s, episode steps: 70, steps per second: 72, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.484 [0.370, 0.530], loss: 8.184627, mean_absolute_error: 40.302826, mean_q: 51.661438
[F[K 192835/500000: episode: 2597, duration: 0.762s, episode steps: 50, steps per second: 66, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.620 [0.000, 4.000], mean observation: 0.509 [0.490, 0.560], loss: 7.620675, mean_absolute_error: 40.844749, mean_q: 52.439995
[F[K 192873/500000: episode: 2598, duration: 0.619s, episode steps: 38, steps per second: 61, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.632 [0.000, 4.000], mean observation: 0.523 [0.470, 0.640], loss: 8.027741, mean_absolute_error: 39.918228, mean_q: 51.298595
[F[K 192970/500000: episode: 2599, duration: 1.474s, episode steps: 97, steps per second: 66, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.493 [0.380, 0.570], loss: 7.853697, mean_absolute_error: 40.564625, mean_q: 52.035046
[F[K 193033/500000: episode: 2600, duration: 0.957s, episode steps: 63, steps per second: 66, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.413 [0.000, 4.000], mean observation: 0.507 [0.470, 0.570], loss: 7.593138, mean_absolute_error: 40.110622, mean_q: 51.419964
[F[K 193118/500000: episode: 2601, duration: 1.280s, episode steps: 85, steps per second: 66, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.953 [0.000, 4.000], mean observation: 0.498 [0.440, 0.550], loss: 7.455436, mean_absolute_error: 40.599899, mean_q: 51.980438
[F[K 193213/500000: episode: 2602, duration: 1.423s, episode steps: 95, steps per second: 67, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.305 [0.000, 4.000], mean observation: 0.492 [0.340, 0.580], loss: 9.039531, mean_absolute_error: 40.033562, mean_q: 51.339134
[F[K 193259/500000: episode: 2603, duration: 0.681s, episode steps: 46, steps per second: 68, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.609 [0.000, 4.000], mean observation: 0.524 [0.470, 0.650], loss: 6.611056, mean_absolute_error: 40.528725, mean_q: 52.202175
[F[K 193305/500000: episode: 2604, duration: 0.646s, episode steps: 46, steps per second: 71, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.217 [0.000, 4.000], mean observation: 0.470 [0.350, 0.530], loss: 8.306745, mean_absolute_error: 39.950985, mean_q: 51.330250
[F[K 193436/500000: episode: 2605, duration: 2.010s, episode steps: 131, steps per second: 65, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.511 [0.470, 0.640], loss: 8.202740, mean_absolute_error: 40.217213, mean_q: 51.595520
[F[K 193473/500000: episode: 2606, duration: 0.541s, episode steps: 37, steps per second: 68, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.730 [0.000, 4.000], mean observation: 0.520 [0.470, 0.650], loss: 8.192094, mean_absolute_error: 40.524132, mean_q: 52.032696
[F[K 193539/500000: episode: 2607, duration: 1.049s, episode steps: 66, steps per second: 63, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.470 [0.000, 4.000], mean observation: 0.502 [0.460, 0.540], loss: 8.936266, mean_absolute_error: 40.585838, mean_q: 52.126553
[F[K 193681/500000: episode: 2608, duration: 2.336s, episode steps: 142, steps per second: 61, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.148 [0.000, 4.000], mean observation: 0.490 [0.410, 0.520], loss: 8.644383, mean_absolute_error: 40.750156, mean_q: 52.281746
[F[K 193749/500000: episode: 2609, duration: 1.189s, episode steps: 68, steps per second: 57, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.368 [0.000, 4.000], mean observation: 0.495 [0.340, 0.600], loss: 8.468040, mean_absolute_error: 40.250607, mean_q: 51.588535
[F[K 193811/500000: episode: 2610, duration: 1.066s, episode steps: 62, steps per second: 58, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.504 [0.420, 0.600], loss: 9.780183, mean_absolute_error: 39.839684, mean_q: 50.982300
[F[K 193951/500000: episode: 2611, duration: 2.234s, episode steps: 140, steps per second: 63, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 0.499 [0.460, 0.530], loss: 8.915237, mean_absolute_error: 40.073757, mean_q: 51.409119
[F[K 194023/500000: episode: 2612, duration: 1.091s, episode steps: 72, steps per second: 66, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.069 [0.000, 4.000], mean observation: 0.496 [0.420, 0.560], loss: 8.809589, mean_absolute_error: 40.067951, mean_q: 51.374508
[F[K 194090/500000: episode: 2613, duration: 1.106s, episode steps: 67, steps per second: 61, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.940 [0.000, 4.000], mean observation: 0.516 [0.470, 0.590], loss: 7.157763, mean_absolute_error: 40.755577, mean_q: 52.361435
[F[K 194156/500000: episode: 2614, duration: 1.045s, episode steps: 66, steps per second: 63, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.788 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 7.885097, mean_absolute_error: 40.380592, mean_q: 51.817635
[F[K 194213/500000: episode: 2615, duration: 0.809s, episode steps: 57, steps per second: 70, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.503 [0.490, 0.530], loss: 9.073449, mean_absolute_error: 40.600574, mean_q: 52.149185
[F[K 194280/500000: episode: 2616, duration: 0.952s, episode steps: 67, steps per second: 70, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.731 [0.000, 4.000], mean observation: 0.497 [0.420, 0.530], loss: 8.569245, mean_absolute_error: 40.072186, mean_q: 51.366585
[F[K 194321/500000: episode: 2617, duration: 0.620s, episode steps: 41, steps per second: 66, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.927 [0.000, 4.000], mean observation: 0.516 [0.480, 0.650], loss: 8.779929, mean_absolute_error: 40.066563, mean_q: 51.461731
[F[K 194417/500000: episode: 2618, duration: 1.552s, episode steps: 96, steps per second: 62, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.135 [0.000, 4.000], mean observation: 0.507 [0.460, 0.580], loss: 8.634212, mean_absolute_error: 40.598316, mean_q: 52.114857
[F[K 194488/500000: episode: 2619, duration: 1.109s, episode steps: 71, steps per second: 64, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.500 [0.440, 0.540], loss: 7.924765, mean_absolute_error: 39.596210, mean_q: 50.874847
[F[K 194550/500000: episode: 2620, duration: 1.056s, episode steps: 62, steps per second: 59, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.710 [0.000, 4.000], mean observation: 0.492 [0.450, 0.530], loss: 7.251305, mean_absolute_error: 40.109226, mean_q: 51.529739
[F[K 194632/500000: episode: 2621, duration: 1.355s, episode steps: 82, steps per second: 61, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.476 [0.000, 4.000], mean observation: 0.491 [0.420, 0.520], loss: 7.765612, mean_absolute_error: 40.583885, mean_q: 52.154011
[F[K 194703/500000: episode: 2622, duration: 1.206s, episode steps: 71, steps per second: 59, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.690 [0.000, 4.000], mean observation: 0.510 [0.490, 0.550], loss: 7.824516, mean_absolute_error: 39.864323, mean_q: 51.189709
[F[K 194791/500000: episode: 2623, duration: 1.382s, episode steps: 88, steps per second: 64, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.409 [0.000, 4.000], mean observation: 0.480 [0.400, 0.520], loss: 8.011060, mean_absolute_error: 40.981400, mean_q: 52.585899
[F[K 194883/500000: episode: 2624, duration: 1.537s, episode steps: 92, steps per second: 60, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.967 [0.000, 4.000], mean observation: 0.513 [0.460, 0.620], loss: 7.571416, mean_absolute_error: 40.793270, mean_q: 52.289623
[F[K 194949/500000: episode: 2625, duration: 1.025s, episode steps: 66, steps per second: 64, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.682 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 7.900023, mean_absolute_error: 40.685200, mean_q: 52.056183
[F[K 195024/500000: episode: 2626, duration: 1.203s, episode steps: 75, steps per second: 62, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.733 [0.000, 4.000], mean observation: 0.497 [0.470, 0.510], loss: 7.056850, mean_absolute_error: 40.572769, mean_q: 52.111588
[F[K 195079/500000: episode: 2627, duration: 0.962s, episode steps: 55, steps per second: 57, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.873 [0.000, 4.000], mean observation: 0.510 [0.420, 0.640], loss: 7.378511, mean_absolute_error: 40.745449, mean_q: 52.400852
[F[K 195148/500000: episode: 2628, duration: 1.173s, episode steps: 69, steps per second: 59, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.348 [0.000, 4.000], mean observation: 0.502 [0.370, 0.650], loss: 7.700740, mean_absolute_error: 40.838760, mean_q: 52.326168
[F[K 195218/500000: episode: 2629, duration: 1.033s, episode steps: 70, steps per second: 68, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.516 [0.470, 0.630], loss: 7.164582, mean_absolute_error: 40.741638, mean_q: 52.354980
[F[K 195344/500000: episode: 2630, duration: 2.267s, episode steps: 126, steps per second: 56, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.325 [0.000, 4.000], mean observation: 0.498 [0.420, 0.550], loss: 7.956064, mean_absolute_error: 40.608704, mean_q: 52.040836
[F[K 195427/500000: episode: 2631, duration: 1.392s, episode steps: 83, steps per second: 60, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.509 [0.470, 0.590], loss: 7.469885, mean_absolute_error: 40.716518, mean_q: 52.332794
[F[K 195506/500000: episode: 2632, duration: 1.196s, episode steps: 79, steps per second: 66, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.848 [0.000, 4.000], mean observation: 0.482 [0.370, 0.510], loss: 7.666755, mean_absolute_error: 40.612106, mean_q: 52.128361
[F[K 195548/500000: episode: 2633, duration: 0.582s, episode steps: 42, steps per second: 72, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.738 [0.000, 4.000], mean observation: 0.508 [0.420, 0.650], loss: 6.702211, mean_absolute_error: 40.764874, mean_q: 52.394333
[F[K 195599/500000: episode: 2634, duration: 0.732s, episode steps: 51, steps per second: 70, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.275 [0.000, 4.000], mean observation: 0.507 [0.450, 0.610], loss: 7.206111, mean_absolute_error: 41.154816, mean_q: 52.853870
[F[K 195755/500000: episode: 2635, duration: 1.940s, episode steps: 156, steps per second: 80, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.801 [0.000, 4.000], mean observation: 0.488 [0.380, 0.550], loss: 8.063417, mean_absolute_error: 40.460075, mean_q: 51.914024
[F[K 195802/500000: episode: 2636, duration: 0.684s, episode steps: 47, steps per second: 69, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.957 [0.000, 4.000], mean observation: 0.526 [0.470, 0.630], loss: 9.887686, mean_absolute_error: 40.586903, mean_q: 52.053780
[F[K 195869/500000: episode: 2637, duration: 0.819s, episode steps: 67, steps per second: 82, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.910 [0.000, 4.000], mean observation: 0.486 [0.420, 0.510], loss: 7.687281, mean_absolute_error: 40.900711, mean_q: 52.517250
[F[K 195935/500000: episode: 2638, duration: 0.914s, episode steps: 66, steps per second: 72, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.591 [0.000, 4.000], mean observation: 0.483 [0.380, 0.530], loss: 8.821559, mean_absolute_error: 40.455204, mean_q: 51.896309
[F[K 195984/500000: episode: 2639, duration: 0.625s, episode steps: 49, steps per second: 78, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.204 [0.000, 4.000], mean observation: 0.493 [0.420, 0.530], loss: 7.264388, mean_absolute_error: 41.392128, mean_q: 53.040848
[F[K 196045/500000: episode: 2640, duration: 0.835s, episode steps: 61, steps per second: 73, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.498 [0.430, 0.540], loss: 7.589027, mean_absolute_error: 41.304783, mean_q: 52.973072
[F[K 196130/500000: episode: 2641, duration: 1.239s, episode steps: 85, steps per second: 69, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 0.521 [0.470, 0.660], loss: 10.621700, mean_absolute_error: 40.899754, mean_q: 52.447330
[F[K 196237/500000: episode: 2642, duration: 1.339s, episode steps: 107, steps per second: 80, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.925 [0.000, 4.000], mean observation: 0.512 [0.480, 0.600], loss: 7.894550, mean_absolute_error: 40.243557, mean_q: 51.744217
[F[K 196283/500000: episode: 2643, duration: 0.620s, episode steps: 46, steps per second: 74, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.957 [0.000, 4.000], mean observation: 0.498 [0.360, 0.620], loss: 8.120115, mean_absolute_error: 41.191307, mean_q: 52.831627
[F[K 196351/500000: episode: 2644, duration: 0.983s, episode steps: 68, steps per second: 69, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.853 [0.000, 4.000], mean observation: 0.491 [0.400, 0.530], loss: 9.173783, mean_absolute_error: 40.421986, mean_q: 51.868660
[F[K 196422/500000: episode: 2645, duration: 1.042s, episode steps: 71, steps per second: 68, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.775 [0.000, 4.000], mean observation: 0.480 [0.380, 0.520], loss: 8.022903, mean_absolute_error: 40.900303, mean_q: 52.448959
[F[K 196545/500000: episode: 2646, duration: 1.704s, episode steps: 123, steps per second: 72, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.740 [0.000, 4.000], mean observation: 0.493 [0.430, 0.540], loss: 8.279489, mean_absolute_error: 41.225399, mean_q: 52.885426
[F[K 196582/500000: episode: 2647, duration: 0.491s, episode steps: 37, steps per second: 75, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.270 [0.000, 4.000], mean observation: 0.499 [0.350, 0.640], loss: 6.952124, mean_absolute_error: 40.537827, mean_q: 51.912075
[F[K 196631/500000: episode: 2648, duration: 0.650s, episode steps: 49, steps per second: 75, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 4.000], mean observation: 0.489 [0.400, 0.530], loss: 7.805792, mean_absolute_error: 40.796619, mean_q: 52.425499
[F[K 196806/500000: episode: 2649, duration: 2.392s, episode steps: 175, steps per second: 73, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.063 [0.000, 4.000], mean observation: 0.502 [0.450, 0.600], loss: 8.787428, mean_absolute_error: 40.405670, mean_q: 51.853928
[F[K 196852/500000: episode: 2650, duration: 0.563s, episode steps: 46, steps per second: 82, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.478 [0.000, 4.000], mean observation: 0.500 [0.370, 0.630], loss: 7.694335, mean_absolute_error: 40.498039, mean_q: 51.932808
[F[K 196904/500000: episode: 2651, duration: 0.795s, episode steps: 52, steps per second: 65, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.327 [0.000, 4.000], mean observation: 0.522 [0.470, 0.620], loss: 8.002076, mean_absolute_error: 40.910526, mean_q: 52.477112
[F[K 196966/500000: episode: 2652, duration: 0.979s, episode steps: 62, steps per second: 63, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.532 [0.000, 4.000], mean observation: 0.495 [0.450, 0.530], loss: 7.185685, mean_absolute_error: 40.245445, mean_q: 51.724106
[F[K 197019/500000: episode: 2653, duration: 0.813s, episode steps: 53, steps per second: 65, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.505 [0.440, 0.610], loss: 8.169547, mean_absolute_error: 40.427822, mean_q: 51.855808
[F[K 197080/500000: episode: 2654, duration: 0.802s, episode steps: 61, steps per second: 76, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.557 [0.000, 4.000], mean observation: 0.493 [0.440, 0.520], loss: 7.365139, mean_absolute_error: 40.394840, mean_q: 51.858238
[F[K 197138/500000: episode: 2655, duration: 0.884s, episode steps: 58, steps per second: 66, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.509 [0.480, 0.570], loss: 9.037309, mean_absolute_error: 39.998833, mean_q: 51.402409
[F[K 197204/500000: episode: 2656, duration: 1.054s, episode steps: 66, steps per second: 63, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.818 [0.000, 4.000], mean observation: 0.511 [0.470, 0.590], loss: 8.071205, mean_absolute_error: 40.343761, mean_q: 51.695625
[F[K 197266/500000: episode: 2657, duration: 1.017s, episode steps: 62, steps per second: 61, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.839 [0.000, 4.000], mean observation: 0.477 [0.400, 0.510], loss: 8.187697, mean_absolute_error: 40.807640, mean_q: 52.346455
[F[K 197353/500000: episode: 2658, duration: 1.294s, episode steps: 87, steps per second: 67, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.897 [0.000, 4.000], mean observation: 0.488 [0.370, 0.520], loss: 8.843547, mean_absolute_error: 40.364597, mean_q: 51.751488
[F[K 197425/500000: episode: 2659, duration: 1.021s, episode steps: 72, steps per second: 70, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.792 [0.000, 4.000], mean observation: 0.514 [0.470, 0.660], loss: 7.915103, mean_absolute_error: 40.311020, mean_q: 51.698601
[F[K 197477/500000: episode: 2660, duration: 0.653s, episode steps: 52, steps per second: 80, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.491 [0.420, 0.530], loss: 7.476789, mean_absolute_error: 41.786381, mean_q: 53.588181
[F[K 197569/500000: episode: 2661, duration: 1.448s, episode steps: 92, steps per second: 64, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.728 [0.000, 4.000], mean observation: 0.490 [0.400, 0.530], loss: 7.681668, mean_absolute_error: 41.138008, mean_q: 52.677460
[F[K 197638/500000: episode: 2662, duration: 0.778s, episode steps: 69, steps per second: 89, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.101 [0.000, 4.000], mean observation: 0.487 [0.410, 0.530], loss: 7.940720, mean_absolute_error: 39.841583, mean_q: 51.169975
[F[K 197703/500000: episode: 2663, duration: 0.955s, episode steps: 65, steps per second: 68, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.769 [0.000, 4.000], mean observation: 0.505 [0.430, 0.590], loss: 8.254317, mean_absolute_error: 40.972408, mean_q: 52.611267
[F[K 197758/500000: episode: 2664, duration: 0.887s, episode steps: 55, steps per second: 62, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.500 [0.360, 0.620], loss: 9.750993, mean_absolute_error: 39.912380, mean_q: 51.218239
[F[K 197803/500000: episode: 2665, duration: 0.691s, episode steps: 45, steps per second: 65, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.556 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 7.914429, mean_absolute_error: 41.181889, mean_q: 52.803711
[F[K 197882/500000: episode: 2666, duration: 1.176s, episode steps: 79, steps per second: 67, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.051 [0.000, 4.000], mean observation: 0.507 [0.470, 0.600], loss: 8.324207, mean_absolute_error: 40.679604, mean_q: 52.132919
[F[K 198002/500000: episode: 2667, duration: 1.733s, episode steps: 120, steps per second: 69, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.117 [0.000, 4.000], mean observation: 0.506 [0.480, 0.560], loss: 8.400131, mean_absolute_error: 40.502987, mean_q: 51.946648
[F[K 198077/500000: episode: 2668, duration: 1.192s, episode steps: 75, steps per second: 63, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.573 [0.000, 4.000], mean observation: 0.506 [0.470, 0.560], loss: 8.733305, mean_absolute_error: 40.191528, mean_q: 51.573654
[F[K 198246/500000: episode: 2669, duration: 2.163s, episode steps: 169, steps per second: 78, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.284 [0.000, 4.000], mean observation: 0.483 [0.370, 0.530], loss: 9.008501, mean_absolute_error: 40.923618, mean_q: 52.394081
[F[K 198296/500000: episode: 2670, duration: 0.732s, episode steps: 50, steps per second: 68, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.140 [0.000, 4.000], mean observation: 0.523 [0.480, 0.650], loss: 7.103032, mean_absolute_error: 40.430042, mean_q: 51.769665
[F[K 198478/500000: episode: 2671, duration: 2.870s, episode steps: 182, steps per second: 63, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.121 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 8.520464, mean_absolute_error: 40.866432, mean_q: 52.413918
[F[K 198566/500000: episode: 2672, duration: 1.517s, episode steps: 88, steps per second: 58, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.491 [0.430, 0.530], loss: 8.568408, mean_absolute_error: 40.829185, mean_q: 52.356644
[F[K 198634/500000: episode: 2673, duration: 1.037s, episode steps: 68, steps per second: 66, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.544 [0.000, 4.000], mean observation: 0.483 [0.370, 0.530], loss: 9.329909, mean_absolute_error: 40.843517, mean_q: 52.341385
[F[K 198707/500000: episode: 2674, duration: 1.253s, episode steps: 73, steps per second: 58, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.795 [0.000, 4.000], mean observation: 0.503 [0.470, 0.560], loss: 8.313129, mean_absolute_error: 41.021358, mean_q: 52.610527
[F[K 198789/500000: episode: 2675, duration: 1.016s, episode steps: 82, steps per second: 81, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.110 [0.000, 4.000], mean observation: 0.489 [0.350, 0.560], loss: 7.832405, mean_absolute_error: 40.754063, mean_q: 52.148544
[F[K 198829/500000: episode: 2676, duration: 0.626s, episode steps: 40, steps per second: 64, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.975 [0.000, 4.000], mean observation: 0.531 [0.490, 0.640], loss: 7.358895, mean_absolute_error: 40.981266, mean_q: 52.488232
[F[K 198891/500000: episode: 2677, duration: 0.913s, episode steps: 62, steps per second: 68, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.493 [0.380, 0.530], loss: 7.835549, mean_absolute_error: 39.703773, mean_q: 51.006603
[F[K 198992/500000: episode: 2678, duration: 1.438s, episode steps: 101, steps per second: 70, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.743 [0.000, 4.000], mean observation: 0.507 [0.450, 0.590], loss: 8.123014, mean_absolute_error: 40.187206, mean_q: 51.568218
[F[K 199026/500000: episode: 2679, duration: 0.601s, episode steps: 34, steps per second: 57, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.382 [0.000, 4.000], mean observation: 0.524 [0.470, 0.620], loss: 8.289217, mean_absolute_error: 40.735798, mean_q: 52.258118
[F[K 199115/500000: episode: 2680, duration: 1.553s, episode steps: 89, steps per second: 57, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.910 [0.000, 4.000], mean observation: 0.492 [0.360, 0.550], loss: 9.820147, mean_absolute_error: 40.468952, mean_q: 51.815681
[F[K 199188/500000: episode: 2681, duration: 1.212s, episode steps: 73, steps per second: 60, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.904 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 9.741949, mean_absolute_error: 40.283825, mean_q: 51.620140
[F[K 199265/500000: episode: 2682, duration: 1.364s, episode steps: 77, steps per second: 56, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.766 [0.000, 4.000], mean observation: 0.507 [0.480, 0.580], loss: 7.943861, mean_absolute_error: 40.159706, mean_q: 51.559597
[F[K 199343/500000: episode: 2683, duration: 1.248s, episode steps: 78, steps per second: 63, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.115 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 8.132373, mean_absolute_error: 40.492916, mean_q: 51.894955
[F[K 199414/500000: episode: 2684, duration: 1.001s, episode steps: 71, steps per second: 71, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.497 [0.360, 0.570], loss: 10.133302, mean_absolute_error: 40.632717, mean_q: 52.113876
[F[K 199474/500000: episode: 2685, duration: 1.010s, episode steps: 60, steps per second: 59, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.017 [0.000, 4.000], mean observation: 0.491 [0.400, 0.530], loss: 8.779864, mean_absolute_error: 40.330544, mean_q: 51.785088
[F[K 199540/500000: episode: 2686, duration: 1.149s, episode steps: 66, steps per second: 57, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.879 [0.000, 4.000], mean observation: 0.498 [0.450, 0.530], loss: 8.369644, mean_absolute_error: 40.512554, mean_q: 51.959904
[F[K 199620/500000: episode: 2687, duration: 1.438s, episode steps: 80, steps per second: 56, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.725 [0.000, 4.000], mean observation: 0.503 [0.430, 0.570], loss: 8.091092, mean_absolute_error: 40.021954, mean_q: 51.367931
[F[K 199664/500000: episode: 2688, duration: 0.775s, episode steps: 44, steps per second: 57, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.727 [0.000, 4.000], mean observation: 0.522 [0.470, 0.610], loss: 8.620581, mean_absolute_error: 40.080246, mean_q: 51.442261
[F[K 199718/500000: episode: 2689, duration: 0.978s, episode steps: 54, steps per second: 55, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.981 [0.000, 4.000], mean observation: 0.496 [0.360, 0.600], loss: 9.072430, mean_absolute_error: 40.139557, mean_q: 51.370796
[F[K 199781/500000: episode: 2690, duration: 1.049s, episode steps: 63, steps per second: 60, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.587 [0.000, 4.000], mean observation: 0.489 [0.380, 0.530], loss: 6.821573, mean_absolute_error: 40.505714, mean_q: 51.933548
[F[K 199863/500000: episode: 2691, duration: 1.117s, episode steps: 82, steps per second: 73, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.485 [0.430, 0.520], loss: 8.714380, mean_absolute_error: 40.836407, mean_q: 52.445343
[F[K 199949/500000: episode: 2692, duration: 1.262s, episode steps: 86, steps per second: 68, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.499 [0.410, 0.580], loss: 8.424994, mean_absolute_error: 40.579334, mean_q: 52.026917
[F[K 200017/500000: episode: 2693, duration: 0.918s, episode steps: 68, steps per second: 74, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.412 [0.000, 4.000], mean observation: 0.488 [0.360, 0.530], loss: 8.313372, mean_absolute_error: 40.179390, mean_q: 51.625324
[F[K 200064/500000: episode: 2694, duration: 0.669s, episode steps: 47, steps per second: 70, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.520 [0.470, 0.610], loss: 8.089550, mean_absolute_error: 39.892292, mean_q: 51.189995
[F[K 200145/500000: episode: 2695, duration: 1.065s, episode steps: 81, steps per second: 76, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.481 [0.000, 4.000], mean observation: 0.488 [0.420, 0.520], loss: 8.661127, mean_absolute_error: 40.213219, mean_q: 51.573700
[F[K 200301/500000: episode: 2696, duration: 2.320s, episode steps: 156, steps per second: 67, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.987 [0.000, 4.000], mean observation: 0.499 [0.380, 0.590], loss: 9.205834, mean_absolute_error: 40.192627, mean_q: 51.533222
[F[K 200341/500000: episode: 2697, duration: 0.607s, episode steps: 40, steps per second: 66, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.925 [0.000, 4.000], mean observation: 0.530 [0.470, 0.650], loss: 7.533096, mean_absolute_error: 40.175789, mean_q: 51.627281
[F[K 200420/500000: episode: 2698, duration: 1.293s, episode steps: 79, steps per second: 61, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.937 [0.000, 4.000], mean observation: 0.495 [0.380, 0.570], loss: 7.727148, mean_absolute_error: 39.797474, mean_q: 51.105026
[F[K 200509/500000: episode: 2699, duration: 1.330s, episode steps: 89, steps per second: 67, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.978 [0.000, 4.000], mean observation: 0.503 [0.410, 0.640], loss: 8.680188, mean_absolute_error: 39.917690, mean_q: 51.240349
[F[K 200581/500000: episode: 2700, duration: 1.184s, episode steps: 72, steps per second: 61, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.236 [0.000, 4.000], mean observation: 0.508 [0.470, 0.600], loss: 9.379537, mean_absolute_error: 39.665127, mean_q: 50.950771
[F[K 200651/500000: episode: 2701, duration: 1.030s, episode steps: 70, steps per second: 68, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.314 [0.000, 4.000], mean observation: 0.479 [0.360, 0.530], loss: 8.505815, mean_absolute_error: 40.690220, mean_q: 52.058216
[F[K 200725/500000: episode: 2702, duration: 1.067s, episode steps: 74, steps per second: 69, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.486 [0.420, 0.530], loss: 10.064604, mean_absolute_error: 39.980732, mean_q: 51.220333
[F[K 200768/500000: episode: 2703, duration: 0.686s, episode steps: 43, steps per second: 63, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.651 [0.000, 4.000], mean observation: 0.504 [0.410, 0.630], loss: 8.972919, mean_absolute_error: 39.703609, mean_q: 50.978260
[F[K 200825/500000: episode: 2704, duration: 0.860s, episode steps: 57, steps per second: 66, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 0.508 [0.480, 0.560], loss: 8.424854, mean_absolute_error: 40.494064, mean_q: 51.953712
[F[K 200973/500000: episode: 2705, duration: 2.399s, episode steps: 148, steps per second: 62, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.365 [0.000, 4.000], mean observation: 0.506 [0.460, 0.620], loss: 8.769992, mean_absolute_error: 40.319153, mean_q: 51.671131
[F[K 201065/500000: episode: 2706, duration: 1.485s, episode steps: 92, steps per second: 62, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 0.506 [0.480, 0.530], loss: 7.861441, mean_absolute_error: 40.401398, mean_q: 51.768410
[F[K 201140/500000: episode: 2707, duration: 1.215s, episode steps: 75, steps per second: 62, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.867 [0.000, 4.000], mean observation: 0.503 [0.450, 0.560], loss: 7.578740, mean_absolute_error: 40.462902, mean_q: 51.951885
[F[K 201216/500000: episode: 2708, duration: 1.003s, episode steps: 76, steps per second: 76, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.474 [0.000, 4.000], mean observation: 0.516 [0.490, 0.620], loss: 8.292229, mean_absolute_error: 39.903992, mean_q: 51.256092
[F[K 201268/500000: episode: 2709, duration: 0.837s, episode steps: 52, steps per second: 62, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.615 [0.000, 4.000], mean observation: 0.500 [0.390, 0.620], loss: 6.438430, mean_absolute_error: 40.046482, mean_q: 51.488712
[F[K 201359/500000: episode: 2710, duration: 1.303s, episode steps: 91, steps per second: 70, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.099 [0.000, 4.000], mean observation: 0.469 [0.330, 0.520], loss: 7.399015, mean_absolute_error: 39.821365, mean_q: 51.182503
[F[K 201432/500000: episode: 2711, duration: 1.159s, episode steps: 73, steps per second: 63, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.342 [0.000, 4.000], mean observation: 0.502 [0.460, 0.540], loss: 7.936270, mean_absolute_error: 40.492989, mean_q: 51.870888
[F[K 201537/500000: episode: 2712, duration: 1.765s, episode steps: 105, steps per second: 59, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.371 [0.000, 4.000], mean observation: 0.497 [0.420, 0.560], loss: 7.893773, mean_absolute_error: 40.059116, mean_q: 51.347698
[F[K 201613/500000: episode: 2713, duration: 1.058s, episode steps: 76, steps per second: 72, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.524 [0.490, 0.630], loss: 8.754273, mean_absolute_error: 40.061886, mean_q: 51.301952
[F[K 201697/500000: episode: 2714, duration: 1.371s, episode steps: 84, steps per second: 61, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.496 [0.460, 0.530], loss: 7.199799, mean_absolute_error: 40.651829, mean_q: 52.169159
[F[K 201762/500000: episode: 2715, duration: 1.010s, episode steps: 65, steps per second: 64, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.338 [0.000, 4.000], mean observation: 0.513 [0.470, 0.640], loss: 7.980542, mean_absolute_error: 40.632908, mean_q: 52.179176
[F[K 201812/500000: episode: 2716, duration: 0.843s, episode steps: 50, steps per second: 59, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.400 [0.000, 4.000], mean observation: 0.528 [0.470, 0.650], loss: 7.470125, mean_absolute_error: 40.858032, mean_q: 52.498402
[F[K 201876/500000: episode: 2717, duration: 1.016s, episode steps: 64, steps per second: 63, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.766 [0.000, 4.000], mean observation: 0.489 [0.360, 0.560], loss: 8.123901, mean_absolute_error: 40.609436, mean_q: 52.062500
[F[K 201941/500000: episode: 2718, duration: 0.870s, episode steps: 65, steps per second: 75, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.077 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 7.602335, mean_absolute_error: 40.989449, mean_q: 52.621063
[F[K 201998/500000: episode: 2719, duration: 0.944s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.579 [0.000, 4.000], mean observation: 0.514 [0.490, 0.620], loss: 9.707991, mean_absolute_error: 39.975426, mean_q: 51.271114
[F[K 202052/500000: episode: 2720, duration: 0.913s, episode steps: 54, steps per second: 59, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.685 [0.000, 4.000], mean observation: 0.502 [0.460, 0.570], loss: 9.356203, mean_absolute_error: 39.973721, mean_q: 51.357330
[F[K 202140/500000: episode: 2721, duration: 1.256s, episode steps: 88, steps per second: 70, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.148 [0.000, 4.000], mean observation: 0.508 [0.430, 0.660], loss: 8.510560, mean_absolute_error: 40.310780, mean_q: 51.599178
[F[K 202224/500000: episode: 2722, duration: 1.154s, episode steps: 84, steps per second: 73, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.179 [0.000, 4.000], mean observation: 0.499 [0.440, 0.540], loss: 8.178523, mean_absolute_error: 39.992371, mean_q: 51.366722
[F[K 202407/500000: episode: 2723, duration: 3.046s, episode steps: 183, steps per second: 60, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 0.486 [0.350, 0.520], loss: 8.262259, mean_absolute_error: 40.066452, mean_q: 51.460907
[F[K 202499/500000: episode: 2724, duration: 1.518s, episode steps: 92, steps per second: 61, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.565 [0.000, 4.000], mean observation: 0.489 [0.430, 0.530], loss: 7.891318, mean_absolute_error: 40.010143, mean_q: 51.317066
[F[K 202543/500000: episode: 2725, duration: 0.706s, episode steps: 44, steps per second: 62, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.636 [0.000, 4.000], mean observation: 0.500 [0.390, 0.620], loss: 9.812376, mean_absolute_error: 39.177822, mean_q: 50.278931
[F[K 202663/500000: episode: 2726, duration: 2.070s, episode steps: 120, steps per second: 58, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.108 [0.000, 4.000], mean observation: 0.489 [0.340, 0.540], loss: 9.547191, mean_absolute_error: 40.340778, mean_q: 51.726070
[F[K 202705/500000: episode: 2727, duration: 0.719s, episode steps: 42, steps per second: 58, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.512 [0.470, 0.620], loss: 6.182601, mean_absolute_error: 40.096733, mean_q: 51.564423
[F[K 202772/500000: episode: 2728, duration: 1.257s, episode steps: 67, steps per second: 53, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.149 [0.000, 4.000], mean observation: 0.496 [0.450, 0.530], loss: 7.664090, mean_absolute_error: 40.465576, mean_q: 52.018353
[F[K 202832/500000: episode: 2729, duration: 1.139s, episode steps: 60, steps per second: 53, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.450 [0.000, 4.000], mean observation: 0.489 [0.430, 0.530], loss: 7.966468, mean_absolute_error: 40.059223, mean_q: 51.311584
[F[K 202938/500000: episode: 2730, duration: 1.923s, episode steps: 106, steps per second: 55, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.575 [0.000, 4.000], mean observation: 0.480 [0.360, 0.530], loss: 7.712425, mean_absolute_error: 40.255066, mean_q: 51.668671
[F[K 203024/500000: episode: 2731, duration: 1.591s, episode steps: 86, steps per second: 54, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.895 [0.000, 4.000], mean observation: 0.489 [0.420, 0.530], loss: 7.882791, mean_absolute_error: 40.386387, mean_q: 51.807186
[F[K 203100/500000: episode: 2732, duration: 1.344s, episode steps: 76, steps per second: 57, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.503 [0.420, 0.590], loss: 8.624250, mean_absolute_error: 40.445770, mean_q: 51.804489
[F[K 203229/500000: episode: 2733, duration: 2.208s, episode steps: 129, steps per second: 58, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.047 [0.000, 4.000], mean observation: 0.475 [0.360, 0.530], loss: 8.211274, mean_absolute_error: 40.445152, mean_q: 51.900822
[F[K 203287/500000: episode: 2734, duration: 0.848s, episode steps: 58, steps per second: 68, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.259 [0.000, 4.000], mean observation: 0.512 [0.470, 0.580], loss: 10.025838, mean_absolute_error: 40.067825, mean_q: 51.237400
[F[K 203353/500000: episode: 2735, duration: 1.075s, episode steps: 66, steps per second: 61, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.273 [0.000, 4.000], mean observation: 0.512 [0.480, 0.570], loss: 9.728268, mean_absolute_error: 40.611828, mean_q: 52.067932
[F[K 203441/500000: episode: 2736, duration: 1.155s, episode steps: 88, steps per second: 76, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.659 [0.000, 4.000], mean observation: 0.509 [0.460, 0.560], loss: 8.276505, mean_absolute_error: 39.873215, mean_q: 51.186867
[F[K 203483/500000: episode: 2737, duration: 0.657s, episode steps: 42, steps per second: 64, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.517 [0.470, 0.610], loss: 8.857945, mean_absolute_error: 39.538166, mean_q: 50.656940
[F[K 203553/500000: episode: 2738, duration: 0.998s, episode steps: 70, steps per second: 70, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.504 [0.470, 0.530], loss: 8.749271, mean_absolute_error: 39.633785, mean_q: 50.916100
[F[K 203689/500000: episode: 2739, duration: 2.200s, episode steps: 136, steps per second: 62, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.265 [0.000, 4.000], mean observation: 0.504 [0.440, 0.640], loss: 7.969030, mean_absolute_error: 40.224586, mean_q: 51.595184
[F[K 203753/500000: episode: 2740, duration: 1.038s, episode steps: 64, steps per second: 62, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.513 [0.490, 0.570], loss: 8.367247, mean_absolute_error: 40.562901, mean_q: 52.061321
[F[K 203813/500000: episode: 2741, duration: 0.954s, episode steps: 60, steps per second: 63, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.150 [0.000, 4.000], mean observation: 0.483 [0.370, 0.530], loss: 8.746326, mean_absolute_error: 40.350533, mean_q: 51.661858
[F[K 203909/500000: episode: 2742, duration: 1.405s, episode steps: 96, steps per second: 68, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 0.495 [0.380, 0.570], loss: 8.441176, mean_absolute_error: 40.341328, mean_q: 51.771076
[F[K 203957/500000: episode: 2743, duration: 0.773s, episode steps: 48, steps per second: 62, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.504 [0.390, 0.650], loss: 7.089973, mean_absolute_error: 41.214634, mean_q: 52.821960
[F[K 203996/500000: episode: 2744, duration: 0.544s, episode steps: 39, steps per second: 72, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.462 [0.000, 4.000], mean observation: 0.516 [0.470, 0.630], loss: 8.053428, mean_absolute_error: 40.161728, mean_q: 51.347507
[F[K 204068/500000: episode: 2745, duration: 0.867s, episode steps: 72, steps per second: 83, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.194 [0.000, 4.000], mean observation: 0.506 [0.470, 0.540], loss: 9.109449, mean_absolute_error: 40.547153, mean_q: 51.889591
[F[K 204125/500000: episode: 2746, duration: 0.698s, episode steps: 57, steps per second: 82, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.522 [0.500, 0.620], loss: 8.858902, mean_absolute_error: 40.210133, mean_q: 51.542038
[F[K 204157/500000: episode: 2747, duration: 0.467s, episode steps: 32, steps per second: 68, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.094 [0.000, 3.000], mean observation: 0.515 [0.470, 0.650], loss: 7.398574, mean_absolute_error: 40.663452, mean_q: 52.051403
[F[K 204262/500000: episode: 2748, duration: 1.368s, episode steps: 105, steps per second: 77, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.479 [0.380, 0.530], loss: 9.112675, mean_absolute_error: 40.266987, mean_q: 51.701527
[F[K 204332/500000: episode: 2749, duration: 0.934s, episode steps: 70, steps per second: 75, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.514 [0.000, 4.000], mean observation: 0.515 [0.490, 0.600], loss: 8.313365, mean_absolute_error: 40.282387, mean_q: 51.626110
[F[K 204398/500000: episode: 2750, duration: 0.859s, episode steps: 66, steps per second: 77, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.682 [0.000, 4.000], mean observation: 0.504 [0.440, 0.570], loss: 7.938903, mean_absolute_error: 39.842701, mean_q: 51.144264
[F[K 204455/500000: episode: 2751, duration: 0.579s, episode steps: 57, steps per second: 98, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.807 [0.000, 4.000], mean observation: 0.485 [0.390, 0.510], loss: 7.292906, mean_absolute_error: 40.531796, mean_q: 52.002193
[F[K 204532/500000: episode: 2752, duration: 1.112s, episode steps: 77, steps per second: 69, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.909 [0.000, 4.000], mean observation: 0.509 [0.470, 0.570], loss: 7.511253, mean_absolute_error: 40.574635, mean_q: 52.102642
[F[K 204614/500000: episode: 2753, duration: 1.210s, episode steps: 82, steps per second: 68, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.220 [0.000, 4.000], mean observation: 0.507 [0.470, 0.570], loss: 8.187417, mean_absolute_error: 40.671101, mean_q: 52.266518
[F[K 204666/500000: episode: 2754, duration: 0.672s, episode steps: 52, steps per second: 77, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.596 [0.000, 4.000], mean observation: 0.491 [0.360, 0.530], loss: 8.849994, mean_absolute_error: 40.281132, mean_q: 51.558830
[F[K 204734/500000: episode: 2755, duration: 0.974s, episode steps: 68, steps per second: 70, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.647 [0.000, 4.000], mean observation: 0.501 [0.480, 0.540], loss: 7.008038, mean_absolute_error: 39.948082, mean_q: 51.217506
[F[K 204784/500000: episode: 2756, duration: 0.752s, episode steps: 50, steps per second: 67, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 0.475 [0.360, 0.530], loss: 8.372846, mean_absolute_error: 40.997177, mean_q: 52.652576
[F[K 204880/500000: episode: 2757, duration: 1.461s, episode steps: 96, steps per second: 66, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.486 [0.350, 0.530], loss: 7.893644, mean_absolute_error: 41.011322, mean_q: 52.586132
[F[K 204939/500000: episode: 2758, duration: 0.820s, episode steps: 59, steps per second: 72, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.486 [0.360, 0.520], loss: 7.939270, mean_absolute_error: 40.401413, mean_q: 51.911903
[F[K 204984/500000: episode: 2759, duration: 0.705s, episode steps: 45, steps per second: 64, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.356 [0.000, 4.000], mean observation: 0.505 [0.400, 0.650], loss: 7.343528, mean_absolute_error: 40.981281, mean_q: 52.686394
[F[K 205076/500000: episode: 2760, duration: 1.441s, episode steps: 92, steps per second: 64, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.924 [0.000, 4.000], mean observation: 0.475 [0.350, 0.530], loss: 8.595261, mean_absolute_error: 40.376720, mean_q: 51.850056
[F[K 205152/500000: episode: 2761, duration: 1.171s, episode steps: 76, steps per second: 65, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.026 [0.000, 4.000], mean observation: 0.506 [0.470, 0.610], loss: 8.110838, mean_absolute_error: 40.380341, mean_q: 51.806530
[F[K 205226/500000: episode: 2762, duration: 1.093s, episode steps: 74, steps per second: 68, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.608 [0.000, 4.000], mean observation: 0.510 [0.440, 0.620], loss: 7.443377, mean_absolute_error: 41.034111, mean_q: 52.668163
[F[K 205288/500000: episode: 2763, duration: 1.013s, episode steps: 62, steps per second: 61, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.677 [0.000, 4.000], mean observation: 0.514 [0.480, 0.600], loss: 8.401573, mean_absolute_error: 40.337875, mean_q: 51.750839
[F[K 205350/500000: episode: 2764, duration: 0.956s, episode steps: 62, steps per second: 65, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.048 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 9.902109, mean_absolute_error: 40.844231, mean_q: 52.332798
[F[K 205435/500000: episode: 2765, duration: 1.274s, episode steps: 85, steps per second: 67, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.412 [0.000, 4.000], mean observation: 0.493 [0.390, 0.560], loss: 7.887726, mean_absolute_error: 41.052353, mean_q: 52.666801
[F[K 205512/500000: episode: 2766, duration: 1.130s, episode steps: 77, steps per second: 68, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.247 [0.000, 4.000], mean observation: 0.500 [0.470, 0.540], loss: 8.702567, mean_absolute_error: 41.079807, mean_q: 52.617970
[F[K 205589/500000: episode: 2767, duration: 1.206s, episode steps: 77, steps per second: 64, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.740 [0.000, 4.000], mean observation: 0.506 [0.450, 0.590], loss: 8.110734, mean_absolute_error: 40.516895, mean_q: 51.859901
[F[K 205701/500000: episode: 2768, duration: 1.817s, episode steps: 112, steps per second: 62, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.973 [0.000, 4.000], mean observation: 0.496 [0.370, 0.600], loss: 7.682588, mean_absolute_error: 40.015007, mean_q: 51.419579
[F[K 205762/500000: episode: 2769, duration: 0.986s, episode steps: 61, steps per second: 62, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.131 [0.000, 4.000], mean observation: 0.496 [0.390, 0.600], loss: 8.042764, mean_absolute_error: 41.408409, mean_q: 53.173077
[F[K 205917/500000: episode: 2770, duration: 2.090s, episode steps: 155, steps per second: 74, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.439 [0.000, 4.000], mean observation: 0.495 [0.380, 0.540], loss: 7.481361, mean_absolute_error: 40.983208, mean_q: 52.557030
[F[K 205964/500000: episode: 2771, duration: 0.605s, episode steps: 47, steps per second: 78, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.508 [0.420, 0.650], loss: 7.196822, mean_absolute_error: 41.788719, mean_q: 53.596321
[F[K 206011/500000: episode: 2772, duration: 0.603s, episode steps: 47, steps per second: 78, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.872 [0.000, 4.000], mean observation: 0.521 [0.470, 0.610], loss: 7.506781, mean_absolute_error: 40.572220, mean_q: 51.958126
[F[K 206084/500000: episode: 2773, duration: 1.038s, episode steps: 73, steps per second: 70, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.849 [0.000, 4.000], mean observation: 0.519 [0.480, 0.620], loss: 8.042852, mean_absolute_error: 41.073761, mean_q: 52.700161
[F[K 206144/500000: episode: 2774, duration: 0.956s, episode steps: 60, steps per second: 63, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.495 [0.420, 0.530], loss: 8.887259, mean_absolute_error: 40.933357, mean_q: 52.605228
[F[K 206195/500000: episode: 2775, duration: 0.896s, episode steps: 51, steps per second: 57, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.471 [0.000, 4.000], mean observation: 0.494 [0.370, 0.570], loss: 10.474782, mean_absolute_error: 41.015610, mean_q: 52.531780
[F[K 206303/500000: episode: 2776, duration: 1.816s, episode steps: 108, steps per second: 59, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.907 [0.000, 4.000], mean observation: 0.487 [0.380, 0.530], loss: 7.739459, mean_absolute_error: 40.271416, mean_q: 51.723076
[F[K 206403/500000: episode: 2777, duration: 1.193s, episode steps: 100, steps per second: 84, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.970 [0.000, 4.000], mean observation: 0.510 [0.460, 0.660], loss: 8.397296, mean_absolute_error: 40.906307, mean_q: 52.386608
[F[K 206533/500000: episode: 2778, duration: 1.766s, episode steps: 130, steps per second: 74, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.062 [0.000, 4.000], mean observation: 0.485 [0.330, 0.540], loss: 7.913210, mean_absolute_error: 40.853291, mean_q: 52.422646
[F[K 206604/500000: episode: 2779, duration: 1.168s, episode steps: 71, steps per second: 61, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.494 [0.380, 0.550], loss: 7.255382, mean_absolute_error: 41.444908, mean_q: 53.133030
[F[K 206683/500000: episode: 2780, duration: 1.020s, episode steps: 79, steps per second: 77, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.487 [0.410, 0.530], loss: 8.698077, mean_absolute_error: 40.582966, mean_q: 52.126144
[F[K 206766/500000: episode: 2781, duration: 1.294s, episode steps: 83, steps per second: 64, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.084 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 7.488555, mean_absolute_error: 40.880199, mean_q: 52.410885
[F[K 206831/500000: episode: 2782, duration: 1.087s, episode steps: 65, steps per second: 60, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.923 [0.000, 4.000], mean observation: 0.491 [0.430, 0.510], loss: 7.947342, mean_absolute_error: 40.853287, mean_q: 52.296082
[F[K 206904/500000: episode: 2783, duration: 1.132s, episode steps: 73, steps per second: 64, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.836 [0.000, 4.000], mean observation: 0.517 [0.470, 0.610], loss: 6.584751, mean_absolute_error: 41.584755, mean_q: 53.333691
[F[K 206950/500000: episode: 2784, duration: 0.624s, episode steps: 46, steps per second: 74, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.435 [0.000, 4.000], mean observation: 0.512 [0.470, 0.650], loss: 8.707578, mean_absolute_error: 40.320473, mean_q: 51.587158
[F[K 207011/500000: episode: 2785, duration: 0.908s, episode steps: 61, steps per second: 67, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.738 [0.000, 4.000], mean observation: 0.510 [0.470, 0.590], loss: 7.770981, mean_absolute_error: 40.413361, mean_q: 51.861397
[F[K 207077/500000: episode: 2786, duration: 0.980s, episode steps: 66, steps per second: 67, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.303 [0.000, 4.000], mean observation: 0.491 [0.410, 0.530], loss: 7.643749, mean_absolute_error: 40.905048, mean_q: 52.384739
[F[K 207147/500000: episode: 2787, duration: 1.133s, episode steps: 70, steps per second: 62, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.508 [0.470, 0.600], loss: 8.603602, mean_absolute_error: 39.847820, mean_q: 51.076740
[F[K 207210/500000: episode: 2788, duration: 0.936s, episode steps: 63, steps per second: 67, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.175 [0.000, 4.000], mean observation: 0.487 [0.410, 0.530], loss: 8.562651, mean_absolute_error: 40.625175, mean_q: 52.024815
[F[K 207285/500000: episode: 2789, duration: 1.254s, episode steps: 75, steps per second: 60, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.973 [0.000, 4.000], mean observation: 0.502 [0.470, 0.550], loss: 8.553896, mean_absolute_error: 40.202847, mean_q: 51.539268
[F[K 207356/500000: episode: 2790, duration: 0.949s, episode steps: 71, steps per second: 75, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.505 [0.450, 0.600], loss: 6.666362, mean_absolute_error: 41.554649, mean_q: 53.364910
[F[K 207518/500000: episode: 2791, duration: 2.587s, episode steps: 162, steps per second: 63, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.049 [0.000, 4.000], mean observation: 0.504 [0.480, 0.530], loss: 9.068295, mean_absolute_error: 40.879429, mean_q: 52.315117
[F[K 207579/500000: episode: 2792, duration: 0.960s, episode steps: 61, steps per second: 64, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.738 [0.000, 4.000], mean observation: 0.497 [0.430, 0.530], loss: 8.392338, mean_absolute_error: 41.441940, mean_q: 53.158684
[F[K 207671/500000: episode: 2793, duration: 1.423s, episode steps: 92, steps per second: 65, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.496 [0.440, 0.530], loss: 7.899408, mean_absolute_error: 41.850189, mean_q: 53.595909
[F[K 207716/500000: episode: 2794, duration: 0.665s, episode steps: 45, steps per second: 68, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.489 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 10.269741, mean_absolute_error: 41.102013, mean_q: 52.582497
[F[K 207804/500000: episode: 2795, duration: 1.308s, episode steps: 88, steps per second: 67, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.909 [0.000, 4.000], mean observation: 0.485 [0.410, 0.520], loss: 7.565910, mean_absolute_error: 41.166286, mean_q: 52.841846
[F[K 207855/500000: episode: 2796, duration: 0.817s, episode steps: 51, steps per second: 62, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.314 [0.000, 4.000], mean observation: 0.508 [0.470, 0.580], loss: 8.326621, mean_absolute_error: 40.451374, mean_q: 51.881351
[F[K 207917/500000: episode: 2797, duration: 1.010s, episode steps: 62, steps per second: 61, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.290 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 8.471015, mean_absolute_error: 40.855209, mean_q: 52.235756
[F[K 208000/500000: episode: 2798, duration: 1.278s, episode steps: 83, steps per second: 65, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.386 [0.000, 4.000], mean observation: 0.481 [0.340, 0.530], loss: 8.210373, mean_absolute_error: 40.717918, mean_q: 52.260361
[F[K 208067/500000: episode: 2799, duration: 1.011s, episode steps: 67, steps per second: 66, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.284 [0.000, 4.000], mean observation: 0.520 [0.480, 0.630], loss: 7.502627, mean_absolute_error: 41.327599, mean_q: 52.938606
[F[K 208112/500000: episode: 2800, duration: 0.731s, episode steps: 45, steps per second: 62, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.178 [0.000, 4.000], mean observation: 0.512 [0.470, 0.630], loss: 8.418085, mean_absolute_error: 42.010086, mean_q: 53.786053
[F[K 208180/500000: episode: 2801, duration: 0.983s, episode steps: 68, steps per second: 69, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.574 [0.000, 4.000], mean observation: 0.499 [0.400, 0.590], loss: 6.901854, mean_absolute_error: 40.556377, mean_q: 51.992401
[F[K 208340/500000: episode: 2802, duration: 2.741s, episode steps: 160, steps per second: 58, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.497 [0.440, 0.550], loss: 8.684456, mean_absolute_error: 40.612076, mean_q: 52.028370
[F[K 208385/500000: episode: 2803, duration: 0.866s, episode steps: 45, steps per second: 52, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.289 [0.000, 4.000], mean observation: 0.485 [0.350, 0.530], loss: 8.883509, mean_absolute_error: 39.903889, mean_q: 51.251076
[F[K 208423/500000: episode: 2804, duration: 0.592s, episode steps: 38, steps per second: 64, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.684 [0.000, 4.000], mean observation: 0.530 [0.480, 0.650], loss: 7.890496, mean_absolute_error: 40.157940, mean_q: 51.590603
[F[K 208485/500000: episode: 2805, duration: 0.956s, episode steps: 62, steps per second: 65, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.065 [0.000, 4.000], mean observation: 0.494 [0.470, 0.510], loss: 6.089850, mean_absolute_error: 40.807022, mean_q: 52.394310
[F[K 208588/500000: episode: 2806, duration: 1.513s, episode steps: 103, steps per second: 68, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.301 [0.000, 4.000], mean observation: 0.486 [0.400, 0.530], loss: 8.796632, mean_absolute_error: 40.485294, mean_q: 51.902615
[F[K 208669/500000: episode: 2807, duration: 1.151s, episode steps: 81, steps per second: 70, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 0.480 [0.380, 0.520], loss: 8.977242, mean_absolute_error: 41.181618, mean_q: 52.784210
[F[K 208734/500000: episode: 2808, duration: 0.978s, episode steps: 65, steps per second: 66, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.846 [0.000, 4.000], mean observation: 0.519 [0.470, 0.650], loss: 9.702309, mean_absolute_error: 40.100723, mean_q: 51.245308
[F[K 208874/500000: episode: 2809, duration: 1.990s, episode steps: 140, steps per second: 70, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.479 [0.370, 0.530], loss: 8.406721, mean_absolute_error: 40.536926, mean_q: 52.012390
[F[K 209059/500000: episode: 2810, duration: 2.735s, episode steps: 185, steps per second: 68, episode reward: 185.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.232 [0.000, 4.000], mean observation: 0.484 [0.310, 0.530], loss: 8.604921, mean_absolute_error: 40.949051, mean_q: 52.514938
[F[K 209109/500000: episode: 2811, duration: 0.648s, episode steps: 50, steps per second: 77, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.040 [0.000, 4.000], mean observation: 0.502 [0.470, 0.580], loss: 8.782655, mean_absolute_error: 40.010269, mean_q: 51.215996
[F[K 209219/500000: episode: 2812, duration: 1.595s, episode steps: 110, steps per second: 69, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.836 [0.000, 4.000], mean observation: 0.506 [0.430, 0.620], loss: 8.907513, mean_absolute_error: 40.613720, mean_q: 52.048492
[F[K 209259/500000: episode: 2813, duration: 0.552s, episode steps: 40, steps per second: 72, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.510 [0.470, 0.630], loss: 8.481206, mean_absolute_error: 41.076736, mean_q: 52.830452
[F[K 209322/500000: episode: 2814, duration: 0.926s, episode steps: 63, steps per second: 68, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 0.502 [0.400, 0.650], loss: 8.407078, mean_absolute_error: 40.470146, mean_q: 51.930103
[F[K 209390/500000: episode: 2815, duration: 0.894s, episode steps: 68, steps per second: 76, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.676 [0.000, 4.000], mean observation: 0.510 [0.480, 0.590], loss: 8.851787, mean_absolute_error: 40.363689, mean_q: 51.813011
[F[K 209469/500000: episode: 2816, duration: 1.084s, episode steps: 79, steps per second: 73, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.076 [0.000, 4.000], mean observation: 0.493 [0.350, 0.560], loss: 8.446195, mean_absolute_error: 40.186516, mean_q: 51.700195
[F[K 209536/500000: episode: 2817, duration: 0.857s, episode steps: 67, steps per second: 78, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.612 [0.000, 4.000], mean observation: 0.512 [0.490, 0.560], loss: 7.341456, mean_absolute_error: 41.282600, mean_q: 52.983601
[F[K 209637/500000: episode: 2818, duration: 1.436s, episode steps: 101, steps per second: 70, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.782 [0.000, 4.000], mean observation: 0.497 [0.450, 0.540], loss: 8.526433, mean_absolute_error: 41.124786, mean_q: 52.775143
[F[K 209747/500000: episode: 2819, duration: 1.596s, episode steps: 110, steps per second: 69, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.485 [0.380, 0.530], loss: 7.753643, mean_absolute_error: 41.307487, mean_q: 53.009079
[F[K 209863/500000: episode: 2820, duration: 1.634s, episode steps: 116, steps per second: 71, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.112 [0.000, 4.000], mean observation: 0.490 [0.390, 0.530], loss: 8.694807, mean_absolute_error: 40.423985, mean_q: 51.853241
[F[K 209927/500000: episode: 2821, duration: 0.990s, episode steps: 64, steps per second: 65, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.469 [0.000, 4.000], mean observation: 0.510 [0.470, 0.570], loss: 8.162543, mean_absolute_error: 40.974503, mean_q: 52.556095
[F[K 209992/500000: episode: 2822, duration: 1.029s, episode steps: 65, steps per second: 63, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.500 [0.390, 0.620], loss: 7.652413, mean_absolute_error: 40.271435, mean_q: 51.729034
[F[K 210047/500000: episode: 2823, duration: 0.710s, episode steps: 55, steps per second: 77, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.273 [0.000, 4.000], mean observation: 0.501 [0.390, 0.600], loss: 7.457288, mean_absolute_error: 40.839344, mean_q: 52.443493
[F[K 210151/500000: episode: 2824, duration: 1.490s, episode steps: 104, steps per second: 70, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.423 [0.000, 4.000], mean observation: 0.491 [0.330, 0.580], loss: 8.095222, mean_absolute_error: 40.785896, mean_q: 52.299519
[F[K 210314/500000: episode: 2825, duration: 2.415s, episode steps: 163, steps per second: 67, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.098 [0.000, 4.000], mean observation: 0.489 [0.430, 0.530], loss: 7.795227, mean_absolute_error: 41.087135, mean_q: 52.703850
[F[K 210379/500000: episode: 2826, duration: 0.852s, episode steps: 65, steps per second: 76, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.569 [0.000, 4.000], mean observation: 0.504 [0.410, 0.600], loss: 8.491899, mean_absolute_error: 41.184563, mean_q: 52.834274
[F[K 210458/500000: episode: 2827, duration: 1.059s, episode steps: 79, steps per second: 75, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.886 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 8.668823, mean_absolute_error: 40.975426, mean_q: 52.638969
[F[K 210554/500000: episode: 2828, duration: 1.392s, episode steps: 96, steps per second: 69, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.792 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 7.834406, mean_absolute_error: 40.354839, mean_q: 51.742779
[F[K 210627/500000: episode: 2829, duration: 1.167s, episode steps: 73, steps per second: 63, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.575 [0.000, 4.000], mean observation: 0.510 [0.480, 0.620], loss: 8.359422, mean_absolute_error: 41.237534, mean_q: 52.877666
[F[K 210688/500000: episode: 2830, duration: 1.022s, episode steps: 61, steps per second: 60, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.656 [0.000, 4.000], mean observation: 0.490 [0.420, 0.530], loss: 10.938560, mean_absolute_error: 40.849766, mean_q: 52.324467
[F[K 210730/500000: episode: 2831, duration: 0.625s, episode steps: 42, steps per second: 67, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.238 [0.000, 4.000], mean observation: 0.506 [0.450, 0.620], loss: 6.817037, mean_absolute_error: 40.397747, mean_q: 52.020187
[F[K 210791/500000: episode: 2832, duration: 0.758s, episode steps: 61, steps per second: 80, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.377 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 8.742061, mean_absolute_error: 41.228443, mean_q: 52.879971
[F[K 210847/500000: episode: 2833, duration: 0.839s, episode steps: 56, steps per second: 67, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.786 [0.000, 4.000], mean observation: 0.497 [0.370, 0.610], loss: 7.380409, mean_absolute_error: 40.696377, mean_q: 52.152561
[F[K 210966/500000: episode: 2834, duration: 1.507s, episode steps: 119, steps per second: 79, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.084 [0.000, 4.000], mean observation: 0.495 [0.420, 0.540], loss: 8.305821, mean_absolute_error: 40.804958, mean_q: 52.399208
[F[K 211106/500000: episode: 2835, duration: 1.951s, episode steps: 140, steps per second: 72, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.657 [0.000, 4.000], mean observation: 0.505 [0.380, 0.640], loss: 7.994963, mean_absolute_error: 41.045185, mean_q: 52.694729
[F[K 211146/500000: episode: 2836, duration: 0.606s, episode steps: 40, steps per second: 66, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.275 [0.000, 4.000], mean observation: 0.522 [0.470, 0.630], loss: 7.541648, mean_absolute_error: 41.913338, mean_q: 53.746635
[F[K 211216/500000: episode: 2837, duration: 0.889s, episode steps: 70, steps per second: 79, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.086 [0.000, 4.000], mean observation: 0.500 [0.450, 0.550], loss: 7.523109, mean_absolute_error: 41.154289, mean_q: 52.852715
[F[K 211280/500000: episode: 2838, duration: 0.927s, episode steps: 64, steps per second: 69, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.531 [0.000, 4.000], mean observation: 0.506 [0.390, 0.630], loss: 8.741367, mean_absolute_error: 40.977650, mean_q: 52.574089
[F[K 211369/500000: episode: 2839, duration: 1.158s, episode steps: 89, steps per second: 77, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.742 [0.000, 4.000], mean observation: 0.482 [0.410, 0.520], loss: 8.745827, mean_absolute_error: 41.082932, mean_q: 52.656322
[F[K 211429/500000: episode: 2840, duration: 1.051s, episode steps: 60, steps per second: 57, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.050 [0.000, 4.000], mean observation: 0.514 [0.470, 0.600], loss: 7.754497, mean_absolute_error: 41.069134, mean_q: 52.687237
[F[K 211480/500000: episode: 2841, duration: 0.903s, episode steps: 51, steps per second: 56, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.843 [0.000, 4.000], mean observation: 0.520 [0.470, 0.600], loss: 7.596929, mean_absolute_error: 40.973129, mean_q: 52.531063
[F[K 211549/500000: episode: 2842, duration: 1.045s, episode steps: 69, steps per second: 66, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.511 [0.470, 0.620], loss: 8.313812, mean_absolute_error: 41.056774, mean_q: 52.629841
[F[K 211602/500000: episode: 2843, duration: 0.912s, episode steps: 53, steps per second: 58, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.906 [0.000, 4.000], mean observation: 0.480 [0.350, 0.530], loss: 7.349700, mean_absolute_error: 41.000179, mean_q: 52.701775
[F[K 211736/500000: episode: 2844, duration: 2.164s, episode steps: 134, steps per second: 62, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.381 [0.000, 4.000], mean observation: 0.481 [0.320, 0.530], loss: 8.441277, mean_absolute_error: 41.310131, mean_q: 53.001850
[F[K 211936/500000: episode: 2845, duration: 3.166s, episode steps: 200, steps per second: 63, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.494 [0.400, 0.590], loss: 8.163183, mean_absolute_error: 41.254623, mean_q: 52.957413
[F[K 211995/500000: episode: 2846, duration: 1.024s, episode steps: 59, steps per second: 58, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.390 [0.000, 4.000], mean observation: 0.502 [0.400, 0.610], loss: 8.771227, mean_absolute_error: 40.879002, mean_q: 52.374214
[F[K 212073/500000: episode: 2847, duration: 1.323s, episode steps: 78, steps per second: 59, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.692 [0.000, 4.000], mean observation: 0.474 [0.360, 0.510], loss: 8.109906, mean_absolute_error: 40.600853, mean_q: 52.067406
[F[K 212115/500000: episode: 2848, duration: 0.724s, episode steps: 42, steps per second: 58, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.738 [0.000, 4.000], mean observation: 0.530 [0.470, 0.650], loss: 9.340478, mean_absolute_error: 40.761578, mean_q: 52.320587
[F[K 212199/500000: episode: 2849, duration: 1.360s, episode steps: 84, steps per second: 62, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.131 [0.000, 4.000], mean observation: 0.489 [0.430, 0.520], loss: 7.880051, mean_absolute_error: 41.209377, mean_q: 52.856823
[F[K 212276/500000: episode: 2850, duration: 1.045s, episode steps: 77, steps per second: 74, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.026 [0.000, 4.000], mean observation: 0.499 [0.470, 0.530], loss: 8.630202, mean_absolute_error: 41.085293, mean_q: 52.694229
[F[K 212401/500000: episode: 2851, duration: 2.051s, episode steps: 125, steps per second: 61, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.509 [0.410, 0.640], loss: 8.261732, mean_absolute_error: 40.935638, mean_q: 52.523094
[F[K 212468/500000: episode: 2852, duration: 1.226s, episode steps: 67, steps per second: 55, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.537 [0.000, 4.000], mean observation: 0.511 [0.480, 0.570], loss: 8.994745, mean_absolute_error: 40.957607, mean_q: 52.439278
[F[K 212641/500000: episode: 2853, duration: 3.069s, episode steps: 173, steps per second: 56, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.295 [0.000, 4.000], mean observation: 0.517 [0.480, 0.620], loss: 7.806062, mean_absolute_error: 41.272099, mean_q: 52.930344
[F[K 212705/500000: episode: 2854, duration: 0.870s, episode steps: 64, steps per second: 74, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.500 [0.380, 0.590], loss: 8.725187, mean_absolute_error: 41.123741, mean_q: 52.705051
[F[K 212761/500000: episode: 2855, duration: 0.862s, episode steps: 56, steps per second: 65, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.505 [0.380, 0.660], loss: 6.955152, mean_absolute_error: 41.495125, mean_q: 53.302952
[F[K 212920/500000: episode: 2856, duration: 2.111s, episode steps: 159, steps per second: 75, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.403 [0.000, 4.000], mean observation: 0.480 [0.320, 0.520], loss: 7.998050, mean_absolute_error: 40.909653, mean_q: 52.441807
[F[K 212972/500000: episode: 2857, duration: 0.770s, episode steps: 52, steps per second: 68, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.516 [0.470, 0.600], loss: 8.385095, mean_absolute_error: 40.490696, mean_q: 51.937767
[F[K 213036/500000: episode: 2858, duration: 0.805s, episode steps: 64, steps per second: 80, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.547 [0.000, 4.000], mean observation: 0.515 [0.480, 0.630], loss: 7.770698, mean_absolute_error: 40.664787, mean_q: 52.259846
[F[K 213096/500000: episode: 2859, duration: 0.822s, episode steps: 60, steps per second: 73, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.217 [0.000, 4.000], mean observation: 0.499 [0.470, 0.530], loss: 9.117970, mean_absolute_error: 40.796116, mean_q: 52.414658
[F[K 213173/500000: episode: 2860, duration: 1.118s, episode steps: 77, steps per second: 69, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.948 [0.000, 4.000], mean observation: 0.496 [0.370, 0.580], loss: 8.370129, mean_absolute_error: 41.538177, mean_q: 53.304661
[F[K 213281/500000: episode: 2861, duration: 1.512s, episode steps: 108, steps per second: 71, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.944 [0.000, 4.000], mean observation: 0.484 [0.370, 0.520], loss: 8.691246, mean_absolute_error: 40.762325, mean_q: 52.249969
[F[K 213354/500000: episode: 2862, duration: 1.076s, episode steps: 73, steps per second: 68, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.890 [0.000, 4.000], mean observation: 0.512 [0.470, 0.630], loss: 8.140428, mean_absolute_error: 41.069107, mean_q: 52.645378
[F[K 213438/500000: episode: 2863, duration: 1.236s, episode steps: 84, steps per second: 68, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.845 [0.000, 4.000], mean observation: 0.501 [0.420, 0.580], loss: 7.614679, mean_absolute_error: 41.397758, mean_q: 53.167866
[F[K 213493/500000: episode: 2864, duration: 0.868s, episode steps: 55, steps per second: 63, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.491 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 7.268787, mean_absolute_error: 41.366348, mean_q: 53.168785
[F[K 213546/500000: episode: 2865, duration: 0.860s, episode steps: 53, steps per second: 62, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.358 [0.000, 4.000], mean observation: 0.520 [0.480, 0.620], loss: 7.493234, mean_absolute_error: 41.705414, mean_q: 53.632210
[F[K 213658/500000: episode: 2866, duration: 1.662s, episode steps: 112, steps per second: 67, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.893 [0.000, 4.000], mean observation: 0.484 [0.380, 0.520], loss: 8.785241, mean_absolute_error: 41.841835, mean_q: 53.686707
[F[K 213752/500000: episode: 2867, duration: 1.335s, episode steps: 94, steps per second: 70, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.394 [0.000, 4.000], mean observation: 0.519 [0.470, 0.650], loss: 7.507748, mean_absolute_error: 41.257236, mean_q: 52.946819
[F[K 213838/500000: episode: 2868, duration: 1.317s, episode steps: 86, steps per second: 65, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.547 [0.000, 4.000], mean observation: 0.509 [0.460, 0.590], loss: 8.531132, mean_absolute_error: 40.981316, mean_q: 52.558002
[F[K 213916/500000: episode: 2869, duration: 1.155s, episode steps: 78, steps per second: 68, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.756 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 8.159597, mean_absolute_error: 41.097054, mean_q: 52.741043
[F[K 213969/500000: episode: 2870, duration: 0.876s, episode steps: 53, steps per second: 61, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.489 [0.380, 0.530], loss: 8.056581, mean_absolute_error: 40.874409, mean_q: 52.396389
[F[K 214027/500000: episode: 2871, duration: 0.954s, episode steps: 58, steps per second: 61, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.897 [0.000, 4.000], mean observation: 0.501 [0.420, 0.600], loss: 8.839465, mean_absolute_error: 41.767323, mean_q: 53.563873
[F[K 214125/500000: episode: 2872, duration: 1.426s, episode steps: 98, steps per second: 69, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.506 [0.460, 0.620], loss: 7.930244, mean_absolute_error: 40.412113, mean_q: 51.897575
[F[K 214254/500000: episode: 2873, duration: 1.931s, episode steps: 129, steps per second: 67, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.264 [0.000, 4.000], mean observation: 0.488 [0.420, 0.530], loss: 8.741497, mean_absolute_error: 41.084515, mean_q: 52.770153
[F[K 214330/500000: episode: 2874, duration: 1.183s, episode steps: 76, steps per second: 64, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.503 [0.460, 0.550], loss: 8.073267, mean_absolute_error: 41.096893, mean_q: 52.711361
[F[K 214403/500000: episode: 2875, duration: 1.149s, episode steps: 73, steps per second: 64, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.205 [0.000, 4.000], mean observation: 0.497 [0.380, 0.570], loss: 9.682405, mean_absolute_error: 41.588764, mean_q: 53.244816
[F[K 214443/500000: episode: 2876, duration: 0.649s, episode steps: 40, steps per second: 62, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.525 [0.470, 0.630], loss: 9.628414, mean_absolute_error: 41.722187, mean_q: 53.658508
[F[K 214499/500000: episode: 2877, duration: 0.625s, episode steps: 56, steps per second: 90, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.411 [0.000, 4.000], mean observation: 0.492 [0.350, 0.570], loss: 9.099372, mean_absolute_error: 41.399284, mean_q: 52.997219
[F[K 214582/500000: episode: 2878, duration: 1.059s, episode steps: 83, steps per second: 78, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.819 [0.000, 4.000], mean observation: 0.488 [0.430, 0.530], loss: 7.717989, mean_absolute_error: 41.351383, mean_q: 53.096844
[F[K 214625/500000: episode: 2879, duration: 0.550s, episode steps: 43, steps per second: 78, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.535 [0.000, 4.000], mean observation: 0.518 [0.470, 0.650], loss: 8.184122, mean_absolute_error: 41.055996, mean_q: 52.717098
[F[K 214729/500000: episode: 2880, duration: 1.218s, episode steps: 104, steps per second: 85, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.423 [0.000, 4.000], mean observation: 0.499 [0.370, 0.600], loss: 9.089726, mean_absolute_error: 41.068378, mean_q: 52.651546
[F[K 214787/500000: episode: 2881, duration: 0.651s, episode steps: 58, steps per second: 89, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.569 [0.000, 4.000], mean observation: 0.498 [0.440, 0.570], loss: 9.233480, mean_absolute_error: 41.138821, mean_q: 52.795799
[F[K 214858/500000: episode: 2882, duration: 0.996s, episode steps: 71, steps per second: 71, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.099 [0.000, 4.000], mean observation: 0.506 [0.470, 0.550], loss: 7.833238, mean_absolute_error: 40.875351, mean_q: 52.470665
[F[K 214990/500000: episode: 2883, duration: 1.693s, episode steps: 132, steps per second: 78, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.886 [0.000, 4.000], mean observation: 0.488 [0.420, 0.530], loss: 8.587580, mean_absolute_error: 40.962547, mean_q: 52.549244
[F[K 215116/500000: episode: 2884, duration: 1.704s, episode steps: 126, steps per second: 74, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.786 [0.000, 4.000], mean observation: 0.490 [0.400, 0.520], loss: 8.245113, mean_absolute_error: 40.974621, mean_q: 52.514912
[F[K 215222/500000: episode: 2885, duration: 1.194s, episode steps: 106, steps per second: 89, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.123 [0.000, 4.000], mean observation: 0.497 [0.380, 0.590], loss: 7.989722, mean_absolute_error: 41.258835, mean_q: 52.954567
[F[K 215295/500000: episode: 2886, duration: 0.868s, episode steps: 73, steps per second: 84, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.233 [0.000, 4.000], mean observation: 0.496 [0.400, 0.570], loss: 8.738654, mean_absolute_error: 41.276340, mean_q: 52.880329
[F[K 215351/500000: episode: 2887, duration: 0.784s, episode steps: 56, steps per second: 71, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.232 [0.000, 4.000], mean observation: 0.493 [0.350, 0.570], loss: 8.771391, mean_absolute_error: 41.203922, mean_q: 52.732170
[F[K 215423/500000: episode: 2888, duration: 0.989s, episode steps: 72, steps per second: 73, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.861 [0.000, 4.000], mean observation: 0.519 [0.490, 0.590], loss: 10.341224, mean_absolute_error: 40.872437, mean_q: 52.410568
[F[K 215496/500000: episode: 2889, duration: 0.946s, episode steps: 73, steps per second: 77, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.315 [0.000, 4.000], mean observation: 0.503 [0.450, 0.570], loss: 7.902945, mean_absolute_error: 41.318703, mean_q: 52.978806
[F[K 215561/500000: episode: 2890, duration: 0.913s, episode steps: 65, steps per second: 71, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.615 [0.000, 4.000], mean observation: 0.494 [0.460, 0.530], loss: 7.609019, mean_absolute_error: 41.722172, mean_q: 53.512959
[F[K 215617/500000: episode: 2891, duration: 0.703s, episode steps: 56, steps per second: 80, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.321 [0.000, 4.000], mean observation: 0.472 [0.350, 0.530], loss: 9.240914, mean_absolute_error: 40.997715, mean_q: 52.532234
[F[K 215684/500000: episode: 2892, duration: 0.857s, episode steps: 67, steps per second: 78, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.164 [0.000, 4.000], mean observation: 0.497 [0.460, 0.530], loss: 6.961105, mean_absolute_error: 41.612995, mean_q: 53.375767
[F[K 215778/500000: episode: 2893, duration: 1.120s, episode steps: 94, steps per second: 84, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.501 [0.460, 0.530], loss: 8.453290, mean_absolute_error: 41.363010, mean_q: 53.018063
[F[K 215836/500000: episode: 2894, duration: 0.851s, episode steps: 58, steps per second: 68, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.741 [0.000, 4.000], mean observation: 0.506 [0.450, 0.610], loss: 9.171966, mean_absolute_error: 41.683865, mean_q: 53.397488
[F[K 215984/500000: episode: 2895, duration: 1.586s, episode steps: 148, steps per second: 93, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.703 [0.000, 4.000], mean observation: 0.493 [0.390, 0.530], loss: 8.934252, mean_absolute_error: 41.565571, mean_q: 53.354603
[F[K 216025/500000: episode: 2896, duration: 0.467s, episode steps: 41, steps per second: 88, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.268 [0.000, 4.000], mean observation: 0.520 [0.470, 0.650], loss: 9.264308, mean_absolute_error: 40.534836, mean_q: 52.139030
[F[K 216112/500000: episode: 2897, duration: 1.165s, episode steps: 87, steps per second: 75, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.483 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 8.424738, mean_absolute_error: 41.039574, mean_q: 52.648842
[F[K 216163/500000: episode: 2898, duration: 0.677s, episode steps: 51, steps per second: 75, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.216 [0.000, 4.000], mean observation: 0.524 [0.480, 0.640], loss: 8.800681, mean_absolute_error: 41.575199, mean_q: 53.204662
[F[K 216219/500000: episode: 2899, duration: 0.707s, episode steps: 56, steps per second: 79, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.536 [0.000, 4.000], mean observation: 0.502 [0.360, 0.650], loss: 8.353139, mean_absolute_error: 40.829960, mean_q: 52.419228
[F[K 216282/500000: episode: 2900, duration: 0.913s, episode steps: 63, steps per second: 69, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.497 [0.430, 0.530], loss: 8.519876, mean_absolute_error: 41.594589, mean_q: 53.264149
[F[K 216389/500000: episode: 2901, duration: 1.523s, episode steps: 107, steps per second: 70, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.514 [0.000, 4.000], mean observation: 0.504 [0.430, 0.620], loss: 8.820244, mean_absolute_error: 41.151482, mean_q: 52.794407
[F[K 216456/500000: episode: 2902, duration: 0.982s, episode steps: 67, steps per second: 68, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.701 [0.000, 4.000], mean observation: 0.498 [0.410, 0.570], loss: 8.701637, mean_absolute_error: 41.348137, mean_q: 53.079151
[F[K 216526/500000: episode: 2903, duration: 0.939s, episode steps: 70, steps per second: 75, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.504 [0.480, 0.530], loss: 8.273903, mean_absolute_error: 41.601460, mean_q: 53.313240
[F[K 216576/500000: episode: 2904, duration: 0.546s, episode steps: 50, steps per second: 92, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.400 [0.000, 4.000], mean observation: 0.520 [0.470, 0.630], loss: 6.676799, mean_absolute_error: 41.276596, mean_q: 53.028461
[F[K 216630/500000: episode: 2905, duration: 0.665s, episode steps: 54, steps per second: 81, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.278 [0.000, 4.000], mean observation: 0.491 [0.360, 0.550], loss: 8.516648, mean_absolute_error: 40.730381, mean_q: 52.245335
[F[K 216688/500000: episode: 2906, duration: 0.795s, episode steps: 58, steps per second: 73, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.498 [0.360, 0.620], loss: 9.462437, mean_absolute_error: 41.656349, mean_q: 53.385456
[F[K 216760/500000: episode: 2907, duration: 1.106s, episode steps: 72, steps per second: 65, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 0.491 [0.380, 0.550], loss: 7.532361, mean_absolute_error: 41.020012, mean_q: 52.601276
[F[K 216876/500000: episode: 2908, duration: 1.442s, episode steps: 116, steps per second: 80, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.345 [0.000, 4.000], mean observation: 0.523 [0.470, 0.640], loss: 8.559852, mean_absolute_error: 40.754028, mean_q: 52.200249
[F[K 216914/500000: episode: 2909, duration: 0.457s, episode steps: 38, steps per second: 83, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.632 [0.000, 4.000], mean observation: 0.529 [0.470, 0.650], loss: 7.461740, mean_absolute_error: 40.421181, mean_q: 51.873096
[F[K 216976/500000: episode: 2910, duration: 0.753s, episode steps: 62, steps per second: 82, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.952 [0.000, 4.000], mean observation: 0.499 [0.430, 0.560], loss: 8.200511, mean_absolute_error: 41.581261, mean_q: 53.321541
[F[K 217053/500000: episode: 2911, duration: 1.019s, episode steps: 77, steps per second: 76, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.130 [0.000, 4.000], mean observation: 0.502 [0.470, 0.530], loss: 8.500816, mean_absolute_error: 40.849522, mean_q: 52.452843
[F[K 217127/500000: episode: 2912, duration: 0.946s, episode steps: 74, steps per second: 78, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.041 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 7.932082, mean_absolute_error: 41.532612, mean_q: 53.322483
[F[K 217168/500000: episode: 2913, duration: 0.547s, episode steps: 41, steps per second: 75, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.585 [0.000, 4.000], mean observation: 0.525 [0.470, 0.640], loss: 9.619336, mean_absolute_error: 41.981293, mean_q: 53.733635
[F[K 217241/500000: episode: 2914, duration: 0.892s, episode steps: 73, steps per second: 82, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.082 [0.000, 4.000], mean observation: 0.489 [0.440, 0.530], loss: 9.946962, mean_absolute_error: 41.481514, mean_q: 53.230839
[F[K 217356/500000: episode: 2915, duration: 1.541s, episode steps: 115, steps per second: 75, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.139 [0.000, 4.000], mean observation: 0.478 [0.340, 0.530], loss: 8.142988, mean_absolute_error: 40.961174, mean_q: 52.574333
[F[K 217398/500000: episode: 2916, duration: 0.559s, episode steps: 42, steps per second: 75, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.881 [0.000, 4.000], mean observation: 0.526 [0.470, 0.650], loss: 8.430088, mean_absolute_error: 40.601540, mean_q: 52.157715
[F[K 217486/500000: episode: 2917, duration: 1.051s, episode steps: 88, steps per second: 84, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.625 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 8.114946, mean_absolute_error: 41.101753, mean_q: 52.727383
[F[K 217594/500000: episode: 2918, duration: 1.351s, episode steps: 108, steps per second: 80, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.778 [0.000, 4.000], mean observation: 0.506 [0.490, 0.550], loss: 7.728080, mean_absolute_error: 41.098976, mean_q: 52.707783
[F[K 217719/500000: episode: 2919, duration: 1.489s, episode steps: 125, steps per second: 84, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.360 [0.000, 4.000], mean observation: 0.489 [0.430, 0.510], loss: 8.478672, mean_absolute_error: 40.897655, mean_q: 52.405121
[F[K 217790/500000: episode: 2920, duration: 0.930s, episode steps: 71, steps per second: 76, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.028 [0.000, 4.000], mean observation: 0.515 [0.470, 0.630], loss: 9.529429, mean_absolute_error: 40.966396, mean_q: 52.479713
[F[K 217896/500000: episode: 2921, duration: 1.232s, episode steps: 106, steps per second: 86, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.528 [0.000, 4.000], mean observation: 0.494 [0.350, 0.600], loss: 8.324347, mean_absolute_error: 40.807102, mean_q: 52.370747
[F[K 217949/500000: episode: 2922, duration: 0.626s, episode steps: 53, steps per second: 85, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.868 [0.000, 4.000], mean observation: 0.504 [0.450, 0.620], loss: 9.340701, mean_absolute_error: 40.967644, mean_q: 52.599201
[F[K 218018/500000: episode: 2923, duration: 0.951s, episode steps: 69, steps per second: 73, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.101 [0.000, 4.000], mean observation: 0.497 [0.360, 0.620], loss: 8.259643, mean_absolute_error: 40.262547, mean_q: 51.537800
[F[K 218074/500000: episode: 2924, duration: 0.791s, episode steps: 56, steps per second: 71, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.506 [0.420, 0.630], loss: 8.024604, mean_absolute_error: 41.271034, mean_q: 52.914368
[F[K 218136/500000: episode: 2925, duration: 0.683s, episode steps: 62, steps per second: 91, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.497 [0.410, 0.590], loss: 9.060352, mean_absolute_error: 40.514702, mean_q: 51.993584
[F[K 218211/500000: episode: 2926, duration: 0.947s, episode steps: 75, steps per second: 79, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.640 [0.000, 4.000], mean observation: 0.476 [0.360, 0.530], loss: 9.054307, mean_absolute_error: 40.652767, mean_q: 52.106014
[F[K 218282/500000: episode: 2927, duration: 0.936s, episode steps: 71, steps per second: 76, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.499 [0.460, 0.530], loss: 7.836185, mean_absolute_error: 40.148827, mean_q: 51.483334
[F[K 218464/500000: episode: 2928, duration: 1.971s, episode steps: 182, steps per second: 92, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.192 [0.000, 4.000], mean observation: 0.496 [0.420, 0.550], loss: 8.895955, mean_absolute_error: 40.664764, mean_q: 52.155090
[F[K 218571/500000: episode: 2929, duration: 1.505s, episode steps: 107, steps per second: 71, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.355 [0.000, 4.000], mean observation: 0.503 [0.470, 0.540], loss: 7.437645, mean_absolute_error: 40.722473, mean_q: 52.224091
[F[K 218657/500000: episode: 2930, duration: 1.058s, episode steps: 86, steps per second: 81, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.570 [0.000, 4.000], mean observation: 0.482 [0.380, 0.530], loss: 7.696681, mean_absolute_error: 40.753540, mean_q: 52.432102
[F[K 218719/500000: episode: 2931, duration: 0.841s, episode steps: 62, steps per second: 74, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.403 [0.000, 4.000], mean observation: 0.479 [0.360, 0.530], loss: 8.147715, mean_absolute_error: 40.397957, mean_q: 51.872486
[F[K 218800/500000: episode: 2932, duration: 1.082s, episode steps: 81, steps per second: 75, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.469 [0.000, 4.000], mean observation: 0.475 [0.370, 0.530], loss: 6.589848, mean_absolute_error: 41.044521, mean_q: 52.705482
[F[K 218870/500000: episode: 2933, duration: 0.926s, episode steps: 70, steps per second: 76, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.757 [0.000, 4.000], mean observation: 0.484 [0.410, 0.530], loss: 8.657494, mean_absolute_error: 40.964184, mean_q: 52.549217
[F[K 218928/500000: episode: 2934, duration: 0.680s, episode steps: 58, steps per second: 85, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.810 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 8.696608, mean_absolute_error: 41.092228, mean_q: 52.643757
[F[K 219022/500000: episode: 2935, duration: 0.931s, episode steps: 94, steps per second: 101, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.564 [0.000, 4.000], mean observation: 0.494 [0.400, 0.550], loss: 9.574919, mean_absolute_error: 40.496620, mean_q: 51.885544
[F[K 219131/500000: episode: 2936, duration: 1.356s, episode steps: 109, steps per second: 80, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.844 [0.000, 4.000], mean observation: 0.516 [0.480, 0.600], loss: 7.743012, mean_absolute_error: 40.533501, mean_q: 52.012802
[F[K 219185/500000: episode: 2937, duration: 0.760s, episode steps: 54, steps per second: 71, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.389 [0.000, 4.000], mean observation: 0.503 [0.470, 0.530], loss: 9.536448, mean_absolute_error: 40.524311, mean_q: 51.862507
[F[K 219259/500000: episode: 2938, duration: 1.017s, episode steps: 74, steps per second: 73, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.405 [0.000, 4.000], mean observation: 0.516 [0.470, 0.650], loss: 7.521157, mean_absolute_error: 40.915916, mean_q: 52.541550
[F[K 219308/500000: episode: 2939, duration: 0.627s, episode steps: 49, steps per second: 78, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.837 [0.000, 4.000], mean observation: 0.524 [0.490, 0.640], loss: 7.158753, mean_absolute_error: 40.781769, mean_q: 52.460594
[F[K 219450/500000: episode: 2940, duration: 1.499s, episode steps: 142, steps per second: 95, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.077 [0.000, 4.000], mean observation: 0.504 [0.470, 0.580], loss: 8.552852, mean_absolute_error: 40.866379, mean_q: 52.410473
[F[K 219574/500000: episode: 2941, duration: 1.449s, episode steps: 124, steps per second: 86, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.895 [0.000, 4.000], mean observation: 0.484 [0.380, 0.530], loss: 7.453326, mean_absolute_error: 40.458912, mean_q: 51.977821
[F[K 219640/500000: episode: 2942, duration: 0.757s, episode steps: 66, steps per second: 87, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.485 [0.000, 4.000], mean observation: 0.479 [0.350, 0.530], loss: 8.814777, mean_absolute_error: 40.919907, mean_q: 52.489910
[F[K 219710/500000: episode: 2943, duration: 1.000s, episode steps: 70, steps per second: 70, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 8.535686, mean_absolute_error: 40.455788, mean_q: 51.969746
[F[K 219777/500000: episode: 2944, duration: 0.973s, episode steps: 67, steps per second: 69, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.701 [0.000, 4.000], mean observation: 0.494 [0.370, 0.610], loss: 8.532110, mean_absolute_error: 40.192242, mean_q: 51.579811
[F[K 219852/500000: episode: 2945, duration: 1.074s, episode steps: 75, steps per second: 70, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 7.616138, mean_absolute_error: 40.773087, mean_q: 52.334152
[F[K 219968/500000: episode: 2946, duration: 1.574s, episode steps: 116, steps per second: 74, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.138 [0.000, 4.000], mean observation: 0.516 [0.480, 0.630], loss: 8.579281, mean_absolute_error: 40.731480, mean_q: 52.119408
[F[K 220023/500000: episode: 2947, duration: 0.777s, episode steps: 55, steps per second: 71, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.236 [0.000, 4.000], mean observation: 0.513 [0.470, 0.590], loss: 8.245630, mean_absolute_error: 41.004532, mean_q: 52.464001
[F[K 220089/500000: episode: 2948, duration: 0.823s, episode steps: 66, steps per second: 80, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.409 [0.000, 4.000], mean observation: 0.488 [0.420, 0.520], loss: 9.629090, mean_absolute_error: 40.570587, mean_q: 51.945766
[F[K 220148/500000: episode: 2949, duration: 0.837s, episode steps: 59, steps per second: 71, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.593 [0.000, 4.000], mean observation: 0.525 [0.480, 0.640], loss: 7.854078, mean_absolute_error: 39.947090, mean_q: 51.226303
[F[K 220195/500000: episode: 2950, duration: 0.738s, episode steps: 47, steps per second: 64, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.404 [0.000, 4.000], mean observation: 0.523 [0.470, 0.620], loss: 8.690966, mean_absolute_error: 40.447594, mean_q: 51.700321
[F[K 220355/500000: episode: 2951, duration: 2.165s, episode steps: 160, steps per second: 74, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.474 [0.360, 0.520], loss: 8.485399, mean_absolute_error: 40.851727, mean_q: 52.339893
[F[K 220425/500000: episode: 2952, duration: 0.856s, episode steps: 70, steps per second: 82, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.514 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 8.366411, mean_absolute_error: 41.059307, mean_q: 52.672676
[F[K 220491/500000: episode: 2953, duration: 0.807s, episode steps: 66, steps per second: 82, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.106 [0.000, 4.000], mean observation: 0.490 [0.450, 0.510], loss: 7.653538, mean_absolute_error: 40.341057, mean_q: 51.660480
[F[K 220551/500000: episode: 2954, duration: 0.865s, episode steps: 60, steps per second: 69, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.367 [0.000, 4.000], mean observation: 0.495 [0.380, 0.540], loss: 9.367930, mean_absolute_error: 40.769962, mean_q: 52.341961
[F[K 220616/500000: episode: 2955, duration: 0.947s, episode steps: 65, steps per second: 69, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.246 [0.000, 4.000], mean observation: 0.497 [0.400, 0.580], loss: 7.508317, mean_absolute_error: 41.549721, mean_q: 53.334805
[F[K 220786/500000: episode: 2956, duration: 2.592s, episode steps: 170, steps per second: 66, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.876 [0.000, 4.000], mean observation: 0.503 [0.470, 0.550], loss: 8.929023, mean_absolute_error: 40.776402, mean_q: 52.300762
[F[K 220950/500000: episode: 2957, duration: 2.293s, episode steps: 164, steps per second: 72, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.963 [0.000, 4.000], mean observation: 0.499 [0.420, 0.590], loss: 8.513301, mean_absolute_error: 40.446674, mean_q: 51.765804
[F[K 221029/500000: episode: 2958, duration: 1.278s, episode steps: 79, steps per second: 62, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.975 [0.000, 4.000], mean observation: 0.470 [0.350, 0.520], loss: 7.917334, mean_absolute_error: 40.627110, mean_q: 52.233459
[F[K 221068/500000: episode: 2959, duration: 0.639s, episode steps: 39, steps per second: 61, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.385 [0.000, 4.000], mean observation: 0.516 [0.470, 0.650], loss: 8.838467, mean_absolute_error: 40.104626, mean_q: 51.528061
[F[K 221131/500000: episode: 2960, duration: 0.859s, episode steps: 63, steps per second: 73, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.032 [0.000, 4.000], mean observation: 0.494 [0.460, 0.530], loss: 9.088830, mean_absolute_error: 40.764248, mean_q: 52.350861
[F[K 221198/500000: episode: 2961, duration: 0.841s, episode steps: 67, steps per second: 80, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.821 [0.000, 4.000], mean observation: 0.500 [0.360, 0.640], loss: 9.412095, mean_absolute_error: 40.516266, mean_q: 51.941509
[F[K 221268/500000: episode: 2962, duration: 0.941s, episode steps: 70, steps per second: 74, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.757 [0.000, 4.000], mean observation: 0.509 [0.470, 0.560], loss: 7.404157, mean_absolute_error: 41.723206, mean_q: 53.428093
[F[K 221425/500000: episode: 2963, duration: 1.969s, episode steps: 157, steps per second: 80, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.293 [0.000, 4.000], mean observation: 0.498 [0.400, 0.590], loss: 8.611506, mean_absolute_error: 40.992702, mean_q: 52.472046
[F[K 221503/500000: episode: 2964, duration: 1.002s, episode steps: 78, steps per second: 78, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.494 [0.330, 0.620], loss: 8.060468, mean_absolute_error: 41.018490, mean_q: 52.550949
[F[K 221616/500000: episode: 2965, duration: 1.319s, episode steps: 113, steps per second: 86, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.920 [0.000, 4.000], mean observation: 0.502 [0.420, 0.590], loss: 9.005340, mean_absolute_error: 40.749153, mean_q: 52.299057
[F[K 221660/500000: episode: 2966, duration: 0.534s, episode steps: 44, steps per second: 82, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.455 [0.000, 4.000], mean observation: 0.523 [0.470, 0.640], loss: 8.768044, mean_absolute_error: 40.407921, mean_q: 51.705662
[F[K 221715/500000: episode: 2967, duration: 0.566s, episode steps: 55, steps per second: 97, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.507 [0.440, 0.630], loss: 7.931538, mean_absolute_error: 40.563522, mean_q: 52.132629
[F[K 221773/500000: episode: 2968, duration: 0.677s, episode steps: 58, steps per second: 86, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.069 [0.000, 4.000], mean observation: 0.517 [0.480, 0.600], loss: 7.886296, mean_absolute_error: 40.855236, mean_q: 52.358402
[F[K 221839/500000: episode: 2969, duration: 0.843s, episode steps: 66, steps per second: 78, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.606 [0.000, 4.000], mean observation: 0.506 [0.460, 0.630], loss: 8.450504, mean_absolute_error: 40.714760, mean_q: 52.211071
[F[K 221946/500000: episode: 2970, duration: 1.360s, episode steps: 107, steps per second: 79, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.140 [0.000, 4.000], mean observation: 0.491 [0.430, 0.520], loss: 7.610654, mean_absolute_error: 40.549229, mean_q: 52.085667
[F[K 222022/500000: episode: 2971, duration: 0.977s, episode steps: 76, steps per second: 78, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.987 [0.000, 4.000], mean observation: 0.509 [0.490, 0.570], loss: 7.577679, mean_absolute_error: 40.521095, mean_q: 52.017303
[F[K 222102/500000: episode: 2972, duration: 1.032s, episode steps: 80, steps per second: 78, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.337 [0.000, 4.000], mean observation: 0.492 [0.420, 0.530], loss: 7.880666, mean_absolute_error: 40.957550, mean_q: 52.609215
[F[K 222252/500000: episode: 2973, duration: 1.542s, episode steps: 150, steps per second: 97, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.498 [0.380, 0.610], loss: 7.824425, mean_absolute_error: 41.109650, mean_q: 52.780014
[F[K 222427/500000: episode: 2974, duration: 2.206s, episode steps: 175, steps per second: 79, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.491 [0.420, 0.550], loss: 7.877238, mean_absolute_error: 41.330513, mean_q: 53.009056
[F[K 222499/500000: episode: 2975, duration: 0.976s, episode steps: 72, steps per second: 74, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.819 [0.000, 4.000], mean observation: 0.485 [0.360, 0.520], loss: 7.841951, mean_absolute_error: 40.650127, mean_q: 52.238537
[F[K 222578/500000: episode: 2976, duration: 1.011s, episode steps: 79, steps per second: 78, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.810 [0.000, 4.000], mean observation: 0.505 [0.470, 0.540], loss: 8.036084, mean_absolute_error: 40.563923, mean_q: 51.915112
[F[K 222650/500000: episode: 2977, duration: 0.854s, episode steps: 72, steps per second: 84, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.542 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 7.687052, mean_absolute_error: 41.167419, mean_q: 52.838669
[F[K 222750/500000: episode: 2978, duration: 1.242s, episode steps: 100, steps per second: 81, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.480 [0.000, 4.000], mean observation: 0.477 [0.340, 0.530], loss: 8.066507, mean_absolute_error: 41.114891, mean_q: 52.734032
[F[K 222815/500000: episode: 2979, duration: 0.818s, episode steps: 65, steps per second: 79, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.485 [0.350, 0.530], loss: 8.511526, mean_absolute_error: 40.579952, mean_q: 52.035568
[F[K 222907/500000: episode: 2980, duration: 1.049s, episode steps: 92, steps per second: 88, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.435 [0.000, 4.000], mean observation: 0.484 [0.400, 0.530], loss: 7.679574, mean_absolute_error: 41.301502, mean_q: 53.025150
[F[K 223037/500000: episode: 2981, duration: 1.592s, episode steps: 130, steps per second: 82, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.731 [0.000, 4.000], mean observation: 0.511 [0.460, 0.610], loss: 8.145061, mean_absolute_error: 40.845402, mean_q: 52.454132
[F[K 223090/500000: episode: 2982, duration: 0.687s, episode steps: 53, steps per second: 77, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.415 [0.000, 4.000], mean observation: 0.509 [0.460, 0.620], loss: 7.400154, mean_absolute_error: 41.348663, mean_q: 52.909908
[F[K 223164/500000: episode: 2983, duration: 0.941s, episode steps: 74, steps per second: 79, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.488 [0.440, 0.510], loss: 8.376298, mean_absolute_error: 41.677078, mean_q: 53.391014
[F[K 223242/500000: episode: 2984, duration: 0.949s, episode steps: 78, steps per second: 82, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.051 [0.000, 4.000], mean observation: 0.477 [0.380, 0.520], loss: 8.633169, mean_absolute_error: 40.809528, mean_q: 52.273487
[F[K 223299/500000: episode: 2985, duration: 0.706s, episode steps: 57, steps per second: 81, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.368 [0.000, 4.000], mean observation: 0.502 [0.420, 0.580], loss: 9.194731, mean_absolute_error: 41.407970, mean_q: 53.140095
[F[K 223336/500000: episode: 2986, duration: 0.469s, episode steps: 37, steps per second: 79, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.757 [0.000, 4.000], mean observation: 0.530 [0.480, 0.650], loss: 8.857227, mean_absolute_error: 41.710354, mean_q: 53.494343
[F[K 223411/500000: episode: 2987, duration: 0.864s, episode steps: 75, steps per second: 87, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.693 [0.000, 4.000], mean observation: 0.501 [0.420, 0.580], loss: 7.482508, mean_absolute_error: 40.731281, mean_q: 52.299339
[F[K 223468/500000: episode: 2988, duration: 0.758s, episode steps: 57, steps per second: 75, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.140 [0.000, 4.000], mean observation: 0.501 [0.380, 0.620], loss: 7.343899, mean_absolute_error: 41.458912, mean_q: 53.117970
[F[K 223596/500000: episode: 2989, duration: 1.645s, episode steps: 128, steps per second: 78, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.727 [0.000, 4.000], mean observation: 0.486 [0.350, 0.530], loss: 8.652911, mean_absolute_error: 41.138145, mean_q: 52.673080
[F[K 223684/500000: episode: 2990, duration: 1.023s, episode steps: 88, steps per second: 86, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.886 [0.000, 4.000], mean observation: 0.499 [0.440, 0.580], loss: 7.810898, mean_absolute_error: 41.521976, mean_q: 53.299393
[F[K 223754/500000: episode: 2991, duration: 0.949s, episode steps: 70, steps per second: 74, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.503 [0.450, 0.600], loss: 8.033590, mean_absolute_error: 40.985336, mean_q: 52.560108
[F[K 223849/500000: episode: 2992, duration: 1.173s, episode steps: 95, steps per second: 81, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.684 [0.000, 4.000], mean observation: 0.508 [0.400, 0.630], loss: 7.464520, mean_absolute_error: 41.212917, mean_q: 52.830727
[F[K 223919/500000: episode: 2993, duration: 0.879s, episode steps: 70, steps per second: 80, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.357 [0.000, 4.000], mean observation: 0.503 [0.460, 0.590], loss: 7.654173, mean_absolute_error: 40.209560, mean_q: 51.560825
[F[K 223983/500000: episode: 2994, duration: 0.840s, episode steps: 64, steps per second: 76, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.495 [0.440, 0.540], loss: 9.084307, mean_absolute_error: 40.905518, mean_q: 52.434673
[F[K 224051/500000: episode: 2995, duration: 0.832s, episode steps: 68, steps per second: 82, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.456 [0.000, 4.000], mean observation: 0.502 [0.470, 0.550], loss: 7.524958, mean_absolute_error: 41.112701, mean_q: 52.754452
[F[K 224114/500000: episode: 2996, duration: 0.831s, episode steps: 63, steps per second: 76, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.486 [0.370, 0.520], loss: 10.375188, mean_absolute_error: 41.161961, mean_q: 52.644894
[F[K 224165/500000: episode: 2997, duration: 0.672s, episode steps: 51, steps per second: 76, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.078 [0.000, 4.000], mean observation: 0.504 [0.390, 0.620], loss: 7.865851, mean_absolute_error: 40.906422, mean_q: 52.511944
[F[K 224206/500000: episode: 2998, duration: 0.540s, episode steps: 41, steps per second: 76, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.707 [0.000, 4.000], mean observation: 0.511 [0.470, 0.630], loss: 7.468128, mean_absolute_error: 41.254013, mean_q: 52.849823
[F[K 224293/500000: episode: 2999, duration: 1.169s, episode steps: 87, steps per second: 74, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.497 [0.430, 0.540], loss: 7.521953, mean_absolute_error: 40.151031, mean_q: 51.606770
[F[K 224365/500000: episode: 3000, duration: 0.897s, episode steps: 72, steps per second: 80, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.507 [0.470, 0.590], loss: 8.555415, mean_absolute_error: 41.195644, mean_q: 52.926010
[F[K 224428/500000: episode: 3001, duration: 0.848s, episode steps: 63, steps per second: 74, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.397 [0.000, 4.000], mean observation: 0.490 [0.420, 0.530], loss: 6.667348, mean_absolute_error: 41.364510, mean_q: 53.097336
[F[K 224501/500000: episode: 3002, duration: 0.909s, episode steps: 73, steps per second: 80, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.479 [0.360, 0.530], loss: 7.580876, mean_absolute_error: 40.940876, mean_q: 52.518818
[F[K 224579/500000: episode: 3003, duration: 1.068s, episode steps: 78, steps per second: 73, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.974 [0.000, 4.000], mean observation: 0.507 [0.470, 0.550], loss: 8.266414, mean_absolute_error: 41.018188, mean_q: 52.595303
[F[K 224626/500000: episode: 3004, duration: 0.660s, episode steps: 47, steps per second: 71, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.234 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 7.878183, mean_absolute_error: 41.164391, mean_q: 52.909084
[F[K 224672/500000: episode: 3005, duration: 0.617s, episode steps: 46, steps per second: 75, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.848 [0.000, 4.000], mean observation: 0.515 [0.470, 0.610], loss: 7.942636, mean_absolute_error: 41.161991, mean_q: 52.784603
[F[K 224770/500000: episode: 3006, duration: 1.222s, episode steps: 98, steps per second: 80, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 8.893339, mean_absolute_error: 41.056427, mean_q: 52.597130
[F[K 224864/500000: episode: 3007, duration: 1.293s, episode steps: 94, steps per second: 73, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.798 [0.000, 4.000], mean observation: 0.496 [0.430, 0.540], loss: 9.474596, mean_absolute_error: 40.598240, mean_q: 52.051926
[F[K 224945/500000: episode: 3008, duration: 1.022s, episode steps: 81, steps per second: 79, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.889 [0.000, 4.000], mean observation: 0.501 [0.440, 0.570], loss: 9.157402, mean_absolute_error: 40.981678, mean_q: 52.599152
[F[K 224988/500000: episode: 3009, duration: 0.444s, episode steps: 43, steps per second: 97, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.047 [0.000, 4.000], mean observation: 0.514 [0.480, 0.640], loss: 9.397984, mean_absolute_error: 41.180626, mean_q: 52.889042
[F[K 225069/500000: episode: 3010, duration: 1.102s, episode steps: 81, steps per second: 74, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.506 [0.000, 4.000], mean observation: 0.508 [0.480, 0.560], loss: 10.000388, mean_absolute_error: 40.694416, mean_q: 52.064056
[F[K 225109/500000: episode: 3011, duration: 0.621s, episode steps: 40, steps per second: 64, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.508 [0.440, 0.640], loss: 7.283010, mean_absolute_error: 39.798088, mean_q: 51.095100
[F[K 225185/500000: episode: 3012, duration: 1.037s, episode steps: 76, steps per second: 73, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.026 [0.000, 4.000], mean observation: 0.501 [0.450, 0.580], loss: 9.054257, mean_absolute_error: 41.447426, mean_q: 53.073601
[F[K 225251/500000: episode: 3013, duration: 0.902s, episode steps: 66, steps per second: 73, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.909 [0.000, 4.000], mean observation: 0.512 [0.490, 0.560], loss: 9.408720, mean_absolute_error: 40.567478, mean_q: 51.939915
[F[K 225400/500000: episode: 3014, duration: 1.869s, episode steps: 149, steps per second: 80, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.336 [0.000, 4.000], mean observation: 0.491 [0.380, 0.530], loss: 8.091626, mean_absolute_error: 40.654507, mean_q: 52.185825
[F[K 225516/500000: episode: 3015, duration: 1.502s, episode steps: 116, steps per second: 77, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.491 [0.410, 0.530], loss: 7.912521, mean_absolute_error: 41.048885, mean_q: 52.649593
[F[K 225576/500000: episode: 3016, duration: 0.668s, episode steps: 60, steps per second: 90, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.510 [0.500, 0.540], loss: 7.495595, mean_absolute_error: 40.428753, mean_q: 51.892895
[F[K 225643/500000: episode: 3017, duration: 0.873s, episode steps: 67, steps per second: 77, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.015 [0.000, 4.000], mean observation: 0.492 [0.410, 0.540], loss: 8.065340, mean_absolute_error: 40.891094, mean_q: 52.404148
[F[K 225725/500000: episode: 3018, duration: 1.177s, episode steps: 82, steps per second: 70, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.037 [0.000, 4.000], mean observation: 0.492 [0.350, 0.590], loss: 8.951577, mean_absolute_error: 40.452682, mean_q: 51.932350
[F[K 225845/500000: episode: 3019, duration: 1.614s, episode steps: 120, steps per second: 74, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.567 [0.000, 4.000], mean observation: 0.503 [0.450, 0.570], loss: 7.758661, mean_absolute_error: 40.965641, mean_q: 52.627964
[F[K 225915/500000: episode: 3020, duration: 0.911s, episode steps: 70, steps per second: 77, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.485 [0.400, 0.520], loss: 7.816211, mean_absolute_error: 40.954567, mean_q: 52.505043
[F[K 225992/500000: episode: 3021, duration: 0.905s, episode steps: 77, steps per second: 85, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.502 [0.360, 0.640], loss: 8.914917, mean_absolute_error: 40.116047, mean_q: 51.416069
[F[K 226052/500000: episode: 3022, duration: 0.720s, episode steps: 60, steps per second: 83, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.467 [0.000, 4.000], mean observation: 0.503 [0.470, 0.570], loss: 7.484827, mean_absolute_error: 40.788948, mean_q: 52.489426
[F[K 226126/500000: episode: 3023, duration: 0.860s, episode steps: 74, steps per second: 86, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.338 [0.000, 4.000], mean observation: 0.502 [0.390, 0.640], loss: 8.419181, mean_absolute_error: 40.369064, mean_q: 51.795033
[F[K 226169/500000: episode: 3024, duration: 0.501s, episode steps: 43, steps per second: 86, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.837 [0.000, 4.000], mean observation: 0.507 [0.440, 0.630], loss: 8.381963, mean_absolute_error: 41.128052, mean_q: 52.831810
[F[K 226266/500000: episode: 3025, duration: 1.121s, episode steps: 97, steps per second: 87, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.021 [0.000, 4.000], mean observation: 0.509 [0.470, 0.570], loss: 9.376064, mean_absolute_error: 40.929928, mean_q: 52.352730
[F[K 226425/500000: episode: 3026, duration: 1.890s, episode steps: 159, steps per second: 84, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.050 [0.000, 4.000], mean observation: 0.482 [0.350, 0.510], loss: 8.122267, mean_absolute_error: 41.041286, mean_q: 52.699188
[F[K 226503/500000: episode: 3027, duration: 0.899s, episode steps: 78, steps per second: 87, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.885 [0.000, 4.000], mean observation: 0.496 [0.400, 0.550], loss: 8.075571, mean_absolute_error: 41.011150, mean_q: 52.598137
[F[K 226557/500000: episode: 3028, duration: 0.651s, episode steps: 54, steps per second: 83, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.389 [0.000, 4.000], mean observation: 0.501 [0.420, 0.600], loss: 8.034851, mean_absolute_error: 41.039692, mean_q: 52.595150
[F[K 226607/500000: episode: 3029, duration: 0.628s, episode steps: 50, steps per second: 80, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.521 [0.470, 0.640], loss: 8.399063, mean_absolute_error: 40.258366, mean_q: 51.755775
[F[K 226672/500000: episode: 3030, duration: 0.746s, episode steps: 65, steps per second: 87, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.495 [0.400, 0.580], loss: 8.537938, mean_absolute_error: 40.906826, mean_q: 52.458019
[F[K 226752/500000: episode: 3031, duration: 0.828s, episode steps: 80, steps per second: 97, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 0.476 [0.350, 0.520], loss: 7.433280, mean_absolute_error: 40.476337, mean_q: 52.137016
[F[K 226826/500000: episode: 3032, duration: 0.876s, episode steps: 74, steps per second: 84, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 9.051762, mean_absolute_error: 40.580639, mean_q: 52.037792
[F[K 226907/500000: episode: 3033, duration: 0.905s, episode steps: 81, steps per second: 90, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.975 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 9.838377, mean_absolute_error: 40.094788, mean_q: 51.401680
[F[K 226995/500000: episode: 3034, duration: 0.994s, episode steps: 88, steps per second: 89, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.670 [0.000, 4.000], mean observation: 0.485 [0.400, 0.530], loss: 7.129873, mean_absolute_error: 40.794041, mean_q: 52.256058
[F[K 227057/500000: episode: 3035, duration: 0.719s, episode steps: 62, steps per second: 86, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.489 [0.410, 0.510], loss: 8.410636, mean_absolute_error: 39.999393, mean_q: 51.332382
[F[K 227229/500000: episode: 3036, duration: 1.703s, episode steps: 172, steps per second: 101, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.481 [0.320, 0.550], loss: 8.656837, mean_absolute_error: 40.803535, mean_q: 52.359955
[F[K 227279/500000: episode: 3037, duration: 0.488s, episode steps: 50, steps per second: 102, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.780 [0.000, 4.000], mean observation: 0.523 [0.470, 0.650], loss: 8.620098, mean_absolute_error: 40.424583, mean_q: 52.011341
[F[K 227331/500000: episode: 3038, duration: 0.534s, episode steps: 52, steps per second: 97, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.673 [0.000, 4.000], mean observation: 0.510 [0.470, 0.580], loss: 7.190007, mean_absolute_error: 39.768311, mean_q: 51.085529
[F[K 227395/500000: episode: 3039, duration: 0.603s, episode steps: 64, steps per second: 106, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.515 [0.490, 0.570], loss: 7.479300, mean_absolute_error: 40.188095, mean_q: 51.512505
[F[K 227501/500000: episode: 3040, duration: 1.115s, episode steps: 106, steps per second: 95, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.160 [0.000, 4.000], mean observation: 0.503 [0.480, 0.550], loss: 8.571607, mean_absolute_error: 40.390942, mean_q: 51.799358
[F[K 227637/500000: episode: 3041, duration: 1.411s, episode steps: 136, steps per second: 96, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.316 [0.000, 4.000], mean observation: 0.506 [0.470, 0.560], loss: 8.386117, mean_absolute_error: 40.199162, mean_q: 51.632973
[F[K 227747/500000: episode: 3042, duration: 1.249s, episode steps: 110, steps per second: 88, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.127 [0.000, 4.000], mean observation: 0.500 [0.340, 0.660], loss: 8.668262, mean_absolute_error: 40.206074, mean_q: 51.606804
[F[K 227808/500000: episode: 3043, duration: 0.685s, episode steps: 61, steps per second: 89, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.098 [0.000, 4.000], mean observation: 0.489 [0.360, 0.530], loss: 8.141113, mean_absolute_error: 40.290024, mean_q: 51.741371
[F[K 227861/500000: episode: 3044, duration: 0.561s, episode steps: 53, steps per second: 94, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.377 [0.000, 4.000], mean observation: 0.513 [0.470, 0.630], loss: 9.254107, mean_absolute_error: 40.291393, mean_q: 51.762447
[F[K 227930/500000: episode: 3045, duration: 0.785s, episode steps: 69, steps per second: 88, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.971 [0.000, 4.000], mean observation: 0.501 [0.470, 0.560], loss: 8.157735, mean_absolute_error: 39.891636, mean_q: 51.258579
[F[K 228008/500000: episode: 3046, duration: 0.845s, episode steps: 78, steps per second: 92, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.936 [0.000, 4.000], mean observation: 0.505 [0.430, 0.590], loss: 7.245193, mean_absolute_error: 39.712551, mean_q: 51.099602
[F[K 228053/500000: episode: 3047, duration: 0.575s, episode steps: 45, steps per second: 78, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.504 [0.410, 0.640], loss: 7.612581, mean_absolute_error: 40.213135, mean_q: 51.690071
[F[K 228114/500000: episode: 3048, duration: 0.740s, episode steps: 61, steps per second: 82, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.541 [0.000, 4.000], mean observation: 0.508 [0.490, 0.550], loss: 8.310438, mean_absolute_error: 39.440109, mean_q: 50.544624
[F[K 228159/500000: episode: 3049, duration: 0.533s, episode steps: 45, steps per second: 84, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.867 [0.000, 4.000], mean observation: 0.506 [0.450, 0.570], loss: 8.962289, mean_absolute_error: 40.513985, mean_q: 51.848747
[F[K 228202/500000: episode: 3050, duration: 0.580s, episode steps: 43, steps per second: 74, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.163 [0.000, 4.000], mean observation: 0.502 [0.390, 0.630], loss: 7.281020, mean_absolute_error: 40.733627, mean_q: 52.413609
[F[K 228280/500000: episode: 3051, duration: 0.817s, episode steps: 78, steps per second: 95, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.485 [0.360, 0.520], loss: 7.338768, mean_absolute_error: 40.057064, mean_q: 51.385647
[F[K 228363/500000: episode: 3052, duration: 0.914s, episode steps: 83, steps per second: 91, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.265 [0.000, 4.000], mean observation: 0.500 [0.420, 0.590], loss: 7.761707, mean_absolute_error: 40.152130, mean_q: 51.613029
[F[K 228509/500000: episode: 3053, duration: 1.508s, episode steps: 146, steps per second: 97, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.356 [0.000, 4.000], mean observation: 0.504 [0.430, 0.610], loss: 7.978904, mean_absolute_error: 41.057510, mean_q: 52.769627
[F[K 228594/500000: episode: 3054, duration: 0.869s, episode steps: 85, steps per second: 98, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.753 [0.000, 4.000], mean observation: 0.513 [0.490, 0.560], loss: 7.817110, mean_absolute_error: 40.166618, mean_q: 51.592205
[F[K 228697/500000: episode: 3055, duration: 1.086s, episode steps: 103, steps per second: 95, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.262 [0.000, 4.000], mean observation: 0.509 [0.450, 0.570], loss: 8.409397, mean_absolute_error: 40.290104, mean_q: 51.754711
[F[K 228763/500000: episode: 3056, duration: 0.661s, episode steps: 66, steps per second: 100, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.348 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 7.350626, mean_absolute_error: 39.926830, mean_q: 51.337154
[F[K 228832/500000: episode: 3057, duration: 0.790s, episode steps: 69, steps per second: 87, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.768 [0.000, 4.000], mean observation: 0.480 [0.370, 0.510], loss: 7.693351, mean_absolute_error: 39.814007, mean_q: 51.280190
[F[K 228903/500000: episode: 3058, duration: 0.793s, episode steps: 71, steps per second: 89, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.085 [0.000, 4.000], mean observation: 0.490 [0.390, 0.520], loss: 7.881968, mean_absolute_error: 41.271378, mean_q: 53.026207
[F[K 228987/500000: episode: 3059, duration: 0.858s, episode steps: 84, steps per second: 98, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.060 [0.000, 4.000], mean observation: 0.507 [0.470, 0.560], loss: 7.244404, mean_absolute_error: 40.090328, mean_q: 51.471161
[F[K 229038/500000: episode: 3060, duration: 0.536s, episode steps: 51, steps per second: 95, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.451 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 7.833080, mean_absolute_error: 40.731049, mean_q: 52.312683
[F[K 229238/500000: episode: 3061, duration: 2.288s, episode steps: 200, steps per second: 87, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.760 [0.000, 4.000], mean observation: 0.483 [0.360, 0.520], loss: 7.395765, mean_absolute_error: 40.481949, mean_q: 51.989845
[F[K 229277/500000: episode: 3062, duration: 0.404s, episode steps: 39, steps per second: 97, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.692 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 10.007706, mean_absolute_error: 40.911598, mean_q: 52.474014
[F[K 229337/500000: episode: 3063, duration: 0.629s, episode steps: 60, steps per second: 95, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.150 [0.000, 4.000], mean observation: 0.508 [0.470, 0.620], loss: 8.117307, mean_absolute_error: 40.386326, mean_q: 51.875057
[F[K 229398/500000: episode: 3064, duration: 0.707s, episode steps: 61, steps per second: 86, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.508 [0.000, 4.000], mean observation: 0.499 [0.420, 0.580], loss: 7.179509, mean_absolute_error: 40.788239, mean_q: 52.361244
[F[K 229457/500000: episode: 3065, duration: 0.718s, episode steps: 59, steps per second: 82, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.506 [0.440, 0.600], loss: 9.133542, mean_absolute_error: 40.355865, mean_q: 51.852722
[F[K 229533/500000: episode: 3066, duration: 0.821s, episode steps: 76, steps per second: 93, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 9.059400, mean_absolute_error: 40.941124, mean_q: 52.394291
[F[K 229592/500000: episode: 3067, duration: 0.738s, episode steps: 59, steps per second: 80, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.610 [0.000, 4.000], mean observation: 0.505 [0.470, 0.540], loss: 10.023008, mean_absolute_error: 40.905159, mean_q: 52.366673
[F[K 229711/500000: episode: 3068, duration: 1.558s, episode steps: 119, steps per second: 76, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.782 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 7.328972, mean_absolute_error: 40.747166, mean_q: 52.251904
[F[K 229781/500000: episode: 3069, duration: 0.897s, episode steps: 70, steps per second: 78, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.515 [0.470, 0.580], loss: 9.129171, mean_absolute_error: 41.130272, mean_q: 52.730667
[F[K 229928/500000: episode: 3070, duration: 1.437s, episode steps: 147, steps per second: 102, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.082 [0.000, 4.000], mean observation: 0.496 [0.400, 0.550], loss: 9.371364, mean_absolute_error: 40.406788, mean_q: 51.832630
[F[K 229985/500000: episode: 3071, duration: 0.785s, episode steps: 57, steps per second: 73, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.506 [0.490, 0.540], loss: 8.824299, mean_absolute_error: 40.100281, mean_q: 51.442078
[F[K 230185/500000: episode: 3072, duration: 2.269s, episode steps: 200, steps per second: 88, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.090 [0.000, 4.000], mean observation: 0.497 [0.450, 0.520], loss: 8.221738, mean_absolute_error: 40.321815, mean_q: 51.690331
[F[K 230246/500000: episode: 3073, duration: 0.751s, episode steps: 61, steps per second: 81, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.507 [0.410, 0.660], loss: 8.506258, mean_absolute_error: 39.545662, mean_q: 50.838448
[F[K 230296/500000: episode: 3074, duration: 0.645s, episode steps: 50, steps per second: 78, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.477 [0.380, 0.530], loss: 7.292277, mean_absolute_error: 40.490185, mean_q: 51.992798
[F[K 230365/500000: episode: 3075, duration: 0.772s, episode steps: 69, steps per second: 89, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.145 [0.000, 4.000], mean observation: 0.519 [0.490, 0.610], loss: 8.635845, mean_absolute_error: 40.379002, mean_q: 51.788933
[F[K 230424/500000: episode: 3076, duration: 0.711s, episode steps: 59, steps per second: 83, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.153 [0.000, 4.000], mean observation: 0.504 [0.460, 0.600], loss: 9.528630, mean_absolute_error: 40.318031, mean_q: 51.715630
[F[K 230503/500000: episode: 3077, duration: 0.845s, episode steps: 79, steps per second: 93, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.506 [0.450, 0.610], loss: 8.450274, mean_absolute_error: 40.479763, mean_q: 51.907444
[F[K 230566/500000: episode: 3078, duration: 0.721s, episode steps: 63, steps per second: 87, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.794 [0.000, 4.000], mean observation: 0.502 [0.470, 0.580], loss: 8.102063, mean_absolute_error: 40.240952, mean_q: 51.659641
[F[K 230739/500000: episode: 3079, duration: 1.931s, episode steps: 173, steps per second: 90, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.277 [0.000, 4.000], mean observation: 0.483 [0.350, 0.510], loss: 8.517558, mean_absolute_error: 40.600925, mean_q: 51.985294
[F[K 230784/500000: episode: 3080, duration: 0.597s, episode steps: 45, steps per second: 75, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.022 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 8.363555, mean_absolute_error: 41.609535, mean_q: 53.338646
[F[K 230850/500000: episode: 3081, duration: 0.898s, episode steps: 66, steps per second: 74, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.076 [0.000, 4.000], mean observation: 0.512 [0.490, 0.590], loss: 7.524586, mean_absolute_error: 40.353031, mean_q: 51.876163
[F[K 230932/500000: episode: 3082, duration: 1.063s, episode steps: 82, steps per second: 77, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.720 [0.000, 4.000], mean observation: 0.495 [0.430, 0.530], loss: 9.009101, mean_absolute_error: 40.019566, mean_q: 51.265579
[F[K 231014/500000: episode: 3083, duration: 1.111s, episode steps: 82, steps per second: 74, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.488 [0.000, 4.000], mean observation: 0.480 [0.360, 0.530], loss: 9.197611, mean_absolute_error: 39.785892, mean_q: 51.057854
[F[K 231079/500000: episode: 3084, duration: 0.853s, episode steps: 65, steps per second: 76, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.308 [0.000, 4.000], mean observation: 0.503 [0.430, 0.600], loss: 9.802186, mean_absolute_error: 40.744015, mean_q: 52.302856
[F[K 231169/500000: episode: 3085, duration: 1.082s, episode steps: 90, steps per second: 83, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.878 [0.000, 4.000], mean observation: 0.502 [0.470, 0.550], loss: 8.642579, mean_absolute_error: 40.085999, mean_q: 51.467514
[F[K 231369/500000: episode: 3086, duration: 2.752s, episode steps: 200, steps per second: 73, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.265 [0.000, 4.000], mean observation: 0.490 [0.390, 0.520], loss: 8.896076, mean_absolute_error: 40.394897, mean_q: 51.784256
[F[K 231465/500000: episode: 3087, duration: 1.280s, episode steps: 96, steps per second: 75, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.073 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 8.533607, mean_absolute_error: 40.313068, mean_q: 51.737835
[F[K 231516/500000: episode: 3088, duration: 0.662s, episode steps: 51, steps per second: 77, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.511 [0.470, 0.620], loss: 8.906043, mean_absolute_error: 39.815914, mean_q: 51.077839
[F[K 231583/500000: episode: 3089, duration: 0.929s, episode steps: 67, steps per second: 72, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.522 [0.000, 4.000], mean observation: 0.487 [0.390, 0.530], loss: 9.347797, mean_absolute_error: 40.336948, mean_q: 51.734253
[F[K 231620/500000: episode: 3090, duration: 0.590s, episode steps: 37, steps per second: 63, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 0.499 [0.350, 0.640], loss: 7.247507, mean_absolute_error: 39.992748, mean_q: 51.396976
[F[K 231773/500000: episode: 3091, duration: 2.074s, episode steps: 153, steps per second: 74, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.052 [0.000, 4.000], mean observation: 0.475 [0.330, 0.530], loss: 7.296047, mean_absolute_error: 40.317356, mean_q: 51.756180
[F[K 231826/500000: episode: 3092, duration: 0.734s, episode steps: 53, steps per second: 72, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.283 [0.000, 4.000], mean observation: 0.505 [0.460, 0.590], loss: 7.328755, mean_absolute_error: 39.790771, mean_q: 51.137253
[F[K 231890/500000: episode: 3093, duration: 0.706s, episode steps: 64, steps per second: 91, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.484 [0.420, 0.510], loss: 7.872138, mean_absolute_error: 40.152603, mean_q: 51.446068
[F[K 232021/500000: episode: 3094, duration: 1.799s, episode steps: 131, steps per second: 73, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.244 [0.000, 4.000], mean observation: 0.507 [0.470, 0.560], loss: 7.831931, mean_absolute_error: 40.376183, mean_q: 51.918243
[F[K 232163/500000: episode: 3095, duration: 1.974s, episode steps: 142, steps per second: 72, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.049 [0.000, 4.000], mean observation: 0.519 [0.480, 0.650], loss: 7.937820, mean_absolute_error: 40.593876, mean_q: 52.192169
[F[K 232230/500000: episode: 3096, duration: 1.004s, episode steps: 67, steps per second: 67, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.498 [0.470, 0.530], loss: 9.404406, mean_absolute_error: 41.138145, mean_q: 52.796722
[F[K 232296/500000: episode: 3097, duration: 0.995s, episode steps: 66, steps per second: 66, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.712 [0.000, 4.000], mean observation: 0.493 [0.390, 0.580], loss: 7.795650, mean_absolute_error: 40.885799, mean_q: 52.429943
[F[K 232400/500000: episode: 3098, duration: 1.646s, episode steps: 104, steps per second: 63, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.135 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 7.923875, mean_absolute_error: 40.226479, mean_q: 51.635979
[F[K 232442/500000: episode: 3099, duration: 0.591s, episode steps: 42, steps per second: 71, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.381 [0.000, 4.000], mean observation: 0.514 [0.470, 0.650], loss: 9.407985, mean_absolute_error: 40.339924, mean_q: 51.744507
[F[K 232502/500000: episode: 3100, duration: 0.908s, episode steps: 60, steps per second: 66, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.767 [0.000, 4.000], mean observation: 0.497 [0.380, 0.560], loss: 9.699223, mean_absolute_error: 40.795349, mean_q: 52.191227
[F[K 232702/500000: episode: 3101, duration: 2.982s, episode steps: 200, steps per second: 67, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.245 [0.000, 4.000], mean observation: 0.495 [0.390, 0.550], loss: 7.858654, mean_absolute_error: 40.268845, mean_q: 51.752422
[F[K 232776/500000: episode: 3102, duration: 1.043s, episode steps: 74, steps per second: 71, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.489 [0.360, 0.530], loss: 7.771430, mean_absolute_error: 40.548058, mean_q: 51.885548
[F[K 232829/500000: episode: 3103, duration: 0.774s, episode steps: 53, steps per second: 68, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.396 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 8.005796, mean_absolute_error: 41.916523, mean_q: 53.764187
[F[K 232881/500000: episode: 3104, duration: 0.797s, episode steps: 52, steps per second: 65, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.769 [0.000, 4.000], mean observation: 0.502 [0.380, 0.630], loss: 8.808696, mean_absolute_error: 40.756779, mean_q: 52.120090
[F[K 232937/500000: episode: 3105, duration: 0.858s, episode steps: 56, steps per second: 65, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 7.575468, mean_absolute_error: 40.574768, mean_q: 52.082584
[F[K 233046/500000: episode: 3106, duration: 1.739s, episode steps: 109, steps per second: 63, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.936 [0.000, 4.000], mean observation: 0.484 [0.380, 0.530], loss: 7.812025, mean_absolute_error: 40.720249, mean_q: 52.319359
[F[K 233092/500000: episode: 3107, duration: 0.753s, episode steps: 46, steps per second: 61, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.543 [0.000, 4.000], mean observation: 0.505 [0.420, 0.630], loss: 8.352139, mean_absolute_error: 39.614380, mean_q: 50.939846
[F[K 233143/500000: episode: 3108, duration: 0.690s, episode steps: 51, steps per second: 74, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.392 [0.000, 4.000], mean observation: 0.516 [0.470, 0.580], loss: 8.900064, mean_absolute_error: 40.817039, mean_q: 52.396034
[F[K 233210/500000: episode: 3109, duration: 0.988s, episode steps: 67, steps per second: 68, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.597 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 7.824060, mean_absolute_error: 40.319427, mean_q: 51.817722
[F[K 233258/500000: episode: 3110, duration: 0.577s, episode steps: 48, steps per second: 83, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.500 [0.360, 0.640], loss: 7.632555, mean_absolute_error: 40.383865, mean_q: 51.876286
[F[K 233327/500000: episode: 3111, duration: 1.145s, episode steps: 69, steps per second: 60, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.491 [0.380, 0.530], loss: 8.120808, mean_absolute_error: 40.203003, mean_q: 51.607300
[F[K 233409/500000: episode: 3112, duration: 1.218s, episode steps: 82, steps per second: 67, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.244 [0.000, 4.000], mean observation: 0.492 [0.400, 0.540], loss: 8.359597, mean_absolute_error: 40.294064, mean_q: 51.736870
[F[K 233456/500000: episode: 3113, duration: 0.702s, episode steps: 47, steps per second: 67, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.872 [0.000, 4.000], mean observation: 0.517 [0.470, 0.610], loss: 8.900760, mean_absolute_error: 41.013371, mean_q: 52.509956
[F[K 233536/500000: episode: 3114, duration: 1.258s, episode steps: 80, steps per second: 64, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.962 [0.000, 4.000], mean observation: 0.498 [0.460, 0.530], loss: 7.762965, mean_absolute_error: 40.766388, mean_q: 52.206318
[F[K 233594/500000: episode: 3115, duration: 0.851s, episode steps: 58, steps per second: 68, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.397 [0.000, 4.000], mean observation: 0.481 [0.390, 0.530], loss: 7.946991, mean_absolute_error: 40.791668, mean_q: 52.378288
[F[K 233666/500000: episode: 3116, duration: 0.946s, episode steps: 72, steps per second: 76, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.236 [0.000, 4.000], mean observation: 0.475 [0.360, 0.530], loss: 7.962836, mean_absolute_error: 40.240513, mean_q: 51.657368
[F[K 233743/500000: episode: 3117, duration: 1.212s, episode steps: 77, steps per second: 64, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.078 [0.000, 4.000], mean observation: 0.513 [0.480, 0.630], loss: 8.069201, mean_absolute_error: 41.106754, mean_q: 52.714188
[F[K 233787/500000: episode: 3118, duration: 0.649s, episode steps: 44, steps per second: 68, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.295 [0.000, 3.000], mean observation: 0.522 [0.470, 0.620], loss: 7.620949, mean_absolute_error: 40.875187, mean_q: 52.372337
[F[K 233838/500000: episode: 3119, duration: 0.756s, episode steps: 51, steps per second: 67, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.176 [0.000, 4.000], mean observation: 0.525 [0.490, 0.650], loss: 8.891052, mean_absolute_error: 40.732574, mean_q: 52.237343
[F[K 233933/500000: episode: 3120, duration: 1.544s, episode steps: 95, steps per second: 62, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.463 [0.000, 4.000], mean observation: 0.488 [0.420, 0.530], loss: 9.113401, mean_absolute_error: 40.477875, mean_q: 51.990822
[F[K 234028/500000: episode: 3121, duration: 1.511s, episode steps: 95, steps per second: 63, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.379 [0.000, 4.000], mean observation: 0.498 [0.460, 0.550], loss: 8.354525, mean_absolute_error: 40.564709, mean_q: 52.088058
[F[K 234146/500000: episode: 3122, duration: 2.076s, episode steps: 118, steps per second: 57, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.034 [0.000, 4.000], mean observation: 0.485 [0.360, 0.530], loss: 7.772258, mean_absolute_error: 40.132286, mean_q: 51.479507
[F[K 234225/500000: episode: 3123, duration: 1.059s, episode steps: 79, steps per second: 75, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.076 [0.000, 4.000], mean observation: 0.490 [0.430, 0.530], loss: 8.599893, mean_absolute_error: 40.059254, mean_q: 51.350605
[F[K 234287/500000: episode: 3124, duration: 0.866s, episode steps: 62, steps per second: 72, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.502 [0.430, 0.560], loss: 6.923810, mean_absolute_error: 40.211830, mean_q: 51.519520
[F[K 234328/500000: episode: 3125, duration: 0.554s, episode steps: 41, steps per second: 74, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.829 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 7.872658, mean_absolute_error: 40.873947, mean_q: 52.500275
[F[K 234389/500000: episode: 3126, duration: 0.873s, episode steps: 61, steps per second: 70, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.689 [0.000, 4.000], mean observation: 0.504 [0.360, 0.640], loss: 8.243633, mean_absolute_error: 40.907055, mean_q: 52.414879
[F[K 234438/500000: episode: 3127, duration: 0.645s, episode steps: 49, steps per second: 76, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.918 [0.000, 4.000], mean observation: 0.518 [0.480, 0.630], loss: 6.667273, mean_absolute_error: 40.620171, mean_q: 52.124191
[F[K 234554/500000: episode: 3128, duration: 1.634s, episode steps: 116, steps per second: 71, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.498 [0.400, 0.550], loss: 8.416489, mean_absolute_error: 40.811501, mean_q: 52.315479
[F[K 234614/500000: episode: 3129, duration: 0.781s, episode steps: 60, steps per second: 77, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.519 [0.480, 0.640], loss: 7.608052, mean_absolute_error: 41.126896, mean_q: 52.750378
[F[K 234660/500000: episode: 3130, duration: 0.654s, episode steps: 46, steps per second: 70, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.739 [0.000, 4.000], mean observation: 0.501 [0.460, 0.530], loss: 7.766374, mean_absolute_error: 40.974606, mean_q: 52.690357
[F[K 234737/500000: episode: 3131, duration: 1.128s, episode steps: 77, steps per second: 68, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.623 [0.000, 4.000], mean observation: 0.504 [0.470, 0.530], loss: 7.510687, mean_absolute_error: 40.557682, mean_q: 51.959133
[F[K 234807/500000: episode: 3132, duration: 0.950s, episode steps: 70, steps per second: 74, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.543 [0.000, 4.000], mean observation: 0.489 [0.430, 0.530], loss: 7.575993, mean_absolute_error: 40.527824, mean_q: 52.062149
[F[K 234888/500000: episode: 3133, duration: 1.067s, episode steps: 81, steps per second: 76, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.185 [0.000, 4.000], mean observation: 0.511 [0.480, 0.620], loss: 8.186968, mean_absolute_error: 41.110645, mean_q: 52.785797
[F[K 234951/500000: episode: 3134, duration: 0.837s, episode steps: 63, steps per second: 75, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.516 [0.490, 0.570], loss: 8.240670, mean_absolute_error: 40.831459, mean_q: 52.456150
[F[K 235000/500000: episode: 3135, duration: 0.725s, episode steps: 49, steps per second: 68, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.408 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 7.930095, mean_absolute_error: 41.286461, mean_q: 52.771057
[F[K 235102/500000: episode: 3136, duration: 1.383s, episode steps: 102, steps per second: 74, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.990 [0.000, 4.000], mean observation: 0.505 [0.470, 0.570], loss: 7.952433, mean_absolute_error: 40.799999, mean_q: 52.402267
[F[K 235210/500000: episode: 3137, duration: 1.589s, episode steps: 108, steps per second: 68, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.620 [0.000, 4.000], mean observation: 0.499 [0.440, 0.530], loss: 8.860798, mean_absolute_error: 40.707226, mean_q: 52.171314
[F[K 235292/500000: episode: 3138, duration: 1.254s, episode steps: 82, steps per second: 65, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.976 [0.000, 4.000], mean observation: 0.497 [0.380, 0.610], loss: 8.128516, mean_absolute_error: 40.766270, mean_q: 52.309834
[F[K 235364/500000: episode: 3139, duration: 1.114s, episode steps: 72, steps per second: 65, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.014 [0.000, 4.000], mean observation: 0.507 [0.440, 0.630], loss: 7.808702, mean_absolute_error: 40.785568, mean_q: 52.240635
[F[K 235437/500000: episode: 3140, duration: 1.163s, episode steps: 73, steps per second: 63, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.356 [0.000, 4.000], mean observation: 0.518 [0.480, 0.640], loss: 7.255949, mean_absolute_error: 41.454437, mean_q: 53.166782
[F[K 235563/500000: episode: 3141, duration: 1.928s, episode steps: 126, steps per second: 65, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.302 [0.000, 4.000], mean observation: 0.484 [0.330, 0.530], loss: 7.810395, mean_absolute_error: 40.604057, mean_q: 52.105965
[F[K 235620/500000: episode: 3142, duration: 0.760s, episode steps: 57, steps per second: 75, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.035 [0.000, 4.000], mean observation: 0.492 [0.380, 0.530], loss: 7.860542, mean_absolute_error: 40.341049, mean_q: 51.792721
[F[K 235709/500000: episode: 3143, duration: 1.290s, episode steps: 89, steps per second: 69, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.483 [0.000, 4.000], mean observation: 0.512 [0.470, 0.610], loss: 8.485699, mean_absolute_error: 40.967484, mean_q: 52.499889
[F[K 235755/500000: episode: 3144, duration: 0.705s, episode steps: 46, steps per second: 65, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.391 [0.000, 4.000], mean observation: 0.501 [0.370, 0.650], loss: 8.021176, mean_absolute_error: 40.284027, mean_q: 51.569279
[F[K 235791/500000: episode: 3145, duration: 0.577s, episode steps: 36, steps per second: 62, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.389 [0.000, 3.000], mean observation: 0.504 [0.390, 0.650], loss: 8.759578, mean_absolute_error: 40.823898, mean_q: 52.367821
[F[K 235963/500000: episode: 3146, duration: 2.502s, episode steps: 172, steps per second: 69, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.523 [0.000, 4.000], mean observation: 0.504 [0.460, 0.570], loss: 7.296253, mean_absolute_error: 41.199150, mean_q: 52.857098
[F[K 236024/500000: episode: 3147, duration: 0.947s, episode steps: 61, steps per second: 64, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.262 [0.000, 4.000], mean observation: 0.513 [0.470, 0.600], loss: 7.678175, mean_absolute_error: 40.692909, mean_q: 52.194386
[F[K 236086/500000: episode: 3148, duration: 0.833s, episode steps: 62, steps per second: 74, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.355 [0.000, 4.000], mean observation: 0.494 [0.410, 0.550], loss: 8.401293, mean_absolute_error: 41.091293, mean_q: 52.702827
[F[K 236174/500000: episode: 3149, duration: 1.326s, episode steps: 88, steps per second: 66, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.977 [0.000, 4.000], mean observation: 0.517 [0.470, 0.630], loss: 7.775386, mean_absolute_error: 41.285934, mean_q: 52.980885
[F[K 236244/500000: episode: 3150, duration: 1.136s, episode steps: 70, steps per second: 62, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.511 [0.490, 0.570], loss: 7.192848, mean_absolute_error: 41.867092, mean_q: 53.694847
[F[K 236294/500000: episode: 3151, duration: 0.829s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.740 [0.000, 4.000], mean observation: 0.525 [0.480, 0.650], loss: 8.890044, mean_absolute_error: 40.971725, mean_q: 52.439873
[F[K 236360/500000: episode: 3152, duration: 1.169s, episode steps: 66, steps per second: 56, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.924 [0.000, 4.000], mean observation: 0.493 [0.430, 0.520], loss: 8.262121, mean_absolute_error: 40.604042, mean_q: 51.912506
[F[K 236396/500000: episode: 3153, duration: 0.593s, episode steps: 36, steps per second: 61, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.472 [0.000, 4.000], mean observation: 0.499 [0.350, 0.640], loss: 9.407671, mean_absolute_error: 42.485126, mean_q: 54.249714
[F[K 236491/500000: episode: 3154, duration: 1.508s, episode steps: 95, steps per second: 63, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.481 [0.350, 0.500], loss: 8.125772, mean_absolute_error: 41.357735, mean_q: 52.975544
[F[K 236530/500000: episode: 3155, duration: 0.504s, episode steps: 39, steps per second: 77, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.512 [0.470, 0.650], loss: 8.819117, mean_absolute_error: 40.814709, mean_q: 52.416790
[F[K 236580/500000: episode: 3156, duration: 0.648s, episode steps: 50, steps per second: 77, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 0.486 [0.430, 0.510], loss: 9.251230, mean_absolute_error: 41.220535, mean_q: 52.697384
[F[K 236629/500000: episode: 3157, duration: 0.617s, episode steps: 49, steps per second: 79, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.878 [0.000, 4.000], mean observation: 0.513 [0.470, 0.630], loss: 9.472260, mean_absolute_error: 42.346626, mean_q: 54.221027
[F[K 236701/500000: episode: 3158, duration: 0.970s, episode steps: 72, steps per second: 74, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.931 [0.000, 4.000], mean observation: 0.486 [0.390, 0.510], loss: 8.036924, mean_absolute_error: 41.413288, mean_q: 53.129593
[F[K 236770/500000: episode: 3159, duration: 0.915s, episode steps: 69, steps per second: 75, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.116 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 7.717915, mean_absolute_error: 41.468544, mean_q: 53.172256
[F[K 236863/500000: episode: 3160, duration: 1.280s, episode steps: 93, steps per second: 73, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.247 [0.000, 4.000], mean observation: 0.520 [0.480, 0.640], loss: 8.497421, mean_absolute_error: 40.916191, mean_q: 52.506336
[F[K 236933/500000: episode: 3161, duration: 0.903s, episode steps: 70, steps per second: 78, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.743 [0.000, 4.000], mean observation: 0.492 [0.370, 0.550], loss: 7.781586, mean_absolute_error: 41.208088, mean_q: 52.832047
[F[K 237025/500000: episode: 3162, duration: 1.311s, episode steps: 92, steps per second: 70, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.098 [0.000, 4.000], mean observation: 0.512 [0.470, 0.630], loss: 7.908157, mean_absolute_error: 41.628178, mean_q: 53.435822
[F[K 237066/500000: episode: 3163, duration: 0.580s, episode steps: 41, steps per second: 71, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.878 [0.000, 4.000], mean observation: 0.524 [0.470, 0.620], loss: 7.993701, mean_absolute_error: 41.636822, mean_q: 53.360935
[F[K 237129/500000: episode: 3164, duration: 0.851s, episode steps: 63, steps per second: 74, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.317 [0.000, 4.000], mean observation: 0.499 [0.440, 0.560], loss: 9.593263, mean_absolute_error: 41.274113, mean_q: 52.980350
[F[K 237201/500000: episode: 3165, duration: 1.050s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 0.489 [0.380, 0.530], loss: 8.756359, mean_absolute_error: 41.612106, mean_q: 53.433224
[F[K 237294/500000: episode: 3166, duration: 1.194s, episode steps: 93, steps per second: 78, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.419 [0.000, 4.000], mean observation: 0.521 [0.470, 0.640], loss: 7.427056, mean_absolute_error: 42.405426, mean_q: 54.297375
[F[K 237364/500000: episode: 3167, duration: 0.854s, episode steps: 70, steps per second: 82, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.499 [0.440, 0.550], loss: 7.435972, mean_absolute_error: 41.127571, mean_q: 52.878941
[F[K 237465/500000: episode: 3168, duration: 1.344s, episode steps: 101, steps per second: 75, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.495 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 8.984297, mean_absolute_error: 40.662495, mean_q: 52.207157
[F[K 237554/500000: episode: 3169, duration: 1.279s, episode steps: 89, steps per second: 70, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.157 [0.000, 4.000], mean observation: 0.487 [0.410, 0.530], loss: 8.389626, mean_absolute_error: 41.286644, mean_q: 53.043869
[F[K 237644/500000: episode: 3170, duration: 1.010s, episode steps: 90, steps per second: 89, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.389 [0.000, 4.000], mean observation: 0.506 [0.440, 0.610], loss: 8.809511, mean_absolute_error: 41.331337, mean_q: 52.971985
[F[K 237704/500000: episode: 3171, duration: 0.758s, episode steps: 60, steps per second: 79, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.783 [0.000, 4.000], mean observation: 0.518 [0.500, 0.600], loss: 7.088891, mean_absolute_error: 40.849964, mean_q: 52.428146
[F[K 237825/500000: episode: 3172, duration: 1.518s, episode steps: 121, steps per second: 80, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.967 [0.000, 4.000], mean observation: 0.501 [0.460, 0.550], loss: 8.687346, mean_absolute_error: 41.112324, mean_q: 52.685097
[F[K 237910/500000: episode: 3173, duration: 1.190s, episode steps: 85, steps per second: 71, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.519 [0.470, 0.620], loss: 8.983871, mean_absolute_error: 41.163692, mean_q: 52.896484
[F[K 237968/500000: episode: 3174, duration: 0.844s, episode steps: 58, steps per second: 69, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.086 [0.000, 4.000], mean observation: 0.495 [0.380, 0.550], loss: 6.994791, mean_absolute_error: 41.011395, mean_q: 52.868748
[F[K 238168/500000: episode: 3175, duration: 2.640s, episode steps: 200, steps per second: 76, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.135 [0.000, 4.000], mean observation: 0.482 [0.370, 0.510], loss: 9.195274, mean_absolute_error: 40.858727, mean_q: 52.405289
[F[K 238341/500000: episode: 3176, duration: 2.311s, episode steps: 173, steps per second: 75, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.133 [0.000, 4.000], mean observation: 0.482 [0.390, 0.510], loss: 8.790980, mean_absolute_error: 40.730244, mean_q: 52.250576
[F[K 238390/500000: episode: 3177, duration: 0.603s, episode steps: 49, steps per second: 81, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.551 [0.000, 4.000], mean observation: 0.518 [0.470, 0.650], loss: 7.767324, mean_absolute_error: 41.566700, mean_q: 53.457554
[F[K 238451/500000: episode: 3178, duration: 0.877s, episode steps: 61, steps per second: 70, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.393 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 7.961205, mean_absolute_error: 40.978504, mean_q: 52.670815
[F[K 238514/500000: episode: 3179, duration: 0.756s, episode steps: 63, steps per second: 83, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.079 [0.000, 4.000], mean observation: 0.499 [0.380, 0.610], loss: 7.060119, mean_absolute_error: 41.325001, mean_q: 53.160183
[F[K 238662/500000: episode: 3180, duration: 1.742s, episode steps: 148, steps per second: 85, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.149 [0.000, 4.000], mean observation: 0.506 [0.440, 0.640], loss: 8.375032, mean_absolute_error: 41.106247, mean_q: 52.834328
[F[K 238737/500000: episode: 3181, duration: 0.883s, episode steps: 75, steps per second: 85, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 0.501 [0.410, 0.570], loss: 8.222387, mean_absolute_error: 40.886032, mean_q: 52.572491
[F[K 238856/500000: episode: 3182, duration: 1.265s, episode steps: 119, steps per second: 94, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.513 [0.000, 4.000], mean observation: 0.506 [0.470, 0.560], loss: 8.232461, mean_absolute_error: 41.332214, mean_q: 53.079586
[F[K 238937/500000: episode: 3183, duration: 0.968s, episode steps: 81, steps per second: 84, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.642 [0.000, 4.000], mean observation: 0.500 [0.390, 0.590], loss: 7.827569, mean_absolute_error: 41.662113, mean_q: 53.498295
[F[K 239029/500000: episode: 3184, duration: 0.991s, episode steps: 92, steps per second: 93, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 0.496 [0.380, 0.600], loss: 8.005099, mean_absolute_error: 41.243214, mean_q: 52.878525
[F[K 239091/500000: episode: 3185, duration: 0.658s, episode steps: 62, steps per second: 94, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.935 [0.000, 4.000], mean observation: 0.496 [0.420, 0.530], loss: 8.390471, mean_absolute_error: 40.812042, mean_q: 52.329876
[F[K 239141/500000: episode: 3186, duration: 0.478s, episode steps: 50, steps per second: 105, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.220 [0.000, 4.000], mean observation: 0.520 [0.470, 0.630], loss: 8.555031, mean_absolute_error: 41.444393, mean_q: 53.102486
[F[K 239218/500000: episode: 3187, duration: 0.769s, episode steps: 77, steps per second: 100, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.104 [0.000, 4.000], mean observation: 0.510 [0.480, 0.550], loss: 8.831757, mean_absolute_error: 41.026772, mean_q: 52.725189
[F[K 239263/500000: episode: 3188, duration: 0.492s, episode steps: 45, steps per second: 91, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.778 [0.000, 4.000], mean observation: 0.520 [0.470, 0.630], loss: 8.443312, mean_absolute_error: 40.558403, mean_q: 51.995144
[F[K 239325/500000: episode: 3189, duration: 0.705s, episode steps: 62, steps per second: 88, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.935 [0.000, 4.000], mean observation: 0.500 [0.450, 0.530], loss: 8.304135, mean_absolute_error: 41.134830, mean_q: 52.779163
[F[K 239497/500000: episode: 3190, duration: 1.854s, episode steps: 172, steps per second: 93, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.267 [0.000, 4.000], mean observation: 0.510 [0.460, 0.600], loss: 9.296293, mean_absolute_error: 41.205761, mean_q: 52.824890
[F[K 239537/500000: episode: 3191, duration: 0.444s, episode steps: 40, steps per second: 90, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.100 [0.000, 4.000], mean observation: 0.512 [0.470, 0.620], loss: 9.246482, mean_absolute_error: 41.443562, mean_q: 53.178413
[F[K 239578/500000: episode: 3192, duration: 0.480s, episode steps: 41, steps per second: 85, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.707 [0.000, 4.000], mean observation: 0.508 [0.450, 0.630], loss: 7.627036, mean_absolute_error: 41.371780, mean_q: 53.054092
[F[K 239671/500000: episode: 3193, duration: 0.988s, episode steps: 93, steps per second: 94, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.022 [0.000, 4.000], mean observation: 0.513 [0.470, 0.620], loss: 7.777579, mean_absolute_error: 40.852215, mean_q: 52.486427
[F[K 239752/500000: episode: 3194, duration: 0.988s, episode steps: 81, steps per second: 82, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 0.482 [0.370, 0.530], loss: 7.615345, mean_absolute_error: 40.993752, mean_q: 52.689102
[F[K 239817/500000: episode: 3195, duration: 0.753s, episode steps: 65, steps per second: 86, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.479 [0.360, 0.520], loss: 6.860364, mean_absolute_error: 40.661362, mean_q: 52.244053
[F[K 239960/500000: episode: 3196, duration: 1.713s, episode steps: 143, steps per second: 83, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.168 [0.000, 4.000], mean observation: 0.516 [0.470, 0.620], loss: 8.590732, mean_absolute_error: 41.217331, mean_q: 52.922508
[F[K 240036/500000: episode: 3197, duration: 0.911s, episode steps: 76, steps per second: 83, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.276 [0.000, 4.000], mean observation: 0.504 [0.470, 0.550], loss: 7.794094, mean_absolute_error: 41.554150, mean_q: 53.359398
[F[K 240134/500000: episode: 3198, duration: 1.106s, episode steps: 98, steps per second: 89, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.031 [0.000, 4.000], mean observation: 0.483 [0.410, 0.510], loss: 8.600453, mean_absolute_error: 40.904160, mean_q: 52.415901
[F[K 240190/500000: episode: 3199, duration: 0.696s, episode steps: 56, steps per second: 80, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.161 [0.000, 4.000], mean observation: 0.482 [0.420, 0.500], loss: 9.524193, mean_absolute_error: 40.738682, mean_q: 52.285740
[F[K 240244/500000: episode: 3200, duration: 0.519s, episode steps: 54, steps per second: 104, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.093 [0.000, 4.000], mean observation: 0.475 [0.370, 0.510], loss: 7.642242, mean_absolute_error: 41.351460, mean_q: 53.060108
[F[K 240331/500000: episode: 3201, duration: 0.932s, episode steps: 87, steps per second: 93, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.515 [0.480, 0.580], loss: 8.149796, mean_absolute_error: 41.257454, mean_q: 52.935795
[F[K 240404/500000: episode: 3202, duration: 0.853s, episode steps: 73, steps per second: 86, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 8.578248, mean_absolute_error: 41.535782, mean_q: 53.409706
[F[K 240499/500000: episode: 3203, duration: 1.204s, episode steps: 95, steps per second: 79, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.442 [0.000, 4.000], mean observation: 0.503 [0.430, 0.590], loss: 8.328439, mean_absolute_error: 41.216385, mean_q: 52.887276
[F[K 240556/500000: episode: 3204, duration: 0.654s, episode steps: 57, steps per second: 87, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 0.517 [0.490, 0.580], loss: 6.883629, mean_absolute_error: 42.007103, mean_q: 53.949734
[F[K 240623/500000: episode: 3205, duration: 0.758s, episode steps: 67, steps per second: 88, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.254 [0.000, 4.000], mean observation: 0.496 [0.380, 0.590], loss: 9.167215, mean_absolute_error: 41.808868, mean_q: 53.514645
[F[K 240667/500000: episode: 3206, duration: 0.517s, episode steps: 44, steps per second: 85, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.523 [0.470, 0.640], loss: 7.905025, mean_absolute_error: 42.185196, mean_q: 54.054298
[F[K 240737/500000: episode: 3207, duration: 0.807s, episode steps: 70, steps per second: 87, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.514 [0.000, 4.000], mean observation: 0.512 [0.480, 0.620], loss: 8.038356, mean_absolute_error: 41.297832, mean_q: 52.927570
[F[K 240795/500000: episode: 3208, duration: 0.663s, episode steps: 58, steps per second: 88, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.155 [0.000, 4.000], mean observation: 0.523 [0.470, 0.630], loss: 9.657641, mean_absolute_error: 41.426220, mean_q: 53.080124
[F[K 240844/500000: episode: 3209, duration: 0.555s, episode steps: 49, steps per second: 88, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 0.511 [0.470, 0.630], loss: 6.748125, mean_absolute_error: 40.855064, mean_q: 52.511681
[F[K 240906/500000: episode: 3210, duration: 0.725s, episode steps: 62, steps per second: 86, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.774 [0.000, 4.000], mean observation: 0.491 [0.390, 0.530], loss: 8.590883, mean_absolute_error: 41.364697, mean_q: 52.951546
[F[K 241013/500000: episode: 3211, duration: 1.223s, episode steps: 107, steps per second: 87, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.813 [0.000, 4.000], mean observation: 0.504 [0.380, 0.650], loss: 8.284335, mean_absolute_error: 41.183823, mean_q: 52.869747
[F[K 241055/500000: episode: 3212, duration: 0.552s, episode steps: 42, steps per second: 76, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.429 [0.000, 4.000], mean observation: 0.512 [0.470, 0.620], loss: 8.240599, mean_absolute_error: 41.383186, mean_q: 52.959553
[F[K 241125/500000: episode: 3213, duration: 0.820s, episode steps: 70, steps per second: 85, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.929 [0.000, 4.000], mean observation: 0.502 [0.460, 0.560], loss: 8.873606, mean_absolute_error: 41.988647, mean_q: 53.932323
[F[K 241210/500000: episode: 3214, duration: 0.978s, episode steps: 85, steps per second: 87, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.882 [0.000, 4.000], mean observation: 0.509 [0.490, 0.580], loss: 10.936342, mean_absolute_error: 41.728863, mean_q: 53.462753
[F[K 241303/500000: episode: 3215, duration: 1.119s, episode steps: 93, steps per second: 83, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.247 [0.000, 4.000], mean observation: 0.505 [0.470, 0.570], loss: 8.490490, mean_absolute_error: 42.223118, mean_q: 54.100544
[F[K 241364/500000: episode: 3216, duration: 0.735s, episode steps: 61, steps per second: 83, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.393 [0.000, 4.000], mean observation: 0.512 [0.480, 0.590], loss: 9.186227, mean_absolute_error: 40.555847, mean_q: 51.991745
[F[K 241406/500000: episode: 3217, duration: 0.528s, episode steps: 42, steps per second: 80, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.643 [0.000, 4.000], mean observation: 0.499 [0.370, 0.630], loss: 7.111513, mean_absolute_error: 41.499130, mean_q: 53.278141
[F[K 241471/500000: episode: 3218, duration: 0.775s, episode steps: 65, steps per second: 84, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.662 [0.000, 4.000], mean observation: 0.508 [0.470, 0.610], loss: 9.039001, mean_absolute_error: 41.598400, mean_q: 53.388649
[F[K 241577/500000: episode: 3219, duration: 1.215s, episode steps: 106, steps per second: 87, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 8.191781, mean_absolute_error: 41.759102, mean_q: 53.458027
[F[K 241691/500000: episode: 3220, duration: 1.337s, episode steps: 114, steps per second: 85, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.482 [0.000, 4.000], mean observation: 0.475 [0.380, 0.520], loss: 7.554314, mean_absolute_error: 41.732559, mean_q: 53.358105
[F[K 241769/500000: episode: 3221, duration: 1.044s, episode steps: 78, steps per second: 75, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.808 [0.000, 4.000], mean observation: 0.503 [0.450, 0.560], loss: 7.936812, mean_absolute_error: 41.303104, mean_q: 52.933098
[F[K 241865/500000: episode: 3222, duration: 1.133s, episode steps: 96, steps per second: 85, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.844 [0.000, 4.000], mean observation: 0.500 [0.450, 0.530], loss: 9.767934, mean_absolute_error: 41.117870, mean_q: 52.697659
[F[K 241922/500000: episode: 3223, duration: 0.698s, episode steps: 57, steps per second: 82, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.649 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 9.633562, mean_absolute_error: 41.495537, mean_q: 53.160538
[F[K 241995/500000: episode: 3224, duration: 0.864s, episode steps: 73, steps per second: 84, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.342 [0.000, 4.000], mean observation: 0.504 [0.480, 0.560], loss: 7.923277, mean_absolute_error: 41.136055, mean_q: 52.705803
[F[K 242049/500000: episode: 3225, duration: 0.652s, episode steps: 54, steps per second: 83, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.352 [0.000, 4.000], mean observation: 0.497 [0.360, 0.610], loss: 12.016168, mean_absolute_error: 41.067749, mean_q: 52.532616
[F[K 242099/500000: episode: 3226, duration: 0.616s, episode steps: 50, steps per second: 81, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 0.518 [0.470, 0.650], loss: 8.641553, mean_absolute_error: 41.933582, mean_q: 53.871971
[F[K 242218/500000: episode: 3227, duration: 1.483s, episode steps: 119, steps per second: 80, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.482 [0.360, 0.520], loss: 7.816417, mean_absolute_error: 41.251045, mean_q: 52.885460
[F[K 242267/500000: episode: 3228, duration: 0.541s, episode steps: 49, steps per second: 91, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.514 [0.480, 0.650], loss: 8.477282, mean_absolute_error: 41.407753, mean_q: 53.021896
[F[K 242330/500000: episode: 3229, duration: 0.720s, episode steps: 63, steps per second: 88, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.365 [0.000, 4.000], mean observation: 0.502 [0.440, 0.590], loss: 8.299824, mean_absolute_error: 41.264118, mean_q: 52.890690
[F[K 242412/500000: episode: 3230, duration: 0.984s, episode steps: 82, steps per second: 83, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.573 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 8.714066, mean_absolute_error: 41.652073, mean_q: 53.395405
[F[K 242612/500000: episode: 3231, duration: 2.265s, episode steps: 200, steps per second: 88, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.507 [0.480, 0.560], loss: 8.028881, mean_absolute_error: 41.370968, mean_q: 53.121445
[F[K 242663/500000: episode: 3232, duration: 0.726s, episode steps: 51, steps per second: 70, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.020 [0.000, 4.000], mean observation: 0.506 [0.480, 0.560], loss: 7.436636, mean_absolute_error: 41.861816, mean_q: 53.729252
[F[K 242726/500000: episode: 3233, duration: 0.766s, episode steps: 63, steps per second: 82, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.444 [0.000, 4.000], mean observation: 0.502 [0.400, 0.590], loss: 7.894291, mean_absolute_error: 41.249504, mean_q: 52.877419
[F[K 242798/500000: episode: 3234, duration: 0.871s, episode steps: 72, steps per second: 83, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.517 [0.480, 0.630], loss: 7.783468, mean_absolute_error: 41.412125, mean_q: 53.179504
[F[K 242874/500000: episode: 3235, duration: 0.919s, episode steps: 76, steps per second: 83, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.504 [0.370, 0.660], loss: 8.958838, mean_absolute_error: 41.767895, mean_q: 53.532696
[F[K 242960/500000: episode: 3236, duration: 1.020s, episode steps: 86, steps per second: 84, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.058 [0.000, 4.000], mean observation: 0.498 [0.340, 0.660], loss: 7.952635, mean_absolute_error: 41.443008, mean_q: 53.044128
[F[K 243035/500000: episode: 3237, duration: 0.839s, episode steps: 75, steps per second: 89, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.600 [0.000, 4.000], mean observation: 0.492 [0.420, 0.530], loss: 8.710535, mean_absolute_error: 41.568523, mean_q: 53.206509
[F[K 243120/500000: episode: 3238, duration: 0.983s, episode steps: 85, steps per second: 86, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.271 [0.000, 4.000], mean observation: 0.504 [0.430, 0.600], loss: 8.183119, mean_absolute_error: 41.236763, mean_q: 52.888752
[F[K 243178/500000: episode: 3239, duration: 0.669s, episode steps: 58, steps per second: 87, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.569 [0.000, 4.000], mean observation: 0.503 [0.420, 0.610], loss: 6.933895, mean_absolute_error: 43.304401, mean_q: 55.496346
[F[K 243258/500000: episode: 3240, duration: 0.948s, episode steps: 80, steps per second: 84, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.550 [0.000, 4.000], mean observation: 0.514 [0.470, 0.640], loss: 7.161882, mean_absolute_error: 42.303432, mean_q: 54.322937
[F[K 243336/500000: episode: 3241, duration: 0.899s, episode steps: 78, steps per second: 87, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.500 [0.410, 0.610], loss: 7.750876, mean_absolute_error: 42.114334, mean_q: 54.068054
[F[K 243386/500000: episode: 3242, duration: 0.594s, episode steps: 50, steps per second: 84, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.040 [0.000, 4.000], mean observation: 0.500 [0.400, 0.570], loss: 8.048119, mean_absolute_error: 41.233158, mean_q: 52.896030
[F[K 243456/500000: episode: 3243, duration: 0.856s, episode steps: 70, steps per second: 82, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.643 [0.000, 4.000], mean observation: 0.507 [0.490, 0.530], loss: 7.560939, mean_absolute_error: 42.176132, mean_q: 54.140015
[F[K 243551/500000: episode: 3244, duration: 1.199s, episode steps: 95, steps per second: 79, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.305 [0.000, 4.000], mean observation: 0.501 [0.470, 0.540], loss: 7.892141, mean_absolute_error: 41.716194, mean_q: 53.508541
[F[K 243633/500000: episode: 3245, duration: 1.032s, episode steps: 82, steps per second: 79, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.963 [0.000, 4.000], mean observation: 0.486 [0.360, 0.530], loss: 9.395674, mean_absolute_error: 41.989208, mean_q: 53.868259
[F[K 243680/500000: episode: 3246, duration: 0.586s, episode steps: 47, steps per second: 80, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.447 [0.000, 4.000], mean observation: 0.509 [0.470, 0.600], loss: 6.651572, mean_absolute_error: 41.698174, mean_q: 53.625057
[F[K 243771/500000: episode: 3247, duration: 1.119s, episode steps: 91, steps per second: 81, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.604 [0.000, 4.000], mean observation: 0.499 [0.400, 0.590], loss: 9.019845, mean_absolute_error: 41.649349, mean_q: 53.393715
[F[K 243900/500000: episode: 3248, duration: 1.609s, episode steps: 129, steps per second: 80, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.155 [0.000, 4.000], mean observation: 0.509 [0.490, 0.600], loss: 8.205351, mean_absolute_error: 41.632725, mean_q: 53.291397
[F[K 243971/500000: episode: 3249, duration: 0.911s, episode steps: 71, steps per second: 78, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.437 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 9.509261, mean_absolute_error: 41.414455, mean_q: 53.093472
[F[K 244084/500000: episode: 3250, duration: 1.420s, episode steps: 113, steps per second: 80, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.584 [0.000, 4.000], mean observation: 0.503 [0.470, 0.540], loss: 9.345245, mean_absolute_error: 41.814976, mean_q: 53.529617
[F[K 244142/500000: episode: 3251, duration: 0.683s, episode steps: 58, steps per second: 85, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.931 [0.000, 4.000], mean observation: 0.485 [0.370, 0.510], loss: 6.992756, mean_absolute_error: 41.754761, mean_q: 53.629589
[F[K 244205/500000: episode: 3252, duration: 0.786s, episode steps: 63, steps per second: 80, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.587 [0.000, 4.000], mean observation: 0.502 [0.490, 0.530], loss: 8.242739, mean_absolute_error: 41.841808, mean_q: 53.641876
[F[K 244290/500000: episode: 3253, duration: 1.114s, episode steps: 85, steps per second: 76, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.929 [0.000, 4.000], mean observation: 0.490 [0.360, 0.540], loss: 6.970487, mean_absolute_error: 42.014614, mean_q: 53.841103
[F[K 244490/500000: episode: 3254, duration: 2.451s, episode steps: 200, steps per second: 82, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.499 [0.410, 0.580], loss: 8.818226, mean_absolute_error: 41.626297, mean_q: 53.335899
[F[K 244581/500000: episode: 3255, duration: 1.208s, episode steps: 91, steps per second: 75, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.198 [0.000, 4.000], mean observation: 0.495 [0.450, 0.530], loss: 8.387936, mean_absolute_error: 41.774925, mean_q: 53.623215
[F[K 244650/500000: episode: 3256, duration: 0.848s, episode steps: 69, steps per second: 81, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.130 [0.000, 4.000], mean observation: 0.470 [0.350, 0.520], loss: 7.666993, mean_absolute_error: 40.873394, mean_q: 52.592690
[F[K 244690/500000: episode: 3257, duration: 0.558s, episode steps: 40, steps per second: 72, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.514 [0.470, 0.610], loss: 9.200561, mean_absolute_error: 41.300285, mean_q: 53.063148
[F[K 244741/500000: episode: 3258, duration: 0.673s, episode steps: 51, steps per second: 76, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.882 [0.000, 4.000], mean observation: 0.500 [0.380, 0.630], loss: 8.155332, mean_absolute_error: 41.567738, mean_q: 53.404934
[F[K 244838/500000: episode: 3259, duration: 1.271s, episode steps: 97, steps per second: 76, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.742 [0.000, 4.000], mean observation: 0.484 [0.370, 0.530], loss: 7.504559, mean_absolute_error: 41.984924, mean_q: 53.930645
[F[K 244923/500000: episode: 3260, duration: 1.074s, episode steps: 85, steps per second: 79, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.082 [0.000, 4.000], mean observation: 0.490 [0.370, 0.530], loss: 9.007479, mean_absolute_error: 41.668903, mean_q: 53.443817
[F[K 245009/500000: episode: 3261, duration: 1.074s, episode steps: 86, steps per second: 80, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.233 [0.000, 4.000], mean observation: 0.511 [0.440, 0.670], loss: 7.442525, mean_absolute_error: 41.985516, mean_q: 53.825523
[F[K 245073/500000: episode: 3262, duration: 0.851s, episode steps: 64, steps per second: 75, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.496 [0.460, 0.530], loss: 8.179373, mean_absolute_error: 41.276787, mean_q: 53.001640
[F[K 245117/500000: episode: 3263, duration: 0.670s, episode steps: 44, steps per second: 66, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.508 [0.430, 0.650], loss: 8.190109, mean_absolute_error: 41.525089, mean_q: 53.277225
[F[K 245216/500000: episode: 3264, duration: 1.362s, episode steps: 99, steps per second: 73, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.646 [0.000, 4.000], mean observation: 0.499 [0.420, 0.570], loss: 7.256967, mean_absolute_error: 42.764343, mean_q: 54.821766
[F[K 245260/500000: episode: 3265, duration: 0.610s, episode steps: 44, steps per second: 72, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.477 [0.000, 3.000], mean observation: 0.524 [0.470, 0.630], loss: 11.120953, mean_absolute_error: 42.346447, mean_q: 54.358688
[F[K 245391/500000: episode: 3266, duration: 1.718s, episode steps: 131, steps per second: 76, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 9.183416, mean_absolute_error: 41.371578, mean_q: 53.098953
[F[K 245522/500000: episode: 3267, duration: 1.625s, episode steps: 131, steps per second: 81, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.130 [0.000, 4.000], mean observation: 0.495 [0.390, 0.580], loss: 7.892582, mean_absolute_error: 42.260132, mean_q: 54.117645
[F[K 245600/500000: episode: 3268, duration: 1.041s, episode steps: 78, steps per second: 75, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.936 [0.000, 4.000], mean observation: 0.512 [0.490, 0.590], loss: 8.555758, mean_absolute_error: 41.578075, mean_q: 53.306633
[F[K 245684/500000: episode: 3269, duration: 1.067s, episode steps: 84, steps per second: 79, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.607 [0.000, 4.000], mean observation: 0.493 [0.450, 0.530], loss: 7.340181, mean_absolute_error: 41.665741, mean_q: 53.547615
[F[K 245755/500000: episode: 3270, duration: 1.053s, episode steps: 71, steps per second: 67, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.479 [0.000, 4.000], mean observation: 0.489 [0.430, 0.520], loss: 7.352947, mean_absolute_error: 42.208637, mean_q: 54.221375
[F[K 245795/500000: episode: 3271, duration: 0.580s, episode steps: 40, steps per second: 69, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.950 [0.000, 4.000], mean observation: 0.506 [0.420, 0.640], loss: 7.411674, mean_absolute_error: 41.724236, mean_q: 53.642235
[F[K 245868/500000: episode: 3272, duration: 0.995s, episode steps: 73, steps per second: 73, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.616 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 7.646236, mean_absolute_error: 41.896088, mean_q: 53.847198
[F[K 245961/500000: episode: 3273, duration: 1.416s, episode steps: 93, steps per second: 66, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.065 [0.000, 4.000], mean observation: 0.507 [0.410, 0.650], loss: 8.877376, mean_absolute_error: 42.151146, mean_q: 54.131260
[F[K 246036/500000: episode: 3274, duration: 0.903s, episode steps: 75, steps per second: 83, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.173 [0.000, 4.000], mean observation: 0.492 [0.430, 0.530], loss: 7.854083, mean_absolute_error: 41.444191, mean_q: 53.221653
[F[K 246117/500000: episode: 3275, duration: 1.178s, episode steps: 81, steps per second: 69, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.086 [0.000, 4.000], mean observation: 0.497 [0.380, 0.570], loss: 8.529435, mean_absolute_error: 41.724888, mean_q: 53.537231
[F[K 246170/500000: episode: 3276, duration: 0.909s, episode steps: 53, steps per second: 58, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.755 [0.000, 4.000], mean observation: 0.508 [0.470, 0.610], loss: 8.298489, mean_absolute_error: 41.290657, mean_q: 53.028385
[F[K 246247/500000: episode: 3277, duration: 1.152s, episode steps: 77, steps per second: 67, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.805 [0.000, 4.000], mean observation: 0.502 [0.440, 0.560], loss: 9.542281, mean_absolute_error: 42.075665, mean_q: 53.834320
[F[K 246318/500000: episode: 3278, duration: 1.120s, episode steps: 71, steps per second: 63, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.085 [0.000, 4.000], mean observation: 0.510 [0.490, 0.570], loss: 8.156708, mean_absolute_error: 41.266277, mean_q: 52.975056
[F[K 246383/500000: episode: 3279, duration: 1.066s, episode steps: 65, steps per second: 61, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.486 [0.390, 0.520], loss: 7.698496, mean_absolute_error: 41.665623, mean_q: 53.428776
[F[K 246456/500000: episode: 3280, duration: 1.226s, episode steps: 73, steps per second: 60, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.505 [0.480, 0.540], loss: 7.946746, mean_absolute_error: 41.744709, mean_q: 53.528225
[F[K 246506/500000: episode: 3281, duration: 0.856s, episode steps: 50, steps per second: 58, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.720 [0.000, 4.000], mean observation: 0.501 [0.370, 0.650], loss: 7.297472, mean_absolute_error: 42.526089, mean_q: 54.564266
[F[K 246586/500000: episode: 3282, duration: 1.428s, episode steps: 80, steps per second: 56, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.312 [0.000, 4.000], mean observation: 0.497 [0.460, 0.530], loss: 9.102165, mean_absolute_error: 41.381611, mean_q: 53.098003
[F[K 246767/500000: episode: 3283, duration: 2.782s, episode steps: 181, steps per second: 65, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.381 [0.000, 4.000], mean observation: 0.509 [0.450, 0.650], loss: 8.507783, mean_absolute_error: 42.008423, mean_q: 53.934586
[F[K 246823/500000: episode: 3284, duration: 0.837s, episode steps: 56, steps per second: 67, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 0.498 [0.430, 0.530], loss: 7.542457, mean_absolute_error: 42.151157, mean_q: 54.084557
[F[K 246893/500000: episode: 3285, duration: 1.187s, episode steps: 70, steps per second: 59, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.502 [0.430, 0.560], loss: 9.551478, mean_absolute_error: 42.055836, mean_q: 53.968323
[F[K 246944/500000: episode: 3286, duration: 0.845s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.804 [0.000, 4.000], mean observation: 0.508 [0.420, 0.650], loss: 8.796926, mean_absolute_error: 42.398693, mean_q: 54.503300
[F[K 247012/500000: episode: 3287, duration: 0.975s, episode steps: 68, steps per second: 70, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.897 [0.000, 4.000], mean observation: 0.520 [0.470, 0.600], loss: 8.288738, mean_absolute_error: 42.049603, mean_q: 54.001217
[F[K 247139/500000: episode: 3288, duration: 2.103s, episode steps: 127, steps per second: 60, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.268 [0.000, 4.000], mean observation: 0.477 [0.350, 0.520], loss: 7.711357, mean_absolute_error: 41.694107, mean_q: 53.582336
[F[K 247208/500000: episode: 3289, duration: 1.123s, episode steps: 69, steps per second: 61, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.506 [0.480, 0.550], loss: 8.978552, mean_absolute_error: 41.981274, mean_q: 53.891365
[F[K 247337/500000: episode: 3290, duration: 2.037s, episode steps: 129, steps per second: 63, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.465 [0.000, 4.000], mean observation: 0.495 [0.460, 0.530], loss: 8.004587, mean_absolute_error: 41.960892, mean_q: 53.920574
[F[K 247388/500000: episode: 3291, duration: 0.899s, episode steps: 51, steps per second: 57, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.902 [0.000, 4.000], mean observation: 0.504 [0.400, 0.640], loss: 7.294438, mean_absolute_error: 41.276817, mean_q: 52.964634
[F[K 247446/500000: episode: 3292, duration: 0.775s, episode steps: 58, steps per second: 75, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.207 [0.000, 4.000], mean observation: 0.519 [0.470, 0.650], loss: 7.675633, mean_absolute_error: 41.942234, mean_q: 53.757767
[F[K 247490/500000: episode: 3293, duration: 0.739s, episode steps: 44, steps per second: 60, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 7.899345, mean_absolute_error: 41.560909, mean_q: 53.242184
[F[K 247536/500000: episode: 3294, duration: 0.729s, episode steps: 46, steps per second: 63, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.761 [0.000, 4.000], mean observation: 0.505 [0.420, 0.630], loss: 8.417940, mean_absolute_error: 42.005238, mean_q: 53.850163
[F[K 247642/500000: episode: 3295, duration: 1.621s, episode steps: 106, steps per second: 65, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.462 [0.000, 4.000], mean observation: 0.516 [0.470, 0.620], loss: 8.286728, mean_absolute_error: 41.889854, mean_q: 53.822617
[F[K 247770/500000: episode: 3296, duration: 2.136s, episode steps: 128, steps per second: 60, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.477 [0.000, 4.000], mean observation: 0.486 [0.330, 0.540], loss: 8.118773, mean_absolute_error: 42.241417, mean_q: 54.198006
[F[K 247827/500000: episode: 3297, duration: 0.981s, episode steps: 57, steps per second: 58, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.842 [0.000, 4.000], mean observation: 0.509 [0.420, 0.640], loss: 9.057025, mean_absolute_error: 41.595383, mean_q: 53.369003
[F[K 247931/500000: episode: 3298, duration: 1.585s, episode steps: 104, steps per second: 66, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.492 [0.370, 0.550], loss: 7.872049, mean_absolute_error: 41.544189, mean_q: 53.293205
[F[K 248079/500000: episode: 3299, duration: 2.428s, episode steps: 148, steps per second: 61, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.230 [0.000, 4.000], mean observation: 0.490 [0.410, 0.530], loss: 7.498759, mean_absolute_error: 41.838055, mean_q: 53.648624
[F[K 248157/500000: episode: 3300, duration: 1.240s, episode steps: 78, steps per second: 63, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.654 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 8.059447, mean_absolute_error: 41.367233, mean_q: 53.012920
[F[K 248201/500000: episode: 3301, duration: 0.753s, episode steps: 44, steps per second: 58, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.545 [0.000, 4.000], mean observation: 0.526 [0.470, 0.650], loss: 8.392129, mean_absolute_error: 41.229286, mean_q: 52.821472
[F[K 248254/500000: episode: 3302, duration: 0.903s, episode steps: 53, steps per second: 59, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.522 [0.470, 0.650], loss: 8.367311, mean_absolute_error: 40.931458, mean_q: 52.457340
[F[K 248324/500000: episode: 3303, duration: 1.257s, episode steps: 70, steps per second: 56, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.986 [0.000, 4.000], mean observation: 0.521 [0.480, 0.660], loss: 9.132720, mean_absolute_error: 41.757214, mean_q: 53.447781
[F[K 248508/500000: episode: 3304, duration: 3.094s, episode steps: 184, steps per second: 59, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.505 [0.000, 4.000], mean observation: 0.502 [0.470, 0.530], loss: 8.519514, mean_absolute_error: 41.662529, mean_q: 53.382328
[F[K 248552/500000: episode: 3305, duration: 0.720s, episode steps: 44, steps per second: 61, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 0.513 [0.470, 0.620], loss: 9.260946, mean_absolute_error: 41.835876, mean_q: 53.637516
[F[K 248607/500000: episode: 3306, duration: 0.938s, episode steps: 55, steps per second: 59, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.600 [0.000, 4.000], mean observation: 0.507 [0.460, 0.610], loss: 8.330415, mean_absolute_error: 41.123589, mean_q: 52.756561
[F[K 248664/500000: episode: 3307, duration: 0.883s, episode steps: 57, steps per second: 65, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.614 [0.000, 4.000], mean observation: 0.499 [0.370, 0.630], loss: 7.356847, mean_absolute_error: 41.685776, mean_q: 53.610428
[F[K 248732/500000: episode: 3308, duration: 1.178s, episode steps: 68, steps per second: 58, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.496 [0.370, 0.610], loss: 8.416867, mean_absolute_error: 42.127464, mean_q: 54.099506
[F[K 248775/500000: episode: 3309, duration: 0.743s, episode steps: 43, steps per second: 58, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.023 [0.000, 4.000], mean observation: 0.512 [0.460, 0.650], loss: 11.467580, mean_absolute_error: 42.039021, mean_q: 53.699566
[F[K 248837/500000: episode: 3310, duration: 1.014s, episode steps: 62, steps per second: 61, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.145 [0.000, 4.000], mean observation: 0.485 [0.400, 0.510], loss: 8.017575, mean_absolute_error: 41.095692, mean_q: 52.695377
[F[K 248920/500000: episode: 3311, duration: 1.214s, episode steps: 83, steps per second: 68, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.024 [0.000, 4.000], mean observation: 0.486 [0.370, 0.530], loss: 8.254718, mean_absolute_error: 41.761887, mean_q: 53.596909
[F[K 248993/500000: episode: 3312, duration: 1.079s, episode steps: 73, steps per second: 68, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.205 [0.000, 4.000], mean observation: 0.496 [0.440, 0.540], loss: 7.686876, mean_absolute_error: 41.346519, mean_q: 53.074631
[F[K 249084/500000: episode: 3313, duration: 1.295s, episode steps: 91, steps per second: 70, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.187 [0.000, 4.000], mean observation: 0.495 [0.400, 0.540], loss: 8.192872, mean_absolute_error: 41.833210, mean_q: 53.660999
[F[K 249147/500000: episode: 3314, duration: 1.034s, episode steps: 63, steps per second: 61, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.159 [0.000, 4.000], mean observation: 0.505 [0.460, 0.600], loss: 8.444520, mean_absolute_error: 41.750847, mean_q: 53.506176
[F[K 249193/500000: episode: 3315, duration: 0.710s, episode steps: 46, steps per second: 65, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.848 [0.000, 4.000], mean observation: 0.515 [0.470, 0.610], loss: 7.826490, mean_absolute_error: 41.178871, mean_q: 52.811153
[F[K 249268/500000: episode: 3316, duration: 1.055s, episode steps: 75, steps per second: 71, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.613 [0.000, 4.000], mean observation: 0.495 [0.360, 0.590], loss: 8.897206, mean_absolute_error: 41.809059, mean_q: 53.650204
[F[K 249344/500000: episode: 3317, duration: 1.152s, episode steps: 76, steps per second: 66, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.368 [0.000, 4.000], mean observation: 0.492 [0.390, 0.560], loss: 7.799814, mean_absolute_error: 41.631466, mean_q: 53.404247
[F[K 249410/500000: episode: 3318, duration: 0.981s, episode steps: 66, steps per second: 67, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.894 [0.000, 4.000], mean observation: 0.499 [0.450, 0.530], loss: 8.538616, mean_absolute_error: 42.299873, mean_q: 54.218540
[F[K 249453/500000: episode: 3319, duration: 0.674s, episode steps: 43, steps per second: 64, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.907 [0.000, 4.000], mean observation: 0.521 [0.470, 0.610], loss: 7.224977, mean_absolute_error: 40.770756, mean_q: 52.307377
[F[K 249513/500000: episode: 3320, duration: 0.813s, episode steps: 60, steps per second: 74, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.617 [0.000, 4.000], mean observation: 0.492 [0.370, 0.530], loss: 9.029171, mean_absolute_error: 41.369083, mean_q: 52.973820
[F[K 249584/500000: episode: 3321, duration: 1.070s, episode steps: 71, steps per second: 66, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.254 [0.000, 4.000], mean observation: 0.479 [0.380, 0.520], loss: 8.825565, mean_absolute_error: 41.177647, mean_q: 52.777126
[F[K 249724/500000: episode: 3322, duration: 2.229s, episode steps: 140, steps per second: 63, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.264 [0.000, 4.000], mean observation: 0.493 [0.360, 0.530], loss: 8.507078, mean_absolute_error: 41.332218, mean_q: 52.987045
[F[K 249780/500000: episode: 3323, duration: 0.931s, episode steps: 56, steps per second: 60, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.536 [0.000, 4.000], mean observation: 0.517 [0.480, 0.610], loss: 8.187773, mean_absolute_error: 42.026699, mean_q: 53.978844
[F[K 249835/500000: episode: 3324, duration: 0.889s, episode steps: 55, steps per second: 62, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.527 [0.000, 4.000], mean observation: 0.503 [0.440, 0.590], loss: 9.777367, mean_absolute_error: 40.844723, mean_q: 52.402061
[F[K 249881/500000: episode: 3325, duration: 0.642s, episode steps: 46, steps per second: 72, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.870 [0.000, 4.000], mean observation: 0.501 [0.360, 0.650], loss: 8.363309, mean_absolute_error: 41.304623, mean_q: 53.109501
[F[K 249984/500000: episode: 3326, duration: 1.677s, episode steps: 103, steps per second: 61, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.500 [0.420, 0.560], loss: 7.533225, mean_absolute_error: 41.585285, mean_q: 53.414192
[F[K 250103/500000: episode: 3327, duration: 1.864s, episode steps: 119, steps per second: 64, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.882 [0.000, 4.000], mean observation: 0.506 [0.490, 0.530], loss: 7.593634, mean_absolute_error: 42.038139, mean_q: 53.997589
[F[K 250170/500000: episode: 3328, duration: 0.929s, episode steps: 67, steps per second: 72, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.090 [0.000, 4.000], mean observation: 0.482 [0.410, 0.500], loss: 8.122378, mean_absolute_error: 41.693607, mean_q: 53.568279
[F[K 250215/500000: episode: 3329, duration: 0.743s, episode steps: 45, steps per second: 61, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.089 [0.000, 4.000], mean observation: 0.531 [0.480, 0.650], loss: 8.602427, mean_absolute_error: 41.958088, mean_q: 53.861099
[F[K 250294/500000: episode: 3330, duration: 1.281s, episode steps: 79, steps per second: 62, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.886 [0.000, 4.000], mean observation: 0.494 [0.470, 0.510], loss: 8.394421, mean_absolute_error: 41.930347, mean_q: 53.705925
[F[K 250365/500000: episode: 3331, duration: 1.006s, episode steps: 71, steps per second: 71, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 7.236399, mean_absolute_error: 41.980648, mean_q: 53.939220
[F[K 250418/500000: episode: 3332, duration: 0.858s, episode steps: 53, steps per second: 62, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.660 [0.000, 4.000], mean observation: 0.516 [0.490, 0.630], loss: 8.496892, mean_absolute_error: 42.095768, mean_q: 53.946621
[F[K 250462/500000: episode: 3333, duration: 0.678s, episode steps: 44, steps per second: 65, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.955 [0.000, 4.000], mean observation: 0.504 [0.410, 0.630], loss: 7.271831, mean_absolute_error: 41.424530, mean_q: 53.331642
[F[K 250516/500000: episode: 3334, duration: 0.831s, episode steps: 54, steps per second: 65, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.963 [0.000, 4.000], mean observation: 0.514 [0.490, 0.570], loss: 8.320683, mean_absolute_error: 42.006084, mean_q: 53.892452
[F[K 250560/500000: episode: 3335, duration: 0.631s, episode steps: 44, steps per second: 70, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.511 [0.450, 0.650], loss: 9.091387, mean_absolute_error: 40.980400, mean_q: 52.497562
[F[K 250604/500000: episode: 3336, duration: 0.621s, episode steps: 44, steps per second: 71, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.159 [0.000, 4.000], mean observation: 0.524 [0.470, 0.650], loss: 8.680080, mean_absolute_error: 41.412888, mean_q: 53.134727
[F[K 250743/500000: episode: 3337, duration: 2.157s, episode steps: 139, steps per second: 64, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.101 [0.000, 4.000], mean observation: 0.504 [0.480, 0.540], loss: 8.875976, mean_absolute_error: 41.312786, mean_q: 52.951313
[F[K 250818/500000: episode: 3338, duration: 1.163s, episode steps: 75, steps per second: 64, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 0.487 [0.390, 0.530], loss: 9.274314, mean_absolute_error: 42.128288, mean_q: 54.138645
[F[K 250853/500000: episode: 3339, duration: 0.552s, episode steps: 35, steps per second: 63, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.629 [0.000, 4.000], mean observation: 0.514 [0.470, 0.650], loss: 8.618385, mean_absolute_error: 41.424091, mean_q: 53.080482
[F[K 250966/500000: episode: 3340, duration: 1.725s, episode steps: 113, steps per second: 65, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.398 [0.000, 4.000], mean observation: 0.504 [0.450, 0.590], loss: 8.204837, mean_absolute_error: 42.096519, mean_q: 53.997658
[F[K 251166/500000: episode: 3341, duration: 3.215s, episode steps: 200, steps per second: 62, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.245 [0.000, 4.000], mean observation: 0.497 [0.390, 0.610], loss: 8.193439, mean_absolute_error: 41.236637, mean_q: 52.942123
[F[K 251280/500000: episode: 3342, duration: 1.947s, episode steps: 114, steps per second: 59, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.693 [0.000, 4.000], mean observation: 0.504 [0.470, 0.560], loss: 7.523745, mean_absolute_error: 41.713657, mean_q: 53.473160
[F[K 251326/500000: episode: 3343, duration: 0.688s, episode steps: 46, steps per second: 67, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.065 [0.000, 4.000], mean observation: 0.531 [0.490, 0.650], loss: 8.161402, mean_absolute_error: 40.942844, mean_q: 52.516380
[F[K 251473/500000: episode: 3344, duration: 2.424s, episode steps: 147, steps per second: 61, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.177 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 8.116234, mean_absolute_error: 41.646420, mean_q: 53.445957
[F[K 251673/500000: episode: 3345, duration: 3.566s, episode steps: 200, steps per second: 56, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.230 [0.000, 4.000], mean observation: 0.499 [0.450, 0.530], loss: 8.683277, mean_absolute_error: 41.412216, mean_q: 53.172340
[F[K 251735/500000: episode: 3346, duration: 1.096s, episode steps: 62, steps per second: 57, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.491 [0.430, 0.510], loss: 8.522348, mean_absolute_error: 41.749756, mean_q: 53.597755
[F[K 251789/500000: episode: 3347, duration: 1.036s, episode steps: 54, steps per second: 52, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.537 [0.000, 4.000], mean observation: 0.507 [0.430, 0.640], loss: 9.922475, mean_absolute_error: 42.336864, mean_q: 54.261597
[F[K 251856/500000: episode: 3348, duration: 1.197s, episode steps: 67, steps per second: 56, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.299 [0.000, 4.000], mean observation: 0.518 [0.480, 0.610], loss: 7.302003, mean_absolute_error: 41.279320, mean_q: 53.062424
[F[K 251903/500000: episode: 3349, duration: 0.781s, episode steps: 47, steps per second: 60, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.319 [0.000, 4.000], mean observation: 0.505 [0.440, 0.580], loss: 7.938904, mean_absolute_error: 41.699459, mean_q: 53.441750
[F[K 251954/500000: episode: 3350, duration: 0.944s, episode steps: 51, steps per second: 54, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.863 [0.000, 4.000], mean observation: 0.491 [0.350, 0.530], loss: 7.359671, mean_absolute_error: 42.316124, mean_q: 54.230156
[F[K 252023/500000: episode: 3351, duration: 1.215s, episode steps: 69, steps per second: 57, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.490 [0.450, 0.530], loss: 10.085790, mean_absolute_error: 41.382637, mean_q: 53.069012
[F[K 252126/500000: episode: 3352, duration: 1.817s, episode steps: 103, steps per second: 57, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.592 [0.000, 4.000], mean observation: 0.494 [0.430, 0.530], loss: 10.066801, mean_absolute_error: 41.251160, mean_q: 52.938686
[F[K 252190/500000: episode: 3353, duration: 0.984s, episode steps: 64, steps per second: 65, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.094 [0.000, 4.000], mean observation: 0.517 [0.480, 0.580], loss: 8.366021, mean_absolute_error: 42.187447, mean_q: 54.109856
[F[K 252277/500000: episode: 3354, duration: 1.494s, episode steps: 87, steps per second: 58, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.046 [0.000, 4.000], mean observation: 0.480 [0.350, 0.510], loss: 8.595836, mean_absolute_error: 41.158279, mean_q: 52.718727
[F[K 252350/500000: episode: 3355, duration: 1.227s, episode steps: 73, steps per second: 59, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.589 [0.000, 4.000], mean observation: 0.498 [0.380, 0.600], loss: 6.512342, mean_absolute_error: 42.143665, mean_q: 54.175011
[F[K 252407/500000: episode: 3356, duration: 1.035s, episode steps: 57, steps per second: 55, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.193 [0.000, 4.000], mean observation: 0.524 [0.470, 0.640], loss: 9.000820, mean_absolute_error: 41.797104, mean_q: 53.616619
[F[K 252480/500000: episode: 3357, duration: 1.327s, episode steps: 73, steps per second: 55, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.781 [0.000, 4.000], mean observation: 0.510 [0.460, 0.590], loss: 7.646483, mean_absolute_error: 41.372704, mean_q: 53.055973
[F[K 252604/500000: episode: 3358, duration: 2.152s, episode steps: 124, steps per second: 58, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.032 [0.000, 4.000], mean observation: 0.492 [0.370, 0.560], loss: 9.443518, mean_absolute_error: 41.494843, mean_q: 53.317726
[F[K 252666/500000: episode: 3359, duration: 1.056s, episode steps: 62, steps per second: 59, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 8.325738, mean_absolute_error: 41.062912, mean_q: 52.561420
[F[K 252768/500000: episode: 3360, duration: 1.765s, episode steps: 102, steps per second: 58, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.225 [0.000, 4.000], mean observation: 0.490 [0.370, 0.560], loss: 7.140436, mean_absolute_error: 42.091179, mean_q: 54.076920
[F[K 252837/500000: episode: 3361, duration: 1.142s, episode steps: 69, steps per second: 60, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.391 [0.000, 4.000], mean observation: 0.479 [0.360, 0.530], loss: 8.218834, mean_absolute_error: 41.478382, mean_q: 53.272926
[F[K 252902/500000: episode: 3362, duration: 1.024s, episode steps: 65, steps per second: 63, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.031 [0.000, 4.000], mean observation: 0.513 [0.490, 0.590], loss: 7.847785, mean_absolute_error: 42.156319, mean_q: 54.114223
[F[K 252984/500000: episode: 3363, duration: 1.273s, episode steps: 82, steps per second: 64, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.183 [0.000, 4.000], mean observation: 0.497 [0.370, 0.630], loss: 7.591966, mean_absolute_error: 41.381130, mean_q: 53.069233
[F[K 253081/500000: episode: 3364, duration: 1.444s, episode steps: 97, steps per second: 67, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.216 [0.000, 4.000], mean observation: 0.502 [0.390, 0.610], loss: 9.402104, mean_absolute_error: 41.674187, mean_q: 53.460308
[F[K 253187/500000: episode: 3365, duration: 1.623s, episode steps: 106, steps per second: 65, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.368 [0.000, 4.000], mean observation: 0.499 [0.360, 0.620], loss: 8.267036, mean_absolute_error: 41.462353, mean_q: 53.252636
[F[K 253256/500000: episode: 3366, duration: 1.077s, episode steps: 69, steps per second: 64, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.217 [0.000, 4.000], mean observation: 0.503 [0.470, 0.530], loss: 7.125400, mean_absolute_error: 41.319004, mean_q: 53.086651
[F[K 253331/500000: episode: 3367, duration: 1.110s, episode steps: 75, steps per second: 68, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 8.421953, mean_absolute_error: 42.045464, mean_q: 54.019859
[F[K 253394/500000: episode: 3368, duration: 1.132s, episode steps: 63, steps per second: 56, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.476 [0.380, 0.510], loss: 8.028233, mean_absolute_error: 42.515316, mean_q: 54.634369
[F[K 253454/500000: episode: 3369, duration: 0.926s, episode steps: 60, steps per second: 65, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.283 [0.000, 4.000], mean observation: 0.505 [0.460, 0.570], loss: 8.857895, mean_absolute_error: 41.252060, mean_q: 52.871601
[F[K 253508/500000: episode: 3370, duration: 0.861s, episode steps: 54, steps per second: 63, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.037 [0.000, 4.000], mean observation: 0.490 [0.360, 0.580], loss: 7.853710, mean_absolute_error: 42.049927, mean_q: 53.996437
[F[K 253573/500000: episode: 3371, duration: 1.032s, episode steps: 65, steps per second: 63, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.062 [0.000, 4.000], mean observation: 0.517 [0.480, 0.650], loss: 8.572027, mean_absolute_error: 41.543510, mean_q: 53.292835
[F[K 253614/500000: episode: 3372, duration: 0.654s, episode steps: 41, steps per second: 63, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.585 [0.000, 4.000], mean observation: 0.525 [0.470, 0.630], loss: 6.940552, mean_absolute_error: 41.767693, mean_q: 53.587814
[F[K 253703/500000: episode: 3373, duration: 1.446s, episode steps: 89, steps per second: 62, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.090 [0.000, 4.000], mean observation: 0.502 [0.470, 0.530], loss: 7.883363, mean_absolute_error: 41.551052, mean_q: 53.367573
[F[K 253762/500000: episode: 3374, duration: 0.859s, episode steps: 59, steps per second: 69, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.847 [0.000, 4.000], mean observation: 0.516 [0.470, 0.640], loss: 8.076491, mean_absolute_error: 42.187366, mean_q: 54.055119
[F[K 253841/500000: episode: 3375, duration: 1.231s, episode steps: 79, steps per second: 64, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.500 [0.440, 0.550], loss: 8.672927, mean_absolute_error: 41.675755, mean_q: 53.497013
[F[K 253924/500000: episode: 3376, duration: 1.311s, episode steps: 83, steps per second: 63, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.498 [0.380, 0.620], loss: 8.580503, mean_absolute_error: 41.754326, mean_q: 53.533634
[F[K 253985/500000: episode: 3377, duration: 1.009s, episode steps: 61, steps per second: 60, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.328 [0.000, 4.000], mean observation: 0.476 [0.370, 0.510], loss: 7.768898, mean_absolute_error: 42.270309, mean_q: 54.359783
[F[K 254055/500000: episode: 3378, duration: 1.146s, episode steps: 70, steps per second: 61, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 7.550277, mean_absolute_error: 41.836079, mean_q: 53.621483
[F[K 254120/500000: episode: 3379, duration: 1.084s, episode steps: 65, steps per second: 60, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.769 [0.000, 4.000], mean observation: 0.502 [0.410, 0.620], loss: 7.564372, mean_absolute_error: 41.351322, mean_q: 53.174351
[F[K 254198/500000: episode: 3380, duration: 1.284s, episode steps: 78, steps per second: 61, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.064 [0.000, 4.000], mean observation: 0.493 [0.400, 0.550], loss: 7.796527, mean_absolute_error: 41.725334, mean_q: 53.520119
[F[K 254269/500000: episode: 3381, duration: 1.031s, episode steps: 71, steps per second: 69, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.648 [0.000, 4.000], mean observation: 0.505 [0.410, 0.630], loss: 8.824491, mean_absolute_error: 41.783142, mean_q: 53.655449
[F[K 254323/500000: episode: 3382, duration: 0.944s, episode steps: 54, steps per second: 57, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.556 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 7.407932, mean_absolute_error: 41.062790, mean_q: 52.661137
[F[K 254367/500000: episode: 3383, duration: 0.773s, episode steps: 44, steps per second: 57, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.545 [0.000, 4.000], mean observation: 0.517 [0.470, 0.610], loss: 6.585947, mean_absolute_error: 42.026241, mean_q: 54.052059
[F[K 254435/500000: episode: 3384, duration: 1.207s, episode steps: 68, steps per second: 56, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.765 [0.000, 4.000], mean observation: 0.513 [0.470, 0.620], loss: 7.359354, mean_absolute_error: 41.917271, mean_q: 53.875416
[F[K 254515/500000: episode: 3385, duration: 1.336s, episode steps: 80, steps per second: 60, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 0.480 [0.390, 0.520], loss: 9.537607, mean_absolute_error: 41.314724, mean_q: 53.053211
[F[K 254627/500000: episode: 3386, duration: 1.771s, episode steps: 112, steps per second: 63, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.205 [0.000, 4.000], mean observation: 0.510 [0.440, 0.610], loss: 7.551888, mean_absolute_error: 41.987213, mean_q: 53.952026
[F[K 254827/500000: episode: 3387, duration: 3.137s, episode steps: 200, steps per second: 64, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.010 [0.000, 4.000], mean observation: 0.495 [0.420, 0.550], loss: 8.617621, mean_absolute_error: 41.633564, mean_q: 53.488575
[F[K 254869/500000: episode: 3388, duration: 0.594s, episode steps: 42, steps per second: 71, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.714 [0.000, 4.000], mean observation: 0.518 [0.470, 0.600], loss: 8.577846, mean_absolute_error: 40.714878, mean_q: 52.136955
[F[K 255032/500000: episode: 3389, duration: 2.741s, episode steps: 163, steps per second: 59, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.313 [0.000, 4.000], mean observation: 0.498 [0.400, 0.550], loss: 7.955019, mean_absolute_error: 41.820900, mean_q: 53.729176
[F[K 255067/500000: episode: 3390, duration: 0.595s, episode steps: 35, steps per second: 59, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.486 [0.000, 4.000], mean observation: 0.506 [0.420, 0.640], loss: 7.048624, mean_absolute_error: 42.142914, mean_q: 54.160233
[F[K 255154/500000: episode: 3391, duration: 1.517s, episode steps: 87, steps per second: 57, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.034 [0.000, 4.000], mean observation: 0.486 [0.430, 0.520], loss: 8.175676, mean_absolute_error: 41.039036, mean_q: 52.627289
[F[K 255217/500000: episode: 3392, duration: 1.049s, episode steps: 63, steps per second: 60, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.497 [0.400, 0.540], loss: 7.728071, mean_absolute_error: 41.660465, mean_q: 53.502819
[F[K 255369/500000: episode: 3393, duration: 2.624s, episode steps: 152, steps per second: 58, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.513 [0.000, 4.000], mean observation: 0.507 [0.460, 0.580], loss: 8.289697, mean_absolute_error: 42.102306, mean_q: 53.931999
[F[K 255461/500000: episode: 3394, duration: 1.593s, episode steps: 92, steps per second: 58, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.717 [0.000, 4.000], mean observation: 0.490 [0.390, 0.530], loss: 7.728922, mean_absolute_error: 41.970032, mean_q: 53.826797
[F[K 255586/500000: episode: 3395, duration: 1.949s, episode steps: 125, steps per second: 64, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.184 [0.000, 4.000], mean observation: 0.498 [0.380, 0.600], loss: 7.665011, mean_absolute_error: 41.237488, mean_q: 52.862785
[F[K 255644/500000: episode: 3396, duration: 1.016s, episode steps: 58, steps per second: 57, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.502 [0.460, 0.550], loss: 7.343657, mean_absolute_error: 41.781620, mean_q: 53.679939
[F[K 255777/500000: episode: 3397, duration: 2.462s, episode steps: 133, steps per second: 54, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.549 [0.000, 4.000], mean observation: 0.520 [0.480, 0.650], loss: 8.449326, mean_absolute_error: 41.932228, mean_q: 53.676773
[F[K 255842/500000: episode: 3398, duration: 1.026s, episode steps: 65, steps per second: 63, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.846 [0.000, 4.000], mean observation: 0.500 [0.460, 0.530], loss: 7.849408, mean_absolute_error: 41.428532, mean_q: 53.285603
[F[K 255912/500000: episode: 3399, duration: 1.270s, episode steps: 70, steps per second: 55, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.529 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 7.784990, mean_absolute_error: 41.878025, mean_q: 53.725677
[F[K 256078/500000: episode: 3400, duration: 2.828s, episode steps: 166, steps per second: 59, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.928 [0.000, 4.000], mean observation: 0.519 [0.480, 0.660], loss: 7.678711, mean_absolute_error: 42.129265, mean_q: 54.146091
[F[K 256158/500000: episode: 3401, duration: 1.483s, episode steps: 80, steps per second: 54, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.050 [0.000, 4.000], mean observation: 0.514 [0.490, 0.640], loss: 9.363054, mean_absolute_error: 41.415352, mean_q: 53.167316
[F[K 256230/500000: episode: 3402, duration: 1.182s, episode steps: 72, steps per second: 61, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 8.470380, mean_absolute_error: 41.900120, mean_q: 53.798325
[F[K 256304/500000: episode: 3403, duration: 1.246s, episode steps: 74, steps per second: 59, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.392 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 7.717152, mean_absolute_error: 41.375267, mean_q: 53.200325
[F[K 256356/500000: episode: 3404, duration: 0.894s, episode steps: 52, steps per second: 58, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.173 [0.000, 4.000], mean observation: 0.491 [0.410, 0.530], loss: 8.570233, mean_absolute_error: 41.768940, mean_q: 53.585331
[F[K 256431/500000: episode: 3405, duration: 1.315s, episode steps: 75, steps per second: 57, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.160 [0.000, 4.000], mean observation: 0.519 [0.470, 0.600], loss: 9.492311, mean_absolute_error: 42.274403, mean_q: 54.045177
[F[K 256492/500000: episode: 3406, duration: 0.951s, episode steps: 61, steps per second: 64, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.803 [0.000, 4.000], mean observation: 0.493 [0.420, 0.530], loss: 8.165318, mean_absolute_error: 41.238094, mean_q: 52.807419
[F[K 256668/500000: episode: 3407, duration: 2.763s, episode steps: 176, steps per second: 64, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.682 [0.000, 4.000], mean observation: 0.482 [0.350, 0.530], loss: 7.825187, mean_absolute_error: 41.838394, mean_q: 53.586376
[F[K 256732/500000: episode: 3408, duration: 0.781s, episode steps: 64, steps per second: 82, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.328 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 7.853797, mean_absolute_error: 42.133995, mean_q: 53.995346
[F[K 256846/500000: episode: 3409, duration: 1.533s, episode steps: 114, steps per second: 74, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.495 [0.440, 0.540], loss: 8.221616, mean_absolute_error: 41.941521, mean_q: 53.792740
[F[K 256909/500000: episode: 3410, duration: 0.929s, episode steps: 63, steps per second: 68, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.508 [0.000, 4.000], mean observation: 0.487 [0.350, 0.530], loss: 7.492369, mean_absolute_error: 41.788265, mean_q: 53.628078
[F[K 256953/500000: episode: 3411, duration: 0.696s, episode steps: 44, steps per second: 63, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.273 [0.000, 4.000], mean observation: 0.522 [0.470, 0.630], loss: 7.236311, mean_absolute_error: 40.567398, mean_q: 52.095646
[F[K 257025/500000: episode: 3412, duration: 0.857s, episode steps: 72, steps per second: 84, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.502 [0.410, 0.620], loss: 8.132119, mean_absolute_error: 41.831623, mean_q: 53.602547
[F[K 257106/500000: episode: 3413, duration: 1.129s, episode steps: 81, steps per second: 72, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.136 [0.000, 4.000], mean observation: 0.514 [0.480, 0.660], loss: 9.232987, mean_absolute_error: 41.598270, mean_q: 53.440094
[F[K 257154/500000: episode: 3414, duration: 0.650s, episode steps: 48, steps per second: 74, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.312 [0.000, 4.000], mean observation: 0.518 [0.470, 0.600], loss: 6.267889, mean_absolute_error: 41.830002, mean_q: 53.766407
[F[K 257213/500000: episode: 3415, duration: 0.889s, episode steps: 59, steps per second: 66, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.136 [0.000, 4.000], mean observation: 0.496 [0.350, 0.600], loss: 8.600454, mean_absolute_error: 41.466259, mean_q: 53.227211
[F[K 257292/500000: episode: 3416, duration: 1.115s, episode steps: 79, steps per second: 71, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.899 [0.000, 4.000], mean observation: 0.499 [0.440, 0.530], loss: 9.712478, mean_absolute_error: 41.802063, mean_q: 53.678524
[F[K 257391/500000: episode: 3417, duration: 1.364s, episode steps: 99, steps per second: 73, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.020 [0.000, 4.000], mean observation: 0.496 [0.470, 0.520], loss: 7.932240, mean_absolute_error: 41.459999, mean_q: 53.228531
[F[K 257456/500000: episode: 3418, duration: 0.914s, episode steps: 65, steps per second: 71, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.846 [0.000, 4.000], mean observation: 0.507 [0.410, 0.640], loss: 8.905301, mean_absolute_error: 41.292809, mean_q: 53.070713
[F[K 257635/500000: episode: 3419, duration: 2.620s, episode steps: 179, steps per second: 68, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.246 [0.000, 4.000], mean observation: 0.488 [0.290, 0.570], loss: 7.623924, mean_absolute_error: 41.654518, mean_q: 53.428787
[F[K 257691/500000: episode: 3420, duration: 0.853s, episode steps: 56, steps per second: 66, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.589 [0.000, 4.000], mean observation: 0.499 [0.370, 0.620], loss: 9.109654, mean_absolute_error: 41.784752, mean_q: 53.568913
[F[K 257766/500000: episode: 3421, duration: 1.141s, episode steps: 75, steps per second: 66, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.787 [0.000, 4.000], mean observation: 0.508 [0.420, 0.660], loss: 7.473649, mean_absolute_error: 41.658028, mean_q: 53.435123
[F[K 257833/500000: episode: 3422, duration: 0.930s, episode steps: 67, steps per second: 72, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.510 [0.490, 0.580], loss: 8.412244, mean_absolute_error: 41.140415, mean_q: 52.759560
[F[K 257886/500000: episode: 3423, duration: 0.726s, episode steps: 53, steps per second: 73, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.849 [0.000, 4.000], mean observation: 0.503 [0.360, 0.640], loss: 9.392355, mean_absolute_error: 41.603565, mean_q: 53.415462
[F[K 257992/500000: episode: 3424, duration: 1.438s, episode steps: 106, steps per second: 74, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.226 [0.000, 4.000], mean observation: 0.514 [0.470, 0.600], loss: 8.901196, mean_absolute_error: 41.953938, mean_q: 53.730335
[F[K 258071/500000: episode: 3425, duration: 1.091s, episode steps: 79, steps per second: 72, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.380 [0.000, 4.000], mean observation: 0.476 [0.360, 0.530], loss: 9.884928, mean_absolute_error: 41.695385, mean_q: 53.504196
[F[K 258188/500000: episode: 3426, duration: 1.455s, episode steps: 117, steps per second: 80, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.077 [0.000, 4.000], mean observation: 0.499 [0.370, 0.590], loss: 7.606252, mean_absolute_error: 42.128632, mean_q: 53.982330
[F[K 258253/500000: episode: 3427, duration: 0.912s, episode steps: 65, steps per second: 71, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.169 [0.000, 4.000], mean observation: 0.492 [0.400, 0.540], loss: 7.245086, mean_absolute_error: 41.316040, mean_q: 52.976120
[F[K 258314/500000: episode: 3428, duration: 0.900s, episode steps: 61, steps per second: 68, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.459 [0.000, 4.000], mean observation: 0.483 [0.400, 0.530], loss: 7.581242, mean_absolute_error: 40.867371, mean_q: 52.478924
[F[K 258391/500000: episode: 3429, duration: 0.958s, episode steps: 77, steps per second: 80, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.013 [0.000, 4.000], mean observation: 0.499 [0.380, 0.610], loss: 7.881260, mean_absolute_error: 42.414272, mean_q: 54.378220
[F[K 258454/500000: episode: 3430, duration: 0.756s, episode steps: 63, steps per second: 83, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.483 [0.400, 0.520], loss: 7.791498, mean_absolute_error: 41.572216, mean_q: 53.334278
[F[K 258529/500000: episode: 3431, duration: 0.879s, episode steps: 75, steps per second: 85, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.240 [0.000, 4.000], mean observation: 0.488 [0.410, 0.510], loss: 8.080775, mean_absolute_error: 42.107639, mean_q: 53.954174
[F[K 258596/500000: episode: 3432, duration: 0.935s, episode steps: 67, steps per second: 72, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.504 [0.480, 0.530], loss: 9.966655, mean_absolute_error: 41.228409, mean_q: 52.772942
[F[K 258647/500000: episode: 3433, duration: 0.701s, episode steps: 51, steps per second: 73, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.255 [0.000, 4.000], mean observation: 0.502 [0.390, 0.620], loss: 8.868867, mean_absolute_error: 41.688831, mean_q: 53.505241
[F[K 258717/500000: episode: 3434, duration: 0.889s, episode steps: 70, steps per second: 79, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.517 [0.490, 0.630], loss: 9.855542, mean_absolute_error: 40.874001, mean_q: 52.378563
[F[K 258778/500000: episode: 3435, duration: 0.821s, episode steps: 61, steps per second: 74, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.607 [0.000, 4.000], mean observation: 0.516 [0.470, 0.640], loss: 7.144665, mean_absolute_error: 40.886681, mean_q: 52.494034
[F[K 258898/500000: episode: 3436, duration: 1.448s, episode steps: 120, steps per second: 83, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.092 [0.000, 4.000], mean observation: 0.481 [0.340, 0.530], loss: 8.028263, mean_absolute_error: 41.359585, mean_q: 52.995907
[F[K 258941/500000: episode: 3437, duration: 0.615s, episode steps: 43, steps per second: 70, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.837 [0.000, 4.000], mean observation: 0.509 [0.470, 0.620], loss: 7.380272, mean_absolute_error: 41.147057, mean_q: 52.867489
[F[K 258989/500000: episode: 3438, duration: 0.655s, episode steps: 48, steps per second: 73, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.505 [0.470, 0.550], loss: 7.777580, mean_absolute_error: 41.341068, mean_q: 53.168842
[F[K 259034/500000: episode: 3439, duration: 0.623s, episode steps: 45, steps per second: 72, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.267 [0.000, 4.000], mean observation: 0.498 [0.370, 0.620], loss: 7.596268, mean_absolute_error: 41.544720, mean_q: 53.442142
[F[K 259141/500000: episode: 3440, duration: 1.418s, episode steps: 107, steps per second: 75, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.991 [0.000, 4.000], mean observation: 0.485 [0.350, 0.540], loss: 7.615916, mean_absolute_error: 41.607880, mean_q: 53.340488
[F[K 259233/500000: episode: 3441, duration: 1.156s, episode steps: 92, steps per second: 80, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.022 [0.000, 4.000], mean observation: 0.492 [0.370, 0.580], loss: 7.913859, mean_absolute_error: 41.290436, mean_q: 53.078205
[F[K 259409/500000: episode: 3442, duration: 2.340s, episode steps: 176, steps per second: 75, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.483 [0.000, 4.000], mean observation: 0.489 [0.340, 0.580], loss: 8.025744, mean_absolute_error: 41.741238, mean_q: 53.497314
[F[K 259471/500000: episode: 3443, duration: 0.805s, episode steps: 62, steps per second: 77, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.194 [0.000, 4.000], mean observation: 0.489 [0.390, 0.520], loss: 7.784079, mean_absolute_error: 41.406013, mean_q: 53.183319
[F[K 259507/500000: episode: 3444, duration: 0.484s, episode steps: 36, steps per second: 74, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.167 [0.000, 3.000], mean observation: 0.508 [0.430, 0.650], loss: 9.597367, mean_absolute_error: 40.305378, mean_q: 51.722988
[F[K 259614/500000: episode: 3445, duration: 1.313s, episode steps: 107, steps per second: 81, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.505 [0.000, 4.000], mean observation: 0.510 [0.490, 0.570], loss: 8.156874, mean_absolute_error: 41.987103, mean_q: 53.925915
[F[K 259688/500000: episode: 3446, duration: 0.977s, episode steps: 74, steps per second: 76, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.518 [0.490, 0.650], loss: 7.179221, mean_absolute_error: 41.740406, mean_q: 53.611809
[F[K 259742/500000: episode: 3447, duration: 0.738s, episode steps: 54, steps per second: 73, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.556 [0.000, 4.000], mean observation: 0.500 [0.410, 0.560], loss: 7.884306, mean_absolute_error: 41.654518, mean_q: 53.406284
[F[K 259918/500000: episode: 3448, duration: 2.228s, episode steps: 176, steps per second: 79, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.989 [0.000, 4.000], mean observation: 0.497 [0.390, 0.560], loss: 8.671111, mean_absolute_error: 41.856113, mean_q: 53.626400
[F[K 259992/500000: episode: 3449, duration: 0.981s, episode steps: 74, steps per second: 75, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.512 [0.480, 0.630], loss: 7.389748, mean_absolute_error: 41.421616, mean_q: 53.171177
[F[K 260133/500000: episode: 3450, duration: 1.949s, episode steps: 141, steps per second: 72, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.652 [0.000, 4.000], mean observation: 0.487 [0.420, 0.520], loss: 8.723710, mean_absolute_error: 41.294765, mean_q: 52.975693
[F[K 260182/500000: episode: 3451, duration: 0.712s, episode steps: 49, steps per second: 69, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.612 [0.000, 4.000], mean observation: 0.504 [0.420, 0.630], loss: 8.397688, mean_absolute_error: 41.301552, mean_q: 52.994698
[F[K 260336/500000: episode: 3452, duration: 2.135s, episode steps: 154, steps per second: 72, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.461 [0.000, 4.000], mean observation: 0.479 [0.340, 0.510], loss: 7.945321, mean_absolute_error: 41.342594, mean_q: 53.092934
[F[K 260512/500000: episode: 3453, duration: 2.270s, episode steps: 176, steps per second: 78, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.307 [0.000, 4.000], mean observation: 0.488 [0.390, 0.530], loss: 7.441797, mean_absolute_error: 41.340382, mean_q: 53.065670
[F[K 260587/500000: episode: 3454, duration: 0.993s, episode steps: 75, steps per second: 76, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.827 [0.000, 4.000], mean observation: 0.492 [0.360, 0.530], loss: 8.654355, mean_absolute_error: 41.539829, mean_q: 53.344093
[F[K 260643/500000: episode: 3455, duration: 0.757s, episode steps: 56, steps per second: 74, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.357 [0.000, 4.000], mean observation: 0.509 [0.470, 0.620], loss: 9.380036, mean_absolute_error: 41.627495, mean_q: 53.350361
[F[K 260710/500000: episode: 3456, duration: 0.921s, episode steps: 67, steps per second: 73, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.791 [0.000, 4.000], mean observation: 0.497 [0.390, 0.610], loss: 7.798178, mean_absolute_error: 41.049236, mean_q: 52.646278
[F[K 260793/500000: episode: 3457, duration: 1.136s, episode steps: 83, steps per second: 73, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.241 [0.000, 4.000], mean observation: 0.504 [0.460, 0.570], loss: 8.328897, mean_absolute_error: 41.202362, mean_q: 52.847126
[F[K 260851/500000: episode: 3458, duration: 0.724s, episode steps: 58, steps per second: 80, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.497 [0.470, 0.530], loss: 7.492559, mean_absolute_error: 41.444332, mean_q: 53.317448
[F[K 260911/500000: episode: 3459, duration: 0.798s, episode steps: 60, steps per second: 75, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.495 [0.450, 0.530], loss: 8.050862, mean_absolute_error: 41.409729, mean_q: 53.111843
[F[K 261002/500000: episode: 3460, duration: 1.306s, episode steps: 91, steps per second: 70, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.176 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 7.908058, mean_absolute_error: 41.588448, mean_q: 53.377071
[F[K 261155/500000: episode: 3461, duration: 1.900s, episode steps: 153, steps per second: 81, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.373 [0.000, 4.000], mean observation: 0.471 [0.310, 0.510], loss: 8.622449, mean_absolute_error: 41.257530, mean_q: 52.938522
[F[K 261227/500000: episode: 3462, duration: 1.046s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.917 [0.000, 4.000], mean observation: 0.494 [0.390, 0.530], loss: 8.024589, mean_absolute_error: 41.740520, mean_q: 53.485676
[F[K 261274/500000: episode: 3463, duration: 0.596s, episode steps: 47, steps per second: 79, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.766 [0.000, 4.000], mean observation: 0.504 [0.430, 0.610], loss: 7.307755, mean_absolute_error: 41.325966, mean_q: 53.085953
[F[K 261359/500000: episode: 3464, duration: 1.181s, episode steps: 85, steps per second: 72, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 0.515 [0.470, 0.640], loss: 8.494380, mean_absolute_error: 41.528408, mean_q: 53.183449
[F[K 261420/500000: episode: 3465, duration: 0.811s, episode steps: 61, steps per second: 75, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.607 [0.000, 4.000], mean observation: 0.496 [0.430, 0.530], loss: 7.597988, mean_absolute_error: 41.943844, mean_q: 53.814709
[F[K 261549/500000: episode: 3466, duration: 1.675s, episode steps: 129, steps per second: 77, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.884 [0.000, 4.000], mean observation: 0.501 [0.390, 0.610], loss: 8.590263, mean_absolute_error: 41.220898, mean_q: 52.753067
[F[K 261604/500000: episode: 3467, duration: 0.704s, episode steps: 55, steps per second: 78, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.527 [0.000, 4.000], mean observation: 0.502 [0.370, 0.650], loss: 7.873025, mean_absolute_error: 41.612438, mean_q: 53.391628
[F[K 261750/500000: episode: 3468, duration: 1.856s, episode steps: 146, steps per second: 79, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.411 [0.000, 4.000], mean observation: 0.479 [0.370, 0.530], loss: 8.665353, mean_absolute_error: 41.471043, mean_q: 53.138798
[F[K 261877/500000: episode: 3469, duration: 1.574s, episode steps: 127, steps per second: 81, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.929 [0.000, 4.000], mean observation: 0.472 [0.330, 0.540], loss: 8.008497, mean_absolute_error: 41.692978, mean_q: 53.470047
[F[K 261992/500000: episode: 3470, duration: 1.553s, episode steps: 115, steps per second: 74, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.322 [0.000, 4.000], mean observation: 0.500 [0.380, 0.600], loss: 8.623335, mean_absolute_error: 41.396374, mean_q: 53.072319
[F[K 262091/500000: episode: 3471, duration: 1.349s, episode steps: 99, steps per second: 73, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.394 [0.000, 4.000], mean observation: 0.492 [0.340, 0.590], loss: 7.218045, mean_absolute_error: 41.608822, mean_q: 53.331963
[F[K 262167/500000: episode: 3472, duration: 1.029s, episode steps: 76, steps per second: 74, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.395 [0.000, 4.000], mean observation: 0.515 [0.480, 0.600], loss: 8.743206, mean_absolute_error: 40.674526, mean_q: 52.109253
[F[K 262363/500000: episode: 3473, duration: 2.568s, episode steps: 196, steps per second: 76, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.235 [0.000, 4.000], mean observation: 0.486 [0.380, 0.540], loss: 8.545205, mean_absolute_error: 41.052681, mean_q: 52.668079
[F[K 262418/500000: episode: 3474, duration: 0.705s, episode steps: 55, steps per second: 78, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.527 [0.000, 4.000], mean observation: 0.517 [0.470, 0.590], loss: 7.508471, mean_absolute_error: 41.727543, mean_q: 53.460888
[F[K 262503/500000: episode: 3475, duration: 1.078s, episode steps: 85, steps per second: 79, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.259 [0.000, 4.000], mean observation: 0.490 [0.410, 0.530], loss: 7.551881, mean_absolute_error: 41.434311, mean_q: 53.142632
[F[K 262626/500000: episode: 3476, duration: 1.425s, episode steps: 123, steps per second: 86, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.772 [0.000, 4.000], mean observation: 0.508 [0.460, 0.620], loss: 6.784710, mean_absolute_error: 41.049294, mean_q: 52.580143
[F[K 262709/500000: episode: 3477, duration: 1.022s, episode steps: 83, steps per second: 81, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.241 [0.000, 4.000], mean observation: 0.495 [0.430, 0.530], loss: 7.177994, mean_absolute_error: 41.335335, mean_q: 53.057281
[F[K 262842/500000: episode: 3478, duration: 1.737s, episode steps: 133, steps per second: 77, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.376 [0.000, 4.000], mean observation: 0.482 [0.360, 0.510], loss: 7.861342, mean_absolute_error: 41.586876, mean_q: 53.256638
[F[K 262880/500000: episode: 3479, duration: 0.528s, episode steps: 38, steps per second: 72, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.263 [0.000, 4.000], mean observation: 0.511 [0.470, 0.630], loss: 6.797711, mean_absolute_error: 41.833118, mean_q: 53.642056
[F[K 262934/500000: episode: 3480, duration: 0.706s, episode steps: 54, steps per second: 76, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.796 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 7.532605, mean_absolute_error: 42.267876, mean_q: 54.194004
[F[K 263007/500000: episode: 3481, duration: 0.983s, episode steps: 73, steps per second: 74, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.822 [0.000, 4.000], mean observation: 0.506 [0.480, 0.560], loss: 8.182702, mean_absolute_error: 41.746040, mean_q: 53.530674
[F[K 263159/500000: episode: 3482, duration: 1.936s, episode steps: 152, steps per second: 79, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.842 [0.000, 4.000], mean observation: 0.497 [0.430, 0.540], loss: 6.898706, mean_absolute_error: 41.602341, mean_q: 53.379131
[F[K 263357/500000: episode: 3483, duration: 2.724s, episode steps: 198, steps per second: 73, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.460 [0.000, 4.000], mean observation: 0.491 [0.360, 0.540], loss: 8.987032, mean_absolute_error: 41.325829, mean_q: 52.894985
[F[K 263421/500000: episode: 3484, duration: 0.895s, episode steps: 64, steps per second: 71, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.516 [0.000, 4.000], mean observation: 0.484 [0.350, 0.530], loss: 9.065935, mean_absolute_error: 41.537125, mean_q: 53.257023
[F[K 263560/500000: episode: 3485, duration: 1.929s, episode steps: 139, steps per second: 72, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.576 [0.000, 4.000], mean observation: 0.484 [0.380, 0.530], loss: 8.197882, mean_absolute_error: 41.302700, mean_q: 52.913261
[F[K 263661/500000: episode: 3486, duration: 1.336s, episode steps: 101, steps per second: 76, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.683 [0.000, 4.000], mean observation: 0.505 [0.420, 0.600], loss: 7.685735, mean_absolute_error: 40.983143, mean_q: 52.539982
[F[K 263718/500000: episode: 3487, duration: 0.731s, episode steps: 57, steps per second: 78, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.684 [0.000, 4.000], mean observation: 0.500 [0.390, 0.620], loss: 7.086929, mean_absolute_error: 41.641769, mean_q: 53.291008
[F[K 263771/500000: episode: 3488, duration: 0.768s, episode steps: 53, steps per second: 69, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.660 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 7.429812, mean_absolute_error: 40.948708, mean_q: 52.592041
[F[K 263836/500000: episode: 3489, duration: 0.842s, episode steps: 65, steps per second: 77, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.954 [0.000, 4.000], mean observation: 0.511 [0.480, 0.580], loss: 7.947696, mean_absolute_error: 41.579700, mean_q: 53.197746
[F[K 263913/500000: episode: 3490, duration: 1.045s, episode steps: 77, steps per second: 74, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.987 [0.000, 4.000], mean observation: 0.492 [0.360, 0.540], loss: 9.179290, mean_absolute_error: 41.527954, mean_q: 53.336342
[F[K 263966/500000: episode: 3491, duration: 0.716s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.547 [0.000, 4.000], mean observation: 0.509 [0.440, 0.650], loss: 6.965342, mean_absolute_error: 41.168331, mean_q: 52.846706
[F[K 264030/500000: episode: 3492, duration: 0.835s, episode steps: 64, steps per second: 77, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.859 [0.000, 4.000], mean observation: 0.511 [0.490, 0.600], loss: 8.550674, mean_absolute_error: 41.488556, mean_q: 53.088882
[F[K 264112/500000: episode: 3493, duration: 1.154s, episode steps: 82, steps per second: 71, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 0.489 [0.350, 0.530], loss: 7.065638, mean_absolute_error: 41.451481, mean_q: 53.221031
[F[K 264223/500000: episode: 3494, duration: 1.515s, episode steps: 111, steps per second: 73, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.099 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 9.055324, mean_absolute_error: 41.334156, mean_q: 52.982491
[F[K 264280/500000: episode: 3495, duration: 0.836s, episode steps: 57, steps per second: 68, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.544 [0.000, 4.000], mean observation: 0.506 [0.480, 0.540], loss: 6.943785, mean_absolute_error: 41.596451, mean_q: 53.351822
[F[K 264345/500000: episode: 3496, duration: 0.876s, episode steps: 65, steps per second: 74, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.292 [0.000, 4.000], mean observation: 0.519 [0.480, 0.610], loss: 9.194862, mean_absolute_error: 41.798206, mean_q: 53.647484
[F[K 264404/500000: episode: 3497, duration: 0.759s, episode steps: 59, steps per second: 78, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.186 [0.000, 4.000], mean observation: 0.506 [0.480, 0.560], loss: 7.614171, mean_absolute_error: 41.077068, mean_q: 52.760910
[F[K 264464/500000: episode: 3498, duration: 0.721s, episode steps: 60, steps per second: 83, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.217 [0.000, 4.000], mean observation: 0.482 [0.410, 0.520], loss: 7.862646, mean_absolute_error: 41.813732, mean_q: 53.579735
[F[K 264562/500000: episode: 3499, duration: 1.269s, episode steps: 98, steps per second: 77, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.612 [0.000, 4.000], mean observation: 0.483 [0.330, 0.530], loss: 7.474682, mean_absolute_error: 41.244469, mean_q: 52.859997
[F[K 264624/500000: episode: 3500, duration: 0.853s, episode steps: 62, steps per second: 73, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.855 [0.000, 4.000], mean observation: 0.490 [0.400, 0.530], loss: 9.109302, mean_absolute_error: 41.581272, mean_q: 53.283817
[F[K 264742/500000: episode: 3501, duration: 1.646s, episode steps: 118, steps per second: 72, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.729 [0.000, 4.000], mean observation: 0.506 [0.470, 0.540], loss: 7.478770, mean_absolute_error: 42.167667, mean_q: 54.029812
[F[K 264922/500000: episode: 3502, duration: 2.415s, episode steps: 180, steps per second: 75, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.944 [0.000, 4.000], mean observation: 0.507 [0.470, 0.600], loss: 8.892453, mean_absolute_error: 41.250824, mean_q: 52.897526
[F[K 265013/500000: episode: 3503, duration: 1.191s, episode steps: 91, steps per second: 76, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.165 [0.000, 4.000], mean observation: 0.491 [0.430, 0.530], loss: 9.216859, mean_absolute_error: 41.252403, mean_q: 52.921635
[F[K 265096/500000: episode: 3504, duration: 1.188s, episode steps: 83, steps per second: 70, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.500 [0.420, 0.600], loss: 7.205686, mean_absolute_error: 41.370586, mean_q: 53.145138
[F[K 265193/500000: episode: 3505, duration: 1.237s, episode steps: 97, steps per second: 78, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.495 [0.470, 0.530], loss: 9.046127, mean_absolute_error: 41.491482, mean_q: 53.244610
[F[K 265275/500000: episode: 3506, duration: 0.955s, episode steps: 82, steps per second: 86, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.817 [0.000, 4.000], mean observation: 0.498 [0.460, 0.530], loss: 8.323107, mean_absolute_error: 41.879890, mean_q: 53.700958
[F[K 265352/500000: episode: 3507, duration: 1.030s, episode steps: 77, steps per second: 75, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.584 [0.000, 4.000], mean observation: 0.490 [0.420, 0.530], loss: 8.083681, mean_absolute_error: 41.334705, mean_q: 52.993793
[F[K 265429/500000: episode: 3508, duration: 1.056s, episode steps: 77, steps per second: 73, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.182 [0.000, 4.000], mean observation: 0.517 [0.480, 0.640], loss: 8.146659, mean_absolute_error: 41.446434, mean_q: 53.127617
[F[K 265476/500000: episode: 3509, duration: 0.718s, episode steps: 47, steps per second: 65, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.936 [0.000, 4.000], mean observation: 0.521 [0.470, 0.640], loss: 7.353341, mean_absolute_error: 42.160625, mean_q: 54.222401
[F[K 265554/500000: episode: 3510, duration: 1.079s, episode steps: 78, steps per second: 72, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.231 [0.000, 4.000], mean observation: 0.474 [0.360, 0.520], loss: 7.680100, mean_absolute_error: 40.787125, mean_q: 52.363865
[F[K 265615/500000: episode: 3511, duration: 0.721s, episode steps: 61, steps per second: 85, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.623 [0.000, 4.000], mean observation: 0.508 [0.480, 0.570], loss: 8.968787, mean_absolute_error: 42.014999, mean_q: 53.799126
[F[K 265732/500000: episode: 3512, duration: 1.643s, episode steps: 117, steps per second: 71, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.325 [0.000, 4.000], mean observation: 0.494 [0.320, 0.620], loss: 8.236105, mean_absolute_error: 41.455330, mean_q: 53.076920
[F[K 265817/500000: episode: 3513, duration: 1.221s, episode steps: 85, steps per second: 70, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.859 [0.000, 4.000], mean observation: 0.516 [0.470, 0.620], loss: 8.058846, mean_absolute_error: 41.832058, mean_q: 53.714413
[F[K 265963/500000: episode: 3514, duration: 1.851s, episode steps: 146, steps per second: 79, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.481 [0.350, 0.520], loss: 8.112803, mean_absolute_error: 41.874668, mean_q: 53.612785
[F[K 266021/500000: episode: 3515, duration: 0.810s, episode steps: 58, steps per second: 72, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.845 [0.000, 4.000], mean observation: 0.509 [0.460, 0.620], loss: 8.820405, mean_absolute_error: 41.538666, mean_q: 53.265488
[F[K 266111/500000: episode: 3516, duration: 1.324s, episode steps: 90, steps per second: 68, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.922 [0.000, 4.000], mean observation: 0.501 [0.410, 0.590], loss: 8.585725, mean_absolute_error: 41.254147, mean_q: 52.939598
[F[K 266249/500000: episode: 3517, duration: 1.929s, episode steps: 138, steps per second: 72, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.022 [0.000, 4.000], mean observation: 0.511 [0.470, 0.630], loss: 8.185906, mean_absolute_error: 41.720181, mean_q: 53.523685
[F[K 266343/500000: episode: 3518, duration: 1.416s, episode steps: 94, steps per second: 66, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.904 [0.000, 4.000], mean observation: 0.488 [0.410, 0.530], loss: 7.852571, mean_absolute_error: 42.103600, mean_q: 53.992336
[F[K 266422/500000: episode: 3519, duration: 1.123s, episode steps: 79, steps per second: 70, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.797 [0.000, 4.000], mean observation: 0.506 [0.470, 0.560], loss: 8.289413, mean_absolute_error: 42.057705, mean_q: 53.952198
[F[K 266485/500000: episode: 3520, duration: 0.969s, episode steps: 63, steps per second: 65, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 0.495 [0.410, 0.530], loss: 8.089622, mean_absolute_error: 41.721519, mean_q: 53.509300
[F[K 266564/500000: episode: 3521, duration: 1.239s, episode steps: 79, steps per second: 64, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 0.517 [0.480, 0.660], loss: 7.737509, mean_absolute_error: 41.709019, mean_q: 53.547184
[F[K 266692/500000: episode: 3522, duration: 1.921s, episode steps: 128, steps per second: 67, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.742 [0.000, 4.000], mean observation: 0.518 [0.470, 0.660], loss: 8.404478, mean_absolute_error: 42.216389, mean_q: 54.139412
[F[K 266756/500000: episode: 3523, duration: 0.908s, episode steps: 64, steps per second: 71, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.502 [0.390, 0.620], loss: 7.723560, mean_absolute_error: 42.198372, mean_q: 54.082733
[F[K 266828/500000: episode: 3524, duration: 1.038s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.222 [0.000, 4.000], mean observation: 0.501 [0.360, 0.640], loss: 8.924656, mean_absolute_error: 41.613235, mean_q: 53.391129
[F[K 266899/500000: episode: 3525, duration: 1.006s, episode steps: 71, steps per second: 71, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.485 [0.370, 0.530], loss: 8.598707, mean_absolute_error: 41.600632, mean_q: 53.427326
[F[K 267026/500000: episode: 3526, duration: 2.016s, episode steps: 127, steps per second: 63, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.181 [0.000, 4.000], mean observation: 0.507 [0.470, 0.570], loss: 7.995380, mean_absolute_error: 42.236851, mean_q: 54.108082
[F[K 267076/500000: episode: 3527, duration: 0.700s, episode steps: 50, steps per second: 71, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.380 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 9.230783, mean_absolute_error: 41.751610, mean_q: 53.393044
[F[K 267153/500000: episode: 3528, duration: 1.022s, episode steps: 77, steps per second: 75, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.247 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 8.879451, mean_absolute_error: 41.457199, mean_q: 53.184708
[F[K 267231/500000: episode: 3529, duration: 1.106s, episode steps: 78, steps per second: 71, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.494 [0.410, 0.550], loss: 6.845041, mean_absolute_error: 41.879021, mean_q: 53.712219
[F[K 267286/500000: episode: 3530, duration: 0.920s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.655 [0.000, 4.000], mean observation: 0.499 [0.420, 0.540], loss: 6.375752, mean_absolute_error: 41.577736, mean_q: 53.364323
[F[K 267334/500000: episode: 3531, duration: 0.743s, episode steps: 48, steps per second: 65, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.520 [0.470, 0.650], loss: 7.323687, mean_absolute_error: 42.226837, mean_q: 54.248646
[F[K 267372/500000: episode: 3532, duration: 0.597s, episode steps: 38, steps per second: 64, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.868 [0.000, 4.000], mean observation: 0.514 [0.470, 0.610], loss: 6.977476, mean_absolute_error: 41.906715, mean_q: 53.672173
[F[K 267520/500000: episode: 3533, duration: 2.027s, episode steps: 148, steps per second: 73, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.507 [0.450, 0.570], loss: 8.175347, mean_absolute_error: 41.907616, mean_q: 53.738365
[F[K 267583/500000: episode: 3534, duration: 0.944s, episode steps: 63, steps per second: 67, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.286 [0.000, 4.000], mean observation: 0.507 [0.470, 0.590], loss: 7.342060, mean_absolute_error: 41.516750, mean_q: 53.244606
[F[K 267663/500000: episode: 3535, duration: 1.152s, episode steps: 80, steps per second: 69, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.476 [0.360, 0.530], loss: 7.477708, mean_absolute_error: 42.117691, mean_q: 53.955620
[F[K 267727/500000: episode: 3536, duration: 0.983s, episode steps: 64, steps per second: 65, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.766 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 7.751857, mean_absolute_error: 41.713356, mean_q: 53.368404
[F[K 267801/500000: episode: 3537, duration: 1.214s, episode steps: 74, steps per second: 61, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.946 [0.000, 4.000], mean observation: 0.488 [0.360, 0.530], loss: 7.278104, mean_absolute_error: 41.698158, mean_q: 53.543182
[F[K 267850/500000: episode: 3538, duration: 0.795s, episode steps: 49, steps per second: 62, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.959 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 8.021026, mean_absolute_error: 42.625011, mean_q: 54.483009
[F[K 267910/500000: episode: 3539, duration: 0.972s, episode steps: 60, steps per second: 62, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.467 [0.000, 4.000], mean observation: 0.517 [0.490, 0.610], loss: 6.667345, mean_absolute_error: 42.163776, mean_q: 54.063377
[F[K 267997/500000: episode: 3540, duration: 1.320s, episode steps: 87, steps per second: 66, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.816 [0.000, 4.000], mean observation: 0.486 [0.370, 0.530], loss: 8.474515, mean_absolute_error: 41.781960, mean_q: 53.590019
[F[K 268038/500000: episode: 3541, duration: 0.688s, episode steps: 41, steps per second: 60, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.341 [0.000, 4.000], mean observation: 0.527 [0.470, 0.650], loss: 9.848056, mean_absolute_error: 41.475559, mean_q: 53.040909
[F[K 268103/500000: episode: 3542, duration: 1.065s, episode steps: 65, steps per second: 61, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.862 [0.000, 4.000], mean observation: 0.497 [0.430, 0.530], loss: 7.795102, mean_absolute_error: 41.907562, mean_q: 53.772114
[F[K 268182/500000: episode: 3543, duration: 1.230s, episode steps: 79, steps per second: 64, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.975 [0.000, 4.000], mean observation: 0.499 [0.470, 0.530], loss: 7.509601, mean_absolute_error: 42.157852, mean_q: 54.131508
[F[K 268318/500000: episode: 3544, duration: 2.160s, episode steps: 136, steps per second: 63, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.279 [0.000, 4.000], mean observation: 0.513 [0.430, 0.680], loss: 7.459357, mean_absolute_error: 41.702000, mean_q: 53.534210
[F[K 268408/500000: episode: 3545, duration: 1.484s, episode steps: 90, steps per second: 61, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.956 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 7.649702, mean_absolute_error: 41.822327, mean_q: 53.688828
[F[K 268485/500000: episode: 3546, duration: 1.300s, episode steps: 77, steps per second: 59, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.818 [0.000, 4.000], mean observation: 0.488 [0.420, 0.530], loss: 7.289357, mean_absolute_error: 41.995445, mean_q: 53.981041
[F[K 268624/500000: episode: 3547, duration: 2.276s, episode steps: 139, steps per second: 61, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.108 [0.000, 4.000], mean observation: 0.479 [0.340, 0.530], loss: 7.510855, mean_absolute_error: 42.099621, mean_q: 54.091648
[F[K 268682/500000: episode: 3548, duration: 0.876s, episode steps: 58, steps per second: 66, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.914 [0.000, 4.000], mean observation: 0.487 [0.360, 0.550], loss: 8.772789, mean_absolute_error: 42.550812, mean_q: 54.488216
[F[K 268735/500000: episode: 3549, duration: 0.873s, episode steps: 53, steps per second: 61, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.453 [0.000, 4.000], mean observation: 0.512 [0.480, 0.560], loss: 8.245917, mean_absolute_error: 41.844585, mean_q: 53.728069
[F[K 268801/500000: episode: 3550, duration: 1.019s, episode steps: 66, steps per second: 65, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.518 [0.470, 0.620], loss: 8.449407, mean_absolute_error: 41.632759, mean_q: 53.311932
[F[K 268879/500000: episode: 3551, duration: 1.319s, episode steps: 78, steps per second: 59, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.821 [0.000, 4.000], mean observation: 0.488 [0.360, 0.550], loss: 10.251904, mean_absolute_error: 42.017296, mean_q: 53.735554
[F[K 268961/500000: episode: 3552, duration: 1.341s, episode steps: 82, steps per second: 61, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.232 [0.000, 4.000], mean observation: 0.504 [0.460, 0.610], loss: 7.780797, mean_absolute_error: 42.528255, mean_q: 54.516159
[F[K 269009/500000: episode: 3553, duration: 0.747s, episode steps: 48, steps per second: 64, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.854 [0.000, 4.000], mean observation: 0.501 [0.360, 0.650], loss: 7.124388, mean_absolute_error: 41.751705, mean_q: 53.540401
[F[K 269093/500000: episode: 3554, duration: 1.292s, episode steps: 84, steps per second: 65, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 0.487 [0.380, 0.530], loss: 7.780369, mean_absolute_error: 41.996662, mean_q: 53.726540
[F[K 269184/500000: episode: 3555, duration: 1.455s, episode steps: 91, steps per second: 63, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.253 [0.000, 4.000], mean observation: 0.481 [0.350, 0.530], loss: 8.217235, mean_absolute_error: 41.832924, mean_q: 53.619881
[F[K 269243/500000: episode: 3556, duration: 0.904s, episode steps: 59, steps per second: 65, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.034 [0.000, 4.000], mean observation: 0.505 [0.460, 0.570], loss: 7.413129, mean_absolute_error: 42.177917, mean_q: 54.081951
[F[K 269363/500000: episode: 3557, duration: 2.149s, episode steps: 120, steps per second: 56, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.505 [0.440, 0.600], loss: 7.898821, mean_absolute_error: 42.112148, mean_q: 53.993607
[F[K 269445/500000: episode: 3558, duration: 1.425s, episode steps: 82, steps per second: 58, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.496 [0.380, 0.610], loss: 8.109214, mean_absolute_error: 41.581841, mean_q: 53.430653
[F[K 269513/500000: episode: 3559, duration: 1.120s, episode steps: 68, steps per second: 61, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.397 [0.000, 4.000], mean observation: 0.489 [0.430, 0.530], loss: 7.976369, mean_absolute_error: 42.317032, mean_q: 54.275707
[F[K 269673/500000: episode: 3560, duration: 2.468s, episode steps: 160, steps per second: 65, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.931 [0.000, 4.000], mean observation: 0.495 [0.440, 0.530], loss: 7.384961, mean_absolute_error: 41.658592, mean_q: 53.385334
[F[K 269750/500000: episode: 3561, duration: 1.301s, episode steps: 77, steps per second: 59, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.481 [0.000, 4.000], mean observation: 0.518 [0.500, 0.630], loss: 7.720169, mean_absolute_error: 41.862858, mean_q: 53.697945
[F[K 269807/500000: episode: 3562, duration: 0.986s, episode steps: 57, steps per second: 58, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.982 [0.000, 4.000], mean observation: 0.522 [0.500, 0.630], loss: 8.104790, mean_absolute_error: 42.109013, mean_q: 53.884254
[F[K 269863/500000: episode: 3563, duration: 0.918s, episode steps: 56, steps per second: 61, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.506 [0.440, 0.610], loss: 8.988478, mean_absolute_error: 42.072712, mean_q: 53.870796
[F[K 269922/500000: episode: 3564, duration: 1.023s, episode steps: 59, steps per second: 58, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.373 [0.000, 4.000], mean observation: 0.500 [0.390, 0.600], loss: 8.326148, mean_absolute_error: 42.684330, mean_q: 54.731792
[F[K 270103/500000: episode: 3565, duration: 3.355s, episode steps: 181, steps per second: 54, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.453 [0.000, 4.000], mean observation: 0.496 [0.400, 0.620], loss: 8.725182, mean_absolute_error: 41.586834, mean_q: 53.305992
[F[K 270141/500000: episode: 3566, duration: 0.535s, episode steps: 38, steps per second: 71, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.895 [0.000, 4.000], mean observation: 0.504 [0.410, 0.630], loss: 9.268979, mean_absolute_error: 42.300346, mean_q: 54.230110
[F[K 270229/500000: episode: 3567, duration: 1.422s, episode steps: 88, steps per second: 62, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.977 [0.000, 4.000], mean observation: 0.482 [0.400, 0.530], loss: 8.294170, mean_absolute_error: 42.112099, mean_q: 54.059628
[F[K 270300/500000: episode: 3568, duration: 1.177s, episode steps: 71, steps per second: 60, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.099 [0.000, 4.000], mean observation: 0.505 [0.480, 0.580], loss: 8.387679, mean_absolute_error: 41.039593, mean_q: 52.585361
[F[K 270420/500000: episode: 3569, duration: 2.007s, episode steps: 120, steps per second: 60, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.542 [0.000, 4.000], mean observation: 0.505 [0.460, 0.570], loss: 8.607076, mean_absolute_error: 41.895184, mean_q: 53.742432
[F[K 270515/500000: episode: 3570, duration: 1.816s, episode steps: 95, steps per second: 52, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.510 [0.480, 0.620], loss: 8.225301, mean_absolute_error: 41.120373, mean_q: 52.686806
[F[K 270570/500000: episode: 3571, duration: 1.008s, episode steps: 55, steps per second: 55, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.927 [0.000, 4.000], mean observation: 0.513 [0.490, 0.610], loss: 10.749627, mean_absolute_error: 41.869053, mean_q: 53.520527
[F[K 270697/500000: episode: 3572, duration: 2.266s, episode steps: 127, steps per second: 56, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.244 [0.000, 4.000], mean observation: 0.513 [0.470, 0.630], loss: 8.074781, mean_absolute_error: 41.946682, mean_q: 53.704472
[F[K 270744/500000: episode: 3573, duration: 0.877s, episode steps: 47, steps per second: 54, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.979 [0.000, 4.000], mean observation: 0.490 [0.400, 0.530], loss: 8.060390, mean_absolute_error: 41.572239, mean_q: 53.195274
[F[K 270811/500000: episode: 3574, duration: 1.164s, episode steps: 67, steps per second: 58, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.866 [0.000, 4.000], mean observation: 0.492 [0.370, 0.530], loss: 8.258840, mean_absolute_error: 41.727352, mean_q: 53.465511
[F[K 270879/500000: episode: 3575, duration: 1.176s, episode steps: 68, steps per second: 58, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.497 [0.390, 0.600], loss: 7.892727, mean_absolute_error: 42.026932, mean_q: 53.886562
[F[K 270926/500000: episode: 3576, duration: 0.894s, episode steps: 47, steps per second: 53, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.489 [0.000, 4.000], mean observation: 0.509 [0.460, 0.630], loss: 7.675779, mean_absolute_error: 41.712170, mean_q: 53.473637
[F[K 271021/500000: episode: 3577, duration: 1.746s, episode steps: 95, steps per second: 54, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.518 [0.490, 0.600], loss: 8.220305, mean_absolute_error: 41.537727, mean_q: 53.288090
[F[K 271086/500000: episode: 3578, duration: 1.213s, episode steps: 65, steps per second: 54, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.662 [0.000, 4.000], mean observation: 0.514 [0.500, 0.580], loss: 9.237944, mean_absolute_error: 41.777779, mean_q: 53.451187
[F[K 271150/500000: episode: 3579, duration: 1.107s, episode steps: 64, steps per second: 58, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.498 [0.350, 0.620], loss: 8.404646, mean_absolute_error: 41.960880, mean_q: 53.684990
[F[K 271224/500000: episode: 3580, duration: 1.140s, episode steps: 74, steps per second: 65, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.905 [0.000, 4.000], mean observation: 0.493 [0.350, 0.590], loss: 6.961693, mean_absolute_error: 41.533886, mean_q: 53.173447
[F[K 271325/500000: episode: 3581, duration: 1.496s, episode steps: 101, steps per second: 68, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.644 [0.000, 4.000], mean observation: 0.506 [0.460, 0.610], loss: 7.293337, mean_absolute_error: 41.416836, mean_q: 53.184116
[F[K 271412/500000: episode: 3582, duration: 1.259s, episode steps: 87, steps per second: 69, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.496 [0.390, 0.560], loss: 8.015117, mean_absolute_error: 41.450329, mean_q: 53.118584
[F[K 271493/500000: episode: 3583, duration: 0.956s, episode steps: 81, steps per second: 85, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.519 [0.000, 4.000], mean observation: 0.493 [0.400, 0.530], loss: 7.899546, mean_absolute_error: 41.507710, mean_q: 53.079372
[F[K 271572/500000: episode: 3584, duration: 1.089s, episode steps: 79, steps per second: 73, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.506 [0.000, 4.000], mean observation: 0.508 [0.440, 0.610], loss: 7.463120, mean_absolute_error: 41.823673, mean_q: 53.539719
[F[K 271655/500000: episode: 3585, duration: 1.268s, episode steps: 83, steps per second: 65, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.060 [0.000, 4.000], mean observation: 0.486 [0.370, 0.530], loss: 7.506404, mean_absolute_error: 42.046516, mean_q: 53.862812
[F[K 271751/500000: episode: 3586, duration: 1.477s, episode steps: 96, steps per second: 65, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.115 [0.000, 4.000], mean observation: 0.512 [0.480, 0.560], loss: 9.612099, mean_absolute_error: 41.776947, mean_q: 53.448009
[F[K 271818/500000: episode: 3587, duration: 0.895s, episode steps: 67, steps per second: 75, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.490 [0.450, 0.510], loss: 8.513323, mean_absolute_error: 41.624222, mean_q: 53.201401
[F[K 271888/500000: episode: 3588, duration: 0.976s, episode steps: 70, steps per second: 72, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.957 [0.000, 4.000], mean observation: 0.498 [0.340, 0.620], loss: 8.046350, mean_absolute_error: 41.289989, mean_q: 52.846771
[F[K 272088/500000: episode: 3589, duration: 2.857s, episode steps: 200, steps per second: 70, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.215 [0.000, 4.000], mean observation: 0.480 [0.340, 0.510], loss: 7.766397, mean_absolute_error: 41.740440, mean_q: 53.483208
[F[K 272139/500000: episode: 3590, duration: 0.760s, episode steps: 51, steps per second: 67, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 7.590913, mean_absolute_error: 41.448639, mean_q: 53.103123
[F[K 272249/500000: episode: 3591, duration: 1.583s, episode steps: 110, steps per second: 69, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 0.496 [0.460, 0.520], loss: 8.334297, mean_absolute_error: 41.947639, mean_q: 53.798588
[F[K 272286/500000: episode: 3592, duration: 0.583s, episode steps: 37, steps per second: 63, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.676 [0.000, 4.000], mean observation: 0.520 [0.470, 0.610], loss: 8.094684, mean_absolute_error: 41.628391, mean_q: 53.228275
[F[K 272350/500000: episode: 3593, duration: 0.906s, episode steps: 64, steps per second: 71, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.497 [0.390, 0.560], loss: 9.207008, mean_absolute_error: 41.438931, mean_q: 53.081100
[F[K 272407/500000: episode: 3594, duration: 0.887s, episode steps: 57, steps per second: 64, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.825 [0.000, 4.000], mean observation: 0.477 [0.350, 0.500], loss: 8.479267, mean_absolute_error: 41.340275, mean_q: 52.876823
[F[K 272499/500000: episode: 3595, duration: 1.491s, episode steps: 92, steps per second: 62, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 9.124073, mean_absolute_error: 41.337627, mean_q: 52.991024
[F[K 272613/500000: episode: 3596, duration: 1.619s, episode steps: 114, steps per second: 70, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.632 [0.000, 4.000], mean observation: 0.502 [0.480, 0.550], loss: 8.656064, mean_absolute_error: 41.592281, mean_q: 53.280670
[F[K 272710/500000: episode: 3597, duration: 1.350s, episode steps: 97, steps per second: 72, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.794 [0.000, 4.000], mean observation: 0.512 [0.490, 0.600], loss: 9.185708, mean_absolute_error: 41.416313, mean_q: 52.956085
[F[K 272769/500000: episode: 3598, duration: 0.886s, episode steps: 59, steps per second: 67, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.271 [0.000, 4.000], mean observation: 0.487 [0.370, 0.530], loss: 8.930944, mean_absolute_error: 40.343349, mean_q: 51.610775
[F[K 272943/500000: episode: 3599, duration: 2.729s, episode steps: 174, steps per second: 64, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 8.006339, mean_absolute_error: 41.620190, mean_q: 53.334473
[F[K 273015/500000: episode: 3600, duration: 1.120s, episode steps: 72, steps per second: 64, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.494 [0.400, 0.560], loss: 8.492673, mean_absolute_error: 41.743408, mean_q: 53.398178
[F[K 273077/500000: episode: 3601, duration: 0.996s, episode steps: 62, steps per second: 62, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.452 [0.000, 4.000], mean observation: 0.494 [0.350, 0.570], loss: 8.902460, mean_absolute_error: 41.468197, mean_q: 53.197006
[F[K 273258/500000: episode: 3602, duration: 2.814s, episode steps: 181, steps per second: 64, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.504 [0.470, 0.580], loss: 8.299679, mean_absolute_error: 41.153069, mean_q: 52.688816
[F[K 273301/500000: episode: 3603, duration: 0.648s, episode steps: 43, steps per second: 66, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.558 [0.000, 4.000], mean observation: 0.513 [0.470, 0.620], loss: 8.294315, mean_absolute_error: 41.905014, mean_q: 53.727539
[F[K 273415/500000: episode: 3604, duration: 1.783s, episode steps: 114, steps per second: 64, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.851 [0.000, 4.000], mean observation: 0.487 [0.380, 0.520], loss: 8.466242, mean_absolute_error: 41.285900, mean_q: 52.888161
[F[K 273615/500000: episode: 3605, duration: 2.537s, episode steps: 200, steps per second: 79, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.990 [0.000, 4.000], mean observation: 0.516 [0.460, 0.710], loss: 7.179075, mean_absolute_error: 41.550217, mean_q: 53.192371
[F[K 273699/500000: episode: 3606, duration: 1.358s, episode steps: 84, steps per second: 62, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.298 [0.000, 4.000], mean observation: 0.490 [0.370, 0.550], loss: 8.227166, mean_absolute_error: 40.701546, mean_q: 52.246738
[F[K 273765/500000: episode: 3607, duration: 1.060s, episode steps: 66, steps per second: 62, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.494 [0.470, 0.510], loss: 8.732725, mean_absolute_error: 41.709785, mean_q: 53.361015
[F[K 273857/500000: episode: 3608, duration: 1.441s, episode steps: 92, steps per second: 64, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.978 [0.000, 4.000], mean observation: 0.506 [0.460, 0.580], loss: 7.801991, mean_absolute_error: 41.436035, mean_q: 53.084373[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
argv[0]=
[F[K 273929/500000: episode: 3609, duration: 1.047s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 0.489 [0.380, 0.540], loss: 8.699949, mean_absolute_error: 41.759712, mean_q: 53.580730
[F[K 274013/500000: episode: 3610, duration: 1.296s, episode steps: 84, steps per second: 65, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.487 [0.420, 0.530], loss: 7.656150, mean_absolute_error: 41.677032, mean_q: 53.474236
[F[K 274093/500000: episode: 3611, duration: 1.192s, episode steps: 80, steps per second: 67, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.484 [0.400, 0.540], loss: 8.430385, mean_absolute_error: 41.755760, mean_q: 53.497284
[F[K 274181/500000: episode: 3612, duration: 1.394s, episode steps: 88, steps per second: 63, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.330 [0.000, 4.000], mean observation: 0.485 [0.390, 0.520], loss: 6.760289, mean_absolute_error: 41.493008, mean_q: 53.139084
[F[K 274245/500000: episode: 3613, duration: 0.863s, episode steps: 64, steps per second: 74, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.844 [0.000, 4.000], mean observation: 0.483 [0.370, 0.530], loss: 8.225536, mean_absolute_error: 41.194557, mean_q: 52.872295
[F[K 274305/500000: episode: 3614, duration: 0.923s, episode steps: 60, steps per second: 65, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.767 [0.000, 4.000], mean observation: 0.504 [0.440, 0.590], loss: 7.817864, mean_absolute_error: 41.870789, mean_q: 53.635738
[F[K 274505/500000: episode: 3615, duration: 2.799s, episode steps: 200, steps per second: 71, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.960 [0.000, 4.000], mean observation: 0.494 [0.350, 0.600], loss: 8.349201, mean_absolute_error: 41.651257, mean_q: 53.403534
[F[K 274594/500000: episode: 3616, duration: 1.177s, episode steps: 89, steps per second: 76, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.573 [0.000, 4.000], mean observation: 0.511 [0.500, 0.580], loss: 9.250847, mean_absolute_error: 41.740826, mean_q: 53.447205
[F[K 274636/500000: episode: 3617, duration: 0.659s, episode steps: 42, steps per second: 64, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.498 [0.360, 0.630], loss: 8.051263, mean_absolute_error: 41.234013, mean_q: 52.751953
[F[K 274741/500000: episode: 3618, duration: 1.342s, episode steps: 105, steps per second: 78, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.876 [0.000, 4.000], mean observation: 0.505 [0.480, 0.560], loss: 8.335375, mean_absolute_error: 41.676285, mean_q: 53.423916
[F[K 274831/500000: episode: 3619, duration: 1.030s, episode steps: 90, steps per second: 87, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.044 [0.000, 4.000], mean observation: 0.493 [0.430, 0.520], loss: 8.186388, mean_absolute_error: 42.015347, mean_q: 53.693974
[F[K 274981/500000: episode: 3620, duration: 1.709s, episode steps: 150, steps per second: 88, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.233 [0.000, 4.000], mean observation: 0.504 [0.420, 0.640], loss: 8.539077, mean_absolute_error: 41.323009, mean_q: 52.931236
[F[K 275090/500000: episode: 3621, duration: 1.263s, episode steps: 109, steps per second: 86, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.018 [0.000, 4.000], mean observation: 0.504 [0.470, 0.570], loss: 7.157492, mean_absolute_error: 42.150436, mean_q: 54.031548
[F[K 275145/500000: episode: 3622, duration: 0.650s, episode steps: 55, steps per second: 85, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.945 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 7.286072, mean_absolute_error: 41.016468, mean_q: 52.448082
[F[K 275220/500000: episode: 3623, duration: 0.859s, episode steps: 75, steps per second: 87, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.787 [0.000, 4.000], mean observation: 0.512 [0.450, 0.630], loss: 7.447188, mean_absolute_error: 41.441990, mean_q: 53.090206
[F[K 275298/500000: episode: 3624, duration: 0.845s, episode steps: 78, steps per second: 92, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.205 [0.000, 4.000], mean observation: 0.501 [0.410, 0.640], loss: 8.105768, mean_absolute_error: 41.570827, mean_q: 53.272762
[F[K 275390/500000: episode: 3625, duration: 1.114s, episode steps: 92, steps per second: 83, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.501 [0.450, 0.560], loss: 7.427446, mean_absolute_error: 41.503567, mean_q: 53.227772
[F[K 275509/500000: episode: 3626, duration: 1.390s, episode steps: 119, steps per second: 86, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.227 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 7.892908, mean_absolute_error: 41.930061, mean_q: 53.673973
[F[K 275601/500000: episode: 3627, duration: 1.009s, episode steps: 92, steps per second: 91, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.913 [0.000, 4.000], mean observation: 0.517 [0.470, 0.610], loss: 7.800220, mean_absolute_error: 41.119949, mean_q: 52.784790
[F[K 275677/500000: episode: 3628, duration: 0.928s, episode steps: 76, steps per second: 82, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.605 [0.000, 4.000], mean observation: 0.479 [0.380, 0.530], loss: 7.936982, mean_absolute_error: 41.595791, mean_q: 53.209084
[F[K 275757/500000: episode: 3629, duration: 0.887s, episode steps: 80, steps per second: 90, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.225 [0.000, 4.000], mean observation: 0.488 [0.440, 0.520], loss: 10.099359, mean_absolute_error: 41.173046, mean_q: 52.792500
[F[K 275849/500000: episode: 3630, duration: 1.074s, episode steps: 92, steps per second: 86, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.826 [0.000, 4.000], mean observation: 0.493 [0.380, 0.530], loss: 8.726382, mean_absolute_error: 41.688435, mean_q: 53.354843
[F[K 275902/500000: episode: 3631, duration: 0.609s, episode steps: 53, steps per second: 87, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.264 [0.000, 4.000], mean observation: 0.513 [0.470, 0.610], loss: 8.601411, mean_absolute_error: 41.151165, mean_q: 52.736488
[F[K 275951/500000: episode: 3632, duration: 0.561s, episode steps: 49, steps per second: 87, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.735 [0.000, 4.000], mean observation: 0.520 [0.470, 0.610], loss: 9.317979, mean_absolute_error: 41.100754, mean_q: 52.725716
[F[K 276024/500000: episode: 3633, duration: 0.828s, episode steps: 73, steps per second: 88, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.178 [0.000, 4.000], mean observation: 0.495 [0.420, 0.560], loss: 7.244156, mean_absolute_error: 41.613106, mean_q: 53.338902
[F[K 276078/500000: episode: 3634, duration: 0.633s, episode steps: 54, steps per second: 85, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.222 [0.000, 4.000], mean observation: 0.498 [0.460, 0.530], loss: 7.345485, mean_absolute_error: 41.888504, mean_q: 53.748955
[F[K 276158/500000: episode: 3635, duration: 0.896s, episode steps: 80, steps per second: 89, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.505 [0.410, 0.630], loss: 9.351851, mean_absolute_error: 41.694294, mean_q: 53.250835
[F[K 276217/500000: episode: 3636, duration: 0.634s, episode steps: 59, steps per second: 93, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.509 [0.450, 0.610], loss: 9.786715, mean_absolute_error: 41.574314, mean_q: 53.216705
[F[K 276379/500000: episode: 3637, duration: 1.594s, episode steps: 162, steps per second: 102, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.265 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 7.069052, mean_absolute_error: 41.737026, mean_q: 53.507126
[F[K 276494/500000: episode: 3638, duration: 1.251s, episode steps: 115, steps per second: 92, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.557 [0.000, 4.000], mean observation: 0.511 [0.460, 0.570], loss: 7.234838, mean_absolute_error: 41.387589, mean_q: 53.151134
[F[K 276560/500000: episode: 3639, duration: 0.775s, episode steps: 66, steps per second: 85, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.182 [0.000, 4.000], mean observation: 0.511 [0.470, 0.580], loss: 7.864745, mean_absolute_error: 41.069195, mean_q: 52.689056
[F[K 276652/500000: episode: 3640, duration: 1.020s, episode steps: 92, steps per second: 90, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.989 [0.000, 4.000], mean observation: 0.502 [0.440, 0.600], loss: 9.584998, mean_absolute_error: 41.140831, mean_q: 52.656036
[F[K 276725/500000: episode: 3641, duration: 0.936s, episode steps: 73, steps per second: 78, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.753 [0.000, 4.000], mean observation: 0.514 [0.490, 0.590], loss: 9.226473, mean_absolute_error: 42.025650, mean_q: 53.910099
[F[K 276779/500000: episode: 3642, duration: 0.623s, episode steps: 54, steps per second: 87, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.537 [0.000, 4.000], mean observation: 0.503 [0.380, 0.640], loss: 7.851955, mean_absolute_error: 40.949753, mean_q: 52.601776
[F[K 276873/500000: episode: 3643, duration: 1.102s, episode steps: 94, steps per second: 85, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.181 [0.000, 4.000], mean observation: 0.488 [0.390, 0.530], loss: 8.528569, mean_absolute_error: 41.896889, mean_q: 53.583740
[F[K 276970/500000: episode: 3644, duration: 1.080s, episode steps: 97, steps per second: 90, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.489 [0.440, 0.520], loss: 7.749928, mean_absolute_error: 41.705528, mean_q: 53.486374
[F[K 277025/500000: episode: 3645, duration: 0.686s, episode steps: 55, steps per second: 80, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.514 [0.470, 0.630], loss: 8.002820, mean_absolute_error: 40.538578, mean_q: 51.914005
[F[K 277088/500000: episode: 3646, duration: 0.715s, episode steps: 63, steps per second: 88, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.635 [0.000, 4.000], mean observation: 0.495 [0.400, 0.550], loss: 6.328721, mean_absolute_error: 41.781311, mean_q: 53.599251
[F[K 277141/500000: episode: 3647, duration: 0.617s, episode steps: 53, steps per second: 86, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.505 [0.400, 0.650], loss: 8.939146, mean_absolute_error: 41.189434, mean_q: 52.563034
[F[K 277189/500000: episode: 3648, duration: 0.546s, episode steps: 48, steps per second: 88, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.042 [0.000, 4.000], mean observation: 0.505 [0.430, 0.620], loss: 8.024644, mean_absolute_error: 40.865170, mean_q: 52.416260
[F[K 277283/500000: episode: 3649, duration: 1.080s, episode steps: 94, steps per second: 87, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.330 [0.000, 4.000], mean observation: 0.507 [0.470, 0.570], loss: 8.114385, mean_absolute_error: 41.730953, mean_q: 53.465374
[F[K 277338/500000: episode: 3650, duration: 0.640s, episode steps: 55, steps per second: 86, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.945 [0.000, 4.000], mean observation: 0.502 [0.410, 0.580], loss: 10.960764, mean_absolute_error: 41.560123, mean_q: 53.250965
[F[K 277414/500000: episode: 3651, duration: 0.923s, episode steps: 76, steps per second: 82, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.987 [0.000, 4.000], mean observation: 0.504 [0.480, 0.530], loss: 8.641253, mean_absolute_error: 41.049782, mean_q: 52.548206
[F[K 277526/500000: episode: 3652, duration: 1.359s, episode steps: 112, steps per second: 82, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 0.489 [0.400, 0.520], loss: 8.258254, mean_absolute_error: 41.789043, mean_q: 53.498440
[F[K 277600/500000: episode: 3653, duration: 0.844s, episode steps: 74, steps per second: 88, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.484 [0.360, 0.530], loss: 7.361824, mean_absolute_error: 41.567787, mean_q: 53.342278
[F[K 277710/500000: episode: 3654, duration: 1.285s, episode steps: 110, steps per second: 86, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.327 [0.000, 4.000], mean observation: 0.480 [0.370, 0.530], loss: 7.803788, mean_absolute_error: 41.615784, mean_q: 53.293095
[F[K 277776/500000: episode: 3655, duration: 0.768s, episode steps: 66, steps per second: 86, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.379 [0.000, 4.000], mean observation: 0.495 [0.350, 0.600], loss: 7.281240, mean_absolute_error: 41.005211, mean_q: 52.594181
[F[K 277835/500000: episode: 3656, duration: 0.709s, episode steps: 59, steps per second: 83, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.496 [0.410, 0.570], loss: 8.396896, mean_absolute_error: 40.736259, mean_q: 52.181347
[F[K 277879/500000: episode: 3657, duration: 0.513s, episode steps: 44, steps per second: 86, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.705 [0.000, 4.000], mean observation: 0.503 [0.420, 0.610], loss: 7.305870, mean_absolute_error: 41.613720, mean_q: 53.348522
[F[K 277957/500000: episode: 3658, duration: 0.933s, episode steps: 78, steps per second: 84, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.090 [0.000, 4.000], mean observation: 0.492 [0.450, 0.530], loss: 7.223817, mean_absolute_error: 42.021309, mean_q: 53.836124
[F[K 278005/500000: episode: 3659, duration: 0.570s, episode steps: 48, steps per second: 84, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.312 [0.000, 4.000], mean observation: 0.521 [0.470, 0.640], loss: 7.623084, mean_absolute_error: 41.058285, mean_q: 52.488758
[F[K 278145/500000: episode: 3660, duration: 1.702s, episode steps: 140, steps per second: 82, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.643 [0.000, 4.000], mean observation: 0.502 [0.440, 0.560], loss: 8.588789, mean_absolute_error: 41.352898, mean_q: 52.952839
[F[K 278254/500000: episode: 3661, duration: 1.287s, episode steps: 109, steps per second: 85, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.716 [0.000, 4.000], mean observation: 0.497 [0.460, 0.520], loss: 9.338169, mean_absolute_error: 41.410233, mean_q: 52.960594
[F[K 278366/500000: episode: 3662, duration: 1.521s, episode steps: 112, steps per second: 74, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 0.495 [0.460, 0.520], loss: 7.445940, mean_absolute_error: 40.843513, mean_q: 52.380623
[F[K 278464/500000: episode: 3663, duration: 1.306s, episode steps: 98, steps per second: 75, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.867 [0.000, 4.000], mean observation: 0.496 [0.420, 0.540], loss: 8.958069, mean_absolute_error: 41.143063, mean_q: 52.758480
[F[K 278544/500000: episode: 3664, duration: 0.981s, episode steps: 80, steps per second: 82, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.975 [0.000, 4.000], mean observation: 0.472 [0.360, 0.520], loss: 7.152148, mean_absolute_error: 41.541603, mean_q: 53.283672
[F[K 278586/500000: episode: 3665, duration: 0.539s, episode steps: 42, steps per second: 78, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.976 [0.000, 4.000], mean observation: 0.523 [0.470, 0.620], loss: 7.539016, mean_absolute_error: 40.713287, mean_q: 52.057205
[F[K 278720/500000: episode: 3666, duration: 1.715s, episode steps: 134, steps per second: 78, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 0.475 [0.330, 0.530], loss: 8.687476, mean_absolute_error: 41.540066, mean_q: 53.143059
[F[K 278757/500000: episode: 3667, duration: 0.471s, episode steps: 37, steps per second: 79, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.525 [0.470, 0.640], loss: 8.692019, mean_absolute_error: 41.593178, mean_q: 53.249138
[F[K 278824/500000: episode: 3668, duration: 0.818s, episode steps: 67, steps per second: 82, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.507 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 7.819686, mean_absolute_error: 41.494331, mean_q: 53.265259
[F[K 278881/500000: episode: 3669, duration: 0.705s, episode steps: 57, steps per second: 81, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.982 [0.000, 4.000], mean observation: 0.473 [0.370, 0.510], loss: 7.334437, mean_absolute_error: 40.737431, mean_q: 52.165810
[F[K 278918/500000: episode: 3670, duration: 0.439s, episode steps: 37, steps per second: 84, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.730 [0.000, 4.000], mean observation: 0.509 [0.450, 0.650], loss: 8.306909, mean_absolute_error: 41.226627, mean_q: 52.865166
[F[K 278975/500000: episode: 3671, duration: 0.783s, episode steps: 57, steps per second: 73, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.895 [0.000, 4.000], mean observation: 0.522 [0.470, 0.650], loss: 7.596781, mean_absolute_error: 42.024918, mean_q: 53.809975
[F[K 279030/500000: episode: 3672, duration: 0.735s, episode steps: 55, steps per second: 75, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.709 [0.000, 4.000], mean observation: 0.505 [0.430, 0.620], loss: 8.293077, mean_absolute_error: 42.072853, mean_q: 53.881657
[F[K 279111/500000: episode: 3673, duration: 1.107s, episode steps: 81, steps per second: 73, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.012 [0.000, 4.000], mean observation: 0.493 [0.440, 0.520], loss: 8.122331, mean_absolute_error: 41.533535, mean_q: 53.191010
[F[K 279201/500000: episode: 3674, duration: 1.125s, episode steps: 90, steps per second: 80, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.778 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 8.237510, mean_absolute_error: 40.996834, mean_q: 52.429344
[F[K 279255/500000: episode: 3675, duration: 0.762s, episode steps: 54, steps per second: 71, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.574 [0.000, 4.000], mean observation: 0.500 [0.460, 0.530], loss: 8.907255, mean_absolute_error: 40.775887, mean_q: 52.138638
[F[K 279334/500000: episode: 3676, duration: 1.046s, episode steps: 79, steps per second: 76, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.152 [0.000, 4.000], mean observation: 0.480 [0.390, 0.510], loss: 7.806363, mean_absolute_error: 41.529778, mean_q: 53.320515
[F[K 279405/500000: episode: 3677, duration: 0.996s, episode steps: 71, steps per second: 71, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.492 [0.450, 0.510], loss: 8.290895, mean_absolute_error: 40.312344, mean_q: 51.580112
[F[K 279487/500000: episode: 3678, duration: 1.123s, episode steps: 82, steps per second: 73, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.902 [0.000, 4.000], mean observation: 0.507 [0.440, 0.650], loss: 7.683359, mean_absolute_error: 41.624653, mean_q: 53.377300
[F[K 279572/500000: episode: 3679, duration: 1.148s, episode steps: 85, steps per second: 74, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.012 [0.000, 4.000], mean observation: 0.502 [0.430, 0.590], loss: 8.378078, mean_absolute_error: 41.465866, mean_q: 53.135410
[F[K 279662/500000: episode: 3680, duration: 1.148s, episode steps: 90, steps per second: 78, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.189 [0.000, 4.000], mean observation: 0.492 [0.380, 0.550], loss: 8.013791, mean_absolute_error: 41.367611, mean_q: 52.957832
[F[K 279734/500000: episode: 3681, duration: 0.920s, episode steps: 72, steps per second: 78, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.389 [0.000, 4.000], mean observation: 0.494 [0.400, 0.550], loss: 7.497272, mean_absolute_error: 40.989899, mean_q: 52.468494
[F[K 279794/500000: episode: 3682, duration: 0.823s, episode steps: 60, steps per second: 73, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.506 [0.470, 0.600], loss: 8.426864, mean_absolute_error: 41.294331, mean_q: 52.959900
[F[K 279940/500000: episode: 3683, duration: 1.928s, episode steps: 146, steps per second: 76, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.164 [0.000, 4.000], mean observation: 0.486 [0.320, 0.550], loss: 7.980183, mean_absolute_error: 41.784252, mean_q: 53.608032
[F[K 280047/500000: episode: 3684, duration: 1.562s, episode steps: 107, steps per second: 69, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.243 [0.000, 4.000], mean observation: 0.484 [0.400, 0.530], loss: 9.154370, mean_absolute_error: 41.309830, mean_q: 52.954453
[F[K 280123/500000: episode: 3685, duration: 1.051s, episode steps: 76, steps per second: 72, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.776 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 7.260207, mean_absolute_error: 41.343250, mean_q: 53.043224
[F[K 280215/500000: episode: 3686, duration: 1.381s, episode steps: 92, steps per second: 67, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.076 [0.000, 4.000], mean observation: 0.483 [0.400, 0.520], loss: 7.484050, mean_absolute_error: 41.154579, mean_q: 52.761391
[F[K 280270/500000: episode: 3687, duration: 0.786s, episode steps: 55, steps per second: 70, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.364 [0.000, 4.000], mean observation: 0.499 [0.440, 0.580], loss: 9.957189, mean_absolute_error: 40.905594, mean_q: 52.478344
[F[K 280362/500000: episode: 3688, duration: 1.261s, episode steps: 92, steps per second: 73, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.283 [0.000, 4.000], mean observation: 0.493 [0.390, 0.550], loss: 9.524023, mean_absolute_error: 41.158539, mean_q: 52.613266
[F[K 280438/500000: episode: 3689, duration: 1.038s, episode steps: 76, steps per second: 73, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.974 [0.000, 4.000], mean observation: 0.503 [0.470, 0.530], loss: 10.424085, mean_absolute_error: 40.879639, mean_q: 52.434139
[F[K 280505/500000: episode: 3690, duration: 0.907s, episode steps: 67, steps per second: 74, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.776 [0.000, 4.000], mean observation: 0.510 [0.430, 0.640], loss: 5.976682, mean_absolute_error: 41.331722, mean_q: 53.127350
[F[K 280581/500000: episode: 3691, duration: 0.978s, episode steps: 76, steps per second: 78, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.421 [0.000, 4.000], mean observation: 0.484 [0.350, 0.530], loss: 7.763038, mean_absolute_error: 41.778854, mean_q: 53.582119
[F[K 280680/500000: episode: 3692, duration: 1.249s, episode steps: 99, steps per second: 79, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.010 [0.000, 4.000], mean observation: 0.513 [0.470, 0.630], loss: 7.558547, mean_absolute_error: 40.694702, mean_q: 52.213055
[F[K 280719/500000: episode: 3693, duration: 0.573s, episode steps: 39, steps per second: 68, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.538 [0.000, 3.000], mean observation: 0.528 [0.480, 0.650], loss: 8.574391, mean_absolute_error: 41.042774, mean_q: 52.602093
[F[K 280871/500000: episode: 3694, duration: 2.041s, episode steps: 152, steps per second: 74, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.059 [0.000, 4.000], mean observation: 0.493 [0.390, 0.560], loss: 9.446079, mean_absolute_error: 41.161106, mean_q: 52.688965
[F[K 280964/500000: episode: 3695, duration: 1.296s, episode steps: 93, steps per second: 72, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.065 [0.000, 4.000], mean observation: 0.506 [0.470, 0.570], loss: 9.024430, mean_absolute_error: 41.561268, mean_q: 53.198112
[F[K 281016/500000: episode: 3696, duration: 0.706s, episode steps: 52, steps per second: 74, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.096 [0.000, 4.000], mean observation: 0.499 [0.380, 0.620], loss: 9.232196, mean_absolute_error: 40.881996, mean_q: 52.506508
[F[K 281088/500000: episode: 3697, duration: 1.143s, episode steps: 72, steps per second: 63, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 0.509 [0.480, 0.570], loss: 8.005816, mean_absolute_error: 40.845627, mean_q: 52.434353
[F[K 281172/500000: episode: 3698, duration: 1.132s, episode steps: 84, steps per second: 74, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.714 [0.000, 4.000], mean observation: 0.494 [0.430, 0.530], loss: 8.618636, mean_absolute_error: 41.098801, mean_q: 52.624176
[F[K 281234/500000: episode: 3699, duration: 0.884s, episode steps: 62, steps per second: 70, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.290 [0.000, 4.000], mean observation: 0.513 [0.470, 0.650], loss: 7.774470, mean_absolute_error: 41.038086, mean_q: 52.662956
[F[K 281301/500000: episode: 3700, duration: 0.936s, episode steps: 67, steps per second: 72, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 7.556176, mean_absolute_error: 40.773800, mean_q: 52.277485
[F[K 281349/500000: episode: 3701, duration: 0.746s, episode steps: 48, steps per second: 64, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.229 [0.000, 4.000], mean observation: 0.508 [0.470, 0.610], loss: 8.878081, mean_absolute_error: 41.151249, mean_q: 52.636414
[F[K 281441/500000: episode: 3702, duration: 1.316s, episode steps: 92, steps per second: 70, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.163 [0.000, 4.000], mean observation: 0.478 [0.360, 0.520], loss: 7.929992, mean_absolute_error: 41.405251, mean_q: 53.028194
[F[K 281519/500000: episode: 3703, duration: 1.068s, episode steps: 78, steps per second: 73, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.397 [0.000, 4.000], mean observation: 0.505 [0.430, 0.620], loss: 8.521880, mean_absolute_error: 40.916508, mean_q: 52.393486
[F[K 281602/500000: episode: 3704, duration: 1.092s, episode steps: 83, steps per second: 76, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.265 [0.000, 4.000], mean observation: 0.482 [0.390, 0.510], loss: 7.736138, mean_absolute_error: 40.794132, mean_q: 52.294487
[F[K 281688/500000: episode: 3705, duration: 1.221s, episode steps: 86, steps per second: 70, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.919 [0.000, 4.000], mean observation: 0.504 [0.450, 0.560], loss: 7.773658, mean_absolute_error: 41.789845, mean_q: 53.621258
[F[K 281797/500000: episode: 3706, duration: 1.667s, episode steps: 109, steps per second: 65, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.422 [0.000, 4.000], mean observation: 0.498 [0.450, 0.530], loss: 8.327924, mean_absolute_error: 41.337788, mean_q: 52.889168
[F[K 281891/500000: episode: 3707, duration: 1.455s, episode steps: 94, steps per second: 65, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.819 [0.000, 4.000], mean observation: 0.499 [0.390, 0.570], loss: 8.387791, mean_absolute_error: 41.245312, mean_q: 52.752903
[F[K 282015/500000: episode: 3708, duration: 1.888s, episode steps: 124, steps per second: 66, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.371 [0.000, 4.000], mean observation: 0.474 [0.350, 0.530], loss: 8.493477, mean_absolute_error: 41.471336, mean_q: 53.074608
[F[K 282049/500000: episode: 3709, duration: 0.569s, episode steps: 34, steps per second: 60, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.647 [0.000, 3.000], mean observation: 0.526 [0.470, 0.640], loss: 7.354990, mean_absolute_error: 40.528671, mean_q: 51.932823
[F[K 282106/500000: episode: 3710, duration: 0.860s, episode steps: 57, steps per second: 66, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.228 [0.000, 4.000], mean observation: 0.510 [0.470, 0.590], loss: 7.385495, mean_absolute_error: 42.335728, mean_q: 54.263706
[F[K 282185/500000: episode: 3711, duration: 1.245s, episode steps: 79, steps per second: 63, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.785 [0.000, 4.000], mean observation: 0.497 [0.380, 0.580], loss: 7.155365, mean_absolute_error: 41.597237, mean_q: 53.354420
[F[K 282243/500000: episode: 3712, duration: 0.942s, episode steps: 58, steps per second: 62, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.501 [0.420, 0.560], loss: 9.802270, mean_absolute_error: 41.356525, mean_q: 52.842041
[F[K 282297/500000: episode: 3713, duration: 0.784s, episode steps: 54, steps per second: 69, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.093 [0.000, 4.000], mean observation: 0.505 [0.410, 0.640], loss: 7.582395, mean_absolute_error: 40.887333, mean_q: 52.340851
[F[K 282464/500000: episode: 3714, duration: 2.245s, episode steps: 167, steps per second: 74, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.138 [0.000, 4.000], mean observation: 0.501 [0.450, 0.560], loss: 7.756704, mean_absolute_error: 41.237297, mean_q: 52.872654
[F[K 282504/500000: episode: 3715, duration: 0.637s, episode steps: 40, steps per second: 63, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.425 [0.000, 4.000], mean observation: 0.520 [0.470, 0.640], loss: 10.828947, mean_absolute_error: 41.470184, mean_q: 53.058968
[F[K 282628/500000: episode: 3716, duration: 2.031s, episode steps: 124, steps per second: 61, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 0.495 [0.410, 0.550], loss: 7.450669, mean_absolute_error: 41.328339, mean_q: 52.908272
[F[K 282692/500000: episode: 3717, duration: 1.070s, episode steps: 64, steps per second: 60, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.517 [0.470, 0.640], loss: 7.860258, mean_absolute_error: 41.763638, mean_q: 53.547020
[F[K 282731/500000: episode: 3718, duration: 0.629s, episode steps: 39, steps per second: 62, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.949 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 8.495870, mean_absolute_error: 41.428570, mean_q: 53.226955
[F[K 282800/500000: episode: 3719, duration: 1.125s, episode steps: 69, steps per second: 61, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.261 [0.000, 4.000], mean observation: 0.512 [0.470, 0.640], loss: 8.464032, mean_absolute_error: 41.561104, mean_q: 53.220364
[F[K 282879/500000: episode: 3720, duration: 1.237s, episode steps: 79, steps per second: 64, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.468 [0.000, 4.000], mean observation: 0.507 [0.420, 0.640], loss: 9.343276, mean_absolute_error: 41.050709, mean_q: 52.692921
[F[K 282931/500000: episode: 3721, duration: 0.866s, episode steps: 52, steps per second: 60, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.827 [0.000, 4.000], mean observation: 0.515 [0.470, 0.620], loss: 9.018678, mean_absolute_error: 41.919575, mean_q: 53.676590
[F[K 283073/500000: episode: 3722, duration: 2.485s, episode steps: 142, steps per second: 57, episode reward: 142.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.509 [0.490, 0.580], loss: 8.421896, mean_absolute_error: 41.459576, mean_q: 53.173161
[F[K 283160/500000: episode: 3723, duration: 1.454s, episode steps: 87, steps per second: 60, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.770 [0.000, 4.000], mean observation: 0.490 [0.350, 0.540], loss: 9.680644, mean_absolute_error: 40.665215, mean_q: 52.053143
[F[K 283238/500000: episode: 3724, duration: 1.297s, episode steps: 78, steps per second: 60, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.974 [0.000, 4.000], mean observation: 0.476 [0.370, 0.530], loss: 8.302585, mean_absolute_error: 40.858181, mean_q: 52.367023
[F[K 283293/500000: episode: 3725, duration: 0.877s, episode steps: 55, steps per second: 63, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.891 [0.000, 4.000], mean observation: 0.527 [0.470, 0.640], loss: 8.501954, mean_absolute_error: 40.884056, mean_q: 52.233894
[F[K 283413/500000: episode: 3726, duration: 1.644s, episode steps: 120, steps per second: 73, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.358 [0.000, 4.000], mean observation: 0.482 [0.360, 0.510], loss: 8.529847, mean_absolute_error: 40.842064, mean_q: 52.361790
[F[K 283487/500000: episode: 3727, duration: 1.087s, episode steps: 74, steps per second: 68, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.515 [0.470, 0.590], loss: 7.979184, mean_absolute_error: 41.393135, mean_q: 52.939198
[F[K 283565/500000: episode: 3728, duration: 1.077s, episode steps: 78, steps per second: 72, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.115 [0.000, 4.000], mean observation: 0.496 [0.410, 0.550], loss: 7.371517, mean_absolute_error: 41.803093, mean_q: 53.549454
[F[K 283630/500000: episode: 3729, duration: 0.890s, episode steps: 65, steps per second: 73, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.138 [0.000, 4.000], mean observation: 0.470 [0.350, 0.510], loss: 7.666133, mean_absolute_error: 41.468521, mean_q: 53.052860
[F[K 283724/500000: episode: 3730, duration: 1.291s, episode steps: 94, steps per second: 73, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.734 [0.000, 4.000], mean observation: 0.508 [0.470, 0.580], loss: 8.948654, mean_absolute_error: 41.489353, mean_q: 53.050537
[F[K 283862/500000: episode: 3731, duration: 1.985s, episode steps: 138, steps per second: 70, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.978 [0.000, 4.000], mean observation: 0.489 [0.360, 0.530], loss: 8.590454, mean_absolute_error: 41.643364, mean_q: 53.311890
[F[K 283924/500000: episode: 3732, duration: 0.853s, episode steps: 62, steps per second: 73, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.048 [0.000, 4.000], mean observation: 0.487 [0.370, 0.530], loss: 9.078931, mean_absolute_error: 41.659218, mean_q: 53.401661
[F[K 284000/500000: episode: 3733, duration: 0.930s, episode steps: 76, steps per second: 82, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.079 [0.000, 4.000], mean observation: 0.484 [0.400, 0.510], loss: 7.886230, mean_absolute_error: 40.631992, mean_q: 52.021755
[F[K 284044/500000: episode: 3734, duration: 0.597s, episode steps: 44, steps per second: 74, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.512 [0.460, 0.650], loss: 9.730597, mean_absolute_error: 40.454361, mean_q: 51.866161
[F[K 284091/500000: episode: 3735, duration: 0.511s, episode steps: 47, steps per second: 92, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.298 [0.000, 4.000], mean observation: 0.495 [0.350, 0.590], loss: 9.303469, mean_absolute_error: 41.229404, mean_q: 52.792046
[F[K 284247/500000: episode: 3736, duration: 2.210s, episode steps: 156, steps per second: 71, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.077 [0.000, 4.000], mean observation: 0.504 [0.440, 0.570], loss: 8.573650, mean_absolute_error: 41.405666, mean_q: 53.107121
[F[K 284303/500000: episode: 3737, duration: 0.773s, episode steps: 56, steps per second: 72, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.499 [0.430, 0.560], loss: 8.358571, mean_absolute_error: 41.251408, mean_q: 52.941689
[F[K 284446/500000: episode: 3738, duration: 1.974s, episode steps: 143, steps per second: 72, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.084 [0.000, 4.000], mean observation: 0.486 [0.370, 0.540], loss: 7.789534, mean_absolute_error: 41.034195, mean_q: 52.673714
[F[K 284497/500000: episode: 3739, duration: 0.696s, episode steps: 51, steps per second: 73, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.804 [0.000, 4.000], mean observation: 0.492 [0.360, 0.530], loss: 8.402648, mean_absolute_error: 41.338799, mean_q: 52.917107
[F[K 284570/500000: episode: 3740, duration: 0.994s, episode steps: 73, steps per second: 73, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.151 [0.000, 4.000], mean observation: 0.505 [0.480, 0.550], loss: 9.503608, mean_absolute_error: 41.386074, mean_q: 52.866673
[F[K 284616/500000: episode: 3741, duration: 0.667s, episode steps: 46, steps per second: 69, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.913 [0.000, 4.000], mean observation: 0.492 [0.370, 0.550], loss: 9.120214, mean_absolute_error: 41.129131, mean_q: 52.547359
[F[K 284702/500000: episode: 3742, duration: 1.173s, episode steps: 86, steps per second: 73, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.105 [0.000, 4.000], mean observation: 0.485 [0.410, 0.520], loss: 6.814275, mean_absolute_error: 41.640751, mean_q: 53.506557
[F[K 284738/500000: episode: 3743, duration: 0.503s, episode steps: 36, steps per second: 72, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.778 [0.000, 4.000], mean observation: 0.520 [0.470, 0.650], loss: 8.354710, mean_absolute_error: 41.548279, mean_q: 53.353821
[F[K 284782/500000: episode: 3744, duration: 0.681s, episode steps: 44, steps per second: 65, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.591 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 9.067214, mean_absolute_error: 41.371883, mean_q: 52.950932
[F[K 284859/500000: episode: 3745, duration: 1.122s, episode steps: 77, steps per second: 69, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.909 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 7.650826, mean_absolute_error: 41.617874, mean_q: 53.352196
[F[K 284987/500000: episode: 3746, duration: 1.819s, episode steps: 128, steps per second: 70, episode reward: 128.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.483 [0.340, 0.530], loss: 7.638749, mean_absolute_error: 41.134407, mean_q: 52.788059
[F[K 285053/500000: episode: 3747, duration: 1.007s, episode steps: 66, steps per second: 66, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.242 [0.000, 4.000], mean observation: 0.485 [0.380, 0.510], loss: 8.492718, mean_absolute_error: 41.894337, mean_q: 53.646679
[F[K 285125/500000: episode: 3748, duration: 1.092s, episode steps: 72, steps per second: 66, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 7.872372, mean_absolute_error: 41.199062, mean_q: 52.917980
[F[K 285191/500000: episode: 3749, duration: 0.903s, episode steps: 66, steps per second: 73, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.182 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 9.390285, mean_absolute_error: 41.798660, mean_q: 53.473557
[F[K 285280/500000: episode: 3750, duration: 1.313s, episode steps: 89, steps per second: 68, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.056 [0.000, 4.000], mean observation: 0.485 [0.400, 0.520], loss: 8.241476, mean_absolute_error: 41.290764, mean_q: 52.835423
[F[K 285359/500000: episode: 3751, duration: 1.193s, episode steps: 79, steps per second: 66, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.051 [0.000, 4.000], mean observation: 0.482 [0.390, 0.530], loss: 7.842203, mean_absolute_error: 41.553009, mean_q: 53.202465
[F[K 285440/500000: episode: 3752, duration: 1.294s, episode steps: 81, steps per second: 63, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.963 [0.000, 4.000], mean observation: 0.506 [0.460, 0.580], loss: 7.266520, mean_absolute_error: 41.163952, mean_q: 52.828346
[F[K 285509/500000: episode: 3753, duration: 1.039s, episode steps: 69, steps per second: 66, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.855 [0.000, 4.000], mean observation: 0.491 [0.390, 0.530], loss: 7.776401, mean_absolute_error: 41.800644, mean_q: 53.553963
[F[K 285591/500000: episode: 3754, duration: 1.069s, episode steps: 82, steps per second: 77, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.378 [0.000, 4.000], mean observation: 0.482 [0.370, 0.520], loss: 7.788256, mean_absolute_error: 41.049152, mean_q: 52.598972
[F[K 285659/500000: episode: 3755, duration: 0.946s, episode steps: 68, steps per second: 72, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.809 [0.000, 4.000], mean observation: 0.507 [0.420, 0.620], loss: 8.380460, mean_absolute_error: 41.487061, mean_q: 53.108978
[F[K 285745/500000: episode: 3756, duration: 1.223s, episode steps: 86, steps per second: 70, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.535 [0.000, 4.000], mean observation: 0.509 [0.470, 0.620], loss: 7.142090, mean_absolute_error: 41.382107, mean_q: 53.002846
[F[K 285804/500000: episode: 3757, duration: 0.834s, episode steps: 59, steps per second: 71, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.441 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 8.954810, mean_absolute_error: 41.215816, mean_q: 52.836235
[F[K 285895/500000: episode: 3758, duration: 1.387s, episode steps: 91, steps per second: 66, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.187 [0.000, 4.000], mean observation: 0.491 [0.450, 0.520], loss: 7.293332, mean_absolute_error: 41.337597, mean_q: 53.044121
[F[K 285940/500000: episode: 3759, duration: 0.715s, episode steps: 45, steps per second: 63, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.489 [0.000, 4.000], mean observation: 0.524 [0.490, 0.610], loss: 8.050275, mean_absolute_error: 42.132751, mean_q: 53.948868
[F[K 286001/500000: episode: 3760, duration: 0.807s, episode steps: 61, steps per second: 76, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.098 [0.000, 4.000], mean observation: 0.514 [0.470, 0.650], loss: 7.741184, mean_absolute_error: 41.105934, mean_q: 52.620609
[F[K 286065/500000: episode: 3761, duration: 0.922s, episode steps: 64, steps per second: 69, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.509 [0.450, 0.610], loss: 7.447548, mean_absolute_error: 41.769814, mean_q: 53.514282
[F[K 286150/500000: episode: 3762, duration: 1.284s, episode steps: 85, steps per second: 66, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.918 [0.000, 4.000], mean observation: 0.506 [0.400, 0.650], loss: 10.583429, mean_absolute_error: 41.438648, mean_q: 52.912724
[F[K 286228/500000: episode: 3763, duration: 1.203s, episode steps: 78, steps per second: 65, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 0.483 [0.360, 0.530], loss: 8.182801, mean_absolute_error: 41.835011, mean_q: 53.521702
[F[K 286353/500000: episode: 3764, duration: 1.848s, episode steps: 125, steps per second: 68, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.152 [0.000, 4.000], mean observation: 0.522 [0.480, 0.670], loss: 7.426315, mean_absolute_error: 41.038231, mean_q: 52.608921
[F[K 286439/500000: episode: 3765, duration: 1.288s, episode steps: 86, steps per second: 67, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.128 [0.000, 4.000], mean observation: 0.506 [0.440, 0.610], loss: 7.846972, mean_absolute_error: 41.150284, mean_q: 52.697613
[F[K 286498/500000: episode: 3766, duration: 0.844s, episode steps: 59, steps per second: 70, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.506 [0.490, 0.530], loss: 8.957595, mean_absolute_error: 41.852234, mean_q: 53.581779
[F[K 286581/500000: episode: 3767, duration: 1.106s, episode steps: 83, steps per second: 75, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.169 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 7.907631, mean_absolute_error: 41.397095, mean_q: 53.056648
[F[K 286654/500000: episode: 3768, duration: 0.855s, episode steps: 73, steps per second: 85, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.205 [0.000, 4.000], mean observation: 0.475 [0.370, 0.530], loss: 7.268648, mean_absolute_error: 41.090172, mean_q: 52.690113
[F[K 286822/500000: episode: 3769, duration: 2.092s, episode steps: 168, steps per second: 80, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.500 [0.450, 0.550], loss: 7.843438, mean_absolute_error: 41.558601, mean_q: 53.265903
[F[K 286993/500000: episode: 3770, duration: 2.302s, episode steps: 171, steps per second: 74, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.327 [0.000, 4.000], mean observation: 0.495 [0.420, 0.540], loss: 7.785070, mean_absolute_error: 41.223576, mean_q: 52.915226
[F[K 287056/500000: episode: 3771, duration: 0.870s, episode steps: 63, steps per second: 72, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.556 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 7.220748, mean_absolute_error: 41.530052, mean_q: 53.330791
[F[K 287098/500000: episode: 3772, duration: 0.588s, episode steps: 42, steps per second: 71, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.619 [0.000, 4.000], mean observation: 0.513 [0.470, 0.650], loss: 8.026514, mean_absolute_error: 40.585613, mean_q: 52.047676
[F[K 287172/500000: episode: 3773, duration: 0.991s, episode steps: 74, steps per second: 75, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.122 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 9.348426, mean_absolute_error: 41.134018, mean_q: 52.709042
[F[K 287343/500000: episode: 3774, duration: 2.298s, episode steps: 171, steps per second: 74, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.099 [0.000, 4.000], mean observation: 0.499 [0.420, 0.570], loss: 8.052547, mean_absolute_error: 41.848415, mean_q: 53.665234
[F[K 287401/500000: episode: 3775, duration: 0.767s, episode steps: 58, steps per second: 76, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.490 [0.420, 0.520], loss: 7.718400, mean_absolute_error: 41.596645, mean_q: 53.376099
[F[K 287510/500000: episode: 3776, duration: 1.449s, episode steps: 109, steps per second: 75, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.046 [0.000, 4.000], mean observation: 0.480 [0.400, 0.520], loss: 8.155108, mean_absolute_error: 41.715237, mean_q: 53.373199
[F[K 287559/500000: episode: 3777, duration: 0.663s, episode steps: 49, steps per second: 74, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.519 [0.470, 0.620], loss: 7.657524, mean_absolute_error: 41.313614, mean_q: 53.054478
[F[K 287601/500000: episode: 3778, duration: 0.496s, episode steps: 42, steps per second: 85, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.929 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 8.639531, mean_absolute_error: 41.580536, mean_q: 53.252068
[F[K 287658/500000: episode: 3779, duration: 0.833s, episode steps: 57, steps per second: 68, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.842 [0.000, 4.000], mean observation: 0.482 [0.350, 0.530], loss: 8.238392, mean_absolute_error: 41.347855, mean_q: 53.083313
[F[K 287741/500000: episode: 3780, duration: 1.062s, episode steps: 83, steps per second: 78, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.514 [0.480, 0.650], loss: 8.492305, mean_absolute_error: 41.454464, mean_q: 53.156704
[F[K 287835/500000: episode: 3781, duration: 1.253s, episode steps: 94, steps per second: 75, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.755 [0.000, 4.000], mean observation: 0.516 [0.480, 0.650], loss: 7.563955, mean_absolute_error: 41.099155, mean_q: 52.710812
[F[K 287899/500000: episode: 3782, duration: 0.771s, episode steps: 64, steps per second: 83, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.078 [0.000, 4.000], mean observation: 0.493 [0.400, 0.550], loss: 7.353367, mean_absolute_error: 41.700291, mean_q: 53.558731
[F[K 287948/500000: episode: 3783, duration: 0.553s, episode steps: 49, steps per second: 89, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.517 [0.470, 0.610], loss: 8.532306, mean_absolute_error: 41.627777, mean_q: 53.382965
[F[K 288049/500000: episode: 3784, duration: 1.136s, episode steps: 101, steps per second: 89, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.743 [0.000, 4.000], mean observation: 0.484 [0.370, 0.530], loss: 7.078700, mean_absolute_error: 41.860474, mean_q: 53.689587
[F[K 288196/500000: episode: 3785, duration: 1.739s, episode steps: 147, steps per second: 85, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.401 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 7.817992, mean_absolute_error: 41.720432, mean_q: 53.465801
[F[K 288284/500000: episode: 3786, duration: 1.029s, episode steps: 88, steps per second: 85, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.920 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 8.031427, mean_absolute_error: 41.602795, mean_q: 53.283997
[F[K 288349/500000: episode: 3787, duration: 0.714s, episode steps: 65, steps per second: 91, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.523 [0.000, 4.000], mean observation: 0.520 [0.500, 0.650], loss: 8.079271, mean_absolute_error: 41.142647, mean_q: 52.772713
[F[K 288462/500000: episode: 3788, duration: 1.118s, episode steps: 113, steps per second: 101, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.965 [0.000, 4.000], mean observation: 0.470 [0.330, 0.530], loss: 9.043385, mean_absolute_error: 41.451866, mean_q: 53.067711
[F[K 288547/500000: episode: 3789, duration: 0.929s, episode steps: 85, steps per second: 91, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.494 [0.390, 0.530], loss: 7.907073, mean_absolute_error: 41.720173, mean_q: 53.541061
[F[K 288638/500000: episode: 3790, duration: 1.079s, episode steps: 91, steps per second: 84, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.011 [0.000, 4.000], mean observation: 0.502 [0.470, 0.540], loss: 9.078989, mean_absolute_error: 41.942089, mean_q: 53.691433
[F[K 288681/500000: episode: 3791, duration: 0.501s, episode steps: 43, steps per second: 86, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.605 [0.000, 4.000], mean observation: 0.510 [0.470, 0.620], loss: 10.528754, mean_absolute_error: 42.150860, mean_q: 53.964447
[F[K 288792/500000: episode: 3792, duration: 1.346s, episode steps: 111, steps per second: 82, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.503 [0.430, 0.590], loss: 8.543497, mean_absolute_error: 41.197945, mean_q: 52.744022
[F[K 288869/500000: episode: 3793, duration: 0.994s, episode steps: 77, steps per second: 77, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.501 [0.450, 0.540], loss: 9.487415, mean_absolute_error: 41.498039, mean_q: 53.246216
[F[K 288928/500000: episode: 3794, duration: 0.750s, episode steps: 59, steps per second: 79, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.864 [0.000, 4.000], mean observation: 0.511 [0.490, 0.550], loss: 8.115323, mean_absolute_error: 40.897038, mean_q: 52.422836
[F[K 289067/500000: episode: 3795, duration: 1.760s, episode steps: 139, steps per second: 79, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.137 [0.000, 4.000], mean observation: 0.493 [0.460, 0.530], loss: 8.278715, mean_absolute_error: 42.016380, mean_q: 53.819733
[F[K 289134/500000: episode: 3796, duration: 0.818s, episode steps: 67, steps per second: 82, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.493 [0.000, 4.000], mean observation: 0.483 [0.410, 0.530], loss: 8.544210, mean_absolute_error: 40.868900, mean_q: 52.244057
[F[K 289184/500000: episode: 3797, duration: 0.594s, episode steps: 50, steps per second: 84, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.740 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 7.210027, mean_absolute_error: 42.005280, mean_q: 53.815239
[F[K 289258/500000: episode: 3798, duration: 0.831s, episode steps: 74, steps per second: 89, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.135 [0.000, 4.000], mean observation: 0.482 [0.420, 0.510], loss: 9.437287, mean_absolute_error: 41.654079, mean_q: 53.436272
[F[K 289378/500000: episode: 3799, duration: 1.435s, episode steps: 120, steps per second: 84, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.633 [0.000, 4.000], mean observation: 0.491 [0.390, 0.530], loss: 7.634837, mean_absolute_error: 41.889893, mean_q: 53.705070
[F[K 289538/500000: episode: 3800, duration: 1.866s, episode steps: 160, steps per second: 86, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.344 [0.000, 4.000], mean observation: 0.499 [0.400, 0.620], loss: 7.607165, mean_absolute_error: 41.328556, mean_q: 53.036430
[F[K 289600/500000: episode: 3801, duration: 0.735s, episode steps: 62, steps per second: 84, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.226 [0.000, 4.000], mean observation: 0.486 [0.420, 0.530], loss: 7.982451, mean_absolute_error: 41.591351, mean_q: 53.247425
[F[K 289680/500000: episode: 3802, duration: 0.895s, episode steps: 80, steps per second: 89, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.150 [0.000, 4.000], mean observation: 0.485 [0.370, 0.520], loss: 7.799309, mean_absolute_error: 42.543629, mean_q: 54.432350
[F[K 289737/500000: episode: 3803, duration: 0.635s, episode steps: 57, steps per second: 90, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.502 [0.440, 0.550], loss: 8.099671, mean_absolute_error: 42.138302, mean_q: 53.963970
[F[K 289807/500000: episode: 3804, duration: 0.802s, episode steps: 70, steps per second: 87, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.506 [0.470, 0.570], loss: 8.102238, mean_absolute_error: 41.674450, mean_q: 53.457405
[F[K 289844/500000: episode: 3805, duration: 0.441s, episode steps: 37, steps per second: 84, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.730 [0.000, 4.000], mean observation: 0.526 [0.470, 0.650], loss: 7.621669, mean_absolute_error: 42.105186, mean_q: 53.832188
[F[K 289909/500000: episode: 3806, duration: 0.751s, episode steps: 65, steps per second: 87, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.277 [0.000, 4.000], mean observation: 0.497 [0.390, 0.550], loss: 8.268833, mean_absolute_error: 41.649010, mean_q: 53.201508
[F[K 289973/500000: episode: 3807, duration: 0.809s, episode steps: 64, steps per second: 79, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.515 [0.490, 0.640], loss: 9.231667, mean_absolute_error: 41.513870, mean_q: 53.288834
[F[K 290165/500000: episode: 3808, duration: 2.392s, episode steps: 192, steps per second: 80, episode reward: 192.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.130 [0.000, 4.000], mean observation: 0.499 [0.420, 0.580], loss: 8.498668, mean_absolute_error: 41.730167, mean_q: 53.402607
[F[K 290306/500000: episode: 3809, duration: 1.738s, episode steps: 141, steps per second: 81, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.447 [0.000, 4.000], mean observation: 0.504 [0.460, 0.560], loss: 8.641562, mean_absolute_error: 40.918327, mean_q: 52.368420
[F[K 290383/500000: episode: 3810, duration: 1.014s, episode steps: 77, steps per second: 76, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.987 [0.000, 4.000], mean observation: 0.488 [0.400, 0.530], loss: 7.357783, mean_absolute_error: 41.373596, mean_q: 53.078789
[F[K 290450/500000: episode: 3811, duration: 0.832s, episode steps: 67, steps per second: 81, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.701 [0.000, 4.000], mean observation: 0.505 [0.460, 0.560], loss: 8.024145, mean_absolute_error: 41.380016, mean_q: 53.006107
[F[K 290519/500000: episode: 3812, duration: 0.889s, episode steps: 69, steps per second: 78, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.518 [0.470, 0.600], loss: 10.419306, mean_absolute_error: 41.508411, mean_q: 53.153683
[F[K 290678/500000: episode: 3813, duration: 2.082s, episode steps: 159, steps per second: 76, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.075 [0.000, 4.000], mean observation: 0.510 [0.470, 0.590], loss: 8.877467, mean_absolute_error: 41.397594, mean_q: 52.937340
[F[K 290760/500000: episode: 3814, duration: 1.099s, episode steps: 82, steps per second: 75, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.024 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 8.342397, mean_absolute_error: 41.721779, mean_q: 53.426662
[F[K 290823/500000: episode: 3815, duration: 0.852s, episode steps: 63, steps per second: 74, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 6.889545, mean_absolute_error: 41.049389, mean_q: 52.723194
[F[K 290866/500000: episode: 3816, duration: 0.589s, episode steps: 43, steps per second: 73, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.814 [0.000, 4.000], mean observation: 0.499 [0.400, 0.620], loss: 10.485769, mean_absolute_error: 41.338001, mean_q: 52.847702
[F[K 290942/500000: episode: 3817, duration: 1.029s, episode steps: 76, steps per second: 74, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.026 [0.000, 4.000], mean observation: 0.490 [0.370, 0.530], loss: 6.851227, mean_absolute_error: 41.288811, mean_q: 52.967743
[F[K 291004/500000: episode: 3818, duration: 0.798s, episode steps: 62, steps per second: 78, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.548 [0.000, 4.000], mean observation: 0.496 [0.360, 0.590], loss: 8.466557, mean_absolute_error: 41.436413, mean_q: 53.049229
[F[K 291084/500000: episode: 3819, duration: 1.092s, episode steps: 80, steps per second: 73, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.175 [0.000, 4.000], mean observation: 0.485 [0.420, 0.530], loss: 9.378081, mean_absolute_error: 41.852123, mean_q: 53.570442
[F[K 291145/500000: episode: 3820, duration: 0.778s, episode steps: 61, steps per second: 78, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.295 [0.000, 4.000], mean observation: 0.486 [0.390, 0.510], loss: 7.026213, mean_absolute_error: 41.702374, mean_q: 53.322182
[F[K 291182/500000: episode: 3821, duration: 0.498s, episode steps: 37, steps per second: 74, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.811 [0.000, 4.000], mean observation: 0.509 [0.470, 0.620], loss: 7.959901, mean_absolute_error: 42.437347, mean_q: 54.255585
[F[K 291223/500000: episode: 3822, duration: 0.543s, episode steps: 41, steps per second: 75, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.503 [0.420, 0.630], loss: 6.691081, mean_absolute_error: 42.072372, mean_q: 53.885490
[F[K 291301/500000: episode: 3823, duration: 1.174s, episode steps: 78, steps per second: 66, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.744 [0.000, 4.000], mean observation: 0.493 [0.450, 0.530], loss: 8.132327, mean_absolute_error: 41.283806, mean_q: 52.823845
[F[K 291343/500000: episode: 3824, duration: 0.544s, episode steps: 42, steps per second: 77, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.048 [0.000, 4.000], mean observation: 0.501 [0.390, 0.640], loss: 7.397419, mean_absolute_error: 40.417351, mean_q: 51.897408
[F[K 291432/500000: episode: 3825, duration: 1.228s, episode steps: 89, steps per second: 72, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.112 [0.000, 4.000], mean observation: 0.489 [0.390, 0.530], loss: 7.870928, mean_absolute_error: 41.207127, mean_q: 52.858753
[F[K 291512/500000: episode: 3826, duration: 1.084s, episode steps: 80, steps per second: 74, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.312 [0.000, 4.000], mean observation: 0.506 [0.430, 0.630], loss: 7.772038, mean_absolute_error: 41.590649, mean_q: 53.358177
[F[K 291599/500000: episode: 3827, duration: 1.089s, episode steps: 87, steps per second: 80, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.368 [0.000, 4.000], mean observation: 0.513 [0.480, 0.580], loss: 7.719533, mean_absolute_error: 41.346836, mean_q: 53.007702
[F[K 291649/500000: episode: 3828, duration: 0.781s, episode steps: 50, steps per second: 64, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.580 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 11.416065, mean_absolute_error: 41.095646, mean_q: 52.703602
[F[K 291807/500000: episode: 3829, duration: 2.190s, episode steps: 158, steps per second: 72, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.497 [0.430, 0.550], loss: 9.249783, mean_absolute_error: 41.249962, mean_q: 52.811028
[F[K 291892/500000: episode: 3830, duration: 1.243s, episode steps: 85, steps per second: 68, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.024 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 8.030297, mean_absolute_error: 40.913486, mean_q: 52.539665
[F[K 291976/500000: episode: 3831, duration: 1.334s, episode steps: 84, steps per second: 63, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.726 [0.000, 4.000], mean observation: 0.505 [0.470, 0.530], loss: 7.958551, mean_absolute_error: 41.425018, mean_q: 53.103607
[F[K 292050/500000: episode: 3832, duration: 1.112s, episode steps: 74, steps per second: 67, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.189 [0.000, 4.000], mean observation: 0.512 [0.460, 0.650], loss: 7.722636, mean_absolute_error: 40.787979, mean_q: 52.260166
[F[K 292120/500000: episode: 3833, duration: 1.001s, episode steps: 70, steps per second: 70, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.786 [0.000, 4.000], mean observation: 0.475 [0.370, 0.520], loss: 7.778163, mean_absolute_error: 41.502346, mean_q: 53.246521
[F[K 292191/500000: episode: 3834, duration: 0.952s, episode steps: 71, steps per second: 75, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.324 [0.000, 4.000], mean observation: 0.496 [0.390, 0.550], loss: 8.056052, mean_absolute_error: 41.366779, mean_q: 52.937996
[F[K 292256/500000: episode: 3835, duration: 0.971s, episode steps: 65, steps per second: 67, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.892 [0.000, 4.000], mean observation: 0.486 [0.370, 0.510], loss: 8.497207, mean_absolute_error: 42.089008, mean_q: 53.912548
[F[K 292294/500000: episode: 3836, duration: 0.565s, episode steps: 38, steps per second: 67, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.526 [0.000, 4.000], mean observation: 0.498 [0.360, 0.630], loss: 7.048879, mean_absolute_error: 41.800617, mean_q: 53.692657
[F[K 292360/500000: episode: 3837, duration: 0.989s, episode steps: 66, steps per second: 67, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.970 [0.000, 4.000], mean observation: 0.492 [0.360, 0.580], loss: 7.177812, mean_absolute_error: 42.028194, mean_q: 53.876297
[F[K 292500/500000: episode: 3838, duration: 2.069s, episode steps: 140, steps per second: 68, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.071 [0.000, 4.000], mean observation: 0.505 [0.380, 0.640], loss: 7.683251, mean_absolute_error: 41.520733, mean_q: 53.178471
[F[K 292542/500000: episode: 3839, duration: 0.614s, episode steps: 42, steps per second: 68, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 8.666227, mean_absolute_error: 41.732658, mean_q: 53.499168
[F[K 292599/500000: episode: 3840, duration: 0.840s, episode steps: 57, steps per second: 68, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.825 [0.000, 4.000], mean observation: 0.500 [0.370, 0.630], loss: 7.711833, mean_absolute_error: 41.236515, mean_q: 52.857670
[F[K 292665/500000: episode: 3841, duration: 1.053s, episode steps: 66, steps per second: 63, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.303 [0.000, 4.000], mean observation: 0.503 [0.480, 0.540], loss: 8.759614, mean_absolute_error: 41.514442, mean_q: 53.120861
[F[K 292754/500000: episode: 3842, duration: 1.451s, episode steps: 89, steps per second: 61, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.180 [0.000, 4.000], mean observation: 0.502 [0.410, 0.620], loss: 7.805554, mean_absolute_error: 41.233425, mean_q: 52.829967
[F[K 292922/500000: episode: 3843, duration: 2.500s, episode steps: 168, steps per second: 67, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.179 [0.000, 4.000], mean observation: 0.499 [0.440, 0.550], loss: 9.120431, mean_absolute_error: 41.366013, mean_q: 52.960075
[F[K 292983/500000: episode: 3844, duration: 0.922s, episode steps: 61, steps per second: 66, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.510 [0.410, 0.650], loss: 7.182266, mean_absolute_error: 42.023735, mean_q: 53.824726
[F[K 293048/500000: episode: 3845, duration: 1.079s, episode steps: 65, steps per second: 60, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.246 [0.000, 4.000], mean observation: 0.489 [0.430, 0.520], loss: 8.347160, mean_absolute_error: 42.052814, mean_q: 53.865665
[F[K 293104/500000: episode: 3846, duration: 0.876s, episode steps: 56, steps per second: 64, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.982 [0.000, 4.000], mean observation: 0.505 [0.400, 0.640], loss: 8.063006, mean_absolute_error: 41.691170, mean_q: 53.283932
[F[K 293177/500000: episode: 3847, duration: 1.101s, episode steps: 73, steps per second: 66, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.685 [0.000, 4.000], mean observation: 0.511 [0.470, 0.580], loss: 7.730175, mean_absolute_error: 41.183121, mean_q: 52.719387
[F[K 293258/500000: episode: 3848, duration: 1.436s, episode steps: 81, steps per second: 56, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.975 [0.000, 4.000], mean observation: 0.507 [0.460, 0.620], loss: 8.031272, mean_absolute_error: 41.753773, mean_q: 53.500198
[F[K 293317/500000: episode: 3849, duration: 0.990s, episode steps: 59, steps per second: 60, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.949 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 8.999654, mean_absolute_error: 41.286530, mean_q: 52.927528
[F[K 293391/500000: episode: 3850, duration: 1.151s, episode steps: 74, steps per second: 64, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.892 [0.000, 4.000], mean observation: 0.509 [0.480, 0.560], loss: 8.624515, mean_absolute_error: 41.855942, mean_q: 53.614025
[F[K 293493/500000: episode: 3851, duration: 1.753s, episode steps: 102, steps per second: 58, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.922 [0.000, 4.000], mean observation: 0.501 [0.410, 0.620], loss: 9.068013, mean_absolute_error: 41.671684, mean_q: 53.336372
[F[K 293575/500000: episode: 3852, duration: 1.257s, episode steps: 82, steps per second: 65, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.256 [0.000, 4.000], mean observation: 0.479 [0.350, 0.530], loss: 7.432469, mean_absolute_error: 41.552742, mean_q: 53.269608
[F[K 293691/500000: episode: 3853, duration: 1.760s, episode steps: 116, steps per second: 66, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.474 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 7.590146, mean_absolute_error: 41.311367, mean_q: 52.907562
[F[K 293754/500000: episode: 3854, duration: 1.077s, episode steps: 63, steps per second: 58, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.048 [0.000, 4.000], mean observation: 0.482 [0.380, 0.530], loss: 10.511500, mean_absolute_error: 41.148422, mean_q: 52.540867
[F[K 293848/500000: episode: 3855, duration: 1.711s, episode steps: 94, steps per second: 55, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.340 [0.000, 4.000], mean observation: 0.480 [0.340, 0.530], loss: 8.764894, mean_absolute_error: 41.356953, mean_q: 52.983124
[F[K 293950/500000: episode: 3856, duration: 1.763s, episode steps: 102, steps per second: 58, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.402 [0.000, 4.000], mean observation: 0.517 [0.500, 0.580], loss: 7.533817, mean_absolute_error: 41.550072, mean_q: 53.214031
[F[K 293997/500000: episode: 3857, duration: 0.766s, episode steps: 47, steps per second: 61, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.638 [0.000, 4.000], mean observation: 0.518 [0.470, 0.650], loss: 8.030301, mean_absolute_error: 41.239079, mean_q: 52.909512
[F[K 294043/500000: episode: 3858, duration: 0.817s, episode steps: 46, steps per second: 56, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.674 [0.000, 4.000], mean observation: 0.501 [0.400, 0.620], loss: 7.774226, mean_absolute_error: 39.903851, mean_q: 51.101559
[F[K 294112/500000: episode: 3859, duration: 1.142s, episode steps: 69, steps per second: 60, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.261 [0.000, 4.000], mean observation: 0.501 [0.430, 0.590], loss: 7.521672, mean_absolute_error: 40.984264, mean_q: 52.399879
[F[K 294186/500000: episode: 3860, duration: 1.258s, episode steps: 74, steps per second: 59, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 8.214719, mean_absolute_error: 41.652874, mean_q: 53.254593
[F[K 294255/500000: episode: 3861, duration: 1.293s, episode steps: 69, steps per second: 53, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.507 [0.000, 4.000], mean observation: 0.499 [0.400, 0.560], loss: 8.310260, mean_absolute_error: 41.431736, mean_q: 53.067471
[F[K 294368/500000: episode: 3862, duration: 1.985s, episode steps: 113, steps per second: 57, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.381 [0.000, 4.000], mean observation: 0.492 [0.360, 0.570], loss: 8.267551, mean_absolute_error: 41.414768, mean_q: 52.947418
[F[K 294418/500000: episode: 3863, duration: 0.796s, episode steps: 50, steps per second: 63, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.840 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 7.338584, mean_absolute_error: 41.862152, mean_q: 53.684654
[F[K 294478/500000: episode: 3864, duration: 0.963s, episode steps: 60, steps per second: 62, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.017 [0.000, 4.000], mean observation: 0.490 [0.400, 0.520], loss: 7.730476, mean_absolute_error: 41.230797, mean_q: 52.645252
[F[K 294560/500000: episode: 3865, duration: 1.503s, episode steps: 82, steps per second: 55, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.427 [0.000, 4.000], mean observation: 0.509 [0.470, 0.620], loss: 8.021578, mean_absolute_error: 41.464336, mean_q: 53.078537
[F[K 294640/500000: episode: 3866, duration: 1.391s, episode steps: 80, steps per second: 58, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.498 [0.410, 0.580], loss: 7.379939, mean_absolute_error: 42.083630, mean_q: 53.902092
[F[K 294718/500000: episode: 3867, duration: 1.320s, episode steps: 78, steps per second: 59, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 0.497 [0.390, 0.590], loss: 8.706331, mean_absolute_error: 41.407383, mean_q: 53.018097
[F[K 294774/500000: episode: 3868, duration: 0.997s, episode steps: 56, steps per second: 56, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.804 [0.000, 4.000], mean observation: 0.498 [0.470, 0.530], loss: 8.438550, mean_absolute_error: 41.358704, mean_q: 53.049076
[F[K 294899/500000: episode: 3869, duration: 2.109s, episode steps: 125, steps per second: 59, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.920 [0.000, 4.000], mean observation: 0.518 [0.480, 0.610], loss: 7.759893, mean_absolute_error: 41.397346, mean_q: 53.086250
[F[K 294974/500000: episode: 3870, duration: 1.091s, episode steps: 75, steps per second: 69, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.040 [0.000, 4.000], mean observation: 0.499 [0.440, 0.560], loss: 8.122597, mean_absolute_error: 41.649590, mean_q: 53.360443
[F[K 295052/500000: episode: 3871, duration: 1.535s, episode steps: 78, steps per second: 51, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.756 [0.000, 4.000], mean observation: 0.481 [0.380, 0.510], loss: 8.122256, mean_absolute_error: 41.293255, mean_q: 52.793587
[F[K 295132/500000: episode: 3872, duration: 1.557s, episode steps: 80, steps per second: 51, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.263 [0.000, 4.000], mean observation: 0.497 [0.380, 0.590], loss: 7.899688, mean_absolute_error: 41.206001, mean_q: 52.785542
[F[K 295208/500000: episode: 3873, duration: 1.494s, episode steps: 76, steps per second: 51, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.303 [0.000, 4.000], mean observation: 0.491 [0.380, 0.530], loss: 9.010798, mean_absolute_error: 40.960625, mean_q: 52.515705
[F[K 295367/500000: episode: 3874, duration: 3.123s, episode steps: 159, steps per second: 51, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.069 [0.000, 4.000], mean observation: 0.495 [0.330, 0.630], loss: 7.740331, mean_absolute_error: 41.737503, mean_q: 53.405918
[F[K 295459/500000: episode: 3875, duration: 1.870s, episode steps: 92, steps per second: 49, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.217 [0.000, 4.000], mean observation: 0.508 [0.440, 0.640], loss: 7.166621, mean_absolute_error: 41.799870, mean_q: 53.558529
[F[K 295624/500000: episode: 3876, duration: 2.923s, episode steps: 165, steps per second: 56, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.097 [0.000, 4.000], mean observation: 0.502 [0.400, 0.610], loss: 7.961305, mean_absolute_error: 42.412140, mean_q: 54.368885
[F[K 295694/500000: episode: 3877, duration: 1.254s, episode steps: 70, steps per second: 56, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.509 [0.470, 0.570], loss: 8.400130, mean_absolute_error: 42.289551, mean_q: 54.136803
[F[K 295761/500000: episode: 3878, duration: 1.258s, episode steps: 67, steps per second: 53, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.104 [0.000, 4.000], mean observation: 0.486 [0.410, 0.520], loss: 8.960084, mean_absolute_error: 41.284752, mean_q: 52.931816
[F[K 295836/500000: episode: 3879, duration: 1.293s, episode steps: 75, steps per second: 58, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.107 [0.000, 4.000], mean observation: 0.503 [0.380, 0.640], loss: 8.271249, mean_absolute_error: 41.443394, mean_q: 53.121082
[F[K 295934/500000: episode: 3880, duration: 1.802s, episode steps: 98, steps per second: 54, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.133 [0.000, 4.000], mean observation: 0.489 [0.440, 0.530], loss: 7.759358, mean_absolute_error: 41.640564, mean_q: 53.340931
[F[K 296036/500000: episode: 3881, duration: 1.878s, episode steps: 102, steps per second: 54, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.206 [0.000, 4.000], mean observation: 0.495 [0.460, 0.530], loss: 7.817599, mean_absolute_error: 42.380825, mean_q: 54.388805
[F[K 296131/500000: episode: 3882, duration: 1.448s, episode steps: 95, steps per second: 66, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.084 [0.000, 4.000], mean observation: 0.477 [0.360, 0.510], loss: 7.162376, mean_absolute_error: 41.785885, mean_q: 53.549995
[F[K 296199/500000: episode: 3883, duration: 1.091s, episode steps: 68, steps per second: 62, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.809 [0.000, 4.000], mean observation: 0.484 [0.360, 0.530], loss: 9.077342, mean_absolute_error: 41.218307, mean_q: 52.800789
[F[K 296257/500000: episode: 3884, duration: 0.946s, episode steps: 58, steps per second: 61, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.505 [0.460, 0.560], loss: 7.125145, mean_absolute_error: 42.561207, mean_q: 54.552555
[F[K 296341/500000: episode: 3885, duration: 1.311s, episode steps: 84, steps per second: 64, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.238 [0.000, 4.000], mean observation: 0.505 [0.450, 0.610], loss: 7.697951, mean_absolute_error: 42.267677, mean_q: 54.086479
[F[K 296403/500000: episode: 3886, duration: 0.917s, episode steps: 62, steps per second: 68, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.210 [0.000, 4.000], mean observation: 0.486 [0.420, 0.510], loss: 10.890817, mean_absolute_error: 41.737770, mean_q: 53.567966
[F[K 296465/500000: episode: 3887, duration: 0.973s, episode steps: 62, steps per second: 64, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.513 [0.470, 0.590], loss: 9.918492, mean_absolute_error: 41.187263, mean_q: 52.841496
[F[K 296623/500000: episode: 3888, duration: 2.467s, episode steps: 158, steps per second: 64, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.348 [0.000, 4.000], mean observation: 0.481 [0.360, 0.510], loss: 8.866041, mean_absolute_error: 41.710407, mean_q: 53.413433
[F[K 296756/500000: episode: 3889, duration: 2.130s, episode steps: 133, steps per second: 62, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.023 [0.000, 4.000], mean observation: 0.500 [0.460, 0.540], loss: 7.718425, mean_absolute_error: 41.792889, mean_q: 53.637848
[F[K 296870/500000: episode: 3890, duration: 1.884s, episode steps: 114, steps per second: 61, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.149 [0.000, 4.000], mean observation: 0.500 [0.460, 0.550], loss: 8.746407, mean_absolute_error: 41.550407, mean_q: 53.241280
[F[K 296943/500000: episode: 3891, duration: 1.166s, episode steps: 73, steps per second: 63, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.082 [0.000, 4.000], mean observation: 0.498 [0.430, 0.570], loss: 9.136334, mean_absolute_error: 42.428761, mean_q: 54.215488
[F[K 297023/500000: episode: 3892, duration: 1.232s, episode steps: 80, steps per second: 65, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.438 [0.000, 4.000], mean observation: 0.505 [0.470, 0.580], loss: 7.585210, mean_absolute_error: 42.469097, mean_q: 54.416786
[F[K 297131/500000: episode: 3893, duration: 1.774s, episode steps: 108, steps per second: 61, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.444 [0.000, 4.000], mean observation: 0.511 [0.470, 0.570], loss: 9.625793, mean_absolute_error: 42.226643, mean_q: 54.063667
[F[K 297217/500000: episode: 3894, duration: 1.369s, episode steps: 86, steps per second: 63, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.023 [0.000, 4.000], mean observation: 0.484 [0.410, 0.520], loss: 7.425508, mean_absolute_error: 42.041409, mean_q: 53.880966
[F[K 297290/500000: episode: 3895, duration: 1.201s, episode steps: 73, steps per second: 61, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.767 [0.000, 4.000], mean observation: 0.502 [0.430, 0.610], loss: 7.674866, mean_absolute_error: 41.210938, mean_q: 52.759212
[F[K 297345/500000: episode: 3896, duration: 0.905s, episode steps: 55, steps per second: 61, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.503 [0.440, 0.600], loss: 10.037679, mean_absolute_error: 41.221943, mean_q: 52.619537
[F[K 297424/500000: episode: 3897, duration: 1.238s, episode steps: 79, steps per second: 64, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.076 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 8.896930, mean_absolute_error: 41.865341, mean_q: 53.673679
[F[K 297486/500000: episode: 3898, duration: 0.941s, episode steps: 62, steps per second: 66, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.968 [0.000, 4.000], mean observation: 0.497 [0.390, 0.570], loss: 8.075801, mean_absolute_error: 41.444347, mean_q: 53.234371
[F[K 297686/500000: episode: 3899, duration: 2.608s, episode steps: 200, steps per second: 77, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.115 [0.000, 4.000], mean observation: 0.510 [0.420, 0.720], loss: 8.195168, mean_absolute_error: 41.515450, mean_q: 53.193203
[F[K 297746/500000: episode: 3900, duration: 0.906s, episode steps: 60, steps per second: 66, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.517 [0.000, 4.000], mean observation: 0.501 [0.360, 0.630], loss: 7.409340, mean_absolute_error: 41.763859, mean_q: 53.529072
[F[K 297835/500000: episode: 3901, duration: 1.397s, episode steps: 89, steps per second: 64, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.169 [0.000, 4.000], mean observation: 0.485 [0.380, 0.530], loss: 7.554024, mean_absolute_error: 42.222702, mean_q: 54.070534
[F[K 297909/500000: episode: 3902, duration: 0.971s, episode steps: 74, steps per second: 76, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.743 [0.000, 4.000], mean observation: 0.471 [0.350, 0.510], loss: 7.625258, mean_absolute_error: 41.790085, mean_q: 53.526402
[F[K 297968/500000: episode: 3903, duration: 0.936s, episode steps: 59, steps per second: 63, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.441 [0.000, 4.000], mean observation: 0.500 [0.350, 0.660], loss: 7.991886, mean_absolute_error: 41.650841, mean_q: 53.392090
[F[K 298133/500000: episode: 3904, duration: 2.356s, episode steps: 165, steps per second: 70, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.782 [0.000, 4.000], mean observation: 0.492 [0.340, 0.530], loss: 7.470462, mean_absolute_error: 41.689194, mean_q: 53.489464
[F[K 298216/500000: episode: 3905, duration: 1.125s, episode steps: 83, steps per second: 74, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.181 [0.000, 4.000], mean observation: 0.517 [0.490, 0.590], loss: 8.198722, mean_absolute_error: 41.935535, mean_q: 53.762199
[F[K 298297/500000: episode: 3906, duration: 1.009s, episode steps: 81, steps per second: 80, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.478 [0.350, 0.500], loss: 7.926222, mean_absolute_error: 41.967934, mean_q: 53.776379
[F[K 298361/500000: episode: 3907, duration: 0.916s, episode steps: 64, steps per second: 70, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.562 [0.000, 4.000], mean observation: 0.520 [0.470, 0.630], loss: 8.992437, mean_absolute_error: 41.695408, mean_q: 53.407532
[F[K 298456/500000: episode: 3908, duration: 1.350s, episode steps: 95, steps per second: 70, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.905 [0.000, 4.000], mean observation: 0.474 [0.370, 0.530], loss: 7.425365, mean_absolute_error: 42.031982, mean_q: 53.958714
[F[K 298572/500000: episode: 3909, duration: 1.424s, episode steps: 116, steps per second: 81, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.845 [0.000, 4.000], mean observation: 0.499 [0.420, 0.590], loss: 8.054777, mean_absolute_error: 41.918236, mean_q: 53.755470
[F[K 298648/500000: episode: 3910, duration: 1.012s, episode steps: 76, steps per second: 75, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.316 [0.000, 4.000], mean observation: 0.503 [0.420, 0.620], loss: 7.737654, mean_absolute_error: 42.226627, mean_q: 54.154411
[F[K 298719/500000: episode: 3911, duration: 1.121s, episode steps: 71, steps per second: 63, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.930 [0.000, 4.000], mean observation: 0.509 [0.460, 0.630], loss: 7.981567, mean_absolute_error: 42.082104, mean_q: 53.825020
[F[K 298804/500000: episode: 3912, duration: 1.043s, episode steps: 85, steps per second: 81, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.976 [0.000, 4.000], mean observation: 0.491 [0.350, 0.580], loss: 8.134681, mean_absolute_error: 42.241711, mean_q: 54.170345
[F[K 298857/500000: episode: 3913, duration: 0.636s, episode steps: 53, steps per second: 83, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.962 [0.000, 4.000], mean observation: 0.521 [0.470, 0.640], loss: 9.357964, mean_absolute_error: 41.378960, mean_q: 53.029037
[F[K 298947/500000: episode: 3914, duration: 0.972s, episode steps: 90, steps per second: 93, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.978 [0.000, 4.000], mean observation: 0.490 [0.370, 0.530], loss: 8.008000, mean_absolute_error: 41.770916, mean_q: 53.593685
[F[K 299013/500000: episode: 3915, duration: 0.821s, episode steps: 66, steps per second: 80, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.258 [0.000, 4.000], mean observation: 0.507 [0.480, 0.580], loss: 6.400799, mean_absolute_error: 42.318054, mean_q: 54.345135
[F[K 299094/500000: episode: 3916, duration: 0.950s, episode steps: 81, steps per second: 85, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.753 [0.000, 4.000], mean observation: 0.478 [0.380, 0.530], loss: 7.709495, mean_absolute_error: 42.383160, mean_q: 54.310909
[F[K 299258/500000: episode: 3917, duration: 1.716s, episode steps: 164, steps per second: 96, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.262 [0.000, 4.000], mean observation: 0.501 [0.440, 0.560], loss: 9.518529, mean_absolute_error: 41.801781, mean_q: 53.516357
[F[K 299333/500000: episode: 3918, duration: 0.880s, episode steps: 75, steps per second: 85, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.880 [0.000, 4.000], mean observation: 0.481 [0.370, 0.530], loss: 7.150851, mean_absolute_error: 42.044155, mean_q: 53.903343
[F[K 299454/500000: episode: 3919, duration: 1.354s, episode steps: 121, steps per second: 89, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.835 [0.000, 4.000], mean observation: 0.480 [0.360, 0.530], loss: 8.465343, mean_absolute_error: 41.856930, mean_q: 53.694870
[F[K 299507/500000: episode: 3920, duration: 0.602s, episode steps: 53, steps per second: 88, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.321 [0.000, 4.000], mean observation: 0.497 [0.410, 0.550], loss: 8.835742, mean_absolute_error: 42.012066, mean_q: 53.744446
[F[K 299627/500000: episode: 3921, duration: 1.472s, episode steps: 120, steps per second: 82, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.350 [0.000, 4.000], mean observation: 0.507 [0.480, 0.560], loss: 8.990678, mean_absolute_error: 42.388271, mean_q: 54.369377
[F[K 299678/500000: episode: 3922, duration: 0.703s, episode steps: 51, steps per second: 73, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.725 [0.000, 4.000], mean observation: 0.514 [0.470, 0.610], loss: 8.440756, mean_absolute_error: 41.222771, mean_q: 52.911884
[F[K 299800/500000: episode: 3923, duration: 1.542s, episode steps: 122, steps per second: 79, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.803 [0.000, 4.000], mean observation: 0.489 [0.380, 0.530], loss: 7.453851, mean_absolute_error: 42.396046, mean_q: 54.327789
[F[K 299890/500000: episode: 3924, duration: 1.099s, episode steps: 90, steps per second: 82, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.486 [0.360, 0.530], loss: 7.172006, mean_absolute_error: 42.207943, mean_q: 54.138313
[F[K 299949/500000: episode: 3925, duration: 0.735s, episode steps: 59, steps per second: 80, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.237 [0.000, 4.000], mean observation: 0.482 [0.400, 0.520], loss: 9.965858, mean_absolute_error: 42.053280, mean_q: 53.807270
[F[K 300037/500000: episode: 3926, duration: 1.148s, episode steps: 88, steps per second: 77, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.977 [0.000, 4.000], mean observation: 0.493 [0.400, 0.530], loss: 8.642553, mean_absolute_error: 42.453136, mean_q: 54.393188
[F[K 300094/500000: episode: 3927, duration: 0.767s, episode steps: 57, steps per second: 74, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.684 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 10.027737, mean_absolute_error: 42.016323, mean_q: 53.816360
[F[K 300189/500000: episode: 3928, duration: 1.199s, episode steps: 95, steps per second: 79, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.032 [0.000, 4.000], mean observation: 0.495 [0.370, 0.580], loss: 8.654073, mean_absolute_error: 41.848061, mean_q: 53.649826
[F[K 300264/500000: episode: 3929, duration: 1.011s, episode steps: 75, steps per second: 74, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 0.512 [0.470, 0.580], loss: 8.316822, mean_absolute_error: 43.050076, mean_q: 55.102695
[F[K 300323/500000: episode: 3930, duration: 0.726s, episode steps: 59, steps per second: 81, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.051 [0.000, 4.000], mean observation: 0.495 [0.430, 0.530], loss: 7.851935, mean_absolute_error: 41.847492, mean_q: 53.566490
[F[K 300387/500000: episode: 3931, duration: 0.855s, episode steps: 64, steps per second: 75, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 0.507 [0.470, 0.540], loss: 9.434668, mean_absolute_error: 42.427094, mean_q: 54.394569
[F[K 300442/500000: episode: 3932, duration: 0.700s, episode steps: 55, steps per second: 79, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.836 [0.000, 4.000], mean observation: 0.521 [0.470, 0.640], loss: 7.805487, mean_absolute_error: 42.454948, mean_q: 54.554367
[F[K 300510/500000: episode: 3933, duration: 0.849s, episode steps: 68, steps per second: 80, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.956 [0.000, 4.000], mean observation: 0.509 [0.490, 0.540], loss: 9.823133, mean_absolute_error: 41.906425, mean_q: 53.722897
[F[K 300551/500000: episode: 3934, duration: 0.549s, episode steps: 41, steps per second: 75, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.268 [0.000, 4.000], mean observation: 0.505 [0.430, 0.650], loss: 8.066284, mean_absolute_error: 41.325493, mean_q: 52.983566
[F[K 300608/500000: episode: 3935, duration: 0.747s, episode steps: 57, steps per second: 76, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.492 [0.400, 0.530], loss: 9.327304, mean_absolute_error: 41.680710, mean_q: 53.409985
[F[K 300666/500000: episode: 3936, duration: 0.789s, episode steps: 58, steps per second: 74, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.276 [0.000, 4.000], mean observation: 0.510 [0.470, 0.650], loss: 8.093054, mean_absolute_error: 41.864914, mean_q: 53.625866
[F[K 300829/500000: episode: 3937, duration: 2.144s, episode steps: 163, steps per second: 76, episode reward: 163.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.074 [0.000, 4.000], mean observation: 0.502 [0.380, 0.590], loss: 9.320699, mean_absolute_error: 42.114056, mean_q: 53.956299
[F[K 300909/500000: episode: 3938, duration: 1.096s, episode steps: 80, steps per second: 73, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 0.484 [0.430, 0.520], loss: 7.309873, mean_absolute_error: 42.203255, mean_q: 54.094482
[F[K 301067/500000: episode: 3939, duration: 2.181s, episode steps: 158, steps per second: 72, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.139 [0.000, 4.000], mean observation: 0.507 [0.420, 0.670], loss: 8.333897, mean_absolute_error: 42.614220, mean_q: 54.548420
[F[K 301135/500000: episode: 3940, duration: 0.888s, episode steps: 68, steps per second: 77, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.500 [0.460, 0.570], loss: 7.783248, mean_absolute_error: 42.548546, mean_q: 54.441124
[F[K 301210/500000: episode: 3941, duration: 0.892s, episode steps: 75, steps per second: 84, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.187 [0.000, 4.000], mean observation: 0.481 [0.370, 0.530], loss: 8.175564, mean_absolute_error: 41.970005, mean_q: 53.865082
[F[K 301345/500000: episode: 3942, duration: 1.754s, episode steps: 135, steps per second: 77, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.993 [0.000, 4.000], mean observation: 0.516 [0.470, 0.630], loss: 7.717698, mean_absolute_error: 42.437683, mean_q: 54.378574
[F[K 301413/500000: episode: 3943, duration: 0.851s, episode steps: 68, steps per second: 80, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.838 [0.000, 4.000], mean observation: 0.506 [0.470, 0.600], loss: 7.551122, mean_absolute_error: 42.258820, mean_q: 54.223492
[F[K 301506/500000: episode: 3944, duration: 1.147s, episode steps: 93, steps per second: 81, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.512 [0.440, 0.650], loss: 8.357284, mean_absolute_error: 42.030415, mean_q: 53.907337
[F[K 301548/500000: episode: 3945, duration: 0.541s, episode steps: 42, steps per second: 78, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.571 [0.000, 4.000], mean observation: 0.506 [0.430, 0.640], loss: 10.542209, mean_absolute_error: 42.021748, mean_q: 53.757069
[F[K 301609/500000: episode: 3946, duration: 0.701s, episode steps: 61, steps per second: 87, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.934 [0.000, 4.000], mean observation: 0.488 [0.440, 0.510], loss: 9.274355, mean_absolute_error: 41.889980, mean_q: 53.514969
[F[K 301658/500000: episode: 3947, duration: 0.662s, episode steps: 49, steps per second: 74, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.367 [0.000, 4.000], mean observation: 0.505 [0.460, 0.600], loss: 8.013390, mean_absolute_error: 42.349312, mean_q: 54.211605
[F[K 301740/500000: episode: 3948, duration: 1.073s, episode steps: 82, steps per second: 76, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.024 [0.000, 4.000], mean observation: 0.499 [0.420, 0.560], loss: 7.762377, mean_absolute_error: 42.127884, mean_q: 54.066059
[F[K 301819/500000: episode: 3949, duration: 1.095s, episode steps: 79, steps per second: 72, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.177 [0.000, 4.000], mean observation: 0.488 [0.420, 0.530], loss: 8.533923, mean_absolute_error: 41.772266, mean_q: 53.492653
[F[K 301903/500000: episode: 3950, duration: 0.913s, episode steps: 84, steps per second: 92, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.496 [0.420, 0.530], loss: 8.506971, mean_absolute_error: 42.259071, mean_q: 54.236107
[F[K 301997/500000: episode: 3951, duration: 1.354s, episode steps: 94, steps per second: 69, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.957 [0.000, 4.000], mean observation: 0.500 [0.440, 0.550], loss: 7.702987, mean_absolute_error: 41.492664, mean_q: 53.185772
[F[K 302067/500000: episode: 3952, duration: 0.987s, episode steps: 70, steps per second: 71, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.501 [0.450, 0.540], loss: 7.792189, mean_absolute_error: 41.731388, mean_q: 53.515415
[F[K 302127/500000: episode: 3953, duration: 0.784s, episode steps: 60, steps per second: 77, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.267 [0.000, 4.000], mean observation: 0.520 [0.480, 0.630], loss: 8.524665, mean_absolute_error: 42.122425, mean_q: 53.854740
[F[K 302178/500000: episode: 3954, duration: 0.752s, episode steps: 51, steps per second: 68, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.647 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 9.703344, mean_absolute_error: 41.970718, mean_q: 53.716621
[F[K 302255/500000: episode: 3955, duration: 1.112s, episode steps: 77, steps per second: 69, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.039 [0.000, 4.000], mean observation: 0.511 [0.490, 0.580], loss: 6.814511, mean_absolute_error: 41.415749, mean_q: 53.189819
[F[K 302330/500000: episode: 3956, duration: 1.114s, episode steps: 75, steps per second: 67, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.040 [0.000, 4.000], mean observation: 0.491 [0.400, 0.530], loss: 7.043200, mean_absolute_error: 42.178268, mean_q: 54.032864
[F[K 302384/500000: episode: 3957, duration: 0.711s, episode steps: 54, steps per second: 76, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 0.512 [0.450, 0.650], loss: 8.353063, mean_absolute_error: 41.423992, mean_q: 53.208721
[F[K 302520/500000: episode: 3958, duration: 1.934s, episode steps: 136, steps per second: 70, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.868 [0.000, 4.000], mean observation: 0.508 [0.470, 0.550], loss: 7.826519, mean_absolute_error: 42.263344, mean_q: 54.199593
[F[K 302598/500000: episode: 3959, duration: 1.172s, episode steps: 78, steps per second: 67, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.346 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 8.818355, mean_absolute_error: 41.939270, mean_q: 53.726192
[F[K 302702/500000: episode: 3960, duration: 1.547s, episode steps: 104, steps per second: 67, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.503 [0.480, 0.540], loss: 8.623440, mean_absolute_error: 42.423969, mean_q: 54.339851
[F[K 302859/500000: episode: 3961, duration: 2.171s, episode steps: 157, steps per second: 72, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.121 [0.000, 4.000], mean observation: 0.509 [0.470, 0.570], loss: 8.748117, mean_absolute_error: 42.011040, mean_q: 53.812923
[F[K 302939/500000: episode: 3962, duration: 1.063s, episode steps: 80, steps per second: 75, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.050 [0.000, 4.000], mean observation: 0.511 [0.470, 0.580], loss: 8.291763, mean_absolute_error: 42.420738, mean_q: 54.402515
[F[K 303011/500000: episode: 3963, duration: 1.013s, episode steps: 72, steps per second: 71, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 0.488 [0.380, 0.530], loss: 9.095705, mean_absolute_error: 42.186569, mean_q: 54.123672
[F[K 303051/500000: episode: 3964, duration: 0.542s, episode steps: 40, steps per second: 74, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.575 [0.000, 4.000], mean observation: 0.503 [0.440, 0.630], loss: 7.689276, mean_absolute_error: 41.526833, mean_q: 53.269592
[F[K 303112/500000: episode: 3965, duration: 0.966s, episode steps: 61, steps per second: 63, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.656 [0.000, 4.000], mean observation: 0.509 [0.490, 0.550], loss: 7.795407, mean_absolute_error: 42.498833, mean_q: 54.580490
[F[K 303172/500000: episode: 3966, duration: 0.727s, episode steps: 60, steps per second: 83, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 0.500 [0.420, 0.580], loss: 7.394109, mean_absolute_error: 42.845356, mean_q: 54.860634
[F[K 303261/500000: episode: 3967, duration: 1.012s, episode steps: 89, steps per second: 88, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.503 [0.460, 0.580], loss: 8.203229, mean_absolute_error: 42.348427, mean_q: 54.350601
[F[K 303319/500000: episode: 3968, duration: 0.715s, episode steps: 58, steps per second: 81, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.490 [0.410, 0.520], loss: 7.816193, mean_absolute_error: 42.516941, mean_q: 54.570152
[F[K 303384/500000: episode: 3969, duration: 0.771s, episode steps: 65, steps per second: 84, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.485 [0.370, 0.520], loss: 9.910542, mean_absolute_error: 41.497849, mean_q: 53.120914
[F[K 303449/500000: episode: 3970, duration: 0.817s, episode steps: 65, steps per second: 80, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.513 [0.470, 0.610], loss: 8.168829, mean_absolute_error: 42.091919, mean_q: 53.948242
[F[K 303491/500000: episode: 3971, duration: 0.470s, episode steps: 42, steps per second: 89, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.519 [0.470, 0.600], loss: 7.949528, mean_absolute_error: 42.315350, mean_q: 54.222645
[F[K 303591/500000: episode: 3972, duration: 1.253s, episode steps: 100, steps per second: 80, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.020 [0.000, 4.000], mean observation: 0.509 [0.420, 0.640], loss: 8.137256, mean_absolute_error: 41.471733, mean_q: 53.157822
[F[K 303694/500000: episode: 3973, duration: 1.359s, episode steps: 103, steps per second: 76, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.447 [0.000, 4.000], mean observation: 0.488 [0.430, 0.530], loss: 9.089618, mean_absolute_error: 42.058163, mean_q: 53.870056
[F[K 303766/500000: episode: 3974, duration: 1.024s, episode steps: 72, steps per second: 70, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 0.489 [0.450, 0.520], loss: 8.231072, mean_absolute_error: 42.088974, mean_q: 53.974625
[F[K 303813/500000: episode: 3975, duration: 0.647s, episode steps: 47, steps per second: 73, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.979 [0.000, 4.000], mean observation: 0.517 [0.470, 0.610], loss: 9.203150, mean_absolute_error: 41.755276, mean_q: 53.555115
[F[K 303900/500000: episode: 3976, duration: 1.375s, episode steps: 87, steps per second: 63, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.138 [0.000, 4.000], mean observation: 0.490 [0.430, 0.530], loss: 8.550273, mean_absolute_error: 41.941647, mean_q: 53.815159
[F[K 303959/500000: episode: 3977, duration: 0.795s, episode steps: 59, steps per second: 74, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.136 [0.000, 4.000], mean observation: 0.483 [0.380, 0.530], loss: 8.234321, mean_absolute_error: 41.983883, mean_q: 53.849854
[F[K 304030/500000: episode: 3978, duration: 1.029s, episode steps: 71, steps per second: 69, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.183 [0.000, 4.000], mean observation: 0.492 [0.400, 0.530], loss: 7.994118, mean_absolute_error: 41.979244, mean_q: 53.777325
[F[K 304120/500000: episode: 3979, duration: 1.249s, episode steps: 90, steps per second: 72, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.501 [0.470, 0.560], loss: 8.255472, mean_absolute_error: 41.494614, mean_q: 53.236355
[F[K 304196/500000: episode: 3980, duration: 1.047s, episode steps: 76, steps per second: 73, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.829 [0.000, 4.000], mean observation: 0.512 [0.470, 0.590], loss: 7.700921, mean_absolute_error: 41.905636, mean_q: 53.759270
[F[K 304269/500000: episode: 3981, duration: 1.012s, episode steps: 73, steps per second: 72, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.466 [0.000, 4.000], mean observation: 0.481 [0.390, 0.530], loss: 8.417350, mean_absolute_error: 41.837723, mean_q: 53.555576
[F[K 304403/500000: episode: 3982, duration: 1.839s, episode steps: 134, steps per second: 73, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.119 [0.000, 4.000], mean observation: 0.491 [0.440, 0.530], loss: 7.928030, mean_absolute_error: 42.200996, mean_q: 54.145638
[F[K 304533/500000: episode: 3983, duration: 1.646s, episode steps: 130, steps per second: 79, episode reward: 130.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.415 [0.000, 4.000], mean observation: 0.472 [0.350, 0.510], loss: 7.834446, mean_absolute_error: 42.463005, mean_q: 54.408211
[F[K 304608/500000: episode: 3984, duration: 1.012s, episode steps: 75, steps per second: 74, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.960 [0.000, 4.000], mean observation: 0.478 [0.390, 0.520], loss: 7.794866, mean_absolute_error: 42.096783, mean_q: 53.957352
[F[K 304655/500000: episode: 3985, duration: 0.641s, episode steps: 47, steps per second: 73, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.213 [0.000, 4.000], mean observation: 0.526 [0.470, 0.640], loss: 8.568100, mean_absolute_error: 41.574718, mean_q: 53.187859
[F[K 304740/500000: episode: 3986, duration: 1.066s, episode steps: 85, steps per second: 80, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.106 [0.000, 4.000], mean observation: 0.500 [0.430, 0.590], loss: 8.093547, mean_absolute_error: 41.744617, mean_q: 53.507015
[F[K 304833/500000: episode: 3987, duration: 1.142s, episode steps: 93, steps per second: 81, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.065 [0.000, 4.000], mean observation: 0.505 [0.470, 0.550], loss: 8.178691, mean_absolute_error: 42.211002, mean_q: 54.100185
[F[K 304886/500000: episode: 3988, duration: 0.746s, episode steps: 53, steps per second: 71, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.679 [0.000, 4.000], mean observation: 0.500 [0.380, 0.630], loss: 8.298887, mean_absolute_error: 42.386833, mean_q: 54.470955
[F[K 304927/500000: episode: 3989, duration: 0.590s, episode steps: 41, steps per second: 69, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.634 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 6.907370, mean_absolute_error: 41.748447, mean_q: 53.538151
[F[K 304994/500000: episode: 3990, duration: 0.803s, episode steps: 67, steps per second: 83, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.866 [0.000, 4.000], mean observation: 0.507 [0.440, 0.600], loss: 8.298107, mean_absolute_error: 42.231842, mean_q: 54.172310
[F[K 305089/500000: episode: 3991, duration: 1.289s, episode steps: 95, steps per second: 74, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.011 [0.000, 4.000], mean observation: 0.497 [0.430, 0.540], loss: 9.187509, mean_absolute_error: 42.735008, mean_q: 54.787792
[F[K 305242/500000: episode: 3992, duration: 2.325s, episode steps: 153, steps per second: 66, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.072 [0.000, 4.000], mean observation: 0.502 [0.450, 0.560], loss: 8.367294, mean_absolute_error: 42.468658, mean_q: 54.421707
[F[K 305335/500000: episode: 3993, duration: 1.374s, episode steps: 93, steps per second: 68, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.989 [0.000, 4.000], mean observation: 0.501 [0.390, 0.610], loss: 8.318789, mean_absolute_error: 42.278721, mean_q: 54.179050
[F[K 305383/500000: episode: 3994, duration: 0.712s, episode steps: 48, steps per second: 67, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 8.845917, mean_absolute_error: 41.550331, mean_q: 53.281891
[F[K 305483/500000: episode: 3995, duration: 1.362s, episode steps: 100, steps per second: 73, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.910 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 9.740143, mean_absolute_error: 41.454620, mean_q: 53.076817
[F[K 305559/500000: episode: 3996, duration: 1.065s, episode steps: 76, steps per second: 71, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.501 [0.450, 0.530], loss: 8.471906, mean_absolute_error: 41.620674, mean_q: 53.408173
[F[K 305604/500000: episode: 3997, duration: 0.659s, episode steps: 45, steps per second: 68, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.978 [0.000, 4.000], mean observation: 0.500 [0.360, 0.640], loss: 9.331042, mean_absolute_error: 42.127411, mean_q: 54.241787
[F[K 305675/500000: episode: 3998, duration: 0.980s, episode steps: 71, steps per second: 72, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.483 [0.370, 0.530], loss: 8.927669, mean_absolute_error: 41.718727, mean_q: 53.400761
[F[K 305716/500000: episode: 3999, duration: 0.577s, episode steps: 41, steps per second: 71, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.244 [0.000, 4.000], mean observation: 0.501 [0.400, 0.640], loss: 7.816192, mean_absolute_error: 41.875942, mean_q: 53.839081
[F[K 305916/500000: episode: 4000, duration: 3.091s, episode steps: 200, steps per second: 65, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.493 [0.430, 0.540], loss: 8.730860, mean_absolute_error: 42.168949, mean_q: 54.102348
[F[K 305986/500000: episode: 4001, duration: 0.946s, episode steps: 70, steps per second: 74, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.886 [0.000, 4.000], mean observation: 0.498 [0.410, 0.560], loss: 7.691848, mean_absolute_error: 42.755741, mean_q: 54.793926
[F[K 306053/500000: episode: 4002, duration: 0.946s, episode steps: 67, steps per second: 71, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.821 [0.000, 4.000], mean observation: 0.496 [0.430, 0.530], loss: 8.940250, mean_absolute_error: 42.156097, mean_q: 53.907021
[F[K 306212/500000: episode: 4003, duration: 2.280s, episode steps: 159, steps per second: 70, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.365 [0.000, 4.000], mean observation: 0.495 [0.410, 0.570], loss: 8.395473, mean_absolute_error: 42.054398, mean_q: 53.980499
[F[K 306304/500000: episode: 4004, duration: 1.372s, episode steps: 92, steps per second: 67, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.402 [0.000, 4.000], mean observation: 0.502 [0.410, 0.610], loss: 10.125036, mean_absolute_error: 42.056259, mean_q: 53.973278
[F[K 306344/500000: episode: 4005, duration: 0.661s, episode steps: 40, steps per second: 60, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.515 [0.470, 0.640], loss: 9.327904, mean_absolute_error: 41.410984, mean_q: 53.065205
[F[K 306400/500000: episode: 4006, duration: 0.882s, episode steps: 56, steps per second: 64, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.696 [0.000, 4.000], mean observation: 0.483 [0.380, 0.530], loss: 8.151999, mean_absolute_error: 41.604729, mean_q: 53.278790
[F[K 306451/500000: episode: 4007, duration: 0.778s, episode steps: 51, steps per second: 66, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.529 [0.000, 4.000], mean observation: 0.508 [0.460, 0.640], loss: 9.487514, mean_absolute_error: 41.941803, mean_q: 53.660088
[F[K 306507/500000: episode: 4008, duration: 0.908s, episode steps: 56, steps per second: 62, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.518 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 8.278213, mean_absolute_error: 41.504524, mean_q: 53.208282
[F[K 306576/500000: episode: 4009, duration: 1.145s, episode steps: 69, steps per second: 60, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.101 [0.000, 4.000], mean observation: 0.503 [0.470, 0.590], loss: 8.599545, mean_absolute_error: 42.046761, mean_q: 53.925083
[F[K 306651/500000: episode: 4010, duration: 1.097s, episode steps: 75, steps per second: 68, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.347 [0.000, 4.000], mean observation: 0.507 [0.440, 0.610], loss: 9.301729, mean_absolute_error: 41.086639, mean_q: 52.550415
[F[K 306723/500000: episode: 4011, duration: 1.120s, episode steps: 72, steps per second: 64, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.361 [0.000, 4.000], mean observation: 0.516 [0.480, 0.610], loss: 8.770817, mean_absolute_error: 41.565384, mean_q: 53.292297
[F[K 306778/500000: episode: 4012, duration: 0.896s, episode steps: 55, steps per second: 61, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.964 [0.000, 4.000], mean observation: 0.529 [0.470, 0.650], loss: 7.729431, mean_absolute_error: 42.258430, mean_q: 54.119728
[F[K 306840/500000: episode: 4013, duration: 1.100s, episode steps: 62, steps per second: 56, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 7.870544, mean_absolute_error: 42.500805, mean_q: 54.556267
[F[K 306901/500000: episode: 4014, duration: 0.919s, episode steps: 61, steps per second: 66, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.328 [0.000, 4.000], mean observation: 0.514 [0.470, 0.650], loss: 10.237863, mean_absolute_error: 41.587276, mean_q: 53.096436
[F[K 306976/500000: episode: 4015, duration: 1.321s, episode steps: 75, steps per second: 57, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.920 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 9.045875, mean_absolute_error: 41.623878, mean_q: 53.263119
[F[K 307026/500000: episode: 4016, duration: 0.886s, episode steps: 50, steps per second: 56, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.499 [0.450, 0.530], loss: 6.605728, mean_absolute_error: 42.493057, mean_q: 54.409809
[F[K 307082/500000: episode: 4017, duration: 0.866s, episode steps: 56, steps per second: 65, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.982 [0.000, 4.000], mean observation: 0.491 [0.380, 0.530], loss: 7.759087, mean_absolute_error: 41.392536, mean_q: 53.091469
[F[K 307138/500000: episode: 4018, duration: 0.801s, episode steps: 56, steps per second: 70, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 0.502 [0.390, 0.600], loss: 7.087573, mean_absolute_error: 41.897156, mean_q: 53.647533
[F[K 307225/500000: episode: 4019, duration: 1.411s, episode steps: 87, steps per second: 62, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.498 [0.460, 0.520], loss: 7.784696, mean_absolute_error: 41.991081, mean_q: 53.844337
[F[K 307293/500000: episode: 4020, duration: 1.255s, episode steps: 68, steps per second: 54, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.147 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 7.058827, mean_absolute_error: 41.864994, mean_q: 53.633514
[F[K 307420/500000: episode: 4021, duration: 2.296s, episode steps: 127, steps per second: 55, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.504 [0.420, 0.650], loss: 8.229738, mean_absolute_error: 41.861351, mean_q: 53.651630
[F[K 307495/500000: episode: 4022, duration: 1.380s, episode steps: 75, steps per second: 54, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.133 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 8.425950, mean_absolute_error: 41.975536, mean_q: 53.863167
[F[K 307588/500000: episode: 4023, duration: 1.620s, episode steps: 93, steps per second: 57, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.817 [0.000, 4.000], mean observation: 0.497 [0.440, 0.540], loss: 7.728704, mean_absolute_error: 41.504196, mean_q: 53.242413
[F[K 307656/500000: episode: 4024, duration: 1.106s, episode steps: 68, steps per second: 61, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 8.161226, mean_absolute_error: 41.899975, mean_q: 53.611744
[F[K 307703/500000: episode: 4025, duration: 0.783s, episode steps: 47, steps per second: 60, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.532 [0.000, 4.000], mean observation: 0.518 [0.470, 0.610], loss: 7.961574, mean_absolute_error: 41.930626, mean_q: 53.841251
[F[K 307761/500000: episode: 4026, duration: 0.885s, episode steps: 58, steps per second: 66, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.879 [0.000, 4.000], mean observation: 0.504 [0.450, 0.610], loss: 6.417044, mean_absolute_error: 41.864388, mean_q: 53.695038
[F[K 307823/500000: episode: 4027, duration: 0.993s, episode steps: 62, steps per second: 62, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.514 [0.480, 0.630], loss: 8.471889, mean_absolute_error: 42.481342, mean_q: 54.453541
[F[K 307935/500000: episode: 4028, duration: 1.826s, episode steps: 112, steps per second: 61, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.884 [0.000, 4.000], mean observation: 0.489 [0.370, 0.520], loss: 8.341005, mean_absolute_error: 41.976997, mean_q: 53.909420
[F[K 308000/500000: episode: 4029, duration: 0.940s, episode steps: 65, steps per second: 69, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.892 [0.000, 4.000], mean observation: 0.497 [0.370, 0.600], loss: 8.565408, mean_absolute_error: 42.548710, mean_q: 54.513947
[F[K 308088/500000: episode: 4030, duration: 1.440s, episode steps: 88, steps per second: 61, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.491 [0.420, 0.530], loss: 8.431034, mean_absolute_error: 42.297745, mean_q: 54.233398
[F[K 308219/500000: episode: 4031, duration: 2.218s, episode steps: 131, steps per second: 59, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.153 [0.000, 4.000], mean observation: 0.526 [0.470, 0.690], loss: 7.539608, mean_absolute_error: 42.014503, mean_q: 53.929966
[F[K 308338/500000: episode: 4032, duration: 1.792s, episode steps: 119, steps per second: 66, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.185 [0.000, 4.000], mean observation: 0.511 [0.480, 0.580], loss: 7.827604, mean_absolute_error: 41.955551, mean_q: 53.812450
[F[K 308410/500000: episode: 4033, duration: 1.025s, episode steps: 72, steps per second: 70, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 7.515187, mean_absolute_error: 41.680103, mean_q: 53.410725
[F[K 308494/500000: episode: 4034, duration: 1.239s, episode steps: 84, steps per second: 68, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.631 [0.000, 4.000], mean observation: 0.487 [0.390, 0.530], loss: 8.459288, mean_absolute_error: 41.967205, mean_q: 53.773849
[F[K 308583/500000: episode: 4035, duration: 1.431s, episode steps: 89, steps per second: 62, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.180 [0.000, 4.000], mean observation: 0.518 [0.490, 0.600], loss: 7.959432, mean_absolute_error: 42.414513, mean_q: 54.279270
[F[K 308670/500000: episode: 4036, duration: 1.310s, episode steps: 87, steps per second: 66, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.621 [0.000, 4.000], mean observation: 0.514 [0.490, 0.590], loss: 8.446670, mean_absolute_error: 41.874462, mean_q: 53.781689
[F[K 308729/500000: episode: 4037, duration: 0.980s, episode steps: 59, steps per second: 60, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.034 [0.000, 4.000], mean observation: 0.501 [0.440, 0.530], loss: 8.687391, mean_absolute_error: 41.700752, mean_q: 53.450916
[F[K 308834/500000: episode: 4038, duration: 1.752s, episode steps: 105, steps per second: 60, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.952 [0.000, 4.000], mean observation: 0.509 [0.480, 0.590], loss: 8.952413, mean_absolute_error: 42.094604, mean_q: 53.914249
[F[K 308893/500000: episode: 4039, duration: 0.893s, episode steps: 59, steps per second: 66, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.136 [0.000, 4.000], mean observation: 0.494 [0.420, 0.530], loss: 8.657722, mean_absolute_error: 41.481113, mean_q: 53.217564
[F[K 308952/500000: episode: 4040, duration: 1.047s, episode steps: 59, steps per second: 56, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.288 [0.000, 4.000], mean observation: 0.519 [0.470, 0.630], loss: 8.902781, mean_absolute_error: 42.731949, mean_q: 54.696491
[F[K 309016/500000: episode: 4041, duration: 0.914s, episode steps: 64, steps per second: 70, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.234 [0.000, 4.000], mean observation: 0.512 [0.480, 0.620], loss: 8.175641, mean_absolute_error: 41.975864, mean_q: 53.761108
[F[K 309090/500000: episode: 4042, duration: 1.172s, episode steps: 74, steps per second: 63, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.743 [0.000, 4.000], mean observation: 0.509 [0.450, 0.600], loss: 9.614367, mean_absolute_error: 42.084698, mean_q: 53.819740
[F[K 309152/500000: episode: 4043, duration: 0.988s, episode steps: 62, steps per second: 63, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.129 [0.000, 4.000], mean observation: 0.479 [0.350, 0.530], loss: 9.772874, mean_absolute_error: 42.638828, mean_q: 54.618221
[F[K 309216/500000: episode: 4044, duration: 1.005s, episode steps: 64, steps per second: 64, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.297 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 7.784225, mean_absolute_error: 42.635056, mean_q: 54.662426
[F[K 309281/500000: episode: 4045, duration: 0.920s, episode steps: 65, steps per second: 71, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.877 [0.000, 4.000], mean observation: 0.492 [0.440, 0.530], loss: 8.619309, mean_absolute_error: 41.630123, mean_q: 53.315350
[F[K 309338/500000: episode: 4046, duration: 0.792s, episode steps: 57, steps per second: 72, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 7.992477, mean_absolute_error: 42.243279, mean_q: 54.257931
[F[K 309405/500000: episode: 4047, duration: 0.952s, episode steps: 67, steps per second: 70, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.104 [0.000, 4.000], mean observation: 0.497 [0.460, 0.530], loss: 8.606119, mean_absolute_error: 42.026215, mean_q: 53.824852
[F[K 309471/500000: episode: 4048, duration: 0.946s, episode steps: 66, steps per second: 70, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.455 [0.000, 4.000], mean observation: 0.495 [0.420, 0.560], loss: 8.148911, mean_absolute_error: 41.545082, mean_q: 53.269005
[F[K 309538/500000: episode: 4049, duration: 0.904s, episode steps: 67, steps per second: 74, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.940 [0.000, 4.000], mean observation: 0.485 [0.410, 0.510], loss: 7.989851, mean_absolute_error: 41.663757, mean_q: 53.262409
[F[K 309631/500000: episode: 4050, duration: 1.352s, episode steps: 93, steps per second: 69, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.613 [0.000, 4.000], mean observation: 0.491 [0.350, 0.560], loss: 7.960113, mean_absolute_error: 43.000221, mean_q: 55.124424
[F[K 309707/500000: episode: 4051, duration: 1.004s, episode steps: 76, steps per second: 76, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.829 [0.000, 4.000], mean observation: 0.487 [0.380, 0.530], loss: 7.513618, mean_absolute_error: 42.308113, mean_q: 54.105110
[F[K 309746/500000: episode: 4052, duration: 0.560s, episode steps: 39, steps per second: 70, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.692 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 8.200577, mean_absolute_error: 41.600708, mean_q: 53.236153
[F[K 309849/500000: episode: 4053, duration: 1.479s, episode steps: 103, steps per second: 70, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 8.612503, mean_absolute_error: 41.609726, mean_q: 53.430038
[F[K 309933/500000: episode: 4054, duration: 1.235s, episode steps: 84, steps per second: 68, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.798 [0.000, 4.000], mean observation: 0.517 [0.470, 0.620], loss: 8.690275, mean_absolute_error: 41.833218, mean_q: 53.556583
[F[K 309995/500000: episode: 4055, duration: 0.991s, episode steps: 62, steps per second: 63, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.919 [0.000, 4.000], mean observation: 0.488 [0.360, 0.530], loss: 8.702105, mean_absolute_error: 42.242462, mean_q: 54.019920
[F[K 310065/500000: episode: 4056, duration: 1.091s, episode steps: 70, steps per second: 64, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.757 [0.000, 4.000], mean observation: 0.507 [0.430, 0.620], loss: 9.342329, mean_absolute_error: 42.013767, mean_q: 53.828667
[F[K 310133/500000: episode: 4057, duration: 1.020s, episode steps: 68, steps per second: 67, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.502 [0.470, 0.540], loss: 7.917809, mean_absolute_error: 42.175873, mean_q: 54.026546
[F[K 310200/500000: episode: 4058, duration: 0.978s, episode steps: 67, steps per second: 69, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 7.150344, mean_absolute_error: 42.075871, mean_q: 53.988564
[F[K 310253/500000: episode: 4059, duration: 0.847s, episode steps: 53, steps per second: 63, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.151 [0.000, 4.000], mean observation: 0.495 [0.370, 0.570], loss: 6.926571, mean_absolute_error: 42.768066, mean_q: 54.819862
[F[K 310323/500000: episode: 4060, duration: 1.036s, episode steps: 70, steps per second: 68, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.671 [0.000, 4.000], mean observation: 0.510 [0.460, 0.650], loss: 7.502732, mean_absolute_error: 42.415745, mean_q: 54.402733
[F[K 310403/500000: episode: 4061, duration: 1.177s, episode steps: 80, steps per second: 68, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.050 [0.000, 4.000], mean observation: 0.479 [0.360, 0.530], loss: 8.626986, mean_absolute_error: 42.860973, mean_q: 54.948284
[F[K 310451/500000: episode: 4062, duration: 0.691s, episode steps: 48, steps per second: 69, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.896 [0.000, 4.000], mean observation: 0.496 [0.380, 0.580], loss: 9.535078, mean_absolute_error: 42.675587, mean_q: 54.649555
[F[K 310601/500000: episode: 4063, duration: 2.176s, episode steps: 150, steps per second: 69, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.483 [0.350, 0.510], loss: 8.176698, mean_absolute_error: 42.401302, mean_q: 54.337414
[F[K 310652/500000: episode: 4064, duration: 0.706s, episode steps: 51, steps per second: 72, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.980 [0.000, 4.000], mean observation: 0.492 [0.420, 0.530], loss: 8.478841, mean_absolute_error: 42.365772, mean_q: 54.292305
[F[K 310746/500000: episode: 4065, duration: 1.422s, episode steps: 94, steps per second: 66, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.840 [0.000, 4.000], mean observation: 0.503 [0.470, 0.590], loss: 8.257565, mean_absolute_error: 43.030090, mean_q: 54.995934
[F[K 310840/500000: episode: 4066, duration: 1.254s, episode steps: 94, steps per second: 75, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.894 [0.000, 4.000], mean observation: 0.497 [0.380, 0.620], loss: 8.251741, mean_absolute_error: 41.930580, mean_q: 53.821350
[F[K 310914/500000: episode: 4067, duration: 1.026s, episode steps: 74, steps per second: 72, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.492 [0.410, 0.520], loss: 8.335842, mean_absolute_error: 43.097202, mean_q: 55.249779
[F[K 310960/500000: episode: 4068, duration: 0.566s, episode steps: 46, steps per second: 81, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.739 [0.000, 4.000], mean observation: 0.496 [0.470, 0.530], loss: 8.900789, mean_absolute_error: 41.931538, mean_q: 53.769291
[F[K 311045/500000: episode: 4069, duration: 1.165s, episode steps: 85, steps per second: 73, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.505 [0.480, 0.530], loss: 8.487466, mean_absolute_error: 42.262924, mean_q: 54.210526
[F[K 311100/500000: episode: 4070, duration: 0.767s, episode steps: 55, steps per second: 72, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.327 [0.000, 4.000], mean observation: 0.478 [0.350, 0.530], loss: 8.013362, mean_absolute_error: 42.575851, mean_q: 54.503410
[F[K 311154/500000: episode: 4071, duration: 0.742s, episode steps: 54, steps per second: 73, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.630 [0.000, 4.000], mean observation: 0.504 [0.450, 0.570], loss: 10.692987, mean_absolute_error: 42.235203, mean_q: 54.030849
[F[K 311220/500000: episode: 4072, duration: 0.897s, episode steps: 66, steps per second: 74, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.894 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 7.732757, mean_absolute_error: 42.277111, mean_q: 54.213165
[F[K 311275/500000: episode: 4073, duration: 0.867s, episode steps: 55, steps per second: 63, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.255 [0.000, 4.000], mean observation: 0.481 [0.410, 0.510], loss: 8.616181, mean_absolute_error: 42.274288, mean_q: 54.120136
[F[K 311372/500000: episode: 4074, duration: 1.433s, episode steps: 97, steps per second: 68, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.474 [0.000, 4.000], mean observation: 0.492 [0.350, 0.560], loss: 8.014267, mean_absolute_error: 41.963707, mean_q: 53.707489
[F[K 311455/500000: episode: 4075, duration: 1.279s, episode steps: 83, steps per second: 65, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.349 [0.000, 4.000], mean observation: 0.524 [0.480, 0.650], loss: 10.156969, mean_absolute_error: 42.108803, mean_q: 53.877052
[F[K 311625/500000: episode: 4076, duration: 2.247s, episode steps: 170, steps per second: 76, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.412 [0.000, 4.000], mean observation: 0.505 [0.450, 0.560], loss: 8.475855, mean_absolute_error: 42.242809, mean_q: 54.109364
[F[K 311698/500000: episode: 4077, duration: 1.097s, episode steps: 73, steps per second: 67, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.233 [0.000, 4.000], mean observation: 0.521 [0.490, 0.640], loss: 7.172920, mean_absolute_error: 42.452324, mean_q: 54.433247
[F[K 311765/500000: episode: 4078, duration: 1.016s, episode steps: 67, steps per second: 66, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.373 [0.000, 4.000], mean observation: 0.494 [0.450, 0.520], loss: 7.921443, mean_absolute_error: 42.736996, mean_q: 54.812752
[F[K 311807/500000: episode: 4079, duration: 0.683s, episode steps: 42, steps per second: 62, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.952 [0.000, 4.000], mean observation: 0.525 [0.470, 0.640], loss: 8.477891, mean_absolute_error: 41.649303, mean_q: 53.344040
[F[K 311860/500000: episode: 4080, duration: 0.788s, episode steps: 53, steps per second: 67, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.226 [0.000, 4.000], mean observation: 0.488 [0.390, 0.510], loss: 8.953576, mean_absolute_error: 42.108532, mean_q: 54.057159
[F[K 311904/500000: episode: 4081, duration: 0.721s, episode steps: 44, steps per second: 61, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.591 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 8.928711, mean_absolute_error: 41.928757, mean_q: 53.773643
[F[K 312014/500000: episode: 4082, duration: 1.655s, episode steps: 110, steps per second: 66, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.545 [0.000, 4.000], mean observation: 0.491 [0.390, 0.530], loss: 8.944636, mean_absolute_error: 42.367931, mean_q: 54.345234
[F[K 312080/500000: episode: 4083, duration: 0.996s, episode steps: 66, steps per second: 66, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.182 [0.000, 4.000], mean observation: 0.502 [0.440, 0.610], loss: 7.997797, mean_absolute_error: 42.171120, mean_q: 54.046188
[F[K 312139/500000: episode: 4084, duration: 0.938s, episode steps: 59, steps per second: 63, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.511 [0.470, 0.580], loss: 8.558898, mean_absolute_error: 41.000992, mean_q: 52.574207
[F[K 312191/500000: episode: 4085, duration: 0.818s, episode steps: 52, steps per second: 64, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.115 [0.000, 4.000], mean observation: 0.500 [0.400, 0.590], loss: 7.799740, mean_absolute_error: 41.938519, mean_q: 53.664238
[F[K 312270/500000: episode: 4086, duration: 1.188s, episode steps: 79, steps per second: 66, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.063 [0.000, 4.000], mean observation: 0.496 [0.410, 0.530], loss: 7.354609, mean_absolute_error: 40.978745, mean_q: 52.516312
[F[K 312381/500000: episode: 4087, duration: 1.570s, episode steps: 111, steps per second: 71, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.955 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 8.815515, mean_absolute_error: 42.106838, mean_q: 53.878731
[F[K 312496/500000: episode: 4088, duration: 1.762s, episode steps: 115, steps per second: 65, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.817 [0.000, 4.000], mean observation: 0.501 [0.360, 0.620], loss: 8.798663, mean_absolute_error: 41.902699, mean_q: 53.750408
[F[K 312558/500000: episode: 4089, duration: 0.888s, episode steps: 62, steps per second: 70, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.522 [0.470, 0.630], loss: 10.567043, mean_absolute_error: 41.355366, mean_q: 52.945217
[F[K 312643/500000: episode: 4090, duration: 1.269s, episode steps: 85, steps per second: 67, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 7.306151, mean_absolute_error: 41.847843, mean_q: 53.612858
[F[K 312790/500000: episode: 4091, duration: 2.177s, episode steps: 147, steps per second: 68, episode reward: 147.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.170 [0.000, 4.000], mean observation: 0.499 [0.430, 0.560], loss: 8.810959, mean_absolute_error: 41.686104, mean_q: 53.497269
[F[K 312856/500000: episode: 4092, duration: 0.959s, episode steps: 66, steps per second: 69, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.212 [0.000, 4.000], mean observation: 0.508 [0.490, 0.550], loss: 7.670774, mean_absolute_error: 41.553780, mean_q: 53.258858
[F[K 312946/500000: episode: 4093, duration: 1.253s, episode steps: 90, steps per second: 72, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 7.405963, mean_absolute_error: 41.513779, mean_q: 53.333008
[F[K 313017/500000: episode: 4094, duration: 0.931s, episode steps: 71, steps per second: 76, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.211 [0.000, 4.000], mean observation: 0.503 [0.470, 0.550], loss: 9.288572, mean_absolute_error: 41.572598, mean_q: 53.334953
[F[K 313081/500000: episode: 4095, duration: 0.824s, episode steps: 64, steps per second: 78, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.906 [0.000, 4.000], mean observation: 0.518 [0.480, 0.650], loss: 8.474665, mean_absolute_error: 41.672722, mean_q: 53.344601
[F[K 313140/500000: episode: 4096, duration: 0.849s, episode steps: 59, steps per second: 70, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.475 [0.000, 4.000], mean observation: 0.484 [0.380, 0.520], loss: 8.522550, mean_absolute_error: 41.674793, mean_q: 53.414703
[F[K 313191/500000: episode: 4097, duration: 0.792s, episode steps: 51, steps per second: 64, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.039 [0.000, 4.000], mean observation: 0.499 [0.380, 0.630], loss: 9.126924, mean_absolute_error: 41.336308, mean_q: 53.024220
[F[K 313258/500000: episode: 4098, duration: 0.956s, episode steps: 67, steps per second: 70, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.254 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 7.916394, mean_absolute_error: 41.800339, mean_q: 53.575958
[F[K 313350/500000: episode: 4099, duration: 1.395s, episode steps: 92, steps per second: 66, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.902 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 8.280871, mean_absolute_error: 41.422825, mean_q: 53.123264
[F[K 313482/500000: episode: 4100, duration: 1.896s, episode steps: 132, steps per second: 70, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 0.504 [0.470, 0.560], loss: 9.049470, mean_absolute_error: 41.835175, mean_q: 53.642742
[F[K 313565/500000: episode: 4101, duration: 1.296s, episode steps: 83, steps per second: 64, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.217 [0.000, 4.000], mean observation: 0.483 [0.360, 0.530], loss: 7.489014, mean_absolute_error: 41.667030, mean_q: 53.506348
[F[K 313615/500000: episode: 4102, duration: 0.838s, episode steps: 50, steps per second: 60, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.940 [0.000, 4.000], mean observation: 0.497 [0.380, 0.620], loss: 8.722292, mean_absolute_error: 42.315598, mean_q: 54.277882
[F[K 313652/500000: episode: 4103, duration: 0.645s, episode steps: 37, steps per second: 57, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 8.629633, mean_absolute_error: 42.010910, mean_q: 53.769531
[F[K 313721/500000: episode: 4104, duration: 1.098s, episode steps: 69, steps per second: 63, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.348 [0.000, 4.000], mean observation: 0.481 [0.360, 0.530], loss: 9.982014, mean_absolute_error: 41.888844, mean_q: 53.730907
[F[K 313808/500000: episode: 4105, duration: 1.090s, episode steps: 87, steps per second: 80, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.149 [0.000, 4.000], mean observation: 0.495 [0.370, 0.590], loss: 8.770159, mean_absolute_error: 41.545326, mean_q: 53.257622
[F[K 313891/500000: episode: 4106, duration: 1.311s, episode steps: 83, steps per second: 63, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.940 [0.000, 4.000], mean observation: 0.505 [0.420, 0.640], loss: 9.115087, mean_absolute_error: 41.646156, mean_q: 53.386276
[F[K 313980/500000: episode: 4107, duration: 1.491s, episode steps: 89, steps per second: 60, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.011 [0.000, 4.000], mean observation: 0.507 [0.460, 0.580], loss: 9.564004, mean_absolute_error: 41.586800, mean_q: 53.277157
[F[K 314091/500000: episode: 4108, duration: 1.451s, episode steps: 111, steps per second: 77, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.901 [0.000, 4.000], mean observation: 0.502 [0.400, 0.610], loss: 9.711114, mean_absolute_error: 41.475063, mean_q: 53.219639
[F[K 314170/500000: episode: 4109, duration: 0.953s, episode steps: 79, steps per second: 83, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.506 [0.000, 4.000], mean observation: 0.504 [0.450, 0.550], loss: 7.863449, mean_absolute_error: 42.077042, mean_q: 54.042358
[F[K 314233/500000: episode: 4110, duration: 0.691s, episode steps: 63, steps per second: 91, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.127 [0.000, 4.000], mean observation: 0.515 [0.480, 0.620], loss: 7.719227, mean_absolute_error: 41.461037, mean_q: 53.330334
[F[K 314323/500000: episode: 4111, duration: 1.290s, episode steps: 90, steps per second: 70, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.900 [0.000, 4.000], mean observation: 0.510 [0.490, 0.570], loss: 8.520530, mean_absolute_error: 41.341724, mean_q: 53.060558
[F[K 314416/500000: episode: 4112, duration: 1.296s, episode steps: 93, steps per second: 72, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.075 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 8.162814, mean_absolute_error: 41.520164, mean_q: 53.294403
[F[K 314513/500000: episode: 4113, duration: 1.282s, episode steps: 97, steps per second: 76, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.866 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 8.394022, mean_absolute_error: 41.873241, mean_q: 53.661774
[F[K 314602/500000: episode: 4114, duration: 1.191s, episode steps: 89, steps per second: 75, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.787 [0.000, 4.000], mean observation: 0.498 [0.390, 0.560], loss: 8.864576, mean_absolute_error: 42.124512, mean_q: 54.003544
[F[K 314657/500000: episode: 4115, duration: 0.746s, episode steps: 55, steps per second: 74, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.498 [0.350, 0.640], loss: 8.668195, mean_absolute_error: 41.596508, mean_q: 53.297462
[F[K 314737/500000: episode: 4116, duration: 1.149s, episode steps: 80, steps per second: 70, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.312 [0.000, 4.000], mean observation: 0.496 [0.370, 0.590], loss: 6.563137, mean_absolute_error: 41.603291, mean_q: 53.453674
[F[K 314806/500000: episode: 4117, duration: 0.860s, episode steps: 69, steps per second: 80, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.464 [0.000, 4.000], mean observation: 0.493 [0.400, 0.530], loss: 8.582552, mean_absolute_error: 41.340321, mean_q: 52.978237
[F[K 314847/500000: episode: 4118, duration: 0.557s, episode steps: 41, steps per second: 74, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.878 [0.000, 4.000], mean observation: 0.522 [0.470, 0.630], loss: 10.105416, mean_absolute_error: 41.423920, mean_q: 53.128132
[F[K 314923/500000: episode: 4119, duration: 0.925s, episode steps: 76, steps per second: 82, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.237 [0.000, 4.000], mean observation: 0.491 [0.440, 0.530], loss: 9.194832, mean_absolute_error: 40.994888, mean_q: 52.602753
[F[K 314982/500000: episode: 4120, duration: 0.786s, episode steps: 59, steps per second: 75, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.513 [0.490, 0.600], loss: 7.799208, mean_absolute_error: 41.961357, mean_q: 53.908833
[F[K 315048/500000: episode: 4121, duration: 0.816s, episode steps: 66, steps per second: 81, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 7.907492, mean_absolute_error: 41.709930, mean_q: 53.543030
[F[K 315115/500000: episode: 4122, duration: 0.810s, episode steps: 67, steps per second: 83, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.030 [0.000, 4.000], mean observation: 0.489 [0.440, 0.520], loss: 8.916205, mean_absolute_error: 41.481411, mean_q: 53.293671
[F[K 315271/500000: episode: 4123, duration: 1.744s, episode steps: 156, steps per second: 89, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.885 [0.000, 4.000], mean observation: 0.500 [0.410, 0.570], loss: 8.568022, mean_absolute_error: 41.520676, mean_q: 53.253380
[F[K 315367/500000: episode: 4124, duration: 1.222s, episode steps: 96, steps per second: 79, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.135 [0.000, 4.000], mean observation: 0.488 [0.390, 0.510], loss: 8.489665, mean_absolute_error: 41.380985, mean_q: 53.143101
[F[K 315498/500000: episode: 4125, duration: 1.651s, episode steps: 131, steps per second: 79, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.366 [0.000, 4.000], mean observation: 0.500 [0.370, 0.630], loss: 8.249273, mean_absolute_error: 42.097820, mean_q: 54.010170
[F[K 315541/500000: episode: 4126, duration: 0.526s, episode steps: 43, steps per second: 82, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.977 [0.000, 4.000], mean observation: 0.524 [0.470, 0.650], loss: 8.042404, mean_absolute_error: 41.482933, mean_q: 53.044537
[F[K 315603/500000: episode: 4127, duration: 0.770s, episode steps: 62, steps per second: 81, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.823 [0.000, 4.000], mean observation: 0.496 [0.390, 0.560], loss: 8.810821, mean_absolute_error: 41.349106, mean_q: 53.014755
[F[K 315706/500000: episode: 4128, duration: 1.127s, episode steps: 103, steps per second: 91, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.500 [0.460, 0.570], loss: 8.970955, mean_absolute_error: 41.314041, mean_q: 52.995983
[F[K 315800/500000: episode: 4129, duration: 1.152s, episode steps: 94, steps per second: 82, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.394 [0.000, 4.000], mean observation: 0.504 [0.470, 0.560], loss: 8.870025, mean_absolute_error: 42.141293, mean_q: 54.035889
[F[K 315900/500000: episode: 4130, duration: 1.262s, episode steps: 100, steps per second: 79, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 10.233901, mean_absolute_error: 41.878727, mean_q: 53.723282
[F[K 315998/500000: episode: 4131, duration: 1.207s, episode steps: 98, steps per second: 81, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.031 [0.000, 4.000], mean observation: 0.477 [0.340, 0.530], loss: 7.761758, mean_absolute_error: 41.473015, mean_q: 53.183201
[F[K 316065/500000: episode: 4132, duration: 0.908s, episode steps: 67, steps per second: 74, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.955 [0.000, 4.000], mean observation: 0.486 [0.350, 0.530], loss: 9.134794, mean_absolute_error: 40.883228, mean_q: 52.461090
[F[K 316114/500000: episode: 4133, duration: 0.619s, episode steps: 49, steps per second: 79, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.041 [0.000, 4.000], mean observation: 0.522 [0.470, 0.620], loss: 8.112708, mean_absolute_error: 41.793152, mean_q: 53.570366
[F[K 316181/500000: episode: 4134, duration: 0.823s, episode steps: 67, steps per second: 81, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.060 [0.000, 4.000], mean observation: 0.487 [0.370, 0.530], loss: 8.988334, mean_absolute_error: 41.513668, mean_q: 53.234837
[F[K 316243/500000: episode: 4135, duration: 0.747s, episode steps: 62, steps per second: 83, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.505 [0.450, 0.580], loss: 8.895911, mean_absolute_error: 41.666210, mean_q: 53.241810
[F[K 316340/500000: episode: 4136, duration: 1.320s, episode steps: 97, steps per second: 74, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.268 [0.000, 4.000], mean observation: 0.479 [0.400, 0.520], loss: 9.330798, mean_absolute_error: 41.912785, mean_q: 53.706089
[F[K 316402/500000: episode: 4137, duration: 0.909s, episode steps: 62, steps per second: 68, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.710 [0.000, 4.000], mean observation: 0.524 [0.470, 0.640], loss: 9.271444, mean_absolute_error: 41.130417, mean_q: 52.809219
[F[K 316471/500000: episode: 4138, duration: 0.883s, episode steps: 69, steps per second: 78, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.870 [0.000, 4.000], mean observation: 0.496 [0.390, 0.600], loss: 8.309649, mean_absolute_error: 41.110405, mean_q: 52.792683
[F[K 316528/500000: episode: 4139, duration: 0.725s, episode steps: 57, steps per second: 79, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.516 [0.470, 0.600], loss: 7.637728, mean_absolute_error: 41.393803, mean_q: 53.137177
[F[K 316588/500000: episode: 4140, duration: 0.825s, episode steps: 60, steps per second: 73, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.283 [0.000, 4.000], mean observation: 0.471 [0.350, 0.510], loss: 8.683254, mean_absolute_error: 42.110088, mean_q: 54.051914
[F[K 316657/500000: episode: 4141, duration: 0.803s, episode steps: 69, steps per second: 86, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.652 [0.000, 4.000], mean observation: 0.480 [0.350, 0.530], loss: 7.962868, mean_absolute_error: 41.104492, mean_q: 52.843830
[F[K 316732/500000: episode: 4142, duration: 0.838s, episode steps: 75, steps per second: 89, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.773 [0.000, 4.000], mean observation: 0.523 [0.490, 0.650], loss: 8.766516, mean_absolute_error: 41.171024, mean_q: 52.845749
[F[K 316808/500000: episode: 4143, duration: 0.975s, episode steps: 76, steps per second: 78, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.510 [0.470, 0.570], loss: 8.875887, mean_absolute_error: 41.617462, mean_q: 53.346527
[F[K 316900/500000: episode: 4144, duration: 1.174s, episode steps: 92, steps per second: 78, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.739 [0.000, 4.000], mean observation: 0.518 [0.470, 0.620], loss: 8.863326, mean_absolute_error: 40.795883, mean_q: 52.370144
[F[K 317059/500000: episode: 4145, duration: 1.967s, episode steps: 159, steps per second: 81, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.050 [0.000, 4.000], mean observation: 0.492 [0.430, 0.540], loss: 8.087664, mean_absolute_error: 41.549686, mean_q: 53.282440
[F[K 317182/500000: episode: 4146, duration: 1.634s, episode steps: 123, steps per second: 75, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.829 [0.000, 4.000], mean observation: 0.516 [0.480, 0.620], loss: 8.630936, mean_absolute_error: 41.281971, mean_q: 52.944401
[F[K 317234/500000: episode: 4147, duration: 0.738s, episode steps: 52, steps per second: 70, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.673 [0.000, 4.000], mean observation: 0.505 [0.450, 0.610], loss: 7.932222, mean_absolute_error: 41.324226, mean_q: 53.066921
[F[K 317365/500000: episode: 4148, duration: 1.765s, episode steps: 131, steps per second: 74, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.603 [0.000, 4.000], mean observation: 0.484 [0.400, 0.530], loss: 7.883237, mean_absolute_error: 41.468719, mean_q: 53.206985
[F[K 317424/500000: episode: 4149, duration: 0.830s, episode steps: 59, steps per second: 71, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.797 [0.000, 4.000], mean observation: 0.484 [0.360, 0.530], loss: 9.270207, mean_absolute_error: 41.179787, mean_q: 52.876747
[F[K 317498/500000: episode: 4150, duration: 1.037s, episode steps: 74, steps per second: 71, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.757 [0.000, 4.000], mean observation: 0.504 [0.430, 0.580], loss: 9.455203, mean_absolute_error: 41.859554, mean_q: 53.759304
[F[K 317552/500000: episode: 4151, duration: 0.768s, episode steps: 54, steps per second: 70, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.759 [0.000, 4.000], mean observation: 0.510 [0.470, 0.630], loss: 9.556383, mean_absolute_error: 41.227978, mean_q: 52.838337
[F[K 317629/500000: episode: 4152, duration: 1.131s, episode steps: 77, steps per second: 68, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.948 [0.000, 4.000], mean observation: 0.495 [0.430, 0.540], loss: 8.596786, mean_absolute_error: 41.446754, mean_q: 53.192326
[F[K 317745/500000: episode: 4153, duration: 1.473s, episode steps: 116, steps per second: 79, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.121 [0.000, 4.000], mean observation: 0.490 [0.410, 0.530], loss: 8.999837, mean_absolute_error: 41.571129, mean_q: 53.371750
[F[K 317808/500000: episode: 4154, duration: 0.847s, episode steps: 63, steps per second: 74, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.222 [0.000, 4.000], mean observation: 0.497 [0.450, 0.530], loss: 8.929104, mean_absolute_error: 41.307682, mean_q: 53.073612
[F[K 317952/500000: episode: 4155, duration: 1.961s, episode steps: 144, steps per second: 73, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.007 [0.000, 4.000], mean observation: 0.498 [0.450, 0.540], loss: 8.085043, mean_absolute_error: 41.400536, mean_q: 53.253853
[F[K 318008/500000: episode: 4156, duration: 0.783s, episode steps: 56, steps per second: 71, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.286 [0.000, 4.000], mean observation: 0.499 [0.420, 0.560], loss: 7.972572, mean_absolute_error: 41.024944, mean_q: 52.603020
[F[K 318085/500000: episode: 4157, duration: 1.030s, episode steps: 77, steps per second: 75, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.519 [0.000, 4.000], mean observation: 0.494 [0.340, 0.570], loss: 8.216520, mean_absolute_error: 40.955013, mean_q: 52.564095
[F[K 318192/500000: episode: 4158, duration: 1.485s, episode steps: 107, steps per second: 72, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.206 [0.000, 4.000], mean observation: 0.512 [0.470, 0.650], loss: 7.975603, mean_absolute_error: 41.659489, mean_q: 53.485546
[F[K 318270/500000: episode: 4159, duration: 1.181s, episode steps: 78, steps per second: 66, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.962 [0.000, 4.000], mean observation: 0.501 [0.430, 0.570], loss: 7.999671, mean_absolute_error: 42.007816, mean_q: 53.942753
[F[K 318347/500000: episode: 4160, duration: 1.125s, episode steps: 77, steps per second: 68, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.883 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 7.854041, mean_absolute_error: 41.588531, mean_q: 53.410358
[F[K 318421/500000: episode: 4161, duration: 1.050s, episode steps: 74, steps per second: 70, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.176 [0.000, 4.000], mean observation: 0.487 [0.410, 0.530], loss: 7.228841, mean_absolute_error: 41.085072, mean_q: 52.721062
[F[K 318490/500000: episode: 4162, duration: 0.897s, episode steps: 69, steps per second: 77, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.058 [0.000, 4.000], mean observation: 0.507 [0.490, 0.560], loss: 8.167648, mean_absolute_error: 42.147861, mean_q: 54.015800
[F[K 318552/500000: episode: 4163, duration: 0.906s, episode steps: 62, steps per second: 68, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.968 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 8.754589, mean_absolute_error: 41.016277, mean_q: 52.557289
[F[K 318607/500000: episode: 4164, duration: 0.784s, episode steps: 55, steps per second: 70, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.145 [0.000, 4.000], mean observation: 0.526 [0.470, 0.650], loss: 8.556177, mean_absolute_error: 41.852139, mean_q: 53.747353
[F[K 318708/500000: episode: 4165, duration: 1.440s, episode steps: 101, steps per second: 70, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.059 [0.000, 4.000], mean observation: 0.491 [0.400, 0.540], loss: 8.782934, mean_absolute_error: 41.637375, mean_q: 53.397400
[F[K 318800/500000: episode: 4166, duration: 1.209s, episode steps: 92, steps per second: 76, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.848 [0.000, 4.000], mean observation: 0.499 [0.410, 0.570], loss: 9.733611, mean_absolute_error: 41.349770, mean_q: 53.086742
[F[K 318863/500000: episode: 4167, duration: 0.758s, episode steps: 63, steps per second: 83, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.810 [0.000, 4.000], mean observation: 0.513 [0.470, 0.650], loss: 8.388146, mean_absolute_error: 41.477211, mean_q: 53.261440
[F[K 318907/500000: episode: 4168, duration: 0.544s, episode steps: 44, steps per second: 81, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.886 [0.000, 4.000], mean observation: 0.501 [0.410, 0.610], loss: 7.898552, mean_absolute_error: 40.275551, mean_q: 51.723034
[F[K 319007/500000: episode: 4169, duration: 1.347s, episode steps: 100, steps per second: 74, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.330 [0.000, 4.000], mean observation: 0.494 [0.370, 0.570], loss: 7.467642, mean_absolute_error: 41.521782, mean_q: 53.293034
[F[K 319056/500000: episode: 4170, duration: 0.680s, episode steps: 49, steps per second: 72, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.612 [0.000, 4.000], mean observation: 0.480 [0.370, 0.530], loss: 9.451055, mean_absolute_error: 41.289799, mean_q: 52.939423
[F[K 319103/500000: episode: 4171, duration: 0.683s, episode steps: 47, steps per second: 69, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.447 [0.000, 4.000], mean observation: 0.500 [0.410, 0.620], loss: 7.780991, mean_absolute_error: 40.661716, mean_q: 52.224697
[F[K 319161/500000: episode: 4172, duration: 0.769s, episode steps: 58, steps per second: 75, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.519 [0.470, 0.630], loss: 8.024319, mean_absolute_error: 42.474892, mean_q: 54.436005
[F[K 319206/500000: episode: 4173, duration: 0.687s, episode steps: 45, steps per second: 65, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.689 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 6.647815, mean_absolute_error: 42.008453, mean_q: 53.806408
[F[K 319287/500000: episode: 4174, duration: 1.191s, episode steps: 81, steps per second: 68, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.877 [0.000, 4.000], mean observation: 0.508 [0.460, 0.600], loss: 7.683264, mean_absolute_error: 41.259609, mean_q: 52.983566
[F[K 319372/500000: episode: 4175, duration: 1.297s, episode steps: 85, steps per second: 66, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 7.756914, mean_absolute_error: 41.496971, mean_q: 53.355286
[F[K 319451/500000: episode: 4176, duration: 1.113s, episode steps: 79, steps per second: 71, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.508 [0.470, 0.560], loss: 8.565599, mean_absolute_error: 41.223721, mean_q: 52.864185
[F[K 319522/500000: episode: 4177, duration: 1.047s, episode steps: 71, steps per second: 68, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.518 [0.470, 0.620], loss: 8.903416, mean_absolute_error: 41.255535, mean_q: 52.949272
[F[K 319578/500000: episode: 4178, duration: 0.842s, episode steps: 56, steps per second: 67, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.018 [0.000, 4.000], mean observation: 0.496 [0.360, 0.600], loss: 8.236091, mean_absolute_error: 41.219135, mean_q: 52.874111
[F[K 319641/500000: episode: 4179, duration: 1.038s, episode steps: 63, steps per second: 61, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.841 [0.000, 4.000], mean observation: 0.489 [0.400, 0.520], loss: 7.732567, mean_absolute_error: 40.829998, mean_q: 52.441982
[F[K 319711/500000: episode: 4180, duration: 0.989s, episode steps: 70, steps per second: 71, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.496 [0.420, 0.560], loss: 8.129643, mean_absolute_error: 41.524151, mean_q: 53.386974
[F[K 319786/500000: episode: 4181, duration: 1.116s, episode steps: 75, steps per second: 67, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.160 [0.000, 4.000], mean observation: 0.483 [0.360, 0.520], loss: 9.533230, mean_absolute_error: 41.295902, mean_q: 52.996593
[F[K 319851/500000: episode: 4182, duration: 0.982s, episode steps: 65, steps per second: 66, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.277 [0.000, 4.000], mean observation: 0.486 [0.380, 0.510], loss: 7.449745, mean_absolute_error: 40.606853, mean_q: 52.206257
[F[K 320007/500000: episode: 4183, duration: 2.207s, episode steps: 156, steps per second: 71, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.705 [0.000, 4.000], mean observation: 0.507 [0.480, 0.560], loss: 8.102349, mean_absolute_error: 40.845821, mean_q: 52.549221
[F[K 320074/500000: episode: 4184, duration: 0.820s, episode steps: 67, steps per second: 82, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.502 [0.420, 0.640], loss: 9.293089, mean_absolute_error: 41.399910, mean_q: 53.102463
[F[K 320145/500000: episode: 4185, duration: 0.948s, episode steps: 71, steps per second: 75, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.502 [0.400, 0.600], loss: 7.906523, mean_absolute_error: 41.840988, mean_q: 53.653393
[F[K 320224/500000: episode: 4186, duration: 0.969s, episode steps: 79, steps per second: 82, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 0.506 [0.470, 0.580], loss: 7.314439, mean_absolute_error: 41.551949, mean_q: 53.451759
[F[K 320285/500000: episode: 4187, duration: 0.836s, episode steps: 61, steps per second: 73, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.529 [0.470, 0.650], loss: 9.851955, mean_absolute_error: 41.597267, mean_q: 53.320389
[F[K 320350/500000: episode: 4188, duration: 0.767s, episode steps: 65, steps per second: 85, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.877 [0.000, 4.000], mean observation: 0.511 [0.480, 0.570], loss: 8.974280, mean_absolute_error: 40.949463, mean_q: 52.649197
[F[K 320491/500000: episode: 4189, duration: 1.999s, episode steps: 141, steps per second: 71, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.596 [0.000, 4.000], mean observation: 0.490 [0.390, 0.540], loss: 7.857881, mean_absolute_error: 41.329102, mean_q: 53.146076
[F[K 320556/500000: episode: 4190, duration: 0.929s, episode steps: 65, steps per second: 70, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.046 [0.000, 4.000], mean observation: 0.514 [0.490, 0.620], loss: 7.981234, mean_absolute_error: 41.607475, mean_q: 53.623291
[F[K 320628/500000: episode: 4191, duration: 1.037s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.491 [0.410, 0.530], loss: 6.739669, mean_absolute_error: 41.678253, mean_q: 53.622173
[F[K 320677/500000: episode: 4192, duration: 0.678s, episode steps: 49, steps per second: 72, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 4.000], mean observation: 0.506 [0.460, 0.560], loss: 8.355133, mean_absolute_error: 41.655205, mean_q: 53.554180
[F[K 320764/500000: episode: 4193, duration: 1.064s, episode steps: 87, steps per second: 82, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.502 [0.470, 0.580], loss: 9.003807, mean_absolute_error: 41.756184, mean_q: 53.559723
[F[K 320880/500000: episode: 4194, duration: 1.548s, episode steps: 116, steps per second: 75, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.905 [0.000, 4.000], mean observation: 0.500 [0.390, 0.570], loss: 7.894645, mean_absolute_error: 40.696251, mean_q: 52.250729
[F[K 321080/500000: episode: 4195, duration: 2.889s, episode steps: 200, steps per second: 69, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.935 [0.000, 4.000], mean observation: 0.489 [0.400, 0.540], loss: 8.501637, mean_absolute_error: 41.326080, mean_q: 53.078716
[F[K 321160/500000: episode: 4196, duration: 1.071s, episode steps: 80, steps per second: 75, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.212 [0.000, 4.000], mean observation: 0.516 [0.480, 0.600], loss: 7.544468, mean_absolute_error: 41.272266, mean_q: 53.080505
[F[K 321207/500000: episode: 4197, duration: 0.744s, episode steps: 47, steps per second: 63, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.979 [0.000, 4.000], mean observation: 0.501 [0.380, 0.640], loss: 9.214210, mean_absolute_error: 42.912083, mean_q: 54.985779
[F[K 321273/500000: episode: 4198, duration: 0.992s, episode steps: 66, steps per second: 67, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.503 [0.400, 0.590], loss: 8.312835, mean_absolute_error: 41.782139, mean_q: 53.600300
[F[K 321368/500000: episode: 4199, duration: 1.245s, episode steps: 95, steps per second: 76, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.568 [0.000, 4.000], mean observation: 0.486 [0.410, 0.530], loss: 7.639291, mean_absolute_error: 41.787815, mean_q: 53.554676
[F[K 321444/500000: episode: 4200, duration: 1.122s, episode steps: 76, steps per second: 68, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.276 [0.000, 4.000], mean observation: 0.512 [0.480, 0.620], loss: 8.407483, mean_absolute_error: 41.549892, mean_q: 53.358501
[F[K 321495/500000: episode: 4201, duration: 0.623s, episode steps: 51, steps per second: 82, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.627 [0.000, 4.000], mean observation: 0.503 [0.450, 0.610], loss: 8.028783, mean_absolute_error: 41.395203, mean_q: 53.161755
[F[K 321649/500000: episode: 4202, duration: 2.308s, episode steps: 154, steps per second: 67, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.506 [0.000, 4.000], mean observation: 0.491 [0.370, 0.550], loss: 8.641968, mean_absolute_error: 42.054382, mean_q: 53.992912
[F[K 321715/500000: episode: 4203, duration: 1.044s, episode steps: 66, steps per second: 63, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.985 [0.000, 4.000], mean observation: 0.510 [0.490, 0.580], loss: 10.722350, mean_absolute_error: 41.529003, mean_q: 53.221684
[F[K 321778/500000: episode: 4204, duration: 0.893s, episode steps: 63, steps per second: 71, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.063 [0.000, 4.000], mean observation: 0.492 [0.370, 0.570], loss: 8.844951, mean_absolute_error: 41.678059, mean_q: 53.547638
[F[K 321829/500000: episode: 4205, duration: 0.794s, episode steps: 51, steps per second: 64, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [0.000, 4.000], mean observation: 0.515 [0.470, 0.640], loss: 9.602544, mean_absolute_error: 41.498066, mean_q: 53.203392
[F[K 321890/500000: episode: 4206, duration: 0.940s, episode steps: 61, steps per second: 65, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 8.675113, mean_absolute_error: 41.192390, mean_q: 52.901516
[F[K 321951/500000: episode: 4207, duration: 0.996s, episode steps: 61, steps per second: 61, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.902 [0.000, 4.000], mean observation: 0.497 [0.440, 0.530], loss: 8.475327, mean_absolute_error: 41.714386, mean_q: 53.535397
[F[K 322085/500000: episode: 4208, duration: 1.924s, episode steps: 134, steps per second: 70, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.134 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 8.640969, mean_absolute_error: 41.906891, mean_q: 53.706757
[F[K 322150/500000: episode: 4209, duration: 1.051s, episode steps: 65, steps per second: 62, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.954 [0.000, 4.000], mean observation: 0.504 [0.420, 0.590], loss: 8.437097, mean_absolute_error: 41.402054, mean_q: 53.163219
[F[K 322244/500000: episode: 4210, duration: 1.460s, episode steps: 94, steps per second: 64, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.606 [0.000, 4.000], mean observation: 0.490 [0.390, 0.520], loss: 8.864273, mean_absolute_error: 41.577759, mean_q: 53.331467
[F[K 322309/500000: episode: 4211, duration: 0.979s, episode steps: 65, steps per second: 66, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.509 [0.460, 0.600], loss: 8.202423, mean_absolute_error: 41.403141, mean_q: 53.065338
[F[K 322374/500000: episode: 4212, duration: 0.776s, episode steps: 65, steps per second: 84, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.031 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 7.115754, mean_absolute_error: 42.020325, mean_q: 53.751099
[F[K 322449/500000: episode: 4213, duration: 1.279s, episode steps: 75, steps per second: 59, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 0.501 [0.460, 0.530], loss: 8.890044, mean_absolute_error: 41.426968, mean_q: 53.234783
[F[K 322533/500000: episode: 4214, duration: 1.350s, episode steps: 84, steps per second: 62, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.417 [0.000, 4.000], mean observation: 0.506 [0.420, 0.630], loss: 8.794977, mean_absolute_error: 41.634022, mean_q: 53.348198
[F[K 322606/500000: episode: 4215, duration: 1.025s, episode steps: 73, steps per second: 71, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.849 [0.000, 4.000], mean observation: 0.503 [0.390, 0.650], loss: 7.434681, mean_absolute_error: 42.056236, mean_q: 54.058239
[F[K 322733/500000: episode: 4216, duration: 1.964s, episode steps: 127, steps per second: 65, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.480 [0.000, 4.000], mean observation: 0.509 [0.470, 0.580], loss: 8.505198, mean_absolute_error: 41.305485, mean_q: 53.085663
[F[K 322802/500000: episode: 4217, duration: 1.166s, episode steps: 69, steps per second: 59, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.159 [0.000, 4.000], mean observation: 0.503 [0.470, 0.580], loss: 8.098634, mean_absolute_error: 41.396988, mean_q: 53.128883
[F[K 322909/500000: episode: 4218, duration: 1.665s, episode steps: 107, steps per second: 64, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.850 [0.000, 4.000], mean observation: 0.504 [0.400, 0.630], loss: 8.838137, mean_absolute_error: 42.062584, mean_q: 53.938454
[F[K 322976/500000: episode: 4219, duration: 1.042s, episode steps: 67, steps per second: 64, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.955 [0.000, 4.000], mean observation: 0.510 [0.490, 0.550], loss: 8.888632, mean_absolute_error: 41.089046, mean_q: 52.729290
[F[K 323026/500000: episode: 4220, duration: 0.848s, episode steps: 50, steps per second: 59, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.880 [0.000, 4.000], mean observation: 0.512 [0.470, 0.640], loss: 7.663248, mean_absolute_error: 41.931328, mean_q: 53.784603
[F[K 323101/500000: episode: 4221, duration: 1.171s, episode steps: 75, steps per second: 64, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.640 [0.000, 4.000], mean observation: 0.505 [0.440, 0.600], loss: 8.846482, mean_absolute_error: 41.696854, mean_q: 53.441975
[F[K 323174/500000: episode: 4222, duration: 1.044s, episode steps: 73, steps per second: 70, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.904 [0.000, 4.000], mean observation: 0.503 [0.420, 0.630], loss: 8.910773, mean_absolute_error: 41.416290, mean_q: 53.045708
[F[K 323232/500000: episode: 4223, duration: 0.887s, episode steps: 58, steps per second: 65, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.741 [0.000, 4.000], mean observation: 0.495 [0.360, 0.590], loss: 8.879953, mean_absolute_error: 41.303185, mean_q: 52.983727
[F[K 323284/500000: episode: 4224, duration: 0.764s, episode steps: 52, steps per second: 68, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.115 [0.000, 4.000], mean observation: 0.507 [0.470, 0.550], loss: 7.777316, mean_absolute_error: 41.788292, mean_q: 53.602543
[F[K 323357/500000: episode: 4225, duration: 1.170s, episode steps: 73, steps per second: 62, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.438 [0.000, 4.000], mean observation: 0.489 [0.390, 0.530], loss: 8.505639, mean_absolute_error: 41.757893, mean_q: 53.747028
[F[K 323503/500000: episode: 4226, duration: 2.030s, episode steps: 146, steps per second: 72, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.233 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 9.185885, mean_absolute_error: 41.480919, mean_q: 53.123627
[F[K 323676/500000: episode: 4227, duration: 2.704s, episode steps: 173, steps per second: 64, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.052 [0.000, 4.000], mean observation: 0.487 [0.410, 0.540], loss: 8.894842, mean_absolute_error: 41.586193, mean_q: 53.319088
[F[K 323746/500000: episode: 4228, duration: 1.027s, episode steps: 70, steps per second: 68, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.493 [0.380, 0.570], loss: 8.788793, mean_absolute_error: 41.498653, mean_q: 53.154343
[F[K 323824/500000: episode: 4229, duration: 1.184s, episode steps: 78, steps per second: 66, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.949 [0.000, 4.000], mean observation: 0.507 [0.460, 0.590], loss: 7.590082, mean_absolute_error: 41.504517, mean_q: 53.324150
[F[K 324007/500000: episode: 4230, duration: 2.574s, episode steps: 183, steps per second: 71, episode reward: 183.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.077 [0.000, 4.000], mean observation: 0.489 [0.390, 0.550], loss: 8.179118, mean_absolute_error: 41.577385, mean_q: 53.380135
[F[K 324083/500000: episode: 4231, duration: 1.113s, episode steps: 76, steps per second: 68, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.211 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 8.889860, mean_absolute_error: 41.726978, mean_q: 53.524502
[F[K 324147/500000: episode: 4232, duration: 0.964s, episode steps: 64, steps per second: 66, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.906 [0.000, 4.000], mean observation: 0.503 [0.410, 0.620], loss: 8.813816, mean_absolute_error: 41.118340, mean_q: 52.735832
[F[K 324209/500000: episode: 4233, duration: 0.759s, episode steps: 62, steps per second: 82, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.048 [0.000, 4.000], mean observation: 0.517 [0.490, 0.600], loss: 9.140034, mean_absolute_error: 41.114758, mean_q: 52.849087
[F[K 324277/500000: episode: 4234, duration: 1.068s, episode steps: 68, steps per second: 64, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.059 [0.000, 4.000], mean observation: 0.514 [0.480, 0.600], loss: 9.269791, mean_absolute_error: 42.265259, mean_q: 54.091553
[F[K 324389/500000: episode: 4235, duration: 1.378s, episode steps: 112, steps per second: 81, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.490 [0.410, 0.530], loss: 8.049555, mean_absolute_error: 41.861004, mean_q: 53.753731
[F[K 324466/500000: episode: 4236, duration: 1.155s, episode steps: 77, steps per second: 67, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.338 [0.000, 4.000], mean observation: 0.486 [0.350, 0.530], loss: 8.084070, mean_absolute_error: 41.734673, mean_q: 53.627289
[F[K 324618/500000: episode: 4237, duration: 1.538s, episode steps: 152, steps per second: 99, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 0.499 [0.350, 0.650], loss: 8.472242, mean_absolute_error: 41.707027, mean_q: 53.530739
[F[K 324665/500000: episode: 4238, duration: 0.752s, episode steps: 47, steps per second: 62, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.521 [0.470, 0.630], loss: 7.942413, mean_absolute_error: 42.336231, mean_q: 54.400352
[F[K 324759/500000: episode: 4239, duration: 1.403s, episode steps: 94, steps per second: 67, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.138 [0.000, 4.000], mean observation: 0.499 [0.390, 0.580], loss: 8.347800, mean_absolute_error: 41.765644, mean_q: 53.519211
[F[K 324813/500000: episode: 4240, duration: 0.802s, episode steps: 54, steps per second: 67, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 8.129710, mean_absolute_error: 40.768974, mean_q: 52.427109
[F[K 324917/500000: episode: 4241, duration: 1.228s, episode steps: 104, steps per second: 85, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.394 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 8.190579, mean_absolute_error: 41.868774, mean_q: 53.732937
[F[K 324988/500000: episode: 4242, duration: 0.801s, episode steps: 71, steps per second: 89, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.535 [0.000, 4.000], mean observation: 0.507 [0.440, 0.650], loss: 9.332482, mean_absolute_error: 41.301216, mean_q: 53.007835
[F[K 325122/500000: episode: 4243, duration: 1.729s, episode steps: 134, steps per second: 78, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.433 [0.000, 4.000], mean observation: 0.479 [0.360, 0.530], loss: 9.070647, mean_absolute_error: 41.557739, mean_q: 53.309711
[F[K 325188/500000: episode: 4244, duration: 0.865s, episode steps: 66, steps per second: 76, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 0.491 [0.360, 0.550], loss: 7.789797, mean_absolute_error: 41.570324, mean_q: 53.381104
[F[K 325256/500000: episode: 4245, duration: 0.848s, episode steps: 68, steps per second: 80, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.338 [0.000, 4.000], mean observation: 0.509 [0.470, 0.580], loss: 7.970349, mean_absolute_error: 41.495720, mean_q: 53.327190
[F[K 325322/500000: episode: 4246, duration: 0.805s, episode steps: 66, steps per second: 82, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.803 [0.000, 3.000], mean observation: 0.493 [0.380, 0.530], loss: 8.466546, mean_absolute_error: 41.799263, mean_q: 53.754436
[F[K 325370/500000: episode: 4247, duration: 0.594s, episode steps: 48, steps per second: 81, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.354 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 8.627065, mean_absolute_error: 41.402344, mean_q: 53.212402
[F[K 325416/500000: episode: 4248, duration: 0.595s, episode steps: 46, steps per second: 77, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.696 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 8.727112, mean_absolute_error: 41.659107, mean_q: 53.541378
[F[K 325477/500000: episode: 4249, duration: 0.746s, episode steps: 61, steps per second: 82, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.496 [0.450, 0.530], loss: 9.029415, mean_absolute_error: 41.493423, mean_q: 53.243069
[F[K 325569/500000: episode: 4250, duration: 1.172s, episode steps: 92, steps per second: 79, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.141 [0.000, 4.000], mean observation: 0.512 [0.470, 0.630], loss: 8.861585, mean_absolute_error: 41.093109, mean_q: 52.766586
[F[K 325647/500000: episode: 4251, duration: 1.043s, episode steps: 78, steps per second: 75, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.505 [0.460, 0.570], loss: 9.223235, mean_absolute_error: 41.215942, mean_q: 52.861687
[F[K 325691/500000: episode: 4252, duration: 0.596s, episode steps: 44, steps per second: 74, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.159 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 9.099622, mean_absolute_error: 41.059155, mean_q: 52.609791
[F[K 325747/500000: episode: 4253, duration: 0.726s, episode steps: 56, steps per second: 77, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.446 [0.000, 4.000], mean observation: 0.527 [0.470, 0.650], loss: 7.820957, mean_absolute_error: 41.351494, mean_q: 53.078072
[F[K 325830/500000: episode: 4254, duration: 1.087s, episode steps: 83, steps per second: 76, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.964 [0.000, 4.000], mean observation: 0.516 [0.480, 0.610], loss: 8.979988, mean_absolute_error: 41.548180, mean_q: 53.353374
[F[K 325916/500000: episode: 4255, duration: 1.074s, episode steps: 86, steps per second: 80, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.081 [0.000, 4.000], mean observation: 0.490 [0.390, 0.530], loss: 8.810682, mean_absolute_error: 41.313808, mean_q: 52.921791
[F[K 325978/500000: episode: 4256, duration: 0.801s, episode steps: 62, steps per second: 77, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.145 [0.000, 4.000], mean observation: 0.496 [0.460, 0.520], loss: 8.548407, mean_absolute_error: 41.377865, mean_q: 53.108124
[F[K 326079/500000: episode: 4257, duration: 1.149s, episode steps: 101, steps per second: 88, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.317 [0.000, 4.000], mean observation: 0.505 [0.380, 0.640], loss: 8.387583, mean_absolute_error: 40.921783, mean_q: 52.593731
[F[K 326138/500000: episode: 4258, duration: 0.722s, episode steps: 59, steps per second: 82, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.695 [0.000, 4.000], mean observation: 0.519 [0.470, 0.630], loss: 8.725527, mean_absolute_error: 41.399101, mean_q: 53.267628
[F[K 326185/500000: episode: 4259, duration: 0.559s, episode steps: 47, steps per second: 84, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.830 [0.000, 4.000], mean observation: 0.511 [0.450, 0.620], loss: 7.093868, mean_absolute_error: 41.902981, mean_q: 53.916988
[F[K 326236/500000: episode: 4260, duration: 0.668s, episode steps: 51, steps per second: 76, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.507 [0.430, 0.650], loss: 7.853196, mean_absolute_error: 41.208004, mean_q: 52.865501
[F[K 326296/500000: episode: 4261, duration: 0.818s, episode steps: 60, steps per second: 73, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 8.261908, mean_absolute_error: 41.645573, mean_q: 53.302925
[F[K 326356/500000: episode: 4262, duration: 0.708s, episode steps: 60, steps per second: 85, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.481 [0.360, 0.520], loss: 8.229562, mean_absolute_error: 41.559940, mean_q: 53.334419
[F[K 326505/500000: episode: 4263, duration: 1.904s, episode steps: 149, steps per second: 78, episode reward: 149.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.564 [0.000, 4.000], mean observation: 0.498 [0.470, 0.520], loss: 7.900755, mean_absolute_error: 40.807922, mean_q: 52.462200
[F[K 326560/500000: episode: 4264, duration: 0.662s, episode steps: 55, steps per second: 83, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.517 [0.470, 0.610], loss: 7.386093, mean_absolute_error: 41.597874, mean_q: 53.297146
[F[K 326631/500000: episode: 4265, duration: 0.983s, episode steps: 71, steps per second: 72, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.494 [0.420, 0.530], loss: 7.637399, mean_absolute_error: 41.474728, mean_q: 53.351540
[F[K 326695/500000: episode: 4266, duration: 0.828s, episode steps: 64, steps per second: 77, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.470 [0.350, 0.510], loss: 8.261410, mean_absolute_error: 41.869175, mean_q: 53.641296
[F[K 326741/500000: episode: 4267, duration: 0.619s, episode steps: 46, steps per second: 74, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.065 [0.000, 4.000], mean observation: 0.513 [0.470, 0.600], loss: 8.031360, mean_absolute_error: 41.591026, mean_q: 53.440865
[F[K 326810/500000: episode: 4268, duration: 0.843s, episode steps: 69, steps per second: 82, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 7.918637, mean_absolute_error: 41.518570, mean_q: 53.340729
[F[K 326889/500000: episode: 4269, duration: 1.006s, episode steps: 79, steps per second: 79, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.684 [0.000, 4.000], mean observation: 0.473 [0.350, 0.530], loss: 10.012700, mean_absolute_error: 41.235348, mean_q: 52.784779
[F[K 326953/500000: episode: 4270, duration: 0.726s, episode steps: 64, steps per second: 88, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.969 [0.000, 4.000], mean observation: 0.487 [0.410, 0.530], loss: 8.916757, mean_absolute_error: 41.361252, mean_q: 52.997894
[F[K 327132/500000: episode: 4271, duration: 1.997s, episode steps: 179, steps per second: 90, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.296 [0.000, 4.000], mean observation: 0.483 [0.360, 0.530], loss: 8.657966, mean_absolute_error: 40.915958, mean_q: 52.486935
[F[K 327179/500000: episode: 4272, duration: 0.639s, episode steps: 47, steps per second: 74, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.494 [0.380, 0.550], loss: 8.310314, mean_absolute_error: 40.360203, mean_q: 51.779339
[F[K 327238/500000: episode: 4273, duration: 0.794s, episode steps: 59, steps per second: 74, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.510 [0.490, 0.560], loss: 7.529887, mean_absolute_error: 40.757511, mean_q: 52.387554
[F[K 327300/500000: episode: 4274, duration: 0.876s, episode steps: 62, steps per second: 71, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.097 [0.000, 4.000], mean observation: 0.503 [0.490, 0.530], loss: 8.160127, mean_absolute_error: 41.548691, mean_q: 53.438763
[F[K 327359/500000: episode: 4275, duration: 0.731s, episode steps: 59, steps per second: 81, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.505 [0.470, 0.570], loss: 7.640790, mean_absolute_error: 41.522644, mean_q: 53.249542
[F[K 327406/500000: episode: 4276, duration: 0.578s, episode steps: 47, steps per second: 81, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.021 [0.000, 4.000], mean observation: 0.516 [0.470, 0.620], loss: 7.662591, mean_absolute_error: 40.632999, mean_q: 52.243656
[F[K 327468/500000: episode: 4277, duration: 0.627s, episode steps: 62, steps per second: 99, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.113 [0.000, 4.000], mean observation: 0.497 [0.460, 0.540], loss: 8.838155, mean_absolute_error: 41.026146, mean_q: 52.599735
[F[K 327624/500000: episode: 4278, duration: 1.657s, episode steps: 156, steps per second: 94, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.026 [0.000, 4.000], mean observation: 0.489 [0.350, 0.550], loss: 8.324919, mean_absolute_error: 40.924934, mean_q: 52.578712
[F[K 327689/500000: episode: 4279, duration: 0.632s, episode steps: 65, steps per second: 103, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.692 [0.000, 4.000], mean observation: 0.492 [0.430, 0.530], loss: 9.154527, mean_absolute_error: 42.269558, mean_q: 54.247944
[F[K 327762/500000: episode: 4280, duration: 0.984s, episode steps: 73, steps per second: 74, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.096 [0.000, 4.000], mean observation: 0.490 [0.430, 0.510], loss: 7.482435, mean_absolute_error: 40.753338, mean_q: 52.381622
[F[K 327822/500000: episode: 4281, duration: 0.813s, episode steps: 60, steps per second: 74, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.033 [0.000, 4.000], mean observation: 0.492 [0.400, 0.530], loss: 9.442922, mean_absolute_error: 41.357555, mean_q: 52.969982
[F[K 327981/500000: episode: 4282, duration: 1.895s, episode steps: 159, steps per second: 84, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.428 [0.000, 4.000], mean observation: 0.504 [0.470, 0.550], loss: 8.215605, mean_absolute_error: 41.574821, mean_q: 53.373573
[F[K 328073/500000: episode: 4283, duration: 1.037s, episode steps: 92, steps per second: 89, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.054 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 7.474109, mean_absolute_error: 41.730145, mean_q: 53.583523
[F[K 328137/500000: episode: 4284, duration: 0.757s, episode steps: 64, steps per second: 85, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.479 [0.370, 0.520], loss: 7.545467, mean_absolute_error: 42.184971, mean_q: 54.097122
[F[K 328201/500000: episode: 4285, duration: 0.885s, episode steps: 64, steps per second: 72, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.812 [0.000, 4.000], mean observation: 0.510 [0.480, 0.580], loss: 8.713606, mean_absolute_error: 40.674171, mean_q: 52.233685
[F[K 328270/500000: episode: 4286, duration: 0.985s, episode steps: 69, steps per second: 70, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.609 [0.000, 4.000], mean observation: 0.481 [0.390, 0.520], loss: 8.814374, mean_absolute_error: 40.528057, mean_q: 51.962601
[F[K 328352/500000: episode: 4287, duration: 1.094s, episode steps: 82, steps per second: 75, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.963 [0.000, 4.000], mean observation: 0.507 [0.400, 0.660], loss: 8.104012, mean_absolute_error: 40.913994, mean_q: 52.404575
[F[K 328408/500000: episode: 4288, duration: 0.739s, episode steps: 56, steps per second: 76, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.357 [0.000, 4.000], mean observation: 0.511 [0.480, 0.560], loss: 9.297035, mean_absolute_error: 41.522957, mean_q: 53.239410
[F[K 328523/500000: episode: 4289, duration: 1.535s, episode steps: 115, steps per second: 75, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.148 [0.000, 4.000], mean observation: 0.503 [0.380, 0.630], loss: 8.819041, mean_absolute_error: 41.452686, mean_q: 53.204079
[F[K 328588/500000: episode: 4290, duration: 0.837s, episode steps: 65, steps per second: 78, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.262 [0.000, 4.000], mean observation: 0.502 [0.400, 0.640], loss: 9.892896, mean_absolute_error: 41.239044, mean_q: 52.877666
[F[K 328650/500000: episode: 4291, duration: 0.823s, episode steps: 62, steps per second: 75, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.952 [0.000, 4.000], mean observation: 0.525 [0.470, 0.650], loss: 8.257177, mean_absolute_error: 41.493401, mean_q: 53.267113
[F[K 328712/500000: episode: 4292, duration: 0.768s, episode steps: 62, steps per second: 81, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 4.000], mean observation: 0.520 [0.470, 0.640], loss: 7.864151, mean_absolute_error: 41.481766, mean_q: 53.255932
[F[K 328778/500000: episode: 4293, duration: 0.937s, episode steps: 66, steps per second: 70, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.955 [0.000, 4.000], mean observation: 0.501 [0.480, 0.530], loss: 7.189240, mean_absolute_error: 41.014961, mean_q: 52.753204
[F[K 328847/500000: episode: 4294, duration: 0.985s, episode steps: 69, steps per second: 70, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.029 [0.000, 4.000], mean observation: 0.510 [0.460, 0.620], loss: 7.150619, mean_absolute_error: 41.918118, mean_q: 53.806095
[F[K 328929/500000: episode: 4295, duration: 1.151s, episode steps: 82, steps per second: 71, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.671 [0.000, 4.000], mean observation: 0.498 [0.420, 0.530], loss: 8.176129, mean_absolute_error: 41.365379, mean_q: 53.148735
[F[K 328986/500000: episode: 4296, duration: 0.731s, episode steps: 57, steps per second: 78, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.035 [0.000, 4.000], mean observation: 0.487 [0.380, 0.530], loss: 7.281070, mean_absolute_error: 41.270958, mean_q: 53.122528
[F[K 329052/500000: episode: 4297, duration: 0.856s, episode steps: 66, steps per second: 77, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 0.473 [0.350, 0.530], loss: 9.403036, mean_absolute_error: 41.142365, mean_q: 52.839371
[F[K 329167/500000: episode: 4298, duration: 1.531s, episode steps: 115, steps per second: 75, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.217 [0.000, 4.000], mean observation: 0.494 [0.430, 0.550], loss: 8.147434, mean_absolute_error: 41.455452, mean_q: 53.196842
[F[K 329215/500000: episode: 4299, duration: 0.690s, episode steps: 48, steps per second: 70, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 10.687630, mean_absolute_error: 40.750141, mean_q: 52.262829
[F[K 329355/500000: episode: 4300, duration: 1.818s, episode steps: 140, steps per second: 77, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.907 [0.000, 4.000], mean observation: 0.509 [0.470, 0.550], loss: 8.062562, mean_absolute_error: 41.764423, mean_q: 53.587479
[F[K 329415/500000: episode: 4301, duration: 0.796s, episode steps: 60, steps per second: 75, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.817 [0.000, 4.000], mean observation: 0.507 [0.490, 0.530], loss: 6.975899, mean_absolute_error: 41.673103, mean_q: 53.320251
[F[K 329486/500000: episode: 4302, duration: 1.015s, episode steps: 71, steps per second: 70, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.113 [0.000, 4.000], mean observation: 0.480 [0.390, 0.530], loss: 7.596320, mean_absolute_error: 41.251030, mean_q: 52.847004
[F[K 329556/500000: episode: 4303, duration: 0.954s, episode steps: 70, steps per second: 73, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.157 [0.000, 4.000], mean observation: 0.488 [0.410, 0.530], loss: 8.587280, mean_absolute_error: 41.410542, mean_q: 53.096272
[F[K 329612/500000: episode: 4304, duration: 0.868s, episode steps: 56, steps per second: 65, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.643 [0.000, 4.000], mean observation: 0.494 [0.370, 0.600], loss: 7.545067, mean_absolute_error: 41.604523, mean_q: 53.426830
[F[K 329683/500000: episode: 4305, duration: 0.964s, episode steps: 71, steps per second: 74, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.502 [0.390, 0.660], loss: 7.278824, mean_absolute_error: 41.517735, mean_q: 53.248539
[F[K 329809/500000: episode: 4306, duration: 1.672s, episode steps: 126, steps per second: 75, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.008 [0.000, 4.000], mean observation: 0.496 [0.440, 0.540], loss: 7.484948, mean_absolute_error: 42.010189, mean_q: 53.929909
[F[K 329954/500000: episode: 4307, duration: 2.287s, episode steps: 145, steps per second: 63, episode reward: 145.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.248 [0.000, 4.000], mean observation: 0.491 [0.420, 0.520], loss: 9.401758, mean_absolute_error: 41.584045, mean_q: 53.323620
[F[K 329996/500000: episode: 4308, duration: 0.659s, episode steps: 42, steps per second: 64, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.286 [0.000, 4.000], mean observation: 0.523 [0.470, 0.630], loss: 9.666143, mean_absolute_error: 41.326790, mean_q: 53.012238
[F[K 330040/500000: episode: 4309, duration: 0.730s, episode steps: 44, steps per second: 60, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.136 [0.000, 4.000], mean observation: 0.523 [0.470, 0.650], loss: 8.092312, mean_absolute_error: 41.775589, mean_q: 53.700954
[F[K 330089/500000: episode: 4310, duration: 0.835s, episode steps: 49, steps per second: 59, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.512 [0.470, 0.580], loss: 9.947901, mean_absolute_error: 41.923264, mean_q: 53.765671
[F[K 330189/500000: episode: 4311, duration: 1.482s, episode steps: 100, steps per second: 67, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.620 [0.000, 4.000], mean observation: 0.501 [0.460, 0.570], loss: 8.347980, mean_absolute_error: 40.964561, mean_q: 52.617332
[F[K 330277/500000: episode: 4312, duration: 1.176s, episode steps: 88, steps per second: 75, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.205 [0.000, 4.000], mean observation: 0.489 [0.450, 0.520], loss: 7.866562, mean_absolute_error: 41.152084, mean_q: 52.816196
[F[K 330354/500000: episode: 4313, duration: 1.126s, episode steps: 77, steps per second: 68, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.961 [0.000, 4.000], mean observation: 0.521 [0.490, 0.650], loss: 9.256530, mean_absolute_error: 41.184361, mean_q: 52.935764
[F[K 330440/500000: episode: 4314, duration: 1.270s, episode steps: 86, steps per second: 68, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.128 [0.000, 4.000], mean observation: 0.491 [0.380, 0.530], loss: 8.477508, mean_absolute_error: 41.320438, mean_q: 53.113464
[F[K 330497/500000: episode: 4315, duration: 0.886s, episode steps: 57, steps per second: 64, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.193 [0.000, 4.000], mean observation: 0.494 [0.380, 0.550], loss: 7.346114, mean_absolute_error: 41.893402, mean_q: 53.917999
[F[K 330547/500000: episode: 4316, duration: 0.798s, episode steps: 50, steps per second: 63, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.880 [0.000, 4.000], mean observation: 0.495 [0.360, 0.590], loss: 8.275715, mean_absolute_error: 41.184071, mean_q: 52.963139
[F[K 330617/500000: episode: 4317, duration: 0.996s, episode steps: 70, steps per second: 70, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.129 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 8.573479, mean_absolute_error: 41.233562, mean_q: 52.943600
[F[K 330694/500000: episode: 4318, duration: 1.082s, episode steps: 77, steps per second: 71, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.078 [0.000, 4.000], mean observation: 0.500 [0.460, 0.540], loss: 8.096190, mean_absolute_error: 41.857578, mean_q: 53.667866
[F[K 330753/500000: episode: 4319, duration: 0.822s, episode steps: 59, steps per second: 72, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.495 [0.430, 0.530], loss: 8.882524, mean_absolute_error: 41.163673, mean_q: 52.802799
[F[K 330826/500000: episode: 4320, duration: 1.109s, episode steps: 73, steps per second: 66, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.753 [0.000, 4.000], mean observation: 0.475 [0.380, 0.510], loss: 9.384013, mean_absolute_error: 40.859093, mean_q: 52.433228
[F[K 330892/500000: episode: 4321, duration: 0.900s, episode steps: 66, steps per second: 73, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.848 [0.000, 4.000], mean observation: 0.515 [0.470, 0.610], loss: 10.545551, mean_absolute_error: 41.654049, mean_q: 53.458778
[F[K 331009/500000: episode: 4322, duration: 1.786s, episode steps: 117, steps per second: 66, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 9.013551, mean_absolute_error: 41.753284, mean_q: 53.598682
[F[K 331093/500000: episode: 4323, duration: 1.411s, episode steps: 84, steps per second: 60, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.060 [0.000, 4.000], mean observation: 0.497 [0.420, 0.560], loss: 8.888026, mean_absolute_error: 40.849194, mean_q: 52.480934
[F[K 331150/500000: episode: 4324, duration: 1.002s, episode steps: 57, steps per second: 57, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.486 [0.410, 0.530], loss: 8.830679, mean_absolute_error: 41.048195, mean_q: 52.720985
[F[K 331195/500000: episode: 4325, duration: 0.770s, episode steps: 45, steps per second: 58, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.689 [0.000, 4.000], mean observation: 0.518 [0.470, 0.600], loss: 7.485630, mean_absolute_error: 41.993412, mean_q: 53.764969
[F[K 331254/500000: episode: 4326, duration: 0.912s, episode steps: 59, steps per second: 65, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.288 [0.000, 4.000], mean observation: 0.476 [0.360, 0.510], loss: 8.301815, mean_absolute_error: 41.036350, mean_q: 52.582500
[F[K 331305/500000: episode: 4327, duration: 0.625s, episode steps: 51, steps per second: 82, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.216 [0.000, 4.000], mean observation: 0.482 [0.390, 0.530], loss: 7.672907, mean_absolute_error: 41.341274, mean_q: 53.001740
[F[K 331392/500000: episode: 4328, duration: 1.405s, episode steps: 87, steps per second: 62, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.497 [0.400, 0.570], loss: 8.301642, mean_absolute_error: 41.956367, mean_q: 53.722034
[F[K 331459/500000: episode: 4329, duration: 0.993s, episode steps: 67, steps per second: 67, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.104 [0.000, 4.000], mean observation: 0.486 [0.420, 0.520], loss: 9.335413, mean_absolute_error: 40.704239, mean_q: 52.098030
[F[K 331547/500000: episode: 4330, duration: 1.506s, episode steps: 88, steps per second: 58, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.864 [0.000, 4.000], mean observation: 0.495 [0.450, 0.510], loss: 8.352914, mean_absolute_error: 41.122616, mean_q: 52.659214
[F[K 331648/500000: episode: 4331, duration: 1.725s, episode steps: 101, steps per second: 59, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.030 [0.000, 4.000], mean observation: 0.499 [0.470, 0.530], loss: 8.716887, mean_absolute_error: 41.421085, mean_q: 53.072453
[F[K 331716/500000: episode: 4332, duration: 1.005s, episode steps: 68, steps per second: 68, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.853 [0.000, 4.000], mean observation: 0.501 [0.470, 0.550], loss: 9.430680, mean_absolute_error: 40.798523, mean_q: 52.303925
[F[K 331789/500000: episode: 4333, duration: 1.109s, episode steps: 73, steps per second: 66, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.110 [0.000, 4.000], mean observation: 0.506 [0.470, 0.570], loss: 9.080780, mean_absolute_error: 41.342766, mean_q: 52.987186
[F[K 331856/500000: episode: 4334, duration: 1.225s, episode steps: 67, steps per second: 55, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.358 [0.000, 4.000], mean observation: 0.483 [0.360, 0.530], loss: 9.531969, mean_absolute_error: 41.197662, mean_q: 52.644817
[F[K 331933/500000: episode: 4335, duration: 1.431s, episode steps: 77, steps per second: 54, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.377 [0.000, 4.000], mean observation: 0.489 [0.410, 0.530], loss: 8.986535, mean_absolute_error: 40.635918, mean_q: 52.030251
[F[K 332020/500000: episode: 4336, duration: 1.622s, episode steps: 87, steps per second: 54, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.195 [0.000, 4.000], mean observation: 0.507 [0.470, 0.570], loss: 7.541832, mean_absolute_error: 41.323635, mean_q: 52.912910