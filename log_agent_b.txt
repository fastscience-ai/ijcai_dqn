(10,)
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_1 (Flatten)          (None, 10)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               5632      
_________________________________________________________________
activation_1 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 512)               262656    
_________________________________________________________________
activation_2 (Activation)    (None, 512)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 5)                 2565      
_________________________________________________________________
activation_3 (Activation)    (None, 5)                 0         
=================================================================
Total params: 270,853
Trainable params: 270,853
Non-trainable params: 0
_________________________________________________________________
None
load model
Training for 500000 steps ...
[F[K     39/500000: episode: 1, duration: 0.562s, episode steps: 39, steps per second: 69, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.498 [0.470, 0.510], loss: 0.880180, mean_absolute_error: 82.667548, mean_q: 104.039556
[F[K    106/500000: episode: 2, duration: 0.322s, episode steps: 67, steps per second: 208, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.284 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 13.879645, mean_absolute_error: 77.133308, mean_q: 97.586159
[F[K    146/500000: episode: 3, duration: 0.208s, episode steps: 40, steps per second: 193, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.625 [0.000, 3.000], mean observation: 0.517 [0.470, 0.630], loss: 19.687922, mean_absolute_error: 74.115128, mean_q: 94.512253
[F[K    198/500000: episode: 4, duration: 0.291s, episode steps: 52, steps per second: 179, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.365 [0.000, 4.000], mean observation: 0.497 [0.360, 0.590], loss: 22.968493, mean_absolute_error: 72.733223, mean_q: 92.759552
[F[K    258/500000: episode: 5, duration: 0.335s, episode steps: 60, steps per second: 179, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.683 [0.000, 4.000], mean observation: 0.509 [0.460, 0.620], loss: 22.424034, mean_absolute_error: 70.933311, mean_q: 90.589096
[F[K    331/500000: episode: 6, duration: 0.404s, episode steps: 73, steps per second: 180, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.548 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 17.801552, mean_absolute_error: 70.952011, mean_q: 90.811714
[F[K    384/500000: episode: 7, duration: 0.352s, episode steps: 53, steps per second: 150, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.113 [0.000, 4.000], mean observation: 0.518 [0.490, 0.590], loss: 16.835354, mean_absolute_error: 69.947899, mean_q: 89.650360
[F[K    443/500000: episode: 8, duration: 0.342s, episode steps: 59, steps per second: 173, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.407 [0.000, 4.000], mean observation: 0.506 [0.460, 0.610], loss: 25.652506, mean_absolute_error: 70.299965, mean_q: 90.050026
[F[K    558/500000: episode: 9, duration: 0.696s, episode steps: 115, steps per second: 165, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.904 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 18.886782, mean_absolute_error: 71.445923, mean_q: 91.824440
[F[K    614/500000: episode: 10, duration: 0.358s, episode steps: 56, steps per second: 157, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.339 [0.000, 4.000], mean observation: 0.476 [0.350, 0.530], loss: 19.104254, mean_absolute_error: 70.787621, mean_q: 91.110245
[F[K    650/500000: episode: 11, duration: 0.258s, episode steps: 36, steps per second: 140, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.917 [0.000, 4.000], mean observation: 0.499 [0.390, 0.580], loss: 16.133413, mean_absolute_error: 69.405670, mean_q: 89.538742
[F[K    705/500000: episode: 12, duration: 0.380s, episode steps: 55, steps per second: 145, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.492 [0.370, 0.590], loss: 17.175287, mean_absolute_error: 69.458130, mean_q: 89.527885
[F[K    764/500000: episode: 13, duration: 0.382s, episode steps: 59, steps per second: 155, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.763 [0.000, 4.000], mean observation: 0.510 [0.490, 0.550], loss: 18.289421, mean_absolute_error: 68.370811, mean_q: 88.106438
[F[K    811/500000: episode: 14, duration: 0.338s, episode steps: 47, steps per second: 139, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.319 [0.000, 4.000], mean observation: 0.506 [0.430, 0.630], loss: 16.321476, mean_absolute_error: 69.228569, mean_q: 89.306267
[F[K    873/500000: episode: 15, duration: 0.490s, episode steps: 62, steps per second: 127, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 0.505 [0.440, 0.610], loss: 21.186390, mean_absolute_error: 68.040779, mean_q: 87.802742
[F[K    924/500000: episode: 16, duration: 0.433s, episode steps: 51, steps per second: 118, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.804 [0.000, 4.000], mean observation: 0.517 [0.500, 0.590], loss: 18.083292, mean_absolute_error: 68.694740, mean_q: 88.739487
[F[K    994/500000: episode: 17, duration: 0.551s, episode steps: 70, steps per second: 127, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.214 [0.000, 4.000], mean observation: 0.500 [0.400, 0.580], loss: 17.236725, mean_absolute_error: 69.384926, mean_q: 89.445671
[F[K   1039/500000: episode: 18, duration: 0.336s, episode steps: 45, steps per second: 134, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.244 [0.000, 4.000], mean observation: 0.510 [0.470, 0.630], loss: 22.778107, mean_absolute_error: 68.358856, mean_q: 88.181862
[F[K   1087/500000: episode: 19, duration: 0.359s, episode steps: 48, steps per second: 134, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.188 [0.000, 3.000], mean observation: 0.504 [0.440, 0.580], loss: 27.564392, mean_absolute_error: 67.993362, mean_q: 87.612160
[F[K   1142/500000: episode: 20, duration: 0.474s, episode steps: 55, steps per second: 116, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.382 [0.000, 4.000], mean observation: 0.493 [0.390, 0.540], loss: 26.385689, mean_absolute_error: 68.449318, mean_q: 88.251793
[F[K   1173/500000: episode: 21, duration: 0.242s, episode steps: 31, steps per second: 128, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.032 [0.000, 3.000], mean observation: 0.525 [0.470, 0.640], loss: 16.765648, mean_absolute_error: 68.485413, mean_q: 88.582108
[F[K   1228/500000: episode: 22, duration: 0.466s, episode steps: 55, steps per second: 118, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.618 [0.000, 4.000], mean observation: 0.492 [0.370, 0.560], loss: 24.643671, mean_absolute_error: 67.610962, mean_q: 87.344002
[F[K   1293/500000: episode: 23, duration: 0.558s, episode steps: 65, steps per second: 116, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.354 [0.000, 4.000], mean observation: 0.514 [0.490, 0.570], loss: 20.569139, mean_absolute_error: 67.116234, mean_q: 86.862595
[F[K   1328/500000: episode: 24, duration: 0.301s, episode steps: 35, steps per second: 116, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.543 [0.000, 3.000], mean observation: 0.515 [0.470, 0.630], loss: 22.282202, mean_absolute_error: 69.045525, mean_q: 88.968140
[F[K   1376/500000: episode: 25, duration: 0.341s, episode steps: 48, steps per second: 141, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.708 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 19.193592, mean_absolute_error: 67.658203, mean_q: 87.373253
[F[K   1415/500000: episode: 26, duration: 0.328s, episode steps: 39, steps per second: 119, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.795 [0.000, 4.000], mean observation: 0.499 [0.430, 0.530], loss: 26.343632, mean_absolute_error: 67.018974, mean_q: 86.343956
[F[K   1497/500000: episode: 27, duration: 0.609s, episode steps: 82, steps per second: 135, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.519 [0.470, 0.630], loss: 20.510832, mean_absolute_error: 67.208099, mean_q: 86.685211
[F[K   1558/500000: episode: 28, duration: 0.503s, episode steps: 61, steps per second: 121, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.820 [0.000, 4.000], mean observation: 0.478 [0.350, 0.520], loss: 20.377731, mean_absolute_error: 66.433907, mean_q: 85.806015
[F[K   1608/500000: episode: 29, duration: 0.431s, episode steps: 50, steps per second: 116, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.300 [0.000, 4.000], mean observation: 0.508 [0.490, 0.540], loss: 22.971098, mean_absolute_error: 66.516258, mean_q: 85.669083
[F[K   1661/500000: episode: 30, duration: 0.513s, episode steps: 53, steps per second: 103, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.499 [0.420, 0.570], loss: 19.090237, mean_absolute_error: 66.528770, mean_q: 85.780380
[F[K   1694/500000: episode: 31, duration: 0.296s, episode steps: 33, steps per second: 111, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.273 [0.000, 3.000], mean observation: 0.526 [0.470, 0.660], loss: 17.293459, mean_absolute_error: 65.876732, mean_q: 85.043549
[F[K   1728/500000: episode: 32, duration: 0.289s, episode steps: 34, steps per second: 118, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.176 [0.000, 3.000], mean observation: 0.526 [0.470, 0.660], loss: 21.272814, mean_absolute_error: 65.055290, mean_q: 84.001404
[F[K   1766/500000: episode: 33, duration: 0.348s, episode steps: 38, steps per second: 109, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.421 [0.000, 4.000], mean observation: 0.487 [0.350, 0.530], loss: 20.877584, mean_absolute_error: 63.863056, mean_q: 82.543144
[F[K   1817/500000: episode: 34, duration: 0.428s, episode steps: 51, steps per second: 119, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.255 [0.000, 4.000], mean observation: 0.482 [0.370, 0.530], loss: 18.053028, mean_absolute_error: 64.370667, mean_q: 83.158867
[F[K   1867/500000: episode: 35, duration: 0.477s, episode steps: 50, steps per second: 105, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.080 [0.000, 4.000], mean observation: 0.511 [0.490, 0.540], loss: 20.567163, mean_absolute_error: 63.110260, mean_q: 81.619217
[F[K   1943/500000: episode: 36, duration: 0.708s, episode steps: 76, steps per second: 107, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.158 [0.000, 4.000], mean observation: 0.492 [0.440, 0.530], loss: 22.172764, mean_absolute_error: 63.808884, mean_q: 82.270958
[F[K   2002/500000: episode: 37, duration: 0.470s, episode steps: 59, steps per second: 126, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.610 [0.000, 4.000], mean observation: 0.513 [0.490, 0.580], loss: 18.534985, mean_absolute_error: 63.209049, mean_q: 81.643982
[F[K   2081/500000: episode: 38, duration: 0.664s, episode steps: 79, steps per second: 119, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.772 [0.000, 4.000], mean observation: 0.515 [0.470, 0.590], loss: 19.313038, mean_absolute_error: 63.943935, mean_q: 82.367027
[F[K   2134/500000: episode: 39, duration: 0.532s, episode steps: 53, steps per second: 100, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.283 [0.000, 4.000], mean observation: 0.495 [0.430, 0.530], loss: 19.557255, mean_absolute_error: 64.551079, mean_q: 83.153641
[F[K   2185/500000: episode: 40, duration: 0.502s, episode steps: 51, steps per second: 102, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.039 [0.000, 4.000], mean observation: 0.474 [0.360, 0.510], loss: 20.934614, mean_absolute_error: 63.004238, mean_q: 81.083946
[F[K   2236/500000: episode: 41, duration: 0.496s, episode steps: 51, steps per second: 103, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.353 [0.000, 4.000], mean observation: 0.502 [0.450, 0.580], loss: 20.413771, mean_absolute_error: 63.914658, mean_q: 82.139168
[F[K   2289/500000: episode: 42, duration: 0.520s, episode steps: 53, steps per second: 102, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.508 [0.430, 0.620], loss: 18.256140, mean_absolute_error: 62.427929, mean_q: 80.393822
[F[K   2338/500000: episode: 43, duration: 0.481s, episode steps: 49, steps per second: 102, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.980 [0.000, 3.000], mean observation: 0.498 [0.360, 0.640], loss: 18.111843, mean_absolute_error: 61.671169, mean_q: 79.499809
[F[K   2400/500000: episode: 44, duration: 0.607s, episode steps: 62, steps per second: 102, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.371 [0.000, 4.000], mean observation: 0.496 [0.360, 0.590], loss: 20.787905, mean_absolute_error: 61.600193, mean_q: 79.228798
[F[K   2471/500000: episode: 45, duration: 0.746s, episode steps: 71, steps per second: 95, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.535 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 17.302610, mean_absolute_error: 61.296745, mean_q: 78.971634
[F[K   2529/500000: episode: 46, duration: 0.519s, episode steps: 58, steps per second: 112, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.276 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 20.024008, mean_absolute_error: 61.665024, mean_q: 79.238411
[F[K   2586/500000: episode: 47, duration: 0.554s, episode steps: 57, steps per second: 103, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.737 [0.000, 4.000], mean observation: 0.520 [0.480, 0.610], loss: 22.808535, mean_absolute_error: 62.260616, mean_q: 79.890152
[F[K   2642/500000: episode: 48, duration: 0.617s, episode steps: 56, steps per second: 91, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.511 [0.460, 0.650], loss: 16.522493, mean_absolute_error: 60.816223, mean_q: 78.165657
[F[K   2696/500000: episode: 49, duration: 0.561s, episode steps: 54, steps per second: 96, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.426 [0.000, 4.000], mean observation: 0.510 [0.490, 0.570], loss: 18.632393, mean_absolute_error: 60.218513, mean_q: 77.512886
[F[K   2771/500000: episode: 50, duration: 0.800s, episode steps: 75, steps per second: 94, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.187 [0.000, 4.000], mean observation: 0.501 [0.440, 0.570], loss: 17.380856, mean_absolute_error: 60.592903, mean_q: 77.932106
[F[K   2837/500000: episode: 51, duration: 0.667s, episode steps: 66, steps per second: 99, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.455 [0.000, 4.000], mean observation: 0.498 [0.470, 0.520], loss: 20.543816, mean_absolute_error: 60.256042, mean_q: 77.362961
[F[K   2899/500000: episode: 52, duration: 0.598s, episode steps: 62, steps per second: 104, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.806 [0.000, 4.000], mean observation: 0.501 [0.440, 0.560], loss: 16.572632, mean_absolute_error: 60.548504, mean_q: 77.762627
[F[K   2952/500000: episode: 53, duration: 0.555s, episode steps: 53, steps per second: 96, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.830 [0.000, 4.000], mean observation: 0.487 [0.420, 0.530], loss: 17.098270, mean_absolute_error: 60.066666, mean_q: 77.110466
[F[K   3006/500000: episode: 54, duration: 0.572s, episode steps: 54, steps per second: 94, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.870 [0.000, 4.000], mean observation: 0.513 [0.500, 0.590], loss: 16.774277, mean_absolute_error: 61.310596, mean_q: 78.728554
[F[K   3047/500000: episode: 55, duration: 0.388s, episode steps: 41, steps per second: 106, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.488 [0.420, 0.530], loss: 15.824840, mean_absolute_error: 60.663567, mean_q: 77.845352
[F[K   3100/500000: episode: 56, duration: 0.525s, episode steps: 53, steps per second: 101, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.321 [0.000, 4.000], mean observation: 0.477 [0.370, 0.530], loss: 19.554564, mean_absolute_error: 59.431068, mean_q: 76.165756
[F[K   3142/500000: episode: 57, duration: 0.445s, episode steps: 42, steps per second: 94, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.905 [0.000, 4.000], mean observation: 0.494 [0.360, 0.560], loss: 18.297024, mean_absolute_error: 59.917915, mean_q: 76.660286
[F[K   3181/500000: episode: 58, duration: 0.452s, episode steps: 39, steps per second: 86, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.795 [0.000, 4.000], mean observation: 0.513 [0.470, 0.640], loss: 19.482347, mean_absolute_error: 58.924988, mean_q: 75.554794
[F[K   3243/500000: episode: 59, duration: 0.670s, episode steps: 62, steps per second: 93, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.048 [0.000, 4.000], mean observation: 0.509 [0.480, 0.590], loss: 17.828171, mean_absolute_error: 58.577408, mean_q: 75.256775
[F[K   3327/500000: episode: 60, duration: 0.912s, episode steps: 84, steps per second: 92, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 18.155529, mean_absolute_error: 58.355858, mean_q: 74.747231
[F[K   3387/500000: episode: 61, duration: 0.669s, episode steps: 60, steps per second: 90, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.817 [0.000, 4.000], mean observation: 0.504 [0.450, 0.590], loss: 18.174109, mean_absolute_error: 58.671459, mean_q: 75.236595
[F[K   3438/500000: episode: 62, duration: 0.605s, episode steps: 51, steps per second: 84, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.294 [0.000, 4.000], mean observation: 0.506 [0.460, 0.560], loss: 17.938276, mean_absolute_error: 57.942101, mean_q: 74.471443
[F[K   3495/500000: episode: 63, duration: 0.675s, episode steps: 57, steps per second: 84, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.511 [0.470, 0.580], loss: 15.914855, mean_absolute_error: 57.489498, mean_q: 73.784363
[F[K   3548/500000: episode: 64, duration: 0.637s, episode steps: 53, steps per second: 83, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.925 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 18.601761, mean_absolute_error: 58.011135, mean_q: 74.278755
[F[K   3586/500000: episode: 65, duration: 0.407s, episode steps: 38, steps per second: 93, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.158 [0.000, 2.000], mean observation: 0.515 [0.470, 0.640], loss: 18.686935, mean_absolute_error: 56.841816, mean_q: 72.857491
[F[K   3637/500000: episode: 66, duration: 0.522s, episode steps: 51, steps per second: 98, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.784 [0.000, 4.000], mean observation: 0.519 [0.470, 0.620], loss: 17.627434, mean_absolute_error: 56.099293, mean_q: 71.972549
[F[K   3684/500000: episode: 67, duration: 0.539s, episode steps: 47, steps per second: 87, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.489 [0.000, 4.000], mean observation: 0.498 [0.360, 0.620], loss: 17.606218, mean_absolute_error: 56.769249, mean_q: 72.912010
[F[K   3753/500000: episode: 68, duration: 0.729s, episode steps: 69, steps per second: 95, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.391 [0.000, 4.000], mean observation: 0.498 [0.440, 0.530], loss: 17.855026, mean_absolute_error: 56.252140, mean_q: 72.187859
[F[K   3810/500000: episode: 69, duration: 0.708s, episode steps: 57, steps per second: 81, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.491 [0.000, 4.000], mean observation: 0.498 [0.460, 0.530], loss: 17.815554, mean_absolute_error: 55.148472, mean_q: 70.861671
[F[K   3871/500000: episode: 70, duration: 0.684s, episode steps: 61, steps per second: 89, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.494 [0.450, 0.520], loss: 15.964568, mean_absolute_error: 55.890232, mean_q: 71.825706
[F[K   3919/500000: episode: 71, duration: 0.613s, episode steps: 48, steps per second: 78, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.042 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 16.789753, mean_absolute_error: 55.838207, mean_q: 71.532509
[F[K   3965/500000: episode: 72, duration: 0.498s, episode steps: 46, steps per second: 92, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.522 [0.000, 3.000], mean observation: 0.519 [0.470, 0.620], loss: 16.141363, mean_absolute_error: 55.935818, mean_q: 71.796638
[F[K   3999/500000: episode: 73, duration: 0.364s, episode steps: 34, steps per second: 93, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.088 [0.000, 3.000], mean observation: 0.518 [0.470, 0.640], loss: 16.710768, mean_absolute_error: 55.066235, mean_q: 70.647011
[F[K   4049/500000: episode: 74, duration: 0.575s, episode steps: 50, steps per second: 87, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.540 [0.000, 4.000], mean observation: 0.500 [0.460, 0.530], loss: 20.350786, mean_absolute_error: 55.114456, mean_q: 70.585663
[F[K   4096/500000: episode: 75, duration: 0.517s, episode steps: 47, steps per second: 91, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.170 [0.000, 4.000], mean observation: 0.483 [0.390, 0.520], loss: 16.298014, mean_absolute_error: 54.425549, mean_q: 69.726013
[F[K   4152/500000: episode: 76, duration: 0.666s, episode steps: 56, steps per second: 84, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.536 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 18.612728, mean_absolute_error: 54.433922, mean_q: 69.765678
[F[K   4200/500000: episode: 77, duration: 0.581s, episode steps: 48, steps per second: 83, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.979 [0.000, 4.000], mean observation: 0.508 [0.470, 0.550], loss: 16.192327, mean_absolute_error: 55.570316, mean_q: 71.241646
[F[K   4262/500000: episode: 78, duration: 0.795s, episode steps: 62, steps per second: 78, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.468 [0.000, 4.000], mean observation: 0.496 [0.410, 0.560], loss: 18.384441, mean_absolute_error: 53.402592, mean_q: 68.543800
[F[K   4328/500000: episode: 79, duration: 0.804s, episode steps: 66, steps per second: 82, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.030 [0.000, 4.000], mean observation: 0.500 [0.400, 0.600], loss: 19.420685, mean_absolute_error: 54.744450, mean_q: 70.129105
[F[K   4386/500000: episode: 80, duration: 0.605s, episode steps: 58, steps per second: 96, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.586 [0.000, 4.000], mean observation: 0.511 [0.470, 0.640], loss: 17.723961, mean_absolute_error: 52.696232, mean_q: 67.501846
[F[K   4441/500000: episode: 81, duration: 0.617s, episode steps: 55, steps per second: 89, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.782 [0.000, 4.000], mean observation: 0.503 [0.410, 0.630], loss: 17.275028, mean_absolute_error: 52.348324, mean_q: 67.103676
[F[K   4509/500000: episode: 82, duration: 0.855s, episode steps: 68, steps per second: 80, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.838 [0.000, 4.000], mean observation: 0.496 [0.390, 0.530], loss: 18.463980, mean_absolute_error: 51.609161, mean_q: 66.338028
[F[K   4568/500000: episode: 83, duration: 0.692s, episode steps: 59, steps per second: 85, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.678 [0.000, 4.000], mean observation: 0.507 [0.470, 0.580], loss: 17.585018, mean_absolute_error: 51.979198, mean_q: 66.524818
[F[K   4620/500000: episode: 84, duration: 0.634s, episode steps: 52, steps per second: 82, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 0.497 [0.450, 0.530], loss: 17.206089, mean_absolute_error: 51.557369, mean_q: 66.182434
[F[K   4680/500000: episode: 85, duration: 0.685s, episode steps: 60, steps per second: 88, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.267 [0.000, 4.000], mean observation: 0.494 [0.370, 0.560], loss: 17.173212, mean_absolute_error: 50.930138, mean_q: 65.332245
[F[K   4749/500000: episode: 86, duration: 0.851s, episode steps: 69, steps per second: 81, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.159 [0.000, 4.000], mean observation: 0.495 [0.460, 0.520], loss: 15.734617, mean_absolute_error: 51.529934, mean_q: 66.050591
[F[K   4799/500000: episode: 87, duration: 0.686s, episode steps: 50, steps per second: 73, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.080 [0.000, 4.000], mean observation: 0.513 [0.470, 0.600], loss: 17.538555, mean_absolute_error: 51.376080, mean_q: 65.867455
[F[K   4843/500000: episode: 88, duration: 0.538s, episode steps: 44, steps per second: 82, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.273 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 16.692116, mean_absolute_error: 50.978283, mean_q: 65.527260
[F[K   4912/500000: episode: 89, duration: 0.837s, episode steps: 69, steps per second: 82, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.507 [0.000, 4.000], mean observation: 0.494 [0.450, 0.510], loss: 19.189392, mean_absolute_error: 52.254749, mean_q: 66.936653
[F[K   4971/500000: episode: 90, duration: 0.810s, episode steps: 59, steps per second: 73, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.864 [0.000, 4.000], mean observation: 0.499 [0.410, 0.590], loss: 18.473808, mean_absolute_error: 50.540283, mean_q: 64.864197
[F[K   5032/500000: episode: 91, duration: 0.754s, episode steps: 61, steps per second: 81, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.836 [0.000, 4.000], mean observation: 0.486 [0.390, 0.520], loss: 17.997988, mean_absolute_error: 49.748478, mean_q: 63.977673
[F[K   5068/500000: episode: 92, duration: 0.475s, episode steps: 36, steps per second: 76, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.111 [0.000, 3.000], mean observation: 0.523 [0.470, 0.650], loss: 17.729296, mean_absolute_error: 49.393639, mean_q: 63.451828
[F[K   5139/500000: episode: 93, duration: 0.867s, episode steps: 71, steps per second: 82, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.493 [0.000, 4.000], mean observation: 0.502 [0.460, 0.550], loss: 18.129135, mean_absolute_error: 49.795673, mean_q: 63.850800
[F[K   5200/500000: episode: 94, duration: 0.771s, episode steps: 61, steps per second: 79, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.082 [0.000, 4.000], mean observation: 0.479 [0.370, 0.520], loss: 17.366663, mean_absolute_error: 49.735954, mean_q: 63.511211
[F[K   5238/500000: episode: 95, duration: 0.493s, episode steps: 38, steps per second: 77, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.395 [0.000, 3.000], mean observation: 0.519 [0.470, 0.630], loss: 17.007683, mean_absolute_error: 50.110645, mean_q: 64.393570
[F[K   5297/500000: episode: 96, duration: 0.794s, episode steps: 59, steps per second: 74, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.441 [0.000, 4.000], mean observation: 0.498 [0.430, 0.560], loss: 19.339018, mean_absolute_error: 50.153770, mean_q: 63.975872
[F[K   5360/500000: episode: 97, duration: 0.760s, episode steps: 63, steps per second: 83, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.746 [0.000, 4.000], mean observation: 0.523 [0.470, 0.620], loss: 18.370356, mean_absolute_error: 48.212639, mean_q: 61.649700
[F[K   5407/500000: episode: 98, duration: 0.568s, episode steps: 47, steps per second: 83, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.021 [0.000, 4.000], mean observation: 0.516 [0.470, 0.600], loss: 19.310734, mean_absolute_error: 47.648125, mean_q: 61.037552
[F[K   5481/500000: episode: 99, duration: 0.971s, episode steps: 74, steps per second: 76, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.595 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 14.887790, mean_absolute_error: 48.503429, mean_q: 62.392235
[F[K   5540/500000: episode: 100, duration: 0.746s, episode steps: 59, steps per second: 79, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.254 [0.000, 4.000], mean observation: 0.497 [0.470, 0.520], loss: 17.541403, mean_absolute_error: 47.956970, mean_q: 61.562508
[F[K   5596/500000: episode: 101, duration: 0.706s, episode steps: 56, steps per second: 79, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.089 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 17.460556, mean_absolute_error: 47.818245, mean_q: 61.282558
[F[K   5625/500000: episode: 102, duration: 0.386s, episode steps: 29, steps per second: 75, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.379 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 16.851357, mean_absolute_error: 47.591797, mean_q: 60.982719
[F[K   5761/500000: episode: 103, duration: 1.645s, episode steps: 136, steps per second: 83, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.257 [0.000, 4.000], mean observation: 0.491 [0.330, 0.560], loss: 15.999167, mean_absolute_error: 47.543663, mean_q: 61.082153
[F[K   5819/500000: episode: 104, duration: 0.741s, episode steps: 58, steps per second: 78, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.724 [0.000, 4.000], mean observation: 0.495 [0.420, 0.530], loss: 15.983768, mean_absolute_error: 47.596813, mean_q: 61.019085
[F[K   5916/500000: episode: 105, duration: 1.237s, episode steps: 97, steps per second: 78, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.979 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 15.556629, mean_absolute_error: 47.145679, mean_q: 60.706306
[F[K   5968/500000: episode: 106, duration: 0.701s, episode steps: 52, steps per second: 74, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.231 [0.000, 4.000], mean observation: 0.514 [0.470, 0.620], loss: 17.177650, mean_absolute_error: 46.893856, mean_q: 60.169617
[F[K   6039/500000: episode: 107, duration: 0.966s, episode steps: 71, steps per second: 73, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.380 [0.000, 4.000], mean observation: 0.501 [0.420, 0.570], loss: 13.920614, mean_absolute_error: 47.507843, mean_q: 61.245804
[F[K   6101/500000: episode: 108, duration: 0.875s, episode steps: 62, steps per second: 71, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.919 [0.000, 4.000], mean observation: 0.492 [0.430, 0.520], loss: 16.846048, mean_absolute_error: 46.728252, mean_q: 59.969116
[F[K   6155/500000: episode: 109, duration: 0.792s, episode steps: 54, steps per second: 68, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.611 [0.000, 4.000], mean observation: 0.500 [0.400, 0.630], loss: 17.921068, mean_absolute_error: 46.537067, mean_q: 59.766556
[F[K   6216/500000: episode: 110, duration: 0.877s, episode steps: 61, steps per second: 70, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.246 [0.000, 4.000], mean observation: 0.504 [0.450, 0.600], loss: 14.490175, mean_absolute_error: 46.608723, mean_q: 59.924973
[F[K   6273/500000: episode: 111, duration: 0.798s, episode steps: 57, steps per second: 71, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.211 [0.000, 4.000], mean observation: 0.490 [0.420, 0.520], loss: 15.756515, mean_absolute_error: 47.217968, mean_q: 60.467010
[F[K   6320/500000: episode: 112, duration: 0.691s, episode steps: 47, steps per second: 68, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.191 [0.000, 4.000], mean observation: 0.475 [0.360, 0.530], loss: 15.078183, mean_absolute_error: 46.765514, mean_q: 60.131130
[F[K   6371/500000: episode: 113, duration: 0.765s, episode steps: 51, steps per second: 67, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.509 [0.460, 0.630], loss: 16.414736, mean_absolute_error: 45.348045, mean_q: 58.490719
[F[K   6420/500000: episode: 114, duration: 0.680s, episode steps: 49, steps per second: 72, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.286 [0.000, 4.000], mean observation: 0.519 [0.470, 0.610], loss: 16.159193, mean_absolute_error: 45.885242, mean_q: 59.107841
[F[K   6496/500000: episode: 115, duration: 1.143s, episode steps: 76, steps per second: 67, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.645 [0.000, 4.000], mean observation: 0.513 [0.470, 0.610], loss: 13.104489, mean_absolute_error: 46.156464, mean_q: 59.355564
[F[K   6543/500000: episode: 116, duration: 0.632s, episode steps: 47, steps per second: 74, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.515 [0.470, 0.640], loss: 16.720766, mean_absolute_error: 45.821629, mean_q: 58.800247
[F[K   6599/500000: episode: 117, duration: 0.795s, episode steps: 56, steps per second: 70, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.661 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 15.032510, mean_absolute_error: 45.634487, mean_q: 58.902813
[F[K   6658/500000: episode: 118, duration: 0.809s, episode steps: 59, steps per second: 73, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.509 [0.470, 0.570], loss: 16.025370, mean_absolute_error: 45.482559, mean_q: 58.687859
[F[K   6705/500000: episode: 119, duration: 0.693s, episode steps: 47, steps per second: 68, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.660 [0.000, 3.000], mean observation: 0.498 [0.370, 0.620], loss: 16.849648, mean_absolute_error: 44.277153, mean_q: 56.875973
[F[K   6743/500000: episode: 120, duration: 0.547s, episode steps: 38, steps per second: 70, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.184 [0.000, 3.000], mean observation: 0.499 [0.360, 0.640], loss: 12.672407, mean_absolute_error: 45.228832, mean_q: 58.215565
[F[K   6804/500000: episode: 121, duration: 0.875s, episode steps: 61, steps per second: 70, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.213 [0.000, 4.000], mean observation: 0.489 [0.420, 0.520], loss: 16.143688, mean_absolute_error: 44.310440, mean_q: 56.945381
[F[K   6846/500000: episode: 122, duration: 0.689s, episode steps: 42, steps per second: 61, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.619 [0.000, 4.000], mean observation: 0.512 [0.470, 0.650], loss: 14.472287, mean_absolute_error: 44.609497, mean_q: 57.393024
[F[K   6909/500000: episode: 123, duration: 0.927s, episode steps: 63, steps per second: 68, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.651 [0.000, 4.000], mean observation: 0.512 [0.470, 0.620], loss: 15.698507, mean_absolute_error: 43.845039, mean_q: 56.242218
[F[K   6946/500000: episode: 124, duration: 0.637s, episode steps: 37, steps per second: 58, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.243 [0.000, 4.000], mean observation: 0.520 [0.470, 0.630], loss: 15.895395, mean_absolute_error: 44.005009, mean_q: 56.397449
[F[K   7001/500000: episode: 125, duration: 0.812s, episode steps: 55, steps per second: 68, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.545 [0.000, 4.000], mean observation: 0.498 [0.390, 0.610], loss: 14.734184, mean_absolute_error: 43.323215, mean_q: 55.875237
[F[K   7097/500000: episode: 126, duration: 1.428s, episode steps: 96, steps per second: 67, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.510 [0.000, 4.000], mean observation: 0.487 [0.380, 0.510], loss: 14.889531, mean_absolute_error: 43.278393, mean_q: 55.783497
[F[K   7147/500000: episode: 127, duration: 0.796s, episode steps: 50, steps per second: 63, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.505 [0.400, 0.650], loss: 11.796588, mean_absolute_error: 42.061073, mean_q: 54.356293
[F[K   7203/500000: episode: 128, duration: 0.914s, episode steps: 56, steps per second: 61, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.179 [0.000, 4.000], mean observation: 0.503 [0.410, 0.630], loss: 14.019052, mean_absolute_error: 45.163967, mean_q: 58.062199
[F[K   7259/500000: episode: 129, duration: 0.949s, episode steps: 56, steps per second: 59, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.286 [0.000, 4.000], mean observation: 0.503 [0.410, 0.630], loss: 15.817263, mean_absolute_error: 44.257587, mean_q: 56.863247
[F[K   7324/500000: episode: 130, duration: 0.995s, episode steps: 65, steps per second: 65, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.708 [0.000, 4.000], mean observation: 0.498 [0.470, 0.520], loss: 13.243170, mean_absolute_error: 42.795040, mean_q: 55.180386
[F[K   7394/500000: episode: 131, duration: 1.198s, episode steps: 70, steps per second: 58, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.496 [0.470, 0.520], loss: 14.220540, mean_absolute_error: 42.609558, mean_q: 54.975800
[F[K   7448/500000: episode: 132, duration: 0.852s, episode steps: 54, steps per second: 63, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.259 [0.000, 4.000], mean observation: 0.505 [0.470, 0.570], loss: 12.715763, mean_absolute_error: 42.393658, mean_q: 54.710285
[F[K   7520/500000: episode: 133, duration: 1.252s, episode steps: 72, steps per second: 58, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.778 [0.000, 4.000], mean observation: 0.508 [0.470, 0.560], loss: 14.485814, mean_absolute_error: 42.236393, mean_q: 54.559937
[F[K   7578/500000: episode: 134, duration: 0.825s, episode steps: 58, steps per second: 70, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.493 [0.470, 0.520], loss: 14.713013, mean_absolute_error: 41.967583, mean_q: 54.138561
[F[K   7618/500000: episode: 135, duration: 0.622s, episode steps: 40, steps per second: 64, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.725 [0.000, 4.000], mean observation: 0.507 [0.450, 0.630], loss: 16.351955, mean_absolute_error: 41.581627, mean_q: 53.459923
[F[K   7657/500000: episode: 136, duration: 0.622s, episode steps: 39, steps per second: 63, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.500 [0.350, 0.640], loss: 13.353435, mean_absolute_error: 42.325745, mean_q: 54.550056
[F[K   7695/500000: episode: 137, duration: 0.621s, episode steps: 38, steps per second: 61, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.184 [0.000, 2.000], mean observation: 0.516 [0.470, 0.640], loss: 14.589746, mean_absolute_error: 42.151196, mean_q: 54.491833
[F[K   7754/500000: episode: 138, duration: 0.898s, episode steps: 59, steps per second: 66, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.495 [0.430, 0.520], loss: 12.834871, mean_absolute_error: 42.062634, mean_q: 54.230473
[F[K   7807/500000: episode: 139, duration: 0.863s, episode steps: 53, steps per second: 61, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.497 [0.400, 0.540], loss: 12.124468, mean_absolute_error: 41.147320, mean_q: 53.117462
[F[K   7857/500000: episode: 140, duration: 0.752s, episode steps: 50, steps per second: 66, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.511 [0.470, 0.570], loss: 13.989605, mean_absolute_error: 42.148781, mean_q: 54.355991
[F[K   7912/500000: episode: 141, duration: 0.618s, episode steps: 55, steps per second: 89, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.345 [0.000, 4.000], mean observation: 0.493 [0.390, 0.550], loss: 15.751687, mean_absolute_error: 41.363499, mean_q: 53.289715
[F[K   7985/500000: episode: 142, duration: 0.990s, episode steps: 73, steps per second: 74, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.397 [0.000, 4.000], mean observation: 0.481 [0.380, 0.520], loss: 13.234107, mean_absolute_error: 41.125637, mean_q: 53.277306
[F[K   8041/500000: episode: 143, duration: 0.736s, episode steps: 56, steps per second: 76, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.982 [0.000, 4.000], mean observation: 0.485 [0.420, 0.520], loss: 13.261067, mean_absolute_error: 40.337166, mean_q: 52.043194
[F[K   8187/500000: episode: 144, duration: 1.885s, episode steps: 146, steps per second: 77, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.479 [0.000, 4.000], mean observation: 0.485 [0.380, 0.530], loss: 13.549891, mean_absolute_error: 40.780521, mean_q: 52.670311
[F[K   8235/500000: episode: 145, duration: 0.603s, episode steps: 48, steps per second: 80, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.487 [0.390, 0.520], loss: 12.361000, mean_absolute_error: 40.979416, mean_q: 52.866318
[F[K   8298/500000: episode: 146, duration: 0.800s, episode steps: 63, steps per second: 79, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.476 [0.000, 4.000], mean observation: 0.486 [0.420, 0.520], loss: 11.518714, mean_absolute_error: 41.330818, mean_q: 53.419693
[F[K   8341/500000: episode: 147, duration: 0.527s, episode steps: 43, steps per second: 82, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.372 [0.000, 4.000], mean observation: 0.490 [0.350, 0.530], loss: 13.502322, mean_absolute_error: 41.384083, mean_q: 53.515987
[F[K   8394/500000: episode: 148, duration: 0.718s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.943 [0.000, 4.000], mean observation: 0.495 [0.400, 0.550], loss: 11.789667, mean_absolute_error: 41.668629, mean_q: 54.132671
[F[K   8435/500000: episode: 149, duration: 0.565s, episode steps: 41, steps per second: 73, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.341 [0.000, 3.000], mean observation: 0.509 [0.470, 0.640], loss: 13.261186, mean_absolute_error: 40.431702, mean_q: 52.482845
[F[K   8491/500000: episode: 150, duration: 0.787s, episode steps: 56, steps per second: 71, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.496 [0.440, 0.530], loss: 13.029406, mean_absolute_error: 40.790871, mean_q: 52.671669
[F[K   8549/500000: episode: 151, duration: 0.769s, episode steps: 58, steps per second: 75, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.552 [0.000, 4.000], mean observation: 0.494 [0.450, 0.520], loss: 14.655252, mean_absolute_error: 40.194126, mean_q: 52.134205
[F[K   8618/500000: episode: 152, duration: 0.817s, episode steps: 69, steps per second: 84, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.623 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 13.125241, mean_absolute_error: 40.430542, mean_q: 52.359287
[F[K   8678/500000: episode: 153, duration: 0.826s, episode steps: 60, steps per second: 73, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.514 [0.470, 0.590], loss: 13.514112, mean_absolute_error: 40.287640, mean_q: 52.017727
[F[K   8719/500000: episode: 154, duration: 0.645s, episode steps: 41, steps per second: 64, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.049 [0.000, 4.000], mean observation: 0.499 [0.450, 0.540], loss: 13.596568, mean_absolute_error: 40.982784, mean_q: 52.807224
[F[K   8772/500000: episode: 155, duration: 0.696s, episode steps: 53, steps per second: 76, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.475 [0.370, 0.530], loss: 12.306231, mean_absolute_error: 40.077141, mean_q: 51.897663
[F[K   8811/500000: episode: 156, duration: 0.491s, episode steps: 39, steps per second: 79, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.410 [0.000, 3.000], mean observation: 0.523 [0.470, 0.630], loss: 12.299728, mean_absolute_error: 40.928349, mean_q: 52.718029
[F[K   8853/500000: episode: 157, duration: 0.594s, episode steps: 42, steps per second: 71, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.810 [0.000, 4.000], mean observation: 0.492 [0.410, 0.540], loss: 13.228086, mean_absolute_error: 40.221310, mean_q: 52.238705
[F[K   8918/500000: episode: 158, duration: 0.905s, episode steps: 65, steps per second: 72, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.677 [0.000, 4.000], mean observation: 0.504 [0.420, 0.620], loss: 13.734672, mean_absolute_error: 39.894382, mean_q: 51.698147
[F[K   8992/500000: episode: 159, duration: 1.150s, episode steps: 74, steps per second: 64, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.297 [0.000, 4.000], mean observation: 0.491 [0.440, 0.530], loss: 12.997526, mean_absolute_error: 39.556408, mean_q: 51.196003
[F[K   9051/500000: episode: 160, duration: 0.915s, episode steps: 59, steps per second: 64, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.407 [0.000, 4.000], mean observation: 0.509 [0.470, 0.590], loss: 13.299514, mean_absolute_error: 39.419888, mean_q: 51.049774
[F[K   9096/500000: episode: 161, duration: 0.669s, episode steps: 45, steps per second: 67, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.489 [0.000, 4.000], mean observation: 0.490 [0.360, 0.550], loss: 10.478493, mean_absolute_error: 41.117779, mean_q: 53.238113
[F[K   9155/500000: episode: 162, duration: 0.813s, episode steps: 59, steps per second: 73, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.847 [0.000, 4.000], mean observation: 0.480 [0.380, 0.530], loss: 12.260923, mean_absolute_error: 40.368732, mean_q: 52.213249
[F[K   9215/500000: episode: 163, duration: 0.943s, episode steps: 60, steps per second: 64, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.300 [0.000, 4.000], mean observation: 0.478 [0.360, 0.520], loss: 12.488225, mean_absolute_error: 40.533169, mean_q: 52.396221
[F[K   9273/500000: episode: 164, duration: 0.762s, episode steps: 58, steps per second: 76, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.914 [0.000, 4.000], mean observation: 0.487 [0.370, 0.520], loss: 13.983169, mean_absolute_error: 39.021603, mean_q: 50.618843
[F[K   9330/500000: episode: 165, duration: 0.785s, episode steps: 57, steps per second: 73, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.333 [0.000, 4.000], mean observation: 0.509 [0.470, 0.590], loss: 12.870174, mean_absolute_error: 40.644943, mean_q: 52.580490
[F[K   9375/500000: episode: 166, duration: 0.628s, episode steps: 45, steps per second: 72, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.133 [0.000, 4.000], mean observation: 0.481 [0.350, 0.510], loss: 11.889250, mean_absolute_error: 40.354958, mean_q: 52.039265
[F[K   9415/500000: episode: 167, duration: 0.612s, episode steps: 40, steps per second: 65, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.775 [0.000, 4.000], mean observation: 0.496 [0.450, 0.530], loss: 10.270973, mean_absolute_error: 39.115959, mean_q: 50.534225
[F[K   9471/500000: episode: 168, duration: 0.785s, episode steps: 56, steps per second: 71, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.714 [0.000, 4.000], mean observation: 0.499 [0.390, 0.600], loss: 12.992637, mean_absolute_error: 39.355679, mean_q: 50.915653
[F[K   9520/500000: episode: 169, duration: 0.596s, episode steps: 49, steps per second: 82, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.898 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 12.063504, mean_absolute_error: 38.588505, mean_q: 50.076241
[F[K   9589/500000: episode: 170, duration: 1.052s, episode steps: 69, steps per second: 66, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.739 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 13.208867, mean_absolute_error: 38.978588, mean_q: 50.506401
[F[K   9645/500000: episode: 171, duration: 0.789s, episode steps: 56, steps per second: 71, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.375 [0.000, 4.000], mean observation: 0.504 [0.470, 0.550], loss: 12.057679, mean_absolute_error: 38.546135, mean_q: 49.973423
[F[K   9694/500000: episode: 172, duration: 0.681s, episode steps: 49, steps per second: 72, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.509 [0.470, 0.550], loss: 11.046105, mean_absolute_error: 39.964874, mean_q: 51.723698
[F[K   9776/500000: episode: 173, duration: 1.129s, episode steps: 82, steps per second: 73, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.537 [0.000, 4.000], mean observation: 0.485 [0.380, 0.530], loss: 13.220515, mean_absolute_error: 40.024124, mean_q: 51.687275
[F[K   9844/500000: episode: 174, duration: 0.861s, episode steps: 68, steps per second: 79, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.618 [0.000, 4.000], mean observation: 0.497 [0.400, 0.550], loss: 11.773136, mean_absolute_error: 38.844269, mean_q: 50.328083
[F[K   9882/500000: episode: 175, duration: 0.537s, episode steps: 38, steps per second: 71, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.105 [0.000, 4.000], mean observation: 0.479 [0.350, 0.510], loss: 13.468489, mean_absolute_error: 39.326561, mean_q: 50.918522
[F[K  10007/500000: episode: 176, duration: 1.934s, episode steps: 125, steps per second: 65, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.368 [0.000, 4.000], mean observation: 0.494 [0.360, 0.570], loss: 12.285847, mean_absolute_error: 39.350853, mean_q: 51.005180
[F[K  10086/500000: episode: 177, duration: 1.202s, episode steps: 79, steps per second: 66, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.557 [0.000, 4.000], mean observation: 0.501 [0.420, 0.590], loss: 11.693971, mean_absolute_error: 38.915833, mean_q: 50.476837
[F[K  10123/500000: episode: 178, duration: 0.559s, episode steps: 37, steps per second: 66, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.432 [0.000, 2.000], mean observation: 0.525 [0.470, 0.630], loss: 13.873514, mean_absolute_error: 38.163048, mean_q: 49.516159
[F[K  10169/500000: episode: 179, duration: 0.659s, episode steps: 46, steps per second: 70, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.326 [0.000, 4.000], mean observation: 0.490 [0.400, 0.530], loss: 10.204721, mean_absolute_error: 38.707863, mean_q: 50.399792
[F[K  10256/500000: episode: 180, duration: 1.317s, episode steps: 87, steps per second: 66, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.805 [0.000, 4.000], mean observation: 0.508 [0.460, 0.570], loss: 13.440697, mean_absolute_error: 38.225155, mean_q: 49.509483
[F[K  10286/500000: episode: 181, duration: 0.492s, episode steps: 30, steps per second: 61, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.333 [0.000, 2.000], mean observation: 0.524 [0.470, 0.650], loss: 12.553838, mean_absolute_error: 37.430370, mean_q: 48.471882
[F[K  10338/500000: episode: 182, duration: 0.728s, episode steps: 52, steps per second: 71, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.731 [0.000, 4.000], mean observation: 0.485 [0.370, 0.530], loss: 11.391648, mean_absolute_error: 38.515396, mean_q: 49.719963
[F[K  10394/500000: episode: 183, duration: 0.804s, episode steps: 56, steps per second: 70, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.643 [0.000, 4.000], mean observation: 0.505 [0.420, 0.600], loss: 10.447072, mean_absolute_error: 37.257915, mean_q: 48.298401
[F[K  10441/500000: episode: 184, duration: 0.607s, episode steps: 47, steps per second: 77, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.319 [0.000, 4.000], mean observation: 0.511 [0.470, 0.560], loss: 12.021621, mean_absolute_error: 37.630371, mean_q: 48.793430
[F[K  10482/500000: episode: 185, duration: 0.604s, episode steps: 41, steps per second: 68, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.244 [0.000, 3.000], mean observation: 0.509 [0.450, 0.650], loss: 11.210214, mean_absolute_error: 38.486374, mean_q: 49.910828
[F[K  10516/500000: episode: 186, duration: 0.451s, episode steps: 34, steps per second: 75, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.147 [0.000, 3.000], mean observation: 0.506 [0.420, 0.650], loss: 11.307600, mean_absolute_error: 37.954208, mean_q: 49.174126
[F[K  10576/500000: episode: 187, duration: 0.729s, episode steps: 60, steps per second: 82, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.317 [0.000, 4.000], mean observation: 0.506 [0.450, 0.580], loss: 11.891093, mean_absolute_error: 37.509655, mean_q: 48.809021
[F[K  10629/500000: episode: 188, duration: 0.743s, episode steps: 53, steps per second: 71, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.906 [0.000, 4.000], mean observation: 0.498 [0.370, 0.610], loss: 12.207212, mean_absolute_error: 38.174026, mean_q: 49.355045
[F[K  10679/500000: episode: 189, duration: 0.700s, episode steps: 50, steps per second: 71, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.940 [0.000, 4.000], mean observation: 0.507 [0.440, 0.620], loss: 10.928162, mean_absolute_error: 37.470428, mean_q: 48.536251
[F[K  10748/500000: episode: 190, duration: 0.999s, episode steps: 69, steps per second: 69, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.841 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 10.878492, mean_absolute_error: 37.617886, mean_q: 48.816154
[F[K  10818/500000: episode: 191, duration: 0.936s, episode steps: 70, steps per second: 75, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 10.951713, mean_absolute_error: 37.868801, mean_q: 49.240997
[F[K  10873/500000: episode: 192, duration: 0.720s, episode steps: 55, steps per second: 76, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.073 [0.000, 4.000], mean observation: 0.514 [0.470, 0.600], loss: 10.316989, mean_absolute_error: 38.641731, mean_q: 50.039173
[F[K  10932/500000: episode: 193, duration: 0.932s, episode steps: 59, steps per second: 63, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.441 [0.000, 3.000], mean observation: 0.519 [0.470, 0.640], loss: 12.793347, mean_absolute_error: 36.832336, mean_q: 47.760899
[F[K  10999/500000: episode: 194, duration: 0.954s, episode steps: 67, steps per second: 70, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.567 [0.000, 4.000], mean observation: 0.486 [0.420, 0.520], loss: 11.112088, mean_absolute_error: 37.362961, mean_q: 48.567669
[F[K  11043/500000: episode: 195, duration: 0.739s, episode steps: 44, steps per second: 60, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.614 [0.000, 4.000], mean observation: 0.498 [0.380, 0.600], loss: 13.241703, mean_absolute_error: 36.941452, mean_q: 47.928078
[F[K  11098/500000: episode: 196, duration: 0.921s, episode steps: 55, steps per second: 60, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.382 [0.000, 4.000], mean observation: 0.492 [0.420, 0.530], loss: 11.940557, mean_absolute_error: 37.544472, mean_q: 48.718128
[F[K  11158/500000: episode: 197, duration: 1.006s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.133 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 12.607465, mean_absolute_error: 37.378517, mean_q: 48.402817
[F[K  11216/500000: episode: 198, duration: 0.881s, episode steps: 58, steps per second: 66, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.507 [0.450, 0.590], loss: 11.419456, mean_absolute_error: 37.274750, mean_q: 48.220901
[F[K  11279/500000: episode: 199, duration: 0.956s, episode steps: 63, steps per second: 66, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.460 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 11.203860, mean_absolute_error: 36.151924, mean_q: 46.762875
[F[K  11337/500000: episode: 200, duration: 0.904s, episode steps: 58, steps per second: 64, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.086 [0.000, 4.000], mean observation: 0.510 [0.490, 0.540], loss: 10.253359, mean_absolute_error: 37.999626, mean_q: 49.193794
[F[K  11385/500000: episode: 201, duration: 0.747s, episode steps: 48, steps per second: 64, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.490 [0.420, 0.530], loss: 11.262580, mean_absolute_error: 35.931839, mean_q: 46.895916
[F[K  11424/500000: episode: 202, duration: 0.559s, episode steps: 39, steps per second: 70, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.128 [0.000, 4.000], mean observation: 0.500 [0.420, 0.540], loss: 10.965998, mean_absolute_error: 37.327198, mean_q: 48.435085
[F[K  11484/500000: episode: 203, duration: 0.957s, episode steps: 60, steps per second: 63, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.950 [0.000, 4.000], mean observation: 0.486 [0.430, 0.520], loss: 13.034211, mean_absolute_error: 36.864159, mean_q: 47.852501
[F[K  11557/500000: episode: 204, duration: 1.103s, episode steps: 73, steps per second: 66, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.495 [0.390, 0.550], loss: 10.614777, mean_absolute_error: 36.993946, mean_q: 47.878456
[F[K  11608/500000: episode: 205, duration: 0.608s, episode steps: 51, steps per second: 84, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.196 [0.000, 4.000], mean observation: 0.501 [0.470, 0.540], loss: 12.816982, mean_absolute_error: 36.748405, mean_q: 47.718521
[F[K  11652/500000: episode: 206, duration: 0.695s, episode steps: 44, steps per second: 63, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.455 [0.000, 4.000], mean observation: 0.515 [0.470, 0.630], loss: 10.080972, mean_absolute_error: 35.988937, mean_q: 46.585484
[F[K  11705/500000: episode: 207, duration: 0.737s, episode steps: 53, steps per second: 72, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.019 [0.000, 4.000], mean observation: 0.494 [0.380, 0.530], loss: 10.858550, mean_absolute_error: 36.392578, mean_q: 47.244694
[F[K  11741/500000: episode: 208, duration: 0.542s, episode steps: 36, steps per second: 66, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.389 [0.000, 2.000], mean observation: 0.528 [0.470, 0.660], loss: 10.056065, mean_absolute_error: 37.850956, mean_q: 49.060593
[F[K  11806/500000: episode: 209, duration: 1.061s, episode steps: 65, steps per second: 61, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.846 [0.000, 4.000], mean observation: 0.520 [0.460, 0.650], loss: 10.748035, mean_absolute_error: 36.742508, mean_q: 47.767002
[F[K  11859/500000: episode: 210, duration: 0.896s, episode steps: 53, steps per second: 59, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.501 [0.420, 0.600], loss: 11.675161, mean_absolute_error: 36.700626, mean_q: 47.593719
[F[K  11900/500000: episode: 211, duration: 0.670s, episode steps: 41, steps per second: 61, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.049 [0.000, 4.000], mean observation: 0.498 [0.360, 0.610], loss: 11.245312, mean_absolute_error: 36.181316, mean_q: 46.668053
[F[K  11963/500000: episode: 212, duration: 0.997s, episode steps: 63, steps per second: 63, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.698 [0.000, 4.000], mean observation: 0.503 [0.470, 0.550], loss: 11.239332, mean_absolute_error: 37.155033, mean_q: 48.107811
[F[K  12016/500000: episode: 213, duration: 0.768s, episode steps: 53, steps per second: 69, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.698 [0.000, 4.000], mean observation: 0.483 [0.360, 0.530], loss: 9.306911, mean_absolute_error: 35.538864, mean_q: 46.305725
[F[K  12081/500000: episode: 214, duration: 0.981s, episode steps: 65, steps per second: 66, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.662 [0.000, 4.000], mean observation: 0.501 [0.420, 0.600], loss: 11.068356, mean_absolute_error: 36.886089, mean_q: 47.861233
[F[K  12144/500000: episode: 215, duration: 0.913s, episode steps: 63, steps per second: 69, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.762 [0.000, 4.000], mean observation: 0.498 [0.400, 0.580], loss: 9.662146, mean_absolute_error: 37.144718, mean_q: 48.223171
[F[K  12199/500000: episode: 216, duration: 0.813s, episode steps: 55, steps per second: 68, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.327 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 12.236105, mean_absolute_error: 35.766762, mean_q: 46.635254
[F[K  12254/500000: episode: 217, duration: 0.831s, episode steps: 55, steps per second: 66, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.513 [0.470, 0.600], loss: 9.083213, mean_absolute_error: 36.848923, mean_q: 47.985756
[F[K  12307/500000: episode: 218, duration: 0.830s, episode steps: 53, steps per second: 64, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.698 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 11.575805, mean_absolute_error: 36.240265, mean_q: 47.001961
[F[K  12358/500000: episode: 219, duration: 0.861s, episode steps: 51, steps per second: 59, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.020 [0.000, 4.000], mean observation: 0.484 [0.390, 0.520], loss: 11.481876, mean_absolute_error: 36.273331, mean_q: 47.000927
[F[K  12430/500000: episode: 220, duration: 1.130s, episode steps: 72, steps per second: 64, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.736 [0.000, 4.000], mean observation: 0.487 [0.420, 0.520], loss: 8.980906, mean_absolute_error: 36.480373, mean_q: 47.423332
[F[K  12466/500000: episode: 221, duration: 0.584s, episode steps: 36, steps per second: 62, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.444 [0.000, 3.000], mean observation: 0.518 [0.470, 0.630], loss: 11.505902, mean_absolute_error: 35.642311, mean_q: 46.387516
[F[K  12517/500000: episode: 222, duration: 0.879s, episode steps: 51, steps per second: 58, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.039 [0.000, 4.000], mean observation: 0.499 [0.390, 0.610], loss: 11.425856, mean_absolute_error: 35.054710, mean_q: 45.663563
[F[K  12550/500000: episode: 223, duration: 0.494s, episode steps: 33, steps per second: 67, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.513 [0.470, 0.650], loss: 13.510664, mean_absolute_error: 35.229912, mean_q: 45.989288
[F[K  12604/500000: episode: 224, duration: 0.867s, episode steps: 54, steps per second: 62, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.593 [0.000, 4.000], mean observation: 0.500 [0.380, 0.630], loss: 12.811883, mean_absolute_error: 35.851711, mean_q: 46.677914
[F[K  12655/500000: episode: 225, duration: 0.779s, episode steps: 51, steps per second: 66, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.843 [0.000, 4.000], mean observation: 0.481 [0.350, 0.530], loss: 10.345163, mean_absolute_error: 35.036976, mean_q: 45.634621
[F[K  12733/500000: episode: 226, duration: 0.994s, episode steps: 78, steps per second: 78, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.705 [0.000, 4.000], mean observation: 0.499 [0.420, 0.560], loss: 11.155803, mean_absolute_error: 35.491394, mean_q: 46.282314
[F[K  12795/500000: episode: 227, duration: 0.964s, episode steps: 62, steps per second: 64, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.758 [0.000, 4.000], mean observation: 0.503 [0.460, 0.570], loss: 11.289577, mean_absolute_error: 35.441383, mean_q: 46.155247
[F[K  12850/500000: episode: 228, duration: 0.828s, episode steps: 55, steps per second: 66, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.527 [0.000, 4.000], mean observation: 0.491 [0.400, 0.520], loss: 9.182220, mean_absolute_error: 35.618538, mean_q: 46.644863
[F[K  12900/500000: episode: 229, duration: 0.699s, episode steps: 50, steps per second: 72, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.480 [0.000, 4.000], mean observation: 0.505 [0.460, 0.600], loss: 10.813962, mean_absolute_error: 35.143036, mean_q: 45.777435
[F[K  12971/500000: episode: 230, duration: 1.114s, episode steps: 71, steps per second: 64, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.493 [0.000, 4.000], mean observation: 0.502 [0.470, 0.550], loss: 10.282213, mean_absolute_error: 35.203983, mean_q: 46.037907
[F[K  13035/500000: episode: 231, duration: 1.021s, episode steps: 64, steps per second: 63, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.438 [0.000, 4.000], mean observation: 0.512 [0.470, 0.610], loss: 11.076163, mean_absolute_error: 35.258457, mean_q: 45.817158
[F[K  13077/500000: episode: 232, duration: 0.519s, episode steps: 42, steps per second: 81, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.548 [0.000, 4.000], mean observation: 0.475 [0.360, 0.520], loss: 11.396719, mean_absolute_error: 35.867195, mean_q: 46.529076
[F[K  13137/500000: episode: 233, duration: 0.829s, episode steps: 60, steps per second: 72, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.283 [0.000, 4.000], mean observation: 0.496 [0.420, 0.540], loss: 9.974337, mean_absolute_error: 34.597481, mean_q: 45.206837
[F[K  13184/500000: episode: 234, duration: 0.588s, episode steps: 47, steps per second: 80, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.596 [0.000, 4.000], mean observation: 0.495 [0.350, 0.590], loss: 9.568475, mean_absolute_error: 35.222198, mean_q: 46.059959
[F[K  13242/500000: episode: 235, duration: 0.800s, episode steps: 58, steps per second: 73, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.414 [0.000, 4.000], mean observation: 0.487 [0.370, 0.530], loss: 8.964012, mean_absolute_error: 34.966488, mean_q: 45.713013
[F[K  13299/500000: episode: 236, duration: 0.925s, episode steps: 57, steps per second: 62, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 0.499 [0.390, 0.560], loss: 10.087695, mean_absolute_error: 35.517342, mean_q: 46.289734
[F[K  13352/500000: episode: 237, duration: 0.784s, episode steps: 53, steps per second: 68, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.132 [0.000, 4.000], mean observation: 0.496 [0.410, 0.560], loss: 9.997502, mean_absolute_error: 35.495644, mean_q: 46.167076
[F[K  13415/500000: episode: 238, duration: 1.023s, episode steps: 63, steps per second: 62, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.206 [0.000, 4.000], mean observation: 0.497 [0.400, 0.580], loss: 10.212468, mean_absolute_error: 35.250446, mean_q: 45.894516
[F[K  13489/500000: episode: 239, duration: 1.082s, episode steps: 74, steps per second: 68, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.027 [0.000, 4.000], mean observation: 0.502 [0.460, 0.530], loss: 9.612743, mean_absolute_error: 34.862610, mean_q: 45.568516
[F[K  13555/500000: episode: 240, duration: 1.040s, episode steps: 66, steps per second: 63, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.409 [0.000, 4.000], mean observation: 0.499 [0.460, 0.540], loss: 8.637516, mean_absolute_error: 35.013119, mean_q: 45.667446
[F[K  13622/500000: episode: 241, duration: 0.997s, episode steps: 67, steps per second: 67, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.090 [0.000, 4.000], mean observation: 0.501 [0.400, 0.610], loss: 9.660373, mean_absolute_error: 34.330582, mean_q: 44.812008
[F[K  13687/500000: episode: 242, duration: 0.929s, episode steps: 65, steps per second: 70, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.503 [0.420, 0.620], loss: 10.149417, mean_absolute_error: 34.443768, mean_q: 45.033592
[F[K  13742/500000: episode: 243, duration: 0.876s, episode steps: 55, steps per second: 63, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.945 [0.000, 4.000], mean observation: 0.501 [0.420, 0.600], loss: 9.399447, mean_absolute_error: 34.950813, mean_q: 45.507305
[F[K  13796/500000: episode: 244, duration: 0.867s, episode steps: 54, steps per second: 62, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.130 [0.000, 4.000], mean observation: 0.508 [0.470, 0.570], loss: 10.277910, mean_absolute_error: 34.218449, mean_q: 44.787434
[F[K  13859/500000: episode: 245, duration: 0.978s, episode steps: 63, steps per second: 64, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.444 [0.000, 4.000], mean observation: 0.505 [0.460, 0.590], loss: 10.180509, mean_absolute_error: 34.904922, mean_q: 45.582535
[F[K  13927/500000: episode: 246, duration: 0.829s, episode steps: 68, steps per second: 82, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.059 [0.000, 4.000], mean observation: 0.485 [0.380, 0.520], loss: 10.053265, mean_absolute_error: 35.055962, mean_q: 45.633350
[F[K  13977/500000: episode: 247, duration: 0.785s, episode steps: 50, steps per second: 64, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 10.406168, mean_absolute_error: 34.594753, mean_q: 44.887131
[F[K  14033/500000: episode: 248, duration: 0.770s, episode steps: 56, steps per second: 73, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.482 [0.000, 4.000], mean observation: 0.472 [0.350, 0.520], loss: 9.500493, mean_absolute_error: 35.111271, mean_q: 45.681793
[F[K  14101/500000: episode: 249, duration: 0.995s, episode steps: 68, steps per second: 68, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.353 [0.000, 4.000], mean observation: 0.482 [0.370, 0.520], loss: 8.913397, mean_absolute_error: 35.068695, mean_q: 45.751045
[F[K  14155/500000: episode: 250, duration: 0.753s, episode steps: 54, steps per second: 72, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.796 [0.000, 4.000], mean observation: 0.519 [0.470, 0.650], loss: 8.795125, mean_absolute_error: 34.871563, mean_q: 45.345264
[F[K  14232/500000: episode: 251, duration: 1.024s, episode steps: 77, steps per second: 75, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.623 [0.000, 4.000], mean observation: 0.495 [0.460, 0.530], loss: 9.247806, mean_absolute_error: 34.963291, mean_q: 45.485577
[F[K  14291/500000: episode: 252, duration: 0.896s, episode steps: 59, steps per second: 66, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.695 [0.000, 4.000], mean observation: 0.523 [0.480, 0.640], loss: 9.190537, mean_absolute_error: 34.319016, mean_q: 44.912632
[F[K  14370/500000: episode: 253, duration: 0.997s, episode steps: 79, steps per second: 79, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.582 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 9.569471, mean_absolute_error: 34.223381, mean_q: 44.609585
[F[K  14437/500000: episode: 254, duration: 0.941s, episode steps: 67, steps per second: 71, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.582 [0.000, 4.000], mean observation: 0.488 [0.400, 0.520], loss: 10.252965, mean_absolute_error: 34.507126, mean_q: 44.984299
[F[K  14515/500000: episode: 255, duration: 1.064s, episode steps: 78, steps per second: 73, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.494 [0.380, 0.530], loss: 7.904813, mean_absolute_error: 34.513901, mean_q: 45.108284
[F[K  14564/500000: episode: 256, duration: 0.747s, episode steps: 49, steps per second: 66, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.499 [0.470, 0.520], loss: 9.287471, mean_absolute_error: 34.882172, mean_q: 45.531124
[F[K  14611/500000: episode: 257, duration: 0.669s, episode steps: 47, steps per second: 70, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.723 [0.000, 4.000], mean observation: 0.502 [0.360, 0.640], loss: 9.132040, mean_absolute_error: 35.063450, mean_q: 45.940990
[F[K  14717/500000: episode: 258, duration: 1.392s, episode steps: 106, steps per second: 76, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.934 [0.000, 4.000], mean observation: 0.488 [0.430, 0.530], loss: 8.516742, mean_absolute_error: 34.320805, mean_q: 44.823006
[F[K  14780/500000: episode: 259, duration: 0.888s, episode steps: 63, steps per second: 71, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.889 [0.000, 4.000], mean observation: 0.508 [0.450, 0.620], loss: 8.335510, mean_absolute_error: 34.763988, mean_q: 45.383881
[F[K  14843/500000: episode: 260, duration: 0.980s, episode steps: 63, steps per second: 64, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.016 [0.000, 4.000], mean observation: 0.489 [0.430, 0.530], loss: 9.577058, mean_absolute_error: 34.674744, mean_q: 45.231171
[F[K  14903/500000: episode: 261, duration: 0.901s, episode steps: 60, steps per second: 67, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.520 [0.480, 0.610], loss: 9.348718, mean_absolute_error: 34.569225, mean_q: 44.951099
[F[K  14991/500000: episode: 262, duration: 1.299s, episode steps: 88, steps per second: 68, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.170 [0.000, 4.000], mean observation: 0.490 [0.430, 0.520], loss: 9.274580, mean_absolute_error: 33.986790, mean_q: 44.374279
[F[K  15047/500000: episode: 263, duration: 0.908s, episode steps: 56, steps per second: 62, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.554 [0.000, 4.000], mean observation: 0.502 [0.410, 0.620], loss: 9.873003, mean_absolute_error: 34.071663, mean_q: 44.527058
[F[K  15117/500000: episode: 264, duration: 1.011s, episode steps: 70, steps per second: 69, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.686 [0.000, 4.000], mean observation: 0.478 [0.360, 0.510], loss: 8.788398, mean_absolute_error: 34.050369, mean_q: 44.522026
[F[K  15157/500000: episode: 265, duration: 0.620s, episode steps: 40, steps per second: 64, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.225 [0.000, 3.000], mean observation: 0.515 [0.470, 0.630], loss: 8.326979, mean_absolute_error: 34.653233, mean_q: 45.280006
[F[K  15212/500000: episode: 266, duration: 0.939s, episode steps: 55, steps per second: 59, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.545 [0.000, 4.000], mean observation: 0.511 [0.460, 0.650], loss: 9.749373, mean_absolute_error: 34.226933, mean_q: 44.730350
[F[K  15267/500000: episode: 267, duration: 0.864s, episode steps: 55, steps per second: 64, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.514 [0.470, 0.620], loss: 8.657722, mean_absolute_error: 34.210941, mean_q: 44.900906
[F[K  15337/500000: episode: 268, duration: 1.032s, episode steps: 70, steps per second: 68, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.257 [0.000, 4.000], mean observation: 0.493 [0.440, 0.520], loss: 9.161408, mean_absolute_error: 34.048523, mean_q: 44.455101
[F[K  15405/500000: episode: 269, duration: 1.049s, episode steps: 68, steps per second: 65, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.206 [0.000, 4.000], mean observation: 0.500 [0.440, 0.570], loss: 9.074858, mean_absolute_error: 34.764591, mean_q: 45.398468
[F[K  15463/500000: episode: 270, duration: 0.800s, episode steps: 58, steps per second: 73, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.513 [0.470, 0.640], loss: 7.874037, mean_absolute_error: 33.540737, mean_q: 44.047741
[F[K  15527/500000: episode: 271, duration: 0.941s, episode steps: 64, steps per second: 68, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.750 [0.000, 4.000], mean observation: 0.505 [0.440, 0.600], loss: 9.430977, mean_absolute_error: 34.226601, mean_q: 44.688660
[F[K  15588/500000: episode: 272, duration: 0.980s, episode steps: 61, steps per second: 62, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.738 [0.000, 4.000], mean observation: 0.480 [0.390, 0.530], loss: 8.057299, mean_absolute_error: 34.937092, mean_q: 45.513466
[F[K  15643/500000: episode: 273, duration: 0.845s, episode steps: 55, steps per second: 65, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 0.484 [0.400, 0.520], loss: 9.286709, mean_absolute_error: 33.512203, mean_q: 43.849422
[F[K  15699/500000: episode: 274, duration: 0.856s, episode steps: 56, steps per second: 65, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.893 [0.000, 4.000], mean observation: 0.511 [0.470, 0.640], loss: 7.419430, mean_absolute_error: 33.662502, mean_q: 44.013401
[F[K  15763/500000: episode: 275, duration: 1.077s, episode steps: 64, steps per second: 59, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.498 [0.400, 0.560], loss: 8.650535, mean_absolute_error: 34.675789, mean_q: 45.064751
[F[K  15843/500000: episode: 276, duration: 1.373s, episode steps: 80, steps per second: 58, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.538 [0.000, 4.000], mean observation: 0.494 [0.400, 0.530], loss: 8.416812, mean_absolute_error: 34.028893, mean_q: 44.443474
[F[K  15897/500000: episode: 277, duration: 0.807s, episode steps: 54, steps per second: 67, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.489 [0.410, 0.530], loss: 7.820249, mean_absolute_error: 33.598713, mean_q: 43.800114
[F[K  15944/500000: episode: 278, duration: 0.706s, episode steps: 47, steps per second: 67, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.809 [0.000, 4.000], mean observation: 0.488 [0.390, 0.530], loss: 8.162127, mean_absolute_error: 33.831150, mean_q: 44.165123
[F[K  16014/500000: episode: 279, duration: 0.998s, episode steps: 70, steps per second: 70, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.484 [0.360, 0.520], loss: 7.313638, mean_absolute_error: 33.334057, mean_q: 43.581772
[F[K  16079/500000: episode: 280, duration: 0.836s, episode steps: 65, steps per second: 78, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.969 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 8.732512, mean_absolute_error: 34.272778, mean_q: 44.687614
[F[K  16138/500000: episode: 281, duration: 0.786s, episode steps: 59, steps per second: 75, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.576 [0.000, 4.000], mean observation: 0.497 [0.420, 0.530], loss: 8.984245, mean_absolute_error: 33.815926, mean_q: 44.044220
[F[K  16188/500000: episode: 282, duration: 0.715s, episode steps: 50, steps per second: 70, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.160 [0.000, 3.000], mean observation: 0.480 [0.360, 0.520], loss: 7.994260, mean_absolute_error: 34.188507, mean_q: 44.565639
[F[K  16244/500000: episode: 283, duration: 0.736s, episode steps: 56, steps per second: 76, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 8.127403, mean_absolute_error: 34.543591, mean_q: 45.106800
[F[K  16320/500000: episode: 284, duration: 1.005s, episode steps: 76, steps per second: 76, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.737 [0.000, 3.000], mean observation: 0.508 [0.460, 0.610], loss: 9.114458, mean_absolute_error: 34.500015, mean_q: 44.951714
[F[K  16378/500000: episode: 285, duration: 0.736s, episode steps: 58, steps per second: 79, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.526 [0.470, 0.650], loss: 9.168099, mean_absolute_error: 33.890511, mean_q: 44.279404
[F[K  16412/500000: episode: 286, duration: 0.461s, episode steps: 34, steps per second: 74, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.588 [0.000, 3.000], mean observation: 0.527 [0.470, 0.650], loss: 7.597571, mean_absolute_error: 33.843792, mean_q: 44.428444
[F[K  16472/500000: episode: 287, duration: 0.759s, episode steps: 60, steps per second: 79, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.717 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 7.979717, mean_absolute_error: 33.335339, mean_q: 43.758190
[F[K  16531/500000: episode: 288, duration: 0.693s, episode steps: 59, steps per second: 85, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.119 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 9.861001, mean_absolute_error: 33.404968, mean_q: 43.760582
[F[K  16599/500000: episode: 289, duration: 0.903s, episode steps: 68, steps per second: 75, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.750 [0.000, 4.000], mean observation: 0.493 [0.440, 0.520], loss: 8.887856, mean_absolute_error: 33.412151, mean_q: 43.733727
[F[K  16652/500000: episode: 290, duration: 0.644s, episode steps: 53, steps per second: 82, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.486 [0.360, 0.520], loss: 8.143521, mean_absolute_error: 33.801823, mean_q: 44.112709
[F[K  16698/500000: episode: 291, duration: 0.647s, episode steps: 46, steps per second: 71, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.348 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 7.446133, mean_absolute_error: 34.575779, mean_q: 45.127956
[F[K  16752/500000: episode: 292, duration: 0.733s, episode steps: 54, steps per second: 74, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.926 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 8.396613, mean_absolute_error: 34.038017, mean_q: 44.230793
[F[K  16794/500000: episode: 293, duration: 0.682s, episode steps: 42, steps per second: 62, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.493 [0.350, 0.560], loss: 9.096431, mean_absolute_error: 33.278069, mean_q: 43.430046
[F[K  16833/500000: episode: 294, duration: 0.576s, episode steps: 39, steps per second: 68, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.795 [0.000, 4.000], mean observation: 0.502 [0.370, 0.650], loss: 6.870315, mean_absolute_error: 33.617493, mean_q: 43.829144
[F[K  16888/500000: episode: 295, duration: 0.791s, episode steps: 55, steps per second: 70, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.127 [0.000, 4.000], mean observation: 0.501 [0.370, 0.640], loss: 8.579800, mean_absolute_error: 33.179909, mean_q: 43.389149
[F[K  16936/500000: episode: 296, duration: 0.621s, episode steps: 48, steps per second: 77, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.896 [0.000, 4.000], mean observation: 0.493 [0.400, 0.540], loss: 8.197739, mean_absolute_error: 33.903831, mean_q: 44.177460
[F[K  16987/500000: episode: 297, duration: 0.596s, episode steps: 51, steps per second: 86, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.176 [0.000, 4.000], mean observation: 0.492 [0.360, 0.560], loss: 9.299664, mean_absolute_error: 33.677677, mean_q: 43.950848
[F[K  17043/500000: episode: 298, duration: 0.903s, episode steps: 56, steps per second: 62, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.089 [0.000, 4.000], mean observation: 0.511 [0.470, 0.620], loss: 9.270929, mean_absolute_error: 33.698795, mean_q: 43.943035
[F[K  17117/500000: episode: 299, duration: 1.095s, episode steps: 74, steps per second: 68, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.649 [0.000, 4.000], mean observation: 0.504 [0.470, 0.550], loss: 9.936528, mean_absolute_error: 33.364857, mean_q: 43.493935
[F[K  17151/500000: episode: 300, duration: 0.490s, episode steps: 34, steps per second: 69, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.882 [0.000, 2.000], mean observation: 0.518 [0.470, 0.650], loss: 7.793704, mean_absolute_error: 32.764698, mean_q: 42.769836
[F[K  17203/500000: episode: 301, duration: 0.785s, episode steps: 52, steps per second: 66, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.135 [0.000, 4.000], mean observation: 0.521 [0.470, 0.630], loss: 9.464767, mean_absolute_error: 33.757439, mean_q: 44.005882
[F[K  17258/500000: episode: 302, duration: 0.860s, episode steps: 55, steps per second: 64, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.491 [0.000, 4.000], mean observation: 0.507 [0.470, 0.550], loss: 8.556260, mean_absolute_error: 33.501335, mean_q: 43.818867
[F[K  17321/500000: episode: 303, duration: 0.777s, episode steps: 63, steps per second: 81, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.495 [0.440, 0.520], loss: 7.492735, mean_absolute_error: 33.582104, mean_q: 43.850769
[F[K  17363/500000: episode: 304, duration: 0.614s, episode steps: 42, steps per second: 68, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.514 [0.470, 0.640], loss: 8.931476, mean_absolute_error: 32.764893, mean_q: 42.820770
[F[K  17402/500000: episode: 305, duration: 0.553s, episode steps: 39, steps per second: 71, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.744 [0.000, 4.000], mean observation: 0.495 [0.370, 0.590], loss: 8.162155, mean_absolute_error: 33.191818, mean_q: 43.434441
[F[K  17465/500000: episode: 306, duration: 0.886s, episode steps: 63, steps per second: 71, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.206 [0.000, 3.000], mean observation: 0.501 [0.470, 0.560], loss: 8.252432, mean_absolute_error: 33.448353, mean_q: 43.600040
[F[K  17521/500000: episode: 307, duration: 0.806s, episode steps: 56, steps per second: 69, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.589 [0.000, 4.000], mean observation: 0.500 [0.370, 0.620], loss: 7.088480, mean_absolute_error: 33.430779, mean_q: 43.617031
[F[K  17572/500000: episode: 308, duration: 0.752s, episode steps: 51, steps per second: 68, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.392 [0.000, 4.000], mean observation: 0.507 [0.470, 0.600], loss: 10.312772, mean_absolute_error: 33.709156, mean_q: 43.821720
[F[K  17655/500000: episode: 309, duration: 1.095s, episode steps: 83, steps per second: 76, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.735 [0.000, 4.000], mean observation: 0.495 [0.370, 0.580], loss: 8.962375, mean_absolute_error: 32.532440, mean_q: 42.437828
[F[K  17692/500000: episode: 310, duration: 0.455s, episode steps: 37, steps per second: 81, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.243 [0.000, 4.000], mean observation: 0.488 [0.350, 0.540], loss: 9.679993, mean_absolute_error: 32.983940, mean_q: 42.869652
[F[K  17732/500000: episode: 311, duration: 0.582s, episode steps: 40, steps per second: 69, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.492 [0.350, 0.590], loss: 7.594831, mean_absolute_error: 33.369225, mean_q: 43.452362
[F[K  17791/500000: episode: 312, duration: 0.862s, episode steps: 59, steps per second: 68, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.898 [0.000, 4.000], mean observation: 0.492 [0.400, 0.540], loss: 9.227447, mean_absolute_error: 33.444504, mean_q: 43.561420
[F[K  17856/500000: episode: 313, duration: 1.013s, episode steps: 65, steps per second: 64, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.738 [0.000, 4.000], mean observation: 0.491 [0.450, 0.520], loss: 8.224632, mean_absolute_error: 33.704002, mean_q: 43.859844
[F[K  17914/500000: episode: 314, duration: 0.885s, episode steps: 58, steps per second: 66, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 0.501 [0.390, 0.620], loss: 8.195056, mean_absolute_error: 32.422966, mean_q: 42.437695
[F[K  17972/500000: episode: 315, duration: 0.879s, episode steps: 58, steps per second: 66, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.345 [0.000, 4.000], mean observation: 0.493 [0.450, 0.520], loss: 7.819860, mean_absolute_error: 33.349609, mean_q: 43.527943
[F[K  18027/500000: episode: 316, duration: 0.757s, episode steps: 55, steps per second: 73, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.964 [0.000, 4.000], mean observation: 0.514 [0.470, 0.610], loss: 8.511481, mean_absolute_error: 32.916969, mean_q: 42.836254
[F[K  18098/500000: episode: 317, duration: 0.852s, episode steps: 71, steps per second: 83, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.511 [0.470, 0.600], loss: 8.506406, mean_absolute_error: 32.430470, mean_q: 42.357349
[F[K  18151/500000: episode: 318, duration: 0.761s, episode steps: 53, steps per second: 70, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.660 [0.000, 4.000], mean observation: 0.513 [0.470, 0.570], loss: 8.341235, mean_absolute_error: 32.917137, mean_q: 42.898636
[F[K  18192/500000: episode: 319, duration: 0.565s, episode steps: 41, steps per second: 73, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.146 [0.000, 4.000], mean observation: 0.494 [0.430, 0.530], loss: 8.425631, mean_absolute_error: 33.379570, mean_q: 43.463982
[F[K  18253/500000: episode: 320, duration: 0.766s, episode steps: 61, steps per second: 80, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.344 [0.000, 4.000], mean observation: 0.492 [0.460, 0.520], loss: 7.930449, mean_absolute_error: 33.124092, mean_q: 43.159645
[F[K  18304/500000: episode: 321, duration: 0.591s, episode steps: 51, steps per second: 86, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.196 [0.000, 4.000], mean observation: 0.512 [0.470, 0.610], loss: 8.690609, mean_absolute_error: 32.824131, mean_q: 42.898811
[F[K  18354/500000: episode: 322, duration: 0.627s, episode steps: 50, steps per second: 80, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.360 [0.000, 4.000], mean observation: 0.484 [0.370, 0.520], loss: 8.022096, mean_absolute_error: 33.468361, mean_q: 43.527348
[F[K  18405/500000: episode: 323, duration: 0.590s, episode steps: 51, steps per second: 86, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.588 [0.000, 4.000], mean observation: 0.502 [0.460, 0.570], loss: 7.577029, mean_absolute_error: 32.697369, mean_q: 42.674843
[F[K  18438/500000: episode: 324, duration: 0.419s, episode steps: 33, steps per second: 79, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.242 [0.000, 3.000], mean observation: 0.512 [0.470, 0.650], loss: 7.840643, mean_absolute_error: 33.623466, mean_q: 43.683224
[F[K  18514/500000: episode: 325, duration: 0.911s, episode steps: 76, steps per second: 83, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.684 [0.000, 4.000], mean observation: 0.503 [0.460, 0.550], loss: 6.850118, mean_absolute_error: 32.600037, mean_q: 42.609901
[F[K  18596/500000: episode: 326, duration: 0.924s, episode steps: 82, steps per second: 89, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.488 [0.000, 4.000], mean observation: 0.493 [0.410, 0.530], loss: 7.780370, mean_absolute_error: 33.015102, mean_q: 42.981247
[F[K  18660/500000: episode: 327, duration: 0.865s, episode steps: 64, steps per second: 74, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.062 [0.000, 4.000], mean observation: 0.487 [0.400, 0.520], loss: 7.670101, mean_absolute_error: 32.991081, mean_q: 42.973316
[F[K  18714/500000: episode: 328, duration: 0.714s, episode steps: 54, steps per second: 76, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.093 [0.000, 4.000], mean observation: 0.502 [0.400, 0.620], loss: 6.936402, mean_absolute_error: 32.577549, mean_q: 42.677803
[F[K  18773/500000: episode: 329, duration: 0.678s, episode steps: 59, steps per second: 87, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.169 [0.000, 4.000], mean observation: 0.508 [0.470, 0.560], loss: 7.695522, mean_absolute_error: 32.646049, mean_q: 42.650082
[F[K  18853/500000: episode: 330, duration: 0.945s, episode steps: 80, steps per second: 85, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.775 [0.000, 4.000], mean observation: 0.505 [0.470, 0.550], loss: 7.429149, mean_absolute_error: 33.371899, mean_q: 43.491608
[F[K  18916/500000: episode: 331, duration: 0.810s, episode steps: 63, steps per second: 78, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.683 [0.000, 4.000], mean observation: 0.497 [0.410, 0.570], loss: 8.485154, mean_absolute_error: 32.651276, mean_q: 42.544708
[F[K  19007/500000: episode: 332, duration: 1.188s, episode steps: 91, steps per second: 77, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.099 [0.000, 4.000], mean observation: 0.506 [0.470, 0.550], loss: 8.001847, mean_absolute_error: 33.301186, mean_q: 43.348755
[F[K  19066/500000: episode: 333, duration: 0.703s, episode steps: 59, steps per second: 84, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.729 [0.000, 4.000], mean observation: 0.487 [0.370, 0.520], loss: 6.849077, mean_absolute_error: 32.534702, mean_q: 42.210777
[F[K  19122/500000: episode: 334, duration: 0.820s, episode steps: 56, steps per second: 68, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.696 [0.000, 4.000], mean observation: 0.502 [0.450, 0.570], loss: 6.665561, mean_absolute_error: 32.601707, mean_q: 42.495586
[F[K  19186/500000: episode: 335, duration: 0.903s, episode steps: 64, steps per second: 71, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.656 [0.000, 4.000], mean observation: 0.490 [0.430, 0.530], loss: 8.243982, mean_absolute_error: 32.530865, mean_q: 42.422550
[F[K  19238/500000: episode: 336, duration: 0.706s, episode steps: 52, steps per second: 74, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.904 [0.000, 4.000], mean observation: 0.500 [0.440, 0.550], loss: 7.395421, mean_absolute_error: 32.557449, mean_q: 42.561157
[F[K  19305/500000: episode: 337, duration: 1.004s, episode steps: 67, steps per second: 67, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.866 [0.000, 4.000], mean observation: 0.500 [0.360, 0.630], loss: 7.853545, mean_absolute_error: 32.905933, mean_q: 42.862858
[F[K  19357/500000: episode: 338, duration: 0.811s, episode steps: 52, steps per second: 64, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.635 [0.000, 3.000], mean observation: 0.511 [0.470, 0.630], loss: 6.920724, mean_absolute_error: 33.084824, mean_q: 43.085606
[F[K  19412/500000: episode: 339, duration: 0.799s, episode steps: 55, steps per second: 69, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.964 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 8.019837, mean_absolute_error: 32.004208, mean_q: 41.740288
[F[K  19469/500000: episode: 340, duration: 0.792s, episode steps: 57, steps per second: 72, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.421 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 6.056973, mean_absolute_error: 33.220398, mean_q: 43.486748
[F[K  19584/500000: episode: 341, duration: 1.666s, episode steps: 115, steps per second: 69, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.139 [0.000, 4.000], mean observation: 0.474 [0.340, 0.540], loss: 7.710809, mean_absolute_error: 32.997658, mean_q: 43.010864
[F[K  19702/500000: episode: 342, duration: 1.482s, episode steps: 118, steps per second: 80, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.627 [0.000, 4.000], mean observation: 0.489 [0.390, 0.530], loss: 8.341877, mean_absolute_error: 32.838058, mean_q: 42.650272
[F[K  19760/500000: episode: 343, duration: 0.848s, episode steps: 58, steps per second: 68, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.086 [0.000, 4.000], mean observation: 0.509 [0.500, 0.540], loss: 7.062817, mean_absolute_error: 32.862095, mean_q: 42.784000
[F[K  19833/500000: episode: 344, duration: 1.009s, episode steps: 73, steps per second: 72, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.658 [0.000, 4.000], mean observation: 0.497 [0.450, 0.530], loss: 7.404184, mean_absolute_error: 33.147789, mean_q: 43.171646
[F[K  19903/500000: episode: 345, duration: 0.961s, episode steps: 70, steps per second: 73, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.614 [0.000, 4.000], mean observation: 0.508 [0.470, 0.580], loss: 7.750734, mean_absolute_error: 33.162727, mean_q: 43.040161
[F[K  19943/500000: episode: 346, duration: 0.548s, episode steps: 40, steps per second: 73, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.575 [0.000, 4.000], mean observation: 0.494 [0.350, 0.580], loss: 6.691285, mean_absolute_error: 32.872059, mean_q: 42.691772
[F[K  20008/500000: episode: 347, duration: 0.844s, episode steps: 65, steps per second: 77, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.031 [0.000, 4.000], mean observation: 0.490 [0.360, 0.530], loss: 7.792972, mean_absolute_error: 32.770535, mean_q: 42.687531
[F[K  20073/500000: episode: 348, duration: 0.975s, episode steps: 65, steps per second: 67, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.031 [0.000, 4.000], mean observation: 0.512 [0.470, 0.560], loss: 6.558708, mean_absolute_error: 32.079334, mean_q: 41.638325
[F[K  20141/500000: episode: 349, duration: 0.928s, episode steps: 68, steps per second: 73, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.492 [0.370, 0.550], loss: 7.391212, mean_absolute_error: 32.959976, mean_q: 42.936630
[F[K  20228/500000: episode: 350, duration: 1.231s, episode steps: 87, steps per second: 71, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.425 [0.000, 4.000], mean observation: 0.502 [0.410, 0.600], loss: 6.535480, mean_absolute_error: 32.581120, mean_q: 42.407879
[F[K  20278/500000: episode: 351, duration: 0.752s, episode steps: 50, steps per second: 67, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.498 [0.370, 0.610], loss: 6.457025, mean_absolute_error: 32.750458, mean_q: 42.611046
[F[K  20352/500000: episode: 352, duration: 1.133s, episode steps: 74, steps per second: 65, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.507 [0.420, 0.650], loss: 7.431698, mean_absolute_error: 33.676933, mean_q: 43.628624
[F[K  20421/500000: episode: 353, duration: 1.011s, episode steps: 69, steps per second: 68, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.449 [0.000, 4.000], mean observation: 0.507 [0.470, 0.600], loss: 7.290900, mean_absolute_error: 32.449390, mean_q: 42.215519
[F[K  20472/500000: episode: 354, duration: 0.796s, episode steps: 51, steps per second: 64, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.294 [0.000, 4.000], mean observation: 0.487 [0.360, 0.540], loss: 8.410888, mean_absolute_error: 32.723431, mean_q: 42.577606
[F[K  20534/500000: episode: 355, duration: 0.804s, episode steps: 62, steps per second: 77, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.903 [0.000, 4.000], mean observation: 0.488 [0.400, 0.530], loss: 8.332941, mean_absolute_error: 32.871090, mean_q: 42.572170
[F[K  20585/500000: episode: 356, duration: 0.773s, episode steps: 51, steps per second: 66, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.522 [0.470, 0.630], loss: 6.346745, mean_absolute_error: 32.910519, mean_q: 42.673599
[F[K  20669/500000: episode: 357, duration: 1.113s, episode steps: 84, steps per second: 75, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.762 [0.000, 4.000], mean observation: 0.489 [0.400, 0.530], loss: 7.913562, mean_absolute_error: 31.888832, mean_q: 41.459358
[F[K  20729/500000: episode: 358, duration: 1.000s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.600 [0.000, 4.000], mean observation: 0.516 [0.470, 0.640], loss: 7.857033, mean_absolute_error: 32.284473, mean_q: 41.888206
[F[K  20779/500000: episode: 359, duration: 0.757s, episode steps: 50, steps per second: 66, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.680 [0.000, 4.000], mean observation: 0.478 [0.360, 0.520], loss: 6.629811, mean_absolute_error: 31.992176, mean_q: 41.584351
[F[K  20840/500000: episode: 360, duration: 0.864s, episode steps: 61, steps per second: 71, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.607 [0.000, 4.000], mean observation: 0.490 [0.380, 0.520], loss: 8.418349, mean_absolute_error: 32.742229, mean_q: 42.356182
[F[K  20925/500000: episode: 361, duration: 1.288s, episode steps: 85, steps per second: 66, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.965 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 7.193557, mean_absolute_error: 32.461060, mean_q: 42.127480
[F[K  20989/500000: episode: 362, duration: 0.895s, episode steps: 64, steps per second: 72, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.359 [0.000, 4.000], mean observation: 0.492 [0.360, 0.550], loss: 7.129316, mean_absolute_error: 32.874519, mean_q: 42.642365
[F[K  21036/500000: episode: 363, duration: 0.710s, episode steps: 47, steps per second: 66, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.404 [0.000, 4.000], mean observation: 0.491 [0.350, 0.560], loss: 7.847013, mean_absolute_error: 32.120846, mean_q: 41.754002
[F[K  21094/500000: episode: 364, duration: 0.788s, episode steps: 58, steps per second: 74, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.172 [0.000, 4.000], mean observation: 0.499 [0.370, 0.620], loss: 7.035175, mean_absolute_error: 33.225483, mean_q: 43.173592
[F[K  21156/500000: episode: 365, duration: 0.960s, episode steps: 62, steps per second: 65, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.403 [0.000, 4.000], mean observation: 0.503 [0.470, 0.530], loss: 7.327726, mean_absolute_error: 32.845669, mean_q: 42.660717
[F[K  21193/500000: episode: 366, duration: 0.580s, episode steps: 37, steps per second: 64, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.520 [0.470, 0.650], loss: 7.712557, mean_absolute_error: 32.365101, mean_q: 42.113968
[F[K  21291/500000: episode: 367, duration: 1.494s, episode steps: 98, steps per second: 66, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.502 [0.440, 0.580], loss: 6.936472, mean_absolute_error: 32.462902, mean_q: 42.182518
[F[K  21335/500000: episode: 368, duration: 0.768s, episode steps: 44, steps per second: 57, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.455 [0.000, 4.000], mean observation: 0.494 [0.350, 0.610], loss: 6.603716, mean_absolute_error: 32.966915, mean_q: 42.771225
[F[K  21389/500000: episode: 369, duration: 0.891s, episode steps: 54, steps per second: 61, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.796 [0.000, 4.000], mean observation: 0.499 [0.470, 0.520], loss: 8.197611, mean_absolute_error: 32.848312, mean_q: 42.625118
[F[K  21455/500000: episode: 370, duration: 1.079s, episode steps: 66, steps per second: 61, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.909 [0.000, 4.000], mean observation: 0.504 [0.390, 0.620], loss: 8.115799, mean_absolute_error: 32.931404, mean_q: 42.676670
[F[K  21532/500000: episode: 371, duration: 1.131s, episode steps: 77, steps per second: 68, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.477 [0.370, 0.520], loss: 7.797956, mean_absolute_error: 32.570736, mean_q: 42.171234
[F[K  21578/500000: episode: 372, duration: 0.783s, episode steps: 46, steps per second: 59, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.848 [0.000, 4.000], mean observation: 0.477 [0.360, 0.520], loss: 7.402584, mean_absolute_error: 32.820351, mean_q: 42.450199
[F[K  21652/500000: episode: 373, duration: 1.113s, episode steps: 74, steps per second: 66, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.919 [0.000, 4.000], mean observation: 0.482 [0.370, 0.530], loss: 7.579093, mean_absolute_error: 32.704933, mean_q: 42.547688
[F[K  21721/500000: episode: 374, duration: 1.134s, episode steps: 69, steps per second: 61, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.174 [0.000, 4.000], mean observation: 0.509 [0.440, 0.650], loss: 8.415935, mean_absolute_error: 32.170269, mean_q: 41.784798
[F[K  21781/500000: episode: 375, duration: 0.911s, episode steps: 60, steps per second: 66, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.519 [0.470, 0.600], loss: 7.485354, mean_absolute_error: 32.633030, mean_q: 42.378983
[F[K  21860/500000: episode: 376, duration: 1.125s, episode steps: 79, steps per second: 70, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.595 [0.000, 4.000], mean observation: 0.502 [0.440, 0.560], loss: 7.632569, mean_absolute_error: 32.740314, mean_q: 42.560631
[F[K  21920/500000: episode: 377, duration: 0.876s, episode steps: 60, steps per second: 68, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.497 [0.470, 0.520], loss: 7.241534, mean_absolute_error: 32.068432, mean_q: 41.658611
[F[K  21996/500000: episode: 378, duration: 1.200s, episode steps: 76, steps per second: 63, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.671 [0.000, 4.000], mean observation: 0.485 [0.410, 0.520], loss: 6.659503, mean_absolute_error: 32.730316, mean_q: 42.383862
[F[K  22081/500000: episode: 379, duration: 1.313s, episode steps: 85, steps per second: 65, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.035 [0.000, 4.000], mean observation: 0.500 [0.450, 0.530], loss: 7.369949, mean_absolute_error: 32.485870, mean_q: 42.199093
[F[K  22158/500000: episode: 380, duration: 1.282s, episode steps: 77, steps per second: 60, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.675 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 7.049919, mean_absolute_error: 32.144444, mean_q: 41.745193
[F[K  22241/500000: episode: 381, duration: 1.208s, episode steps: 83, steps per second: 69, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.373 [0.000, 4.000], mean observation: 0.486 [0.400, 0.510], loss: 7.621226, mean_absolute_error: 33.021652, mean_q: 42.731007
[F[K  22307/500000: episode: 382, duration: 1.036s, episode steps: 66, steps per second: 64, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.909 [0.000, 4.000], mean observation: 0.488 [0.370, 0.520], loss: 7.481387, mean_absolute_error: 33.006237, mean_q: 42.679199
[F[K  22356/500000: episode: 383, duration: 0.690s, episode steps: 49, steps per second: 71, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.673 [0.000, 4.000], mean observation: 0.498 [0.360, 0.610], loss: 7.156259, mean_absolute_error: 32.450592, mean_q: 42.078926
[F[K  22429/500000: episode: 384, duration: 1.177s, episode steps: 73, steps per second: 62, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.475 [0.360, 0.520], loss: 6.576167, mean_absolute_error: 32.728981, mean_q: 42.485386
[F[K  22465/500000: episode: 385, duration: 0.534s, episode steps: 36, steps per second: 67, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.361 [0.000, 3.000], mean observation: 0.522 [0.470, 0.630], loss: 7.799895, mean_absolute_error: 31.879337, mean_q: 41.519318
[F[K  22538/500000: episode: 386, duration: 1.132s, episode steps: 73, steps per second: 65, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.274 [0.000, 4.000], mean observation: 0.508 [0.470, 0.560], loss: 6.655629, mean_absolute_error: 32.873615, mean_q: 42.635715
[F[K  22608/500000: episode: 387, duration: 1.049s, episode steps: 70, steps per second: 67, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.586 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 6.383691, mean_absolute_error: 33.118839, mean_q: 43.172344
[F[K  22663/500000: episode: 388, duration: 0.942s, episode steps: 55, steps per second: 58, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.506 [0.460, 0.630], loss: 6.539505, mean_absolute_error: 32.531193, mean_q: 42.253174
[F[K  22700/500000: episode: 389, duration: 0.616s, episode steps: 37, steps per second: 60, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.405 [0.000, 3.000], mean observation: 0.521 [0.470, 0.640], loss: 6.364184, mean_absolute_error: 33.671471, mean_q: 43.560780
[F[K  22770/500000: episode: 390, duration: 1.074s, episode steps: 70, steps per second: 65, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.686 [0.000, 4.000], mean observation: 0.502 [0.400, 0.630], loss: 6.847263, mean_absolute_error: 32.794903, mean_q: 42.633213
[F[K  22829/500000: episode: 391, duration: 0.705s, episode steps: 59, steps per second: 84, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.514 [0.470, 0.650], loss: 7.907372, mean_absolute_error: 33.110043, mean_q: 42.908508
[F[K  22912/500000: episode: 392, duration: 1.166s, episode steps: 83, steps per second: 71, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.386 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 7.659021, mean_absolute_error: 32.561386, mean_q: 42.269482
[F[K  22971/500000: episode: 393, duration: 0.948s, episode steps: 59, steps per second: 62, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.288 [0.000, 4.000], mean observation: 0.498 [0.390, 0.590], loss: 7.801937, mean_absolute_error: 32.701675, mean_q: 42.424614
[F[K  23038/500000: episode: 394, duration: 1.103s, episode steps: 67, steps per second: 61, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.164 [0.000, 4.000], mean observation: 0.518 [0.460, 0.610], loss: 6.928407, mean_absolute_error: 32.036064, mean_q: 41.615860
[F[K  23117/500000: episode: 395, duration: 1.156s, episode steps: 79, steps per second: 68, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.975 [0.000, 4.000], mean observation: 0.488 [0.400, 0.530], loss: 6.880970, mean_absolute_error: 32.952980, mean_q: 42.689095
[F[K  23170/500000: episode: 396, duration: 0.716s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.528 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 7.867242, mean_absolute_error: 32.806366, mean_q: 42.366669
[F[K  23252/500000: episode: 397, duration: 1.195s, episode steps: 82, steps per second: 69, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.878 [0.000, 4.000], mean observation: 0.518 [0.460, 0.590], loss: 7.658136, mean_absolute_error: 32.618324, mean_q: 42.168514
[F[K  23337/500000: episode: 398, duration: 1.196s, episode steps: 85, steps per second: 71, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.529 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 6.974502, mean_absolute_error: 32.083096, mean_q: 41.668732
[F[K  23398/500000: episode: 399, duration: 0.826s, episode steps: 61, steps per second: 74, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.311 [0.000, 4.000], mean observation: 0.503 [0.440, 0.600], loss: 6.586282, mean_absolute_error: 32.226475, mean_q: 41.828743
[F[K  23466/500000: episode: 400, duration: 0.871s, episode steps: 68, steps per second: 78, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.000 [0.000, 4.000], mean observation: 0.513 [0.460, 0.610], loss: 6.900424, mean_absolute_error: 32.337402, mean_q: 41.818855
[F[K  23512/500000: episode: 401, duration: 0.667s, episode steps: 46, steps per second: 69, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.348 [0.000, 4.000], mean observation: 0.496 [0.350, 0.600], loss: 6.179203, mean_absolute_error: 32.633228, mean_q: 42.320518
[F[K  23568/500000: episode: 402, duration: 0.827s, episode steps: 56, steps per second: 68, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.321 [0.000, 4.000], mean observation: 0.498 [0.350, 0.620], loss: 7.219209, mean_absolute_error: 32.143291, mean_q: 41.653381
[F[K  23674/500000: episode: 403, duration: 1.359s, episode steps: 106, steps per second: 78, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.494 [0.400, 0.550], loss: 7.060904, mean_absolute_error: 32.646660, mean_q: 42.337730
[F[K  23737/500000: episode: 404, duration: 0.874s, episode steps: 63, steps per second: 72, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.905 [0.000, 4.000], mean observation: 0.491 [0.430, 0.530], loss: 7.333377, mean_absolute_error: 32.086994, mean_q: 41.440571
[F[K  23795/500000: episode: 405, duration: 0.838s, episode steps: 58, steps per second: 69, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.491 [0.400, 0.530], loss: 7.761229, mean_absolute_error: 32.952675, mean_q: 42.599918
[F[K  23864/500000: episode: 406, duration: 0.934s, episode steps: 69, steps per second: 74, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.043 [0.000, 4.000], mean observation: 0.508 [0.480, 0.560], loss: 6.798709, mean_absolute_error: 32.740765, mean_q: 42.486084
[F[K  23936/500000: episode: 407, duration: 1.014s, episode steps: 72, steps per second: 71, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.507 [0.450, 0.630], loss: 7.199377, mean_absolute_error: 32.691772, mean_q: 42.193188
[F[K  24008/500000: episode: 408, duration: 0.852s, episode steps: 72, steps per second: 84, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.736 [0.000, 4.000], mean observation: 0.515 [0.460, 0.600], loss: 7.135081, mean_absolute_error: 32.585918, mean_q: 42.139668
[F[K  24075/500000: episode: 409, duration: 0.686s, episode steps: 67, steps per second: 98, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.254 [0.000, 4.000], mean observation: 0.499 [0.380, 0.580], loss: 7.593466, mean_absolute_error: 32.817196, mean_q: 42.538250
[F[K  24135/500000: episode: 410, duration: 0.749s, episode steps: 60, steps per second: 80, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.567 [0.000, 4.000], mean observation: 0.520 [0.470, 0.640], loss: 7.111509, mean_absolute_error: 32.012493, mean_q: 41.338039
[F[K  24231/500000: episode: 411, duration: 1.273s, episode steps: 96, steps per second: 75, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.427 [0.000, 4.000], mean observation: 0.494 [0.380, 0.540], loss: 7.351185, mean_absolute_error: 32.815716, mean_q: 42.399204
[F[K  24303/500000: episode: 412, duration: 0.804s, episode steps: 72, steps per second: 90, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.639 [0.000, 4.000], mean observation: 0.481 [0.350, 0.510], loss: 6.405286, mean_absolute_error: 32.581039, mean_q: 42.317966
[F[K  24364/500000: episode: 413, duration: 0.809s, episode steps: 61, steps per second: 75, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.459 [0.000, 4.000], mean observation: 0.485 [0.350, 0.510], loss: 7.474352, mean_absolute_error: 32.771725, mean_q: 42.456940
[F[K  24444/500000: episode: 414, duration: 1.051s, episode steps: 80, steps per second: 76, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.562 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 8.145134, mean_absolute_error: 31.878925, mean_q: 41.223030
[F[K  24488/500000: episode: 415, duration: 0.565s, episode steps: 44, steps per second: 78, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 0.511 [0.470, 0.640], loss: 7.063102, mean_absolute_error: 32.488461, mean_q: 42.119553
[F[K  24547/500000: episode: 416, duration: 0.892s, episode steps: 59, steps per second: 66, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.305 [0.000, 4.000], mean observation: 0.495 [0.420, 0.540], loss: 5.500268, mean_absolute_error: 32.761841, mean_q: 42.371426
[F[K  24616/500000: episode: 417, duration: 0.686s, episode steps: 69, steps per second: 101, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.609 [0.000, 4.000], mean observation: 0.504 [0.470, 0.530], loss: 6.398212, mean_absolute_error: 32.672501, mean_q: 42.299976
[F[K  24676/500000: episode: 418, duration: 0.772s, episode steps: 60, steps per second: 78, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.475 [0.360, 0.520], loss: 6.737863, mean_absolute_error: 32.355892, mean_q: 41.953659
[F[K  24736/500000: episode: 419, duration: 0.779s, episode steps: 60, steps per second: 77, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.517 [0.000, 4.000], mean observation: 0.503 [0.400, 0.650], loss: 6.917916, mean_absolute_error: 31.800457, mean_q: 41.435539
[F[K  24800/500000: episode: 420, duration: 0.752s, episode steps: 64, steps per second: 85, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.203 [0.000, 4.000], mean observation: 0.499 [0.460, 0.560], loss: 7.123480, mean_absolute_error: 31.947994, mean_q: 41.566002
[F[K  24861/500000: episode: 421, duration: 0.784s, episode steps: 61, steps per second: 78, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.541 [0.000, 4.000], mean observation: 0.504 [0.440, 0.600], loss: 7.266786, mean_absolute_error: 32.732243, mean_q: 42.398643
[F[K  24912/500000: episode: 422, duration: 0.640s, episode steps: 51, steps per second: 80, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.157 [0.000, 4.000], mean observation: 0.496 [0.370, 0.630], loss: 7.578182, mean_absolute_error: 32.409241, mean_q: 41.834450
[F[K  24970/500000: episode: 423, duration: 0.847s, episode steps: 58, steps per second: 68, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 6.865261, mean_absolute_error: 31.767092, mean_q: 41.061409
[F[K  25024/500000: episode: 424, duration: 0.724s, episode steps: 54, steps per second: 75, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.426 [0.000, 4.000], mean observation: 0.494 [0.360, 0.580], loss: 7.510843, mean_absolute_error: 32.203915, mean_q: 41.549507
[F[K  25072/500000: episode: 425, duration: 0.640s, episode steps: 48, steps per second: 75, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.562 [0.000, 4.000], mean observation: 0.499 [0.360, 0.620], loss: 5.163434, mean_absolute_error: 32.339352, mean_q: 41.861797
[F[K  25127/500000: episode: 426, duration: 0.852s, episode steps: 55, steps per second: 65, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.273 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 6.469337, mean_absolute_error: 32.382278, mean_q: 42.117489
[F[K  25197/500000: episode: 427, duration: 0.996s, episode steps: 70, steps per second: 70, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.100 [0.000, 4.000], mean observation: 0.488 [0.420, 0.530], loss: 7.182830, mean_absolute_error: 31.861601, mean_q: 41.288891
[F[K  25306/500000: episode: 428, duration: 1.511s, episode steps: 109, steps per second: 72, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.413 [0.000, 4.000], mean observation: 0.478 [0.370, 0.520], loss: 6.916543, mean_absolute_error: 32.022324, mean_q: 41.518005
[F[K  25363/500000: episode: 429, duration: 0.786s, episode steps: 57, steps per second: 73, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.035 [0.000, 4.000], mean observation: 0.508 [0.450, 0.640], loss: 6.836374, mean_absolute_error: 32.047489, mean_q: 41.465057
[F[K  25427/500000: episode: 430, duration: 0.879s, episode steps: 64, steps per second: 73, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.094 [0.000, 4.000], mean observation: 0.516 [0.470, 0.600], loss: 7.133834, mean_absolute_error: 32.090702, mean_q: 41.577702
[F[K  25467/500000: episode: 431, duration: 0.603s, episode steps: 40, steps per second: 66, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.450 [0.000, 3.000], mean observation: 0.513 [0.470, 0.660], loss: 6.790252, mean_absolute_error: 31.753674, mean_q: 41.238533
[F[K  25545/500000: episode: 432, duration: 1.121s, episode steps: 78, steps per second: 70, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.487 [0.000, 4.000], mean observation: 0.474 [0.360, 0.520], loss: 8.150845, mean_absolute_error: 32.531166, mean_q: 42.212757
[F[K  25609/500000: episode: 433, duration: 0.877s, episode steps: 64, steps per second: 73, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.490 [0.430, 0.520], loss: 7.379170, mean_absolute_error: 32.736443, mean_q: 42.282921
[F[K  25662/500000: episode: 434, duration: 0.779s, episode steps: 53, steps per second: 68, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.623 [0.000, 4.000], mean observation: 0.504 [0.420, 0.650], loss: 7.816740, mean_absolute_error: 32.697464, mean_q: 42.431133
[F[K  25723/500000: episode: 435, duration: 0.896s, episode steps: 61, steps per second: 68, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.836 [0.000, 4.000], mean observation: 0.495 [0.410, 0.550], loss: 7.253281, mean_absolute_error: 32.158295, mean_q: 41.634586
[F[K  25776/500000: episode: 436, duration: 0.713s, episode steps: 53, steps per second: 74, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.849 [0.000, 4.000], mean observation: 0.495 [0.360, 0.600], loss: 6.875270, mean_absolute_error: 32.257042, mean_q: 41.703419
[F[K  25910/500000: episode: 437, duration: 1.905s, episode steps: 134, steps per second: 70, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.060 [0.000, 4.000], mean observation: 0.512 [0.470, 0.620], loss: 6.522353, mean_absolute_error: 32.034412, mean_q: 41.426189
[F[K  25982/500000: episode: 438, duration: 1.053s, episode steps: 72, steps per second: 68, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.458 [0.000, 4.000], mean observation: 0.499 [0.380, 0.610], loss: 6.798494, mean_absolute_error: 32.276772, mean_q: 41.633125
[F[K  26057/500000: episode: 439, duration: 1.118s, episode steps: 75, steps per second: 67, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.187 [0.000, 4.000], mean observation: 0.482 [0.390, 0.520], loss: 6.808389, mean_absolute_error: 32.579994, mean_q: 42.273659
[F[K  26097/500000: episode: 440, duration: 0.696s, episode steps: 40, steps per second: 57, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.300 [0.000, 3.000], mean observation: 0.502 [0.400, 0.640], loss: 6.654746, mean_absolute_error: 32.285194, mean_q: 41.828674
[F[K  26175/500000: episode: 441, duration: 1.009s, episode steps: 78, steps per second: 77, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.141 [0.000, 4.000], mean observation: 0.502 [0.400, 0.610], loss: 7.279445, mean_absolute_error: 32.050140, mean_q: 41.448406
[F[K  26239/500000: episode: 442, duration: 0.937s, episode steps: 64, steps per second: 68, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.062 [0.000, 4.000], mean observation: 0.481 [0.370, 0.530], loss: 6.838084, mean_absolute_error: 32.282135, mean_q: 41.633347
[F[K  26325/500000: episode: 443, duration: 1.114s, episode steps: 86, steps per second: 77, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.070 [0.000, 4.000], mean observation: 0.511 [0.450, 0.570], loss: 6.988643, mean_absolute_error: 31.931519, mean_q: 41.235737
[F[K  26376/500000: episode: 444, duration: 0.669s, episode steps: 51, steps per second: 76, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.686 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 6.858205, mean_absolute_error: 32.468140, mean_q: 42.054276
[F[K  26445/500000: episode: 445, duration: 0.886s, episode steps: 69, steps per second: 78, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.478 [0.000, 4.000], mean observation: 0.507 [0.450, 0.590], loss: 6.541341, mean_absolute_error: 32.727814, mean_q: 42.407768
[F[K  26586/500000: episode: 446, duration: 1.910s, episode steps: 141, steps per second: 74, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.482 [0.000, 4.000], mean observation: 0.489 [0.410, 0.520], loss: 6.963518, mean_absolute_error: 32.349747, mean_q: 41.829468
[F[K  26624/500000: episode: 447, duration: 0.527s, episode steps: 38, steps per second: 72, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.289 [0.000, 2.000], mean observation: 0.524 [0.470, 0.650], loss: 7.689681, mean_absolute_error: 32.479336, mean_q: 41.935059
[F[K  26664/500000: episode: 448, duration: 0.614s, episode steps: 40, steps per second: 65, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 3.000], mean observation: 0.505 [0.400, 0.650], loss: 7.205657, mean_absolute_error: 32.760479, mean_q: 42.431690
[F[K  26722/500000: episode: 449, duration: 0.869s, episode steps: 58, steps per second: 67, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.491 [0.390, 0.530], loss: 6.744329, mean_absolute_error: 31.887363, mean_q: 41.290348
[F[K  26789/500000: episode: 450, duration: 0.937s, episode steps: 67, steps per second: 72, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.501 [0.470, 0.520], loss: 7.460481, mean_absolute_error: 32.426922, mean_q: 41.875282
[F[K  26852/500000: episode: 451, duration: 0.898s, episode steps: 63, steps per second: 70, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.095 [0.000, 4.000], mean observation: 0.496 [0.460, 0.520], loss: 7.266120, mean_absolute_error: 32.348267, mean_q: 41.823818
[F[K  26922/500000: episode: 452, duration: 1.049s, episode steps: 70, steps per second: 67, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.114 [0.000, 4.000], mean observation: 0.493 [0.410, 0.530], loss: 6.386498, mean_absolute_error: 31.625051, mean_q: 40.977642
[F[K  26961/500000: episode: 453, duration: 0.620s, episode steps: 39, steps per second: 63, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.359 [0.000, 3.000], mean observation: 0.521 [0.470, 0.650], loss: 6.268639, mean_absolute_error: 32.744186, mean_q: 42.441326
[F[K  27041/500000: episode: 454, duration: 0.909s, episode steps: 80, steps per second: 88, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.163 [0.000, 4.000], mean observation: 0.487 [0.420, 0.520], loss: 7.633138, mean_absolute_error: 32.706352, mean_q: 42.151020
[F[K  27114/500000: episode: 455, duration: 1.112s, episode steps: 73, steps per second: 66, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.014 [0.000, 4.000], mean observation: 0.511 [0.470, 0.580], loss: 6.741822, mean_absolute_error: 32.479935, mean_q: 41.991165
[F[K  27175/500000: episode: 456, duration: 0.849s, episode steps: 61, steps per second: 72, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.344 [0.000, 4.000], mean observation: 0.492 [0.360, 0.570], loss: 7.046937, mean_absolute_error: 32.345348, mean_q: 41.762787
[F[K  27260/500000: episode: 457, duration: 1.155s, episode steps: 85, steps per second: 74, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.871 [0.000, 4.000], mean observation: 0.501 [0.430, 0.570], loss: 6.846034, mean_absolute_error: 32.586567, mean_q: 42.094292
[F[K  27367/500000: episode: 458, duration: 1.302s, episode steps: 107, steps per second: 82, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.514 [0.000, 4.000], mean observation: 0.492 [0.370, 0.540], loss: 7.608544, mean_absolute_error: 32.591858, mean_q: 42.069485
[F[K  27425/500000: episode: 459, duration: 0.941s, episode steps: 58, steps per second: 62, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.486 [0.360, 0.520], loss: 6.773897, mean_absolute_error: 32.250183, mean_q: 41.825939
[F[K  27485/500000: episode: 460, duration: 1.034s, episode steps: 60, steps per second: 58, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.533 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 6.520942, mean_absolute_error: 32.830498, mean_q: 42.579403
[F[K  27543/500000: episode: 461, duration: 0.805s, episode steps: 58, steps per second: 72, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.509 [0.460, 0.630], loss: 6.859969, mean_absolute_error: 33.352238, mean_q: 43.047852
[F[K  27622/500000: episode: 462, duration: 1.119s, episode steps: 79, steps per second: 71, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.013 [0.000, 4.000], mean observation: 0.508 [0.440, 0.650], loss: 6.461817, mean_absolute_error: 32.653667, mean_q: 42.146622
[F[K  27709/500000: episode: 463, duration: 1.321s, episode steps: 87, steps per second: 66, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.034 [0.000, 4.000], mean observation: 0.512 [0.460, 0.620], loss: 7.083107, mean_absolute_error: 32.824917, mean_q: 42.312759
[F[K  27816/500000: episode: 464, duration: 1.761s, episode steps: 107, steps per second: 61, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.327 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 6.405225, mean_absolute_error: 32.928276, mean_q: 42.453377
[F[K  27865/500000: episode: 465, duration: 0.724s, episode steps: 49, steps per second: 68, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.484 [0.390, 0.530], loss: 6.215185, mean_absolute_error: 32.687366, mean_q: 42.185944
[F[K  27899/500000: episode: 466, duration: 0.500s, episode steps: 34, steps per second: 68, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.412 [0.000, 4.000], mean observation: 0.510 [0.460, 0.650], loss: 7.120855, mean_absolute_error: 33.239685, mean_q: 42.763191
[F[K  27942/500000: episode: 467, duration: 0.688s, episode steps: 43, steps per second: 62, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.512 [0.000, 3.000], mean observation: 0.504 [0.400, 0.640], loss: 6.393944, mean_absolute_error: 32.115913, mean_q: 41.483658
[F[K  28000/500000: episode: 468, duration: 0.954s, episode steps: 58, steps per second: 61, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.672 [0.000, 4.000], mean observation: 0.495 [0.420, 0.540], loss: 6.591856, mean_absolute_error: 33.323853, mean_q: 43.014442
[F[K  28047/500000: episode: 469, duration: 0.761s, episode steps: 47, steps per second: 62, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.404 [0.000, 4.000], mean observation: 0.503 [0.350, 0.650], loss: 5.953319, mean_absolute_error: 33.086079, mean_q: 42.781128
[F[K  28124/500000: episode: 470, duration: 1.094s, episode steps: 77, steps per second: 70, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.455 [0.000, 4.000], mean observation: 0.486 [0.370, 0.520], loss: 6.184224, mean_absolute_error: 33.205215, mean_q: 42.955345
[F[K  28175/500000: episode: 471, duration: 0.750s, episode steps: 51, steps per second: 68, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.961 [0.000, 4.000], mean observation: 0.511 [0.470, 0.650], loss: 6.930194, mean_absolute_error: 32.828152, mean_q: 42.526417
[F[K  28248/500000: episode: 472, duration: 1.134s, episode steps: 73, steps per second: 64, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 7.071003, mean_absolute_error: 33.039268, mean_q: 42.459099
[F[K  28318/500000: episode: 473, duration: 1.001s, episode steps: 70, steps per second: 70, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.504 [0.430, 0.580], loss: 7.125305, mean_absolute_error: 32.301392, mean_q: 41.821655
[F[K  28381/500000: episode: 474, duration: 1.037s, episode steps: 63, steps per second: 61, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.937 [0.000, 4.000], mean observation: 0.480 [0.380, 0.530], loss: 6.914865, mean_absolute_error: 33.504250, mean_q: 43.327217
[F[K  28466/500000: episode: 475, duration: 1.357s, episode steps: 85, steps per second: 63, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.812 [0.000, 4.000], mean observation: 0.524 [0.470, 0.620], loss: 6.501381, mean_absolute_error: 32.839207, mean_q: 42.450836
[F[K  28524/500000: episode: 476, duration: 1.004s, episode steps: 58, steps per second: 58, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.362 [0.000, 4.000], mean observation: 0.512 [0.470, 0.610], loss: 7.034398, mean_absolute_error: 32.811207, mean_q: 42.447517
[F[K  28623/500000: episode: 477, duration: 1.615s, episode steps: 99, steps per second: 61, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.394 [0.000, 4.000], mean observation: 0.486 [0.370, 0.520], loss: 6.606063, mean_absolute_error: 33.183952, mean_q: 42.866661
[F[K  28692/500000: episode: 478, duration: 1.289s, episode steps: 69, steps per second: 54, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.507 [0.000, 4.000], mean observation: 0.520 [0.480, 0.630], loss: 6.880584, mean_absolute_error: 32.303410, mean_q: 41.663265
[F[K  28747/500000: episode: 479, duration: 0.873s, episode steps: 55, steps per second: 63, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 7.409672, mean_absolute_error: 32.293892, mean_q: 41.754101
[F[K  28812/500000: episode: 480, duration: 0.847s, episode steps: 65, steps per second: 77, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.569 [0.000, 4.000], mean observation: 0.499 [0.470, 0.530], loss: 7.050456, mean_absolute_error: 33.124565, mean_q: 42.768669
[F[K  28863/500000: episode: 481, duration: 0.699s, episode steps: 51, steps per second: 73, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.078 [0.000, 4.000], mean observation: 0.513 [0.490, 0.560], loss: 7.021407, mean_absolute_error: 32.055683, mean_q: 41.484322
[F[K  28917/500000: episode: 482, duration: 0.809s, episode steps: 54, steps per second: 67, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 6.371143, mean_absolute_error: 32.669590, mean_q: 42.198025
[F[K  28995/500000: episode: 483, duration: 1.397s, episode steps: 78, steps per second: 56, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.692 [0.000, 4.000], mean observation: 0.505 [0.470, 0.540], loss: 6.563138, mean_absolute_error: 32.490601, mean_q: 41.923702
[F[K  29062/500000: episode: 484, duration: 1.124s, episode steps: 67, steps per second: 60, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.746 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 6.583462, mean_absolute_error: 32.547321, mean_q: 42.040092
[F[K  29120/500000: episode: 485, duration: 1.050s, episode steps: 58, steps per second: 55, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.569 [0.000, 4.000], mean observation: 0.492 [0.450, 0.520], loss: 7.052084, mean_absolute_error: 33.159637, mean_q: 42.800686
[F[K  29184/500000: episode: 486, duration: 1.068s, episode steps: 64, steps per second: 60, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.507 [0.470, 0.560], loss: 7.171471, mean_absolute_error: 32.728973, mean_q: 42.120422
[F[K  29264/500000: episode: 487, duration: 1.250s, episode steps: 80, steps per second: 64, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.587 [0.000, 4.000], mean observation: 0.489 [0.410, 0.530], loss: 6.839220, mean_absolute_error: 32.127903, mean_q: 41.489368
[F[K  29348/500000: episode: 488, duration: 1.256s, episode steps: 84, steps per second: 67, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.486 [0.400, 0.530], loss: 7.213036, mean_absolute_error: 32.742577, mean_q: 42.187626
[F[K  29423/500000: episode: 489, duration: 1.347s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.560 [0.000, 4.000], mean observation: 0.476 [0.360, 0.510], loss: 6.820023, mean_absolute_error: 32.628414, mean_q: 42.222980
[F[K  29491/500000: episode: 490, duration: 1.088s, episode steps: 68, steps per second: 62, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.779 [0.000, 4.000], mean observation: 0.520 [0.470, 0.620], loss: 6.814783, mean_absolute_error: 32.067253, mean_q: 41.429970
[F[K  29542/500000: episode: 491, duration: 0.856s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.686 [0.000, 4.000], mean observation: 0.512 [0.470, 0.630], loss: 6.228592, mean_absolute_error: 32.891220, mean_q: 42.298229
[F[K  29622/500000: episode: 492, duration: 1.360s, episode steps: 80, steps per second: 59, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.498 [0.430, 0.550], loss: 7.472673, mean_absolute_error: 32.625587, mean_q: 42.011555
[F[K  29661/500000: episode: 493, duration: 0.754s, episode steps: 39, steps per second: 52, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.462 [0.000, 4.000], mean observation: 0.499 [0.370, 0.620], loss: 5.882514, mean_absolute_error: 32.479050, mean_q: 42.021015
[F[K  29719/500000: episode: 494, duration: 1.100s, episode steps: 58, steps per second: 53, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.948 [0.000, 4.000], mean observation: 0.499 [0.430, 0.530], loss: 5.888043, mean_absolute_error: 32.892918, mean_q: 42.595417
[F[K  29794/500000: episode: 495, duration: 1.342s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.488 [0.370, 0.520], loss: 6.650421, mean_absolute_error: 32.232388, mean_q: 41.530327
[F[K  29861/500000: episode: 496, duration: 1.331s, episode steps: 67, steps per second: 50, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.433 [0.000, 4.000], mean observation: 0.492 [0.380, 0.530], loss: 6.944656, mean_absolute_error: 32.798462, mean_q: 42.218548
[F[K  29925/500000: episode: 497, duration: 1.172s, episode steps: 64, steps per second: 55, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.499 [0.390, 0.570], loss: 6.777335, mean_absolute_error: 32.852482, mean_q: 42.334747
[F[K  29998/500000: episode: 498, duration: 1.386s, episode steps: 73, steps per second: 53, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.616 [0.000, 4.000], mean observation: 0.496 [0.390, 0.580], loss: 7.756411, mean_absolute_error: 32.662800, mean_q: 42.040367
[F[K  30047/500000: episode: 499, duration: 0.763s, episode steps: 49, steps per second: 64, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.020 [0.000, 4.000], mean observation: 0.501 [0.350, 0.650], loss: 6.790166, mean_absolute_error: 32.998993, mean_q: 42.514214
[F[K  30105/500000: episode: 500, duration: 1.018s, episode steps: 58, steps per second: 57, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.138 [0.000, 4.000], mean observation: 0.491 [0.440, 0.530], loss: 7.239130, mean_absolute_error: 32.967495, mean_q: 42.570648
[F[K  30167/500000: episode: 501, duration: 1.039s, episode steps: 62, steps per second: 60, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.677 [0.000, 4.000], mean observation: 0.476 [0.360, 0.510], loss: 7.622503, mean_absolute_error: 32.706734, mean_q: 42.090492
[F[K  30225/500000: episode: 502, duration: 1.074s, episode steps: 58, steps per second: 54, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 0.513 [0.500, 0.580], loss: 6.330790, mean_absolute_error: 32.496162, mean_q: 42.049610
[F[K  30279/500000: episode: 503, duration: 0.866s, episode steps: 54, steps per second: 62, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.519 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 7.383067, mean_absolute_error: 32.859203, mean_q: 42.291828
[F[K  30374/500000: episode: 504, duration: 1.468s, episode steps: 95, steps per second: 65, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.842 [0.000, 4.000], mean observation: 0.510 [0.460, 0.600], loss: 7.229906, mean_absolute_error: 32.313499, mean_q: 41.923317
[F[K  30412/500000: episode: 505, duration: 0.680s, episode steps: 38, steps per second: 56, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.842 [0.000, 4.000], mean observation: 0.516 [0.470, 0.640], loss: 7.505024, mean_absolute_error: 32.993652, mean_q: 42.480751
[F[K  30477/500000: episode: 506, duration: 1.127s, episode steps: 65, steps per second: 58, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.477 [0.000, 4.000], mean observation: 0.516 [0.480, 0.620], loss: 7.828338, mean_absolute_error: 31.882936, mean_q: 41.147583
[F[K  30544/500000: episode: 507, duration: 1.222s, episode steps: 67, steps per second: 55, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.119 [0.000, 4.000], mean observation: 0.489 [0.420, 0.520], loss: 6.285261, mean_absolute_error: 32.093487, mean_q: 41.577549
[F[K  30619/500000: episode: 508, duration: 1.245s, episode steps: 75, steps per second: 60, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.693 [0.000, 4.000], mean observation: 0.487 [0.370, 0.530], loss: 7.205559, mean_absolute_error: 32.797363, mean_q: 42.363224
[F[K  30677/500000: episode: 509, duration: 0.924s, episode steps: 58, steps per second: 63, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.638 [0.000, 4.000], mean observation: 0.502 [0.440, 0.590], loss: 7.213269, mean_absolute_error: 32.393219, mean_q: 41.650478
[F[K  30740/500000: episode: 510, duration: 0.952s, episode steps: 63, steps per second: 66, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.889 [0.000, 4.000], mean observation: 0.493 [0.390, 0.530], loss: 7.691432, mean_absolute_error: 31.892853, mean_q: 41.213219
[F[K  30812/500000: episode: 511, duration: 1.042s, episode steps: 72, steps per second: 69, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.944 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 6.500491, mean_absolute_error: 32.066669, mean_q: 41.469299
[F[K  30872/500000: episode: 512, duration: 0.986s, episode steps: 60, steps per second: 61, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.533 [0.000, 4.000], mean observation: 0.493 [0.450, 0.520], loss: 6.204349, mean_absolute_error: 32.738659, mean_q: 42.366463
[F[K  30931/500000: episode: 513, duration: 1.001s, episode steps: 59, steps per second: 59, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.661 [0.000, 4.000], mean observation: 0.493 [0.440, 0.530], loss: 7.032190, mean_absolute_error: 32.894398, mean_q: 42.460743
[F[K  30986/500000: episode: 514, duration: 0.739s, episode steps: 55, steps per second: 74, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.509 [0.000, 4.000], mean observation: 0.518 [0.470, 0.600], loss: 6.250786, mean_absolute_error: 33.309593, mean_q: 43.026131
[F[K  31019/500000: episode: 515, duration: 0.602s, episode steps: 33, steps per second: 55, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.515 [0.000, 4.000], mean observation: 0.529 [0.470, 0.650], loss: 7.877086, mean_absolute_error: 32.021584, mean_q: 41.246567
[F[K  31081/500000: episode: 516, duration: 0.976s, episode steps: 62, steps per second: 64, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.774 [0.000, 4.000], mean observation: 0.485 [0.370, 0.520], loss: 7.186603, mean_absolute_error: 32.131084, mean_q: 41.482193
[F[K  31138/500000: episode: 517, duration: 0.898s, episode steps: 57, steps per second: 63, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.140 [0.000, 4.000], mean observation: 0.486 [0.380, 0.520], loss: 6.781368, mean_absolute_error: 32.472576, mean_q: 41.909973
[F[K  31216/500000: episode: 518, duration: 1.224s, episode steps: 78, steps per second: 64, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.603 [0.000, 4.000], mean observation: 0.509 [0.440, 0.650], loss: 7.507965, mean_absolute_error: 32.771198, mean_q: 42.231007
[F[K  31286/500000: episode: 519, duration: 1.105s, episode steps: 70, steps per second: 63, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.943 [0.000, 4.000], mean observation: 0.498 [0.430, 0.530], loss: 6.489201, mean_absolute_error: 33.289795, mean_q: 43.008198
[F[K  31357/500000: episode: 520, duration: 1.144s, episode steps: 71, steps per second: 62, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.451 [0.000, 4.000], mean observation: 0.502 [0.450, 0.580], loss: 5.852083, mean_absolute_error: 32.724113, mean_q: 42.271198
[F[K  31426/500000: episode: 521, duration: 1.102s, episode steps: 69, steps per second: 63, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.246 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 7.528317, mean_absolute_error: 32.505936, mean_q: 41.860355
[F[K  31502/500000: episode: 522, duration: 1.490s, episode steps: 76, steps per second: 51, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 3.053 [0.000, 4.000], mean observation: 0.516 [0.460, 0.590], loss: 7.075277, mean_absolute_error: 32.750626, mean_q: 42.228481
[F[K  31574/500000: episode: 523, duration: 1.008s, episode steps: 72, steps per second: 71, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.505 [0.470, 0.580], loss: 7.202960, mean_absolute_error: 32.359329, mean_q: 41.735733
[F[K  31631/500000: episode: 524, duration: 0.928s, episode steps: 57, steps per second: 61, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.484 [0.360, 0.510], loss: 6.955624, mean_absolute_error: 32.993778, mean_q: 42.607113
[F[K  31690/500000: episode: 525, duration: 0.869s, episode steps: 59, steps per second: 68, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.492 [0.000, 4.000], mean observation: 0.498 [0.400, 0.590], loss: 7.242697, mean_absolute_error: 31.921137, mean_q: 41.273640
[F[K  31741/500000: episode: 526, duration: 0.783s, episode steps: 51, steps per second: 65, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.784 [0.000, 4.000], mean observation: 0.495 [0.370, 0.580], loss: 7.688806, mean_absolute_error: 32.550648, mean_q: 41.992077
[F[K  31795/500000: episode: 527, duration: 1.022s, episode steps: 54, steps per second: 53, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.574 [0.000, 4.000], mean observation: 0.482 [0.360, 0.510], loss: 7.009366, mean_absolute_error: 32.828785, mean_q: 42.303638
[F[K  31859/500000: episode: 528, duration: 1.146s, episode steps: 64, steps per second: 56, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.609 [0.000, 4.000], mean observation: 0.505 [0.420, 0.650], loss: 6.799933, mean_absolute_error: 32.636822, mean_q: 42.217583
[F[K  31905/500000: episode: 529, duration: 0.747s, episode steps: 46, steps per second: 62, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.457 [0.000, 4.000], mean observation: 0.501 [0.350, 0.630], loss: 6.280911, mean_absolute_error: 32.676319, mean_q: 42.160465
[F[K  31983/500000: episode: 530, duration: 1.276s, episode steps: 78, steps per second: 61, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.769 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 6.727055, mean_absolute_error: 32.862381, mean_q: 42.315544
[F[K  32042/500000: episode: 531, duration: 1.014s, episode steps: 59, steps per second: 58, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.983 [0.000, 4.000], mean observation: 0.513 [0.470, 0.580], loss: 6.421870, mean_absolute_error: 32.466911, mean_q: 41.938358
[F[K  32150/500000: episode: 532, duration: 1.891s, episode steps: 108, steps per second: 57, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.398 [0.000, 4.000], mean observation: 0.493 [0.410, 0.560], loss: 6.569229, mean_absolute_error: 33.053864, mean_q: 42.597004
[F[K  32213/500000: episode: 533, duration: 1.036s, episode steps: 63, steps per second: 61, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.762 [0.000, 4.000], mean observation: 0.478 [0.370, 0.530], loss: 6.299146, mean_absolute_error: 32.428265, mean_q: 41.759926
[F[K  32281/500000: episode: 534, duration: 1.161s, episode steps: 68, steps per second: 59, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.265 [0.000, 4.000], mean observation: 0.503 [0.480, 0.530], loss: 7.251789, mean_absolute_error: 32.415020, mean_q: 41.763683
[F[K  32340/500000: episode: 535, duration: 1.001s, episode steps: 59, steps per second: 59, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.502 [0.410, 0.600], loss: 6.992939, mean_absolute_error: 33.366150, mean_q: 42.962124
[F[K  32375/500000: episode: 536, duration: 0.525s, episode steps: 35, steps per second: 67, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.524 [0.470, 0.640], loss: 7.689710, mean_absolute_error: 31.322670, mean_q: 40.559658
[F[K  32442/500000: episode: 537, duration: 1.071s, episode steps: 67, steps per second: 63, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.687 [0.000, 4.000], mean observation: 0.484 [0.370, 0.530], loss: 7.142883, mean_absolute_error: 31.910496, mean_q: 41.068150
[F[K  32534/500000: episode: 538, duration: 1.408s, episode steps: 92, steps per second: 65, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.293 [0.000, 4.000], mean observation: 0.503 [0.420, 0.590], loss: 6.669806, mean_absolute_error: 33.021732, mean_q: 42.623291
[F[K  32625/500000: episode: 539, duration: 1.544s, episode steps: 91, steps per second: 59, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.758 [0.000, 4.000], mean observation: 0.507 [0.460, 0.590], loss: 7.801382, mean_absolute_error: 32.411324, mean_q: 41.798649
[F[K  32672/500000: episode: 540, duration: 0.685s, episode steps: 47, steps per second: 69, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.340 [0.000, 4.000], mean observation: 0.504 [0.390, 0.640], loss: 6.873294, mean_absolute_error: 32.419682, mean_q: 41.696819
[F[K  32747/500000: episode: 541, duration: 1.322s, episode steps: 75, steps per second: 57, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.680 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 6.941433, mean_absolute_error: 32.226154, mean_q: 41.617939
[F[K  32807/500000: episode: 542, duration: 1.153s, episode steps: 60, steps per second: 52, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.067 [0.000, 4.000], mean observation: 0.519 [0.490, 0.600], loss: 7.840165, mean_absolute_error: 32.428143, mean_q: 41.780399
[F[K  32863/500000: episode: 543, duration: 0.900s, episode steps: 56, steps per second: 62, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.839 [0.000, 4.000], mean observation: 0.501 [0.360, 0.640], loss: 6.645829, mean_absolute_error: 32.788872, mean_q: 42.379963
[F[K  32912/500000: episode: 544, duration: 0.875s, episode steps: 49, steps per second: 56, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.776 [0.000, 4.000], mean observation: 0.510 [0.470, 0.570], loss: 6.071425, mean_absolute_error: 32.433929, mean_q: 41.928272
[F[K  32993/500000: episode: 545, duration: 1.360s, episode steps: 81, steps per second: 60, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.951 [0.000, 4.000], mean observation: 0.489 [0.410, 0.530], loss: 7.502013, mean_absolute_error: 32.763779, mean_q: 42.171520
[F[K  33049/500000: episode: 546, duration: 0.746s, episode steps: 56, steps per second: 75, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.857 [0.000, 4.000], mean observation: 0.504 [0.370, 0.650], loss: 7.605680, mean_absolute_error: 32.275661, mean_q: 41.739159
[F[K  33134/500000: episode: 547, duration: 1.170s, episode steps: 85, steps per second: 73, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.506 [0.000, 4.000], mean observation: 0.503 [0.470, 0.540], loss: 7.429387, mean_absolute_error: 32.767830, mean_q: 42.312984
[F[K  33206/500000: episode: 548, duration: 0.993s, episode steps: 72, steps per second: 73, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.361 [0.000, 4.000], mean observation: 0.497 [0.380, 0.570], loss: 6.390696, mean_absolute_error: 32.367622, mean_q: 41.737148
[F[K  33263/500000: episode: 549, duration: 0.776s, episode steps: 57, steps per second: 73, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.421 [0.000, 4.000], mean observation: 0.513 [0.470, 0.630], loss: 6.403009, mean_absolute_error: 33.136429, mean_q: 42.613800
[F[K  33318/500000: episode: 550, duration: 0.836s, episode steps: 55, steps per second: 66, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.545 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 7.452799, mean_absolute_error: 32.492519, mean_q: 41.844833
[F[K  33381/500000: episode: 551, duration: 0.990s, episode steps: 63, steps per second: 64, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.111 [0.000, 4.000], mean observation: 0.502 [0.360, 0.660], loss: 6.891512, mean_absolute_error: 32.216934, mean_q: 41.601982
[F[K  33421/500000: episode: 552, duration: 0.727s, episode steps: 40, steps per second: 55, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.425 [0.000, 3.000], mean observation: 0.505 [0.410, 0.660], loss: 8.387975, mean_absolute_error: 32.451103, mean_q: 41.752865
[F[K  33492/500000: episode: 553, duration: 1.069s, episode steps: 71, steps per second: 66, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.634 [0.000, 4.000], mean observation: 0.511 [0.480, 0.600], loss: 7.227019, mean_absolute_error: 32.967079, mean_q: 42.441807
[F[K  33568/500000: episode: 554, duration: 1.110s, episode steps: 76, steps per second: 68, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.395 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 6.899919, mean_absolute_error: 32.168354, mean_q: 41.453548
[F[K  33621/500000: episode: 555, duration: 0.710s, episode steps: 53, steps per second: 75, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.151 [0.000, 4.000], mean observation: 0.512 [0.470, 0.650], loss: 7.361931, mean_absolute_error: 33.220669, mean_q: 42.764278
[F[K  33705/500000: episode: 556, duration: 1.267s, episode steps: 84, steps per second: 66, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.499 [0.410, 0.550], loss: 7.079241, mean_absolute_error: 32.374413, mean_q: 41.746380
[F[K  33764/500000: episode: 557, duration: 0.950s, episode steps: 59, steps per second: 62, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.831 [0.000, 4.000], mean observation: 0.500 [0.370, 0.620], loss: 6.527539, mean_absolute_error: 32.182201, mean_q: 41.623020
[F[K  33847/500000: episode: 558, duration: 1.253s, episode steps: 83, steps per second: 66, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.024 [0.000, 4.000], mean observation: 0.490 [0.390, 0.530], loss: 6.409442, mean_absolute_error: 32.388641, mean_q: 41.776108
[F[K  33908/500000: episode: 559, duration: 1.006s, episode steps: 61, steps per second: 61, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.197 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 8.070268, mean_absolute_error: 32.353436, mean_q: 41.762360
[F[K  34072/500000: episode: 560, duration: 2.722s, episode steps: 164, steps per second: 60, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.707 [0.000, 4.000], mean observation: 0.491 [0.420, 0.520], loss: 6.967979, mean_absolute_error: 32.528046, mean_q: 42.045471
[F[K  34124/500000: episode: 561, duration: 0.823s, episode steps: 52, steps per second: 63, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.404 [0.000, 4.000], mean observation: 0.474 [0.350, 0.530], loss: 6.327648, mean_absolute_error: 32.979588, mean_q: 42.566460
[F[K  34194/500000: episode: 562, duration: 1.119s, episode steps: 70, steps per second: 63, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.929 [0.000, 4.000], mean observation: 0.499 [0.420, 0.560], loss: 6.888008, mean_absolute_error: 32.618614, mean_q: 42.119194
[F[K  34313/500000: episode: 563, duration: 1.690s, episode steps: 119, steps per second: 70, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.008 [0.000, 4.000], mean observation: 0.490 [0.390, 0.530], loss: 6.516085, mean_absolute_error: 32.829739, mean_q: 42.354156
[F[K  34415/500000: episode: 564, duration: 1.511s, episode steps: 102, steps per second: 67, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.255 [0.000, 4.000], mean observation: 0.509 [0.480, 0.560], loss: 6.610120, mean_absolute_error: 33.322445, mean_q: 42.996109
[F[K  34480/500000: episode: 565, duration: 0.966s, episode steps: 65, steps per second: 67, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.508 [0.460, 0.620], loss: 7.864338, mean_absolute_error: 33.354103, mean_q: 42.903534
[F[K  34539/500000: episode: 566, duration: 0.861s, episode steps: 59, steps per second: 69, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.509 [0.490, 0.570], loss: 7.032303, mean_absolute_error: 32.135921, mean_q: 41.482697
[F[K  34588/500000: episode: 567, duration: 0.918s, episode steps: 49, steps per second: 53, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.653 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 6.821255, mean_absolute_error: 33.036404, mean_q: 42.461948
[F[K  34650/500000: episode: 568, duration: 0.968s, episode steps: 62, steps per second: 64, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.419 [0.000, 4.000], mean observation: 0.511 [0.470, 0.620], loss: 7.093364, mean_absolute_error: 33.794254, mean_q: 43.458687
[F[K  34687/500000: episode: 569, duration: 0.499s, episode steps: 37, steps per second: 74, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.405 [0.000, 4.000], mean observation: 0.515 [0.470, 0.650], loss: 6.902080, mean_absolute_error: 32.120781, mean_q: 41.477112
[F[K  34740/500000: episode: 570, duration: 0.759s, episode steps: 53, steps per second: 70, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.566 [0.000, 4.000], mean observation: 0.503 [0.470, 0.520], loss: 7.369252, mean_absolute_error: 32.927723, mean_q: 42.329407
[F[K  34809/500000: episode: 571, duration: 1.110s, episode steps: 69, steps per second: 62, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.203 [0.000, 4.000], mean observation: 0.502 [0.480, 0.530], loss: 6.867334, mean_absolute_error: 32.390301, mean_q: 41.630848
[F[K  34879/500000: episode: 572, duration: 1.231s, episode steps: 70, steps per second: 57, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.114 [0.000, 4.000], mean observation: 0.506 [0.440, 0.620], loss: 6.124451, mean_absolute_error: 32.736649, mean_q: 42.312763
[F[K  34968/500000: episode: 573, duration: 1.180s, episode steps: 89, steps per second: 75, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.427 [0.000, 4.000], mean observation: 0.497 [0.360, 0.600], loss: 6.961177, mean_absolute_error: 33.159149, mean_q: 42.750225
[F[K  35041/500000: episode: 574, duration: 1.115s, episode steps: 73, steps per second: 65, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.493 [0.000, 4.000], mean observation: 0.510 [0.470, 0.600], loss: 7.522821, mean_absolute_error: 33.226574, mean_q: 42.785927
[F[K  35125/500000: episode: 575, duration: 1.595s, episode steps: 84, steps per second: 53, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.485 [0.370, 0.530], loss: 7.212514, mean_absolute_error: 32.278000, mean_q: 41.683392
[F[K  35204/500000: episode: 576, duration: 1.363s, episode steps: 79, steps per second: 58, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.177 [0.000, 4.000], mean observation: 0.494 [0.390, 0.530], loss: 6.137192, mean_absolute_error: 33.350609, mean_q: 43.060154
[F[K  35256/500000: episode: 577, duration: 0.840s, episode steps: 52, steps per second: 62, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.077 [0.000, 4.000], mean observation: 0.477 [0.360, 0.510], loss: 8.305326, mean_absolute_error: 32.980511, mean_q: 42.442505
[F[K  35329/500000: episode: 578, duration: 1.176s, episode steps: 73, steps per second: 62, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.890 [0.000, 4.000], mean observation: 0.518 [0.490, 0.640], loss: 6.923439, mean_absolute_error: 33.489120, mean_q: 43.101501
[F[K  35410/500000: episode: 579, duration: 1.259s, episode steps: 81, steps per second: 64, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.210 [0.000, 4.000], mean observation: 0.499 [0.410, 0.560], loss: 7.154980, mean_absolute_error: 32.903236, mean_q: 42.461842
[F[K  35449/500000: episode: 580, duration: 0.711s, episode steps: 39, steps per second: 55, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.692 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 7.048663, mean_absolute_error: 33.143913, mean_q: 42.715046
[F[K  35483/500000: episode: 581, duration: 0.633s, episode steps: 34, steps per second: 54, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.706 [0.000, 4.000], mean observation: 0.525 [0.470, 0.630], loss: 7.225416, mean_absolute_error: 32.564301, mean_q: 41.949944
[F[K  35596/500000: episode: 582, duration: 2.253s, episode steps: 113, steps per second: 50, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.027 [0.000, 4.000], mean observation: 0.507 [0.470, 0.550], loss: 6.808949, mean_absolute_error: 32.558941, mean_q: 41.991905
[F[K  35716/500000: episode: 583, duration: 1.836s, episode steps: 120, steps per second: 65, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.450 [0.000, 4.000], mean observation: 0.503 [0.460, 0.550], loss: 6.260818, mean_absolute_error: 32.651363, mean_q: 42.199619
[F[K  35808/500000: episode: 584, duration: 1.418s, episode steps: 92, steps per second: 65, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.483 [0.410, 0.530], loss: 6.491560, mean_absolute_error: 32.744884, mean_q: 42.277840
[F[K  35865/500000: episode: 585, duration: 0.948s, episode steps: 57, steps per second: 60, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.579 [0.000, 3.000], mean observation: 0.475 [0.350, 0.510], loss: 6.291553, mean_absolute_error: 32.633797, mean_q: 42.138103
[F[K  35947/500000: episode: 586, duration: 1.563s, episode steps: 82, steps per second: 52, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.890 [0.000, 4.000], mean observation: 0.513 [0.490, 0.590], loss: 7.209206, mean_absolute_error: 33.363392, mean_q: 42.920811
[F[K  36003/500000: episode: 587, duration: 1.036s, episode steps: 56, steps per second: 54, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.536 [0.000, 4.000], mean observation: 0.506 [0.440, 0.620], loss: 7.238225, mean_absolute_error: 33.054661, mean_q: 42.635925
[F[K  36060/500000: episode: 588, duration: 1.014s, episode steps: 57, steps per second: 56, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.479 [0.380, 0.500], loss: 6.990194, mean_absolute_error: 32.591885, mean_q: 42.009403
[F[K  36135/500000: episode: 589, duration: 1.409s, episode steps: 75, steps per second: 53, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.013 [0.000, 4.000], mean observation: 0.485 [0.380, 0.530], loss: 6.450394, mean_absolute_error: 32.768250, mean_q: 42.376064
[F[K  36189/500000: episode: 590, duration: 0.954s, episode steps: 54, steps per second: 57, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.037 [0.000, 4.000], mean observation: 0.506 [0.470, 0.550], loss: 6.429981, mean_absolute_error: 33.473324, mean_q: 43.209873
[F[K  36229/500000: episode: 591, duration: 0.745s, episode steps: 40, steps per second: 54, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.675 [0.000, 3.000], mean observation: 0.529 [0.470, 0.650], loss: 8.853994, mean_absolute_error: 32.384617, mean_q: 41.781307
[F[K  36335/500000: episode: 592, duration: 1.740s, episode steps: 106, steps per second: 61, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.113 [0.000, 4.000], mean observation: 0.480 [0.380, 0.530], loss: 6.720509, mean_absolute_error: 32.946117, mean_q: 42.532810
[F[K  36415/500000: episode: 593, duration: 1.415s, episode steps: 80, steps per second: 57, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.863 [0.000, 4.000], mean observation: 0.505 [0.470, 0.540], loss: 7.339622, mean_absolute_error: 33.702709, mean_q: 43.312984
[F[K  36450/500000: episode: 594, duration: 0.582s, episode steps: 35, steps per second: 60, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.657 [0.000, 4.000], mean observation: 0.516 [0.470, 0.640], loss: 6.861295, mean_absolute_error: 32.681919, mean_q: 42.061443
[F[K  36510/500000: episode: 595, duration: 0.968s, episode steps: 60, steps per second: 62, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.350 [0.000, 4.000], mean observation: 0.501 [0.390, 0.630], loss: 7.255494, mean_absolute_error: 33.733471, mean_q: 43.488464
[F[K  36593/500000: episode: 596, duration: 1.516s, episode steps: 83, steps per second: 55, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.783 [0.000, 4.000], mean observation: 0.505 [0.450, 0.630], loss: 7.139926, mean_absolute_error: 33.046780, mean_q: 42.653645
[F[K  36644/500000: episode: 597, duration: 0.997s, episode steps: 51, steps per second: 51, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.503 [0.460, 0.540], loss: 6.563695, mean_absolute_error: 32.710480, mean_q: 42.351925
[F[K  36688/500000: episode: 598, duration: 0.777s, episode steps: 44, steps per second: 57, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.136 [0.000, 4.000], mean observation: 0.492 [0.370, 0.560], loss: 7.135811, mean_absolute_error: 33.077404, mean_q: 42.645622
[F[K  36759/500000: episode: 599, duration: 1.417s, episode steps: 71, steps per second: 50, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.502 [0.410, 0.580], loss: 7.302557, mean_absolute_error: 33.162632, mean_q: 42.823746
[F[K  36838/500000: episode: 600, duration: 1.156s, episode steps: 79, steps per second: 68, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.038 [0.000, 4.000], mean observation: 0.488 [0.380, 0.520], loss: 7.187745, mean_absolute_error: 32.861725, mean_q: 42.324520
[F[K  36878/500000: episode: 601, duration: 0.747s, episode steps: 40, steps per second: 54, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.650 [0.000, 4.000], mean observation: 0.520 [0.470, 0.640], loss: 5.514205, mean_absolute_error: 33.236065, mean_q: 42.854198
[F[K  36945/500000: episode: 602, duration: 1.212s, episode steps: 67, steps per second: 55, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.388 [0.000, 4.000], mean observation: 0.513 [0.470, 0.600], loss: 6.815567, mean_absolute_error: 32.896053, mean_q: 42.418415
[F[K  37053/500000: episode: 603, duration: 1.933s, episode steps: 108, steps per second: 56, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.926 [0.000, 4.000], mean observation: 0.493 [0.390, 0.550], loss: 7.213546, mean_absolute_error: 33.709759, mean_q: 43.396980
[F[K  37113/500000: episode: 604, duration: 1.136s, episode steps: 60, steps per second: 53, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.492 [0.380, 0.530], loss: 7.269808, mean_absolute_error: 32.996300, mean_q: 42.627277
[F[K  37147/500000: episode: 605, duration: 0.682s, episode steps: 34, steps per second: 50, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.353 [0.000, 3.000], mean observation: 0.524 [0.470, 0.640], loss: 6.881671, mean_absolute_error: 32.893074, mean_q: 42.395737
[F[K  37210/500000: episode: 606, duration: 1.172s, episode steps: 63, steps per second: 54, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.498 [0.370, 0.600], loss: 7.026285, mean_absolute_error: 33.424286, mean_q: 43.094448
[F[K  37277/500000: episode: 607, duration: 1.278s, episode steps: 67, steps per second: 52, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.015 [0.000, 4.000], mean observation: 0.483 [0.380, 0.520], loss: 7.091552, mean_absolute_error: 33.410076, mean_q: 43.032696
[F[K  37338/500000: episode: 608, duration: 1.204s, episode steps: 61, steps per second: 51, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.721 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 7.866737, mean_absolute_error: 33.669491, mean_q: 43.321373
[F[K  37414/500000: episode: 609, duration: 1.351s, episode steps: 76, steps per second: 56, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.329 [0.000, 4.000], mean observation: 0.481 [0.390, 0.530], loss: 7.655848, mean_absolute_error: 33.063366, mean_q: 42.553562
[F[K  37508/500000: episode: 610, duration: 1.694s, episode steps: 94, steps per second: 55, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.085 [0.000, 4.000], mean observation: 0.502 [0.370, 0.630], loss: 6.758432, mean_absolute_error: 33.351276, mean_q: 42.946548
[F[K  37587/500000: episode: 611, duration: 1.460s, episode steps: 79, steps per second: 54, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.480 [0.360, 0.510], loss: 6.801140, mean_absolute_error: 33.540707, mean_q: 43.208759
[F[K  37650/500000: episode: 612, duration: 0.995s, episode steps: 63, steps per second: 63, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.222 [0.000, 4.000], mean observation: 0.489 [0.360, 0.530], loss: 6.791060, mean_absolute_error: 33.705082, mean_q: 43.395710
[F[K  37727/500000: episode: 613, duration: 1.441s, episode steps: 77, steps per second: 53, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.805 [0.000, 4.000], mean observation: 0.508 [0.470, 0.600], loss: 5.933116, mean_absolute_error: 33.973751, mean_q: 43.834705
[F[K  37814/500000: episode: 614, duration: 1.211s, episode steps: 87, steps per second: 72, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.491 [0.370, 0.550], loss: 6.706150, mean_absolute_error: 33.340843, mean_q: 42.981483
[F[K  37884/500000: episode: 615, duration: 1.060s, episode steps: 70, steps per second: 66, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.629 [0.000, 4.000], mean observation: 0.491 [0.390, 0.540], loss: 7.710241, mean_absolute_error: 33.512802, mean_q: 43.092152
[F[K  37962/500000: episode: 616, duration: 1.573s, episode steps: 78, steps per second: 50, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.385 [0.000, 4.000], mean observation: 0.501 [0.470, 0.520], loss: 7.413935, mean_absolute_error: 33.050011, mean_q: 42.514080
[F[K  38023/500000: episode: 617, duration: 1.167s, episode steps: 61, steps per second: 52, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.049 [0.000, 4.000], mean observation: 0.482 [0.390, 0.530], loss: 6.859160, mean_absolute_error: 33.402962, mean_q: 43.081551
[F[K  38146/500000: episode: 618, duration: 2.666s, episode steps: 123, steps per second: 46, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.927 [0.000, 4.000], mean observation: 0.493 [0.380, 0.560], loss: 6.572037, mean_absolute_error: 34.086456, mean_q: 43.856922
[F[K  38232/500000: episode: 619, duration: 1.602s, episode steps: 86, steps per second: 54, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.860 [0.000, 4.000], mean observation: 0.497 [0.400, 0.550], loss: 7.235960, mean_absolute_error: 33.882717, mean_q: 43.602543
[F[K  38286/500000: episode: 620, duration: 1.040s, episode steps: 54, steps per second: 52, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.741 [0.000, 4.000], mean observation: 0.473 [0.350, 0.530], loss: 7.253485, mean_absolute_error: 33.553913, mean_q: 43.222321
[F[K  38341/500000: episode: 621, duration: 1.010s, episode steps: 55, steps per second: 54, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.055 [0.000, 4.000], mean observation: 0.498 [0.430, 0.570], loss: 8.245598, mean_absolute_error: 33.127586, mean_q: 42.499939
[F[K  38375/500000: episode: 622, duration: 0.734s, episode steps: 34, steps per second: 46, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.706 [0.000, 4.000], mean observation: 0.522 [0.470, 0.640], loss: 7.833344, mean_absolute_error: 33.793797, mean_q: 43.427856
[F[K  38430/500000: episode: 623, duration: 1.165s, episode steps: 55, steps per second: 47, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.509 [0.460, 0.640], loss: 7.270304, mean_absolute_error: 33.972908, mean_q: 43.742821
[F[K  38496/500000: episode: 624, duration: 1.240s, episode steps: 66, steps per second: 53, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.712 [0.000, 4.000], mean observation: 0.492 [0.400, 0.530], loss: 7.895766, mean_absolute_error: 34.643879, mean_q: 44.603901
[F[K  38539/500000: episode: 625, duration: 0.795s, episode steps: 43, steps per second: 54, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.442 [0.000, 4.000], mean observation: 0.502 [0.400, 0.630], loss: 7.261788, mean_absolute_error: 33.198868, mean_q: 42.717045
[F[K  38611/500000: episode: 626, duration: 1.413s, episode steps: 72, steps per second: 51, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.493 [0.420, 0.530], loss: 6.955918, mean_absolute_error: 34.015396, mean_q: 43.830002
[F[K  38679/500000: episode: 627, duration: 1.280s, episode steps: 68, steps per second: 53, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.559 [0.000, 4.000], mean observation: 0.478 [0.380, 0.530], loss: 7.085588, mean_absolute_error: 33.759434, mean_q: 43.354851
[F[K  38725/500000: episode: 628, duration: 0.782s, episode steps: 46, steps per second: 59, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.130 [0.000, 4.000], mean observation: 0.497 [0.380, 0.590], loss: 8.176556, mean_absolute_error: 33.453773, mean_q: 43.009056
[F[K  38761/500000: episode: 629, duration: 0.596s, episode steps: 36, steps per second: 60, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.639 [0.000, 3.000], mean observation: 0.502 [0.370, 0.650], loss: 8.061509, mean_absolute_error: 33.056358, mean_q: 42.583118
[F[K  38826/500000: episode: 630, duration: 1.281s, episode steps: 65, steps per second: 51, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.969 [0.000, 4.000], mean observation: 0.507 [0.420, 0.640], loss: 6.199723, mean_absolute_error: 34.055359, mean_q: 43.843624
[F[K  38871/500000: episode: 631, duration: 0.806s, episode steps: 45, steps per second: 56, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 0.500 [0.370, 0.630], loss: 8.040756, mean_absolute_error: 33.851082, mean_q: 43.490276
[F[K  38959/500000: episode: 632, duration: 1.731s, episode steps: 88, steps per second: 51, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.487 [0.410, 0.520], loss: 6.570430, mean_absolute_error: 33.500912, mean_q: 43.148312
[F[K  39023/500000: episode: 633, duration: 1.246s, episode steps: 64, steps per second: 51, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.016 [0.000, 4.000], mean observation: 0.507 [0.470, 0.570], loss: 7.881549, mean_absolute_error: 33.798920, mean_q: 43.478527
[F[K  39066/500000: episode: 634, duration: 0.927s, episode steps: 43, steps per second: 46, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.512 [0.000, 4.000], mean observation: 0.502 [0.350, 0.640], loss: 6.505945, mean_absolute_error: 33.593449, mean_q: 43.273640
[F[K  39142/500000: episode: 635, duration: 1.485s, episode steps: 76, steps per second: 51, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.737 [0.000, 4.000], mean observation: 0.520 [0.500, 0.600], loss: 7.715109, mean_absolute_error: 34.020981, mean_q: 43.637539
[F[K  39190/500000: episode: 636, duration: 1.029s, episode steps: 48, steps per second: 47, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.417 [0.000, 4.000], mean observation: 0.485 [0.410, 0.530], loss: 7.425217, mean_absolute_error: 34.126625, mean_q: 43.788013
[F[K  39258/500000: episode: 637, duration: 1.325s, episode steps: 68, steps per second: 51, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.485 [0.400, 0.530], loss: 7.586465, mean_absolute_error: 33.666824, mean_q: 43.210552
[F[K  39303/500000: episode: 638, duration: 0.767s, episode steps: 45, steps per second: 59, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.556 [0.000, 4.000], mean observation: 0.483 [0.360, 0.530], loss: 7.890407, mean_absolute_error: 34.371410, mean_q: 44.144444
[F[K  39342/500000: episode: 639, duration: 0.768s, episode steps: 39, steps per second: 51, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.103 [0.000, 4.000], mean observation: 0.476 [0.370, 0.530], loss: 7.288988, mean_absolute_error: 33.706287, mean_q: 43.220928
[F[K  39380/500000: episode: 640, duration: 0.760s, episode steps: 38, steps per second: 50, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.789 [0.000, 4.000], mean observation: 0.506 [0.410, 0.650], loss: 8.098776, mean_absolute_error: 33.882248, mean_q: 43.450146
[F[K  39463/500000: episode: 641, duration: 1.619s, episode steps: 83, steps per second: 51, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.675 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 7.310702, mean_absolute_error: 33.486908, mean_q: 42.962280
[F[K  39518/500000: episode: 642, duration: 1.021s, episode steps: 55, steps per second: 54, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.691 [0.000, 4.000], mean observation: 0.477 [0.350, 0.530], loss: 7.491937, mean_absolute_error: 33.273201, mean_q: 42.696625
[F[K  39585/500000: episode: 643, duration: 1.253s, episode steps: 67, steps per second: 53, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.687 [0.000, 4.000], mean observation: 0.481 [0.360, 0.510], loss: 8.671549, mean_absolute_error: 32.823845, mean_q: 42.086758
[F[K  39634/500000: episode: 644, duration: 0.950s, episode steps: 49, steps per second: 52, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.510 [0.000, 4.000], mean observation: 0.503 [0.400, 0.630], loss: 7.129864, mean_absolute_error: 33.785728, mean_q: 43.457344
[F[K  39690/500000: episode: 645, duration: 1.121s, episode steps: 56, steps per second: 50, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.375 [0.000, 4.000], mean observation: 0.485 [0.350, 0.520], loss: 6.960770, mean_absolute_error: 32.923870, mean_q: 42.479397
[F[K  39741/500000: episode: 646, duration: 1.026s, episode steps: 51, steps per second: 50, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.495 [0.460, 0.530], loss: 7.774726, mean_absolute_error: 33.777615, mean_q: 43.520340
[F[K  39781/500000: episode: 647, duration: 0.884s, episode steps: 40, steps per second: 45, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.575 [0.000, 4.000], mean observation: 0.496 [0.350, 0.590], loss: 7.109281, mean_absolute_error: 33.078777, mean_q: 42.732430
[F[K  39855/500000: episode: 648, duration: 1.546s, episode steps: 74, steps per second: 48, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.757 [0.000, 4.000], mean observation: 0.476 [0.360, 0.530], loss: 7.020782, mean_absolute_error: 33.222290, mean_q: 42.819660
[F[K  39920/500000: episode: 649, duration: 1.466s, episode steps: 65, steps per second: 44, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.431 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 7.859684, mean_absolute_error: 32.874966, mean_q: 42.423203
[F[K  39976/500000: episode: 650, duration: 1.095s, episode steps: 56, steps per second: 51, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.321 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 7.659383, mean_absolute_error: 33.807583, mean_q: 43.562656
[F[K  40014/500000: episode: 651, duration: 0.607s, episode steps: 38, steps per second: 63, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.605 [0.000, 4.000], mean observation: 0.514 [0.470, 0.640], loss: 7.580317, mean_absolute_error: 33.470856, mean_q: 43.005219
[F[K  40073/500000: episode: 652, duration: 1.039s, episode steps: 59, steps per second: 57, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.519 [0.470, 0.600], loss: 7.341636, mean_absolute_error: 33.629818, mean_q: 43.310223
[F[K  40145/500000: episode: 653, duration: 1.411s, episode steps: 72, steps per second: 51, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.306 [0.000, 4.000], mean observation: 0.504 [0.470, 0.540], loss: 7.013945, mean_absolute_error: 33.482468, mean_q: 43.203106
[F[K  40218/500000: episode: 654, duration: 1.185s, episode steps: 73, steps per second: 62, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.452 [0.000, 4.000], mean observation: 0.518 [0.480, 0.600], loss: 6.773786, mean_absolute_error: 33.603218, mean_q: 43.285378
[F[K  40281/500000: episode: 655, duration: 1.377s, episode steps: 63, steps per second: 46, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.873 [0.000, 4.000], mean observation: 0.492 [0.440, 0.520], loss: 6.639144, mean_absolute_error: 34.278732, mean_q: 44.111027
[F[K  40381/500000: episode: 656, duration: 1.969s, episode steps: 100, steps per second: 51, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.630 [0.000, 4.000], mean observation: 0.498 [0.460, 0.520], loss: 6.927851, mean_absolute_error: 33.719120, mean_q: 43.430702
[F[K  40454/500000: episode: 657, duration: 1.343s, episode steps: 73, steps per second: 54, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.904 [0.000, 4.000], mean observation: 0.490 [0.380, 0.530], loss: 7.798903, mean_absolute_error: 33.433090, mean_q: 42.978374
[F[K  40532/500000: episode: 658, duration: 1.311s, episode steps: 78, steps per second: 60, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.385 [0.000, 4.000], mean observation: 0.494 [0.410, 0.530], loss: 7.355482, mean_absolute_error: 33.577042, mean_q: 43.266365
[F[K  40589/500000: episode: 659, duration: 1.103s, episode steps: 57, steps per second: 52, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.281 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 7.183122, mean_absolute_error: 33.090668, mean_q: 42.441666
[F[K  40626/500000: episode: 660, duration: 0.740s, episode steps: 37, steps per second: 50, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.811 [0.000, 4.000], mean observation: 0.517 [0.470, 0.640], loss: 6.616217, mean_absolute_error: 33.206841, mean_q: 42.803795
[F[K  40682/500000: episode: 661, duration: 0.946s, episode steps: 56, steps per second: 59, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.499 [0.400, 0.570], loss: 7.012268, mean_absolute_error: 33.289928, mean_q: 42.797474
[F[K  40760/500000: episode: 662, duration: 1.618s, episode steps: 78, steps per second: 48, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.962 [0.000, 4.000], mean observation: 0.498 [0.410, 0.590], loss: 7.452739, mean_absolute_error: 33.061733, mean_q: 42.580818
[F[K  40832/500000: episode: 663, duration: 1.308s, episode steps: 72, steps per second: 55, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.931 [0.000, 4.000], mean observation: 0.510 [0.460, 0.600], loss: 8.004757, mean_absolute_error: 33.648148, mean_q: 43.291794
[F[K  40892/500000: episode: 664, duration: 1.027s, episode steps: 60, steps per second: 58, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.517 [0.000, 4.000], mean observation: 0.515 [0.470, 0.640], loss: 7.272231, mean_absolute_error: 33.688416, mean_q: 43.332821
[F[K  40955/500000: episode: 665, duration: 1.210s, episode steps: 63, steps per second: 52, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.381 [0.000, 4.000], mean observation: 0.489 [0.390, 0.530], loss: 6.430815, mean_absolute_error: 34.204437, mean_q: 44.105747
[F[K  41012/500000: episode: 666, duration: 1.088s, episode steps: 57, steps per second: 52, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.486 [0.350, 0.520], loss: 7.458460, mean_absolute_error: 33.119389, mean_q: 42.695000
[F[K  41082/500000: episode: 667, duration: 1.102s, episode steps: 70, steps per second: 64, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.314 [0.000, 4.000], mean observation: 0.509 [0.470, 0.570], loss: 6.709987, mean_absolute_error: 34.020107, mean_q: 43.845470
[F[K  41136/500000: episode: 668, duration: 0.939s, episode steps: 54, steps per second: 58, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.704 [0.000, 4.000], mean observation: 0.487 [0.360, 0.530], loss: 7.431130, mean_absolute_error: 34.025639, mean_q: 43.735260
[F[K  41200/500000: episode: 669, duration: 1.001s, episode steps: 64, steps per second: 64, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.109 [0.000, 4.000], mean observation: 0.499 [0.440, 0.560], loss: 7.284459, mean_absolute_error: 33.056580, mean_q: 42.594986
[F[K  41337/500000: episode: 670, duration: 2.483s, episode steps: 137, steps per second: 55, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.358 [0.000, 4.000], mean observation: 0.516 [0.480, 0.580], loss: 6.601000, mean_absolute_error: 33.391201, mean_q: 42.969383
[F[K  41384/500000: episode: 671, duration: 0.883s, episode steps: 47, steps per second: 53, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.660 [0.000, 4.000], mean observation: 0.497 [0.360, 0.600], loss: 8.195226, mean_absolute_error: 33.854435, mean_q: 43.513153
[F[K  41481/500000: episode: 672, duration: 1.738s, episode steps: 97, steps per second: 56, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.454 [0.000, 4.000], mean observation: 0.511 [0.460, 0.650], loss: 6.891311, mean_absolute_error: 34.488098, mean_q: 44.367107
[F[K  41544/500000: episode: 673, duration: 1.173s, episode steps: 63, steps per second: 54, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.873 [0.000, 4.000], mean observation: 0.509 [0.470, 0.590], loss: 7.032918, mean_absolute_error: 33.734470, mean_q: 43.486767
[F[K  41615/500000: episode: 674, duration: 1.272s, episode steps: 71, steps per second: 56, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.775 [0.000, 4.000], mean observation: 0.480 [0.380, 0.530], loss: 7.487533, mean_absolute_error: 33.590820, mean_q: 43.295307
[F[K  41664/500000: episode: 675, duration: 0.979s, episode steps: 49, steps per second: 50, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 7.384152, mean_absolute_error: 33.617748, mean_q: 43.285919
[F[K  41767/500000: episode: 676, duration: 2.045s, episode steps: 103, steps per second: 50, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.806 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 7.424376, mean_absolute_error: 33.849129, mean_q: 43.567871
[F[K  41861/500000: episode: 677, duration: 1.490s, episode steps: 94, steps per second: 63, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.518 [0.490, 0.600], loss: 7.581399, mean_absolute_error: 33.301758, mean_q: 42.817524
[F[K  41910/500000: episode: 678, duration: 1.068s, episode steps: 49, steps per second: 46, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.571 [0.000, 4.000], mean observation: 0.498 [0.400, 0.580], loss: 6.192968, mean_absolute_error: 34.680107, mean_q: 44.617512
[F[K  41975/500000: episode: 679, duration: 1.444s, episode steps: 65, steps per second: 45, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.369 [0.000, 4.000], mean observation: 0.509 [0.470, 0.580], loss: 6.848940, mean_absolute_error: 33.445797, mean_q: 43.208427
[F[K  42038/500000: episode: 680, duration: 1.196s, episode steps: 63, steps per second: 53, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.841 [0.000, 4.000], mean observation: 0.486 [0.360, 0.530], loss: 7.038531, mean_absolute_error: 33.902187, mean_q: 43.631592
[F[K  42095/500000: episode: 681, duration: 0.970s, episode steps: 57, steps per second: 59, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.456 [0.000, 4.000], mean observation: 0.509 [0.460, 0.640], loss: 8.394102, mean_absolute_error: 33.599049, mean_q: 43.186657
[F[K  42177/500000: episode: 682, duration: 1.767s, episode steps: 82, steps per second: 46, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.939 [0.000, 4.000], mean observation: 0.502 [0.410, 0.590], loss: 8.326180, mean_absolute_error: 34.190681, mean_q: 43.841152
[F[K  42212/500000: episode: 683, duration: 0.727s, episode steps: 35, steps per second: 48, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.400 [0.000, 4.000], mean observation: 0.527 [0.470, 0.650], loss: 7.305960, mean_absolute_error: 33.954247, mean_q: 43.679661
[F[K  42309/500000: episode: 684, duration: 1.857s, episode steps: 97, steps per second: 52, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.814 [0.000, 4.000], mean observation: 0.502 [0.460, 0.540], loss: 6.795717, mean_absolute_error: 34.154373, mean_q: 43.838074
[F[K  42370/500000: episode: 685, duration: 1.084s, episode steps: 61, steps per second: 56, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.525 [0.000, 4.000], mean observation: 0.497 [0.450, 0.530], loss: 7.593721, mean_absolute_error: 33.628040, mean_q: 43.138493
[F[K  42424/500000: episode: 686, duration: 1.275s, episode steps: 54, steps per second: 42, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.741 [0.000, 3.000], mean observation: 0.512 [0.470, 0.620], loss: 7.074923, mean_absolute_error: 33.892902, mean_q: 43.550739
[F[K  42494/500000: episode: 687, duration: 1.602s, episode steps: 70, steps per second: 44, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.214 [0.000, 4.000], mean observation: 0.514 [0.480, 0.610], loss: 6.990828, mean_absolute_error: 33.617573, mean_q: 43.375416
[F[K  42556/500000: episode: 688, duration: 1.251s, episode steps: 62, steps per second: 50, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.290 [0.000, 4.000], mean observation: 0.493 [0.440, 0.520], loss: 7.835794, mean_absolute_error: 33.790817, mean_q: 43.525234
[F[K  42615/500000: episode: 689, duration: 1.196s, episode steps: 59, steps per second: 49, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.153 [0.000, 4.000], mean observation: 0.514 [0.470, 0.620], loss: 8.189783, mean_absolute_error: 33.743576, mean_q: 43.371914
[F[K  42688/500000: episode: 690, duration: 1.527s, episode steps: 73, steps per second: 48, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.712 [0.000, 4.000], mean observation: 0.502 [0.380, 0.620], loss: 7.003098, mean_absolute_error: 33.202435, mean_q: 42.781479
[F[K  42749/500000: episode: 691, duration: 1.183s, episode steps: 61, steps per second: 52, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.885 [0.000, 4.000], mean observation: 0.494 [0.360, 0.600], loss: 6.672740, mean_absolute_error: 33.084682, mean_q: 42.612251
[F[K  42827/500000: episode: 692, duration: 1.638s, episode steps: 78, steps per second: 48, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.859 [0.000, 4.000], mean observation: 0.507 [0.490, 0.550], loss: 6.128401, mean_absolute_error: 33.512459, mean_q: 43.096027
[F[K  42897/500000: episode: 693, duration: 1.499s, episode steps: 70, steps per second: 47, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.229 [0.000, 4.000], mean observation: 0.500 [0.470, 0.520], loss: 6.032387, mean_absolute_error: 33.808956, mean_q: 43.571659
[F[K  42951/500000: episode: 694, duration: 1.085s, episode steps: 54, steps per second: 50, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 8.052201, mean_absolute_error: 33.188446, mean_q: 42.722805
[F[K  42996/500000: episode: 695, duration: 0.842s, episode steps: 45, steps per second: 53, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.733 [0.000, 4.000], mean observation: 0.524 [0.470, 0.640], loss: 7.221125, mean_absolute_error: 34.626316, mean_q: 44.612637
[F[K  43052/500000: episode: 696, duration: 1.185s, episode steps: 56, steps per second: 47, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.804 [0.000, 4.000], mean observation: 0.518 [0.490, 0.610], loss: 6.840360, mean_absolute_error: 34.245228, mean_q: 44.173279
[F[K  43119/500000: episode: 697, duration: 1.298s, episode steps: 67, steps per second: 52, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.493 [0.000, 4.000], mean observation: 0.475 [0.360, 0.510], loss: 7.118385, mean_absolute_error: 33.610943, mean_q: 43.347904
[F[K  43154/500000: episode: 698, duration: 0.761s, episode steps: 35, steps per second: 46, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.519 [0.470, 0.640], loss: 7.631795, mean_absolute_error: 34.037617, mean_q: 43.730923
[F[K  43213/500000: episode: 699, duration: 1.235s, episode steps: 59, steps per second: 48, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.525 [0.000, 4.000], mean observation: 0.509 [0.450, 0.640], loss: 7.300697, mean_absolute_error: 33.630112, mean_q: 43.301208
[F[K  43279/500000: episode: 700, duration: 1.232s, episode steps: 66, steps per second: 54, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.045 [0.000, 4.000], mean observation: 0.486 [0.430, 0.510], loss: 6.784496, mean_absolute_error: 33.624237, mean_q: 43.281109
[F[K  43382/500000: episode: 701, duration: 1.418s, episode steps: 103, steps per second: 73, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.699 [0.000, 4.000], mean observation: 0.504 [0.470, 0.580], loss: 7.023204, mean_absolute_error: 33.924389, mean_q: 43.640846
[F[K  43456/500000: episode: 702, duration: 1.036s, episode steps: 74, steps per second: 71, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.784 [0.000, 4.000], mean observation: 0.493 [0.410, 0.530], loss: 6.848051, mean_absolute_error: 33.997841, mean_q: 43.716789
[F[K  43518/500000: episode: 703, duration: 1.263s, episode steps: 62, steps per second: 49, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.758 [0.000, 4.000], mean observation: 0.496 [0.380, 0.590], loss: 7.964422, mean_absolute_error: 34.528976, mean_q: 44.368969
[F[K  43610/500000: episode: 704, duration: 1.734s, episode steps: 92, steps per second: 53, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 0.508 [0.480, 0.560], loss: 7.882782, mean_absolute_error: 34.426777, mean_q: 44.251705
[F[K  43677/500000: episode: 705, duration: 1.252s, episode steps: 67, steps per second: 54, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.791 [0.000, 4.000], mean observation: 0.506 [0.490, 0.530], loss: 7.729029, mean_absolute_error: 34.622520, mean_q: 44.600086
[F[K  43770/500000: episode: 706, duration: 1.967s, episode steps: 93, steps per second: 47, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.828 [0.000, 4.000], mean observation: 0.473 [0.360, 0.510], loss: 6.595705, mean_absolute_error: 33.879887, mean_q: 43.714115
[F[K  43825/500000: episode: 707, duration: 1.275s, episode steps: 55, steps per second: 43, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.091 [0.000, 4.000], mean observation: 0.514 [0.490, 0.570], loss: 7.385752, mean_absolute_error: 34.664272, mean_q: 44.668819
[F[K  43876/500000: episode: 708, duration: 1.190s, episode steps: 51, steps per second: 43, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 0.518 [0.470, 0.620], loss: 8.054588, mean_absolute_error: 34.705093, mean_q: 44.693184
[F[K  43936/500000: episode: 709, duration: 0.996s, episode steps: 60, steps per second: 60, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.067 [0.000, 4.000], mean observation: 0.498 [0.360, 0.600], loss: 8.052087, mean_absolute_error: 33.739204, mean_q: 43.237381
[F[K  44011/500000: episode: 710, duration: 1.568s, episode steps: 75, steps per second: 48, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.120 [0.000, 4.000], mean observation: 0.516 [0.470, 0.600], loss: 7.847856, mean_absolute_error: 34.150429, mean_q: 43.943684
[F[K  44089/500000: episode: 711, duration: 1.772s, episode steps: 78, steps per second: 44, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.192 [0.000, 4.000], mean observation: 0.508 [0.490, 0.550], loss: 7.049277, mean_absolute_error: 35.028412, mean_q: 45.105598
[F[K  44120/500000: episode: 712, duration: 0.626s, episode steps: 31, steps per second: 49, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.419 [0.000, 3.000], mean observation: 0.525 [0.470, 0.650], loss: 7.848083, mean_absolute_error: 34.074459, mean_q: 43.943729
[F[K  44192/500000: episode: 713, duration: 1.324s, episode steps: 72, steps per second: 54, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.611 [0.000, 4.000], mean observation: 0.505 [0.490, 0.530], loss: 8.156789, mean_absolute_error: 33.946262, mean_q: 43.659279
[F[K  44244/500000: episode: 714, duration: 1.087s, episode steps: 52, steps per second: 48, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.327 [0.000, 4.000], mean observation: 0.505 [0.470, 0.560], loss: 6.687219, mean_absolute_error: 34.166157, mean_q: 43.992771
[F[K  44314/500000: episode: 715, duration: 1.476s, episode steps: 70, steps per second: 47, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.512 [0.460, 0.640], loss: 7.057974, mean_absolute_error: 34.608562, mean_q: 44.513466
[F[K  44397/500000: episode: 716, duration: 1.708s, episode steps: 83, steps per second: 49, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.723 [0.000, 4.000], mean observation: 0.505 [0.420, 0.610], loss: 8.399649, mean_absolute_error: 34.392525, mean_q: 44.179661
[F[K  44438/500000: episode: 717, duration: 0.808s, episode steps: 41, steps per second: 51, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.499 [0.370, 0.610], loss: 7.214740, mean_absolute_error: 34.520302, mean_q: 44.586010
[F[K  44489/500000: episode: 718, duration: 1.003s, episode steps: 51, steps per second: 51, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.804 [0.000, 4.000], mean observation: 0.498 [0.410, 0.530], loss: 7.377209, mean_absolute_error: 34.767776, mean_q: 44.543381
[F[K  44569/500000: episode: 719, duration: 1.529s, episode steps: 80, steps per second: 52, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.675 [0.000, 4.000], mean observation: 0.495 [0.450, 0.530], loss: 8.027674, mean_absolute_error: 33.978188, mean_q: 43.730705
[F[K  44630/500000: episode: 720, duration: 1.290s, episode steps: 61, steps per second: 47, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.934 [0.000, 4.000], mean observation: 0.517 [0.500, 0.570], loss: 7.026193, mean_absolute_error: 33.909927, mean_q: 43.733250
[F[K  44697/500000: episode: 721, duration: 1.249s, episode steps: 67, steps per second: 54, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.746 [0.000, 4.000], mean observation: 0.497 [0.440, 0.530], loss: 7.664381, mean_absolute_error: 34.487602, mean_q: 44.437801
[F[K  44745/500000: episode: 722, duration: 0.897s, episode steps: 48, steps per second: 54, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.646 [0.000, 4.000], mean observation: 0.507 [0.450, 0.630], loss: 6.558449, mean_absolute_error: 34.141682, mean_q: 44.045788
[F[K  44811/500000: episode: 723, duration: 1.345s, episode steps: 66, steps per second: 49, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.955 [0.000, 4.000], mean observation: 0.486 [0.380, 0.530], loss: 7.042640, mean_absolute_error: 35.000259, mean_q: 45.027523
[F[K  44898/500000: episode: 724, duration: 1.782s, episode steps: 87, steps per second: 49, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.218 [0.000, 4.000], mean observation: 0.511 [0.460, 0.650], loss: 7.631938, mean_absolute_error: 34.119045, mean_q: 43.978935
[F[K  44932/500000: episode: 725, duration: 0.777s, episode steps: 34, steps per second: 44, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.618 [0.000, 4.000], mean observation: 0.513 [0.470, 0.640], loss: 7.226535, mean_absolute_error: 34.332592, mean_q: 44.265072
[F[K  44999/500000: episode: 726, duration: 1.283s, episode steps: 67, steps per second: 52, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.284 [0.000, 4.000], mean observation: 0.503 [0.470, 0.570], loss: 8.023012, mean_absolute_error: 34.306763, mean_q: 43.986397
[F[K  45059/500000: episode: 727, duration: 1.307s, episode steps: 60, steps per second: 46, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.550 [0.000, 4.000], mean observation: 0.515 [0.470, 0.650], loss: 7.415081, mean_absolute_error: 34.834194, mean_q: 44.874229
[F[K  45128/500000: episode: 728, duration: 1.637s, episode steps: 69, steps per second: 42, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.232 [0.000, 4.000], mean observation: 0.502 [0.400, 0.600], loss: 8.089157, mean_absolute_error: 33.850552, mean_q: 43.558891
[F[K  45210/500000: episode: 729, duration: 1.735s, episode steps: 82, steps per second: 47, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.732 [0.000, 4.000], mean observation: 0.488 [0.400, 0.530], loss: 7.257740, mean_absolute_error: 34.268559, mean_q: 44.046459
[F[K  45342/500000: episode: 730, duration: 2.702s, episode steps: 132, steps per second: 49, episode reward: 132.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.356 [0.000, 4.000], mean observation: 0.516 [0.460, 0.640], loss: 7.055757, mean_absolute_error: 33.841942, mean_q: 43.586624
[F[K  45499/500000: episode: 731, duration: 2.944s, episode steps: 157, steps per second: 53, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.561 [0.000, 4.000], mean observation: 0.512 [0.450, 0.630], loss: 7.239924, mean_absolute_error: 34.764107, mean_q: 44.700245
[F[K  45573/500000: episode: 732, duration: 1.521s, episode steps: 74, steps per second: 49, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.568 [0.000, 4.000], mean observation: 0.488 [0.390, 0.530], loss: 7.130670, mean_absolute_error: 34.875965, mean_q: 44.919270
[F[K  45643/500000: episode: 733, duration: 1.314s, episode steps: 70, steps per second: 53, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.886 [0.000, 4.000], mean observation: 0.514 [0.470, 0.590], loss: 8.615720, mean_absolute_error: 34.618206, mean_q: 44.438541
[F[K  45723/500000: episode: 734, duration: 1.554s, episode steps: 80, steps per second: 51, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.508 [0.470, 0.580], loss: 7.992358, mean_absolute_error: 34.308167, mean_q: 44.146080
[F[K  45787/500000: episode: 735, duration: 1.040s, episode steps: 64, steps per second: 62, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.906 [0.000, 4.000], mean observation: 0.492 [0.380, 0.540], loss: 8.243723, mean_absolute_error: 34.030701, mean_q: 43.748764
[F[K  45853/500000: episode: 736, duration: 1.228s, episode steps: 66, steps per second: 54, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.517 [0.490, 0.620], loss: 8.718695, mean_absolute_error: 34.088203, mean_q: 43.697712
[F[K  45935/500000: episode: 737, duration: 1.603s, episode steps: 82, steps per second: 51, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.671 [0.000, 3.000], mean observation: 0.491 [0.380, 0.550], loss: 7.223481, mean_absolute_error: 33.956501, mean_q: 43.751160
[F[K  46015/500000: episode: 738, duration: 1.716s, episode steps: 80, steps per second: 47, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.112 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 7.840089, mean_absolute_error: 34.815723, mean_q: 44.744461
[F[K  46125/500000: episode: 739, duration: 1.969s, episode steps: 110, steps per second: 56, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.255 [0.000, 4.000], mean observation: 0.495 [0.400, 0.530], loss: 7.714468, mean_absolute_error: 34.414841, mean_q: 44.153679
[F[K  46161/500000: episode: 740, duration: 0.698s, episode steps: 36, steps per second: 52, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.472 [0.000, 3.000], mean observation: 0.515 [0.470, 0.640], loss: 7.353700, mean_absolute_error: 33.151745, mean_q: 42.745785
[F[K  46332/500000: episode: 741, duration: 2.803s, episode steps: 171, steps per second: 61, episode reward: 171.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.596 [0.000, 4.000], mean observation: 0.496 [0.440, 0.560], loss: 8.062405, mean_absolute_error: 34.429611, mean_q: 44.280239
[F[K  46500/500000: episode: 742, duration: 3.344s, episode steps: 168, steps per second: 50, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.030 [0.000, 4.000], mean observation: 0.486 [0.380, 0.540], loss: 7.442426, mean_absolute_error: 34.092731, mean_q: 43.853951
[F[K  46591/500000: episode: 743, duration: 1.687s, episode steps: 91, steps per second: 54, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.500 [0.440, 0.580], loss: 7.932396, mean_absolute_error: 34.890541, mean_q: 44.772076
[F[K  46692/500000: episode: 744, duration: 1.744s, episode steps: 101, steps per second: 58, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.287 [0.000, 4.000], mean observation: 0.506 [0.470, 0.610], loss: 7.003779, mean_absolute_error: 34.750061, mean_q: 44.537025
[F[K  46741/500000: episode: 745, duration: 0.759s, episode steps: 49, steps per second: 65, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.776 [0.000, 4.000], mean observation: 0.503 [0.410, 0.630], loss: 6.427719, mean_absolute_error: 34.303753, mean_q: 44.188866
[F[K  46808/500000: episode: 746, duration: 1.263s, episode steps: 67, steps per second: 53, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.821 [0.000, 4.000], mean observation: 0.496 [0.380, 0.590], loss: 6.476997, mean_absolute_error: 34.137291, mean_q: 43.856342
[F[K  46846/500000: episode: 747, duration: 0.828s, episode steps: 38, steps per second: 46, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.474 [0.000, 4.000], mean observation: 0.517 [0.470, 0.650], loss: 7.155987, mean_absolute_error: 35.119923, mean_q: 45.273724
[F[K  46905/500000: episode: 748, duration: 1.304s, episode steps: 59, steps per second: 45, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.915 [0.000, 4.000], mean observation: 0.510 [0.470, 0.610], loss: 7.124753, mean_absolute_error: 34.906185, mean_q: 44.978821
[F[K  46983/500000: episode: 749, duration: 1.642s, episode steps: 78, steps per second: 48, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.423 [0.000, 4.000], mean observation: 0.502 [0.390, 0.630], loss: 7.558753, mean_absolute_error: 34.855495, mean_q: 44.769901
[F[K  47066/500000: episode: 750, duration: 1.723s, episode steps: 83, steps per second: 48, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.627 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 6.978390, mean_absolute_error: 34.729073, mean_q: 44.660820
[F[K  47112/500000: episode: 751, duration: 0.943s, episode steps: 46, steps per second: 49, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.304 [0.000, 4.000], mean observation: 0.516 [0.470, 0.620], loss: 7.581903, mean_absolute_error: 34.541245, mean_q: 44.312077
[F[K  47193/500000: episode: 752, duration: 1.565s, episode steps: 81, steps per second: 52, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.012 [0.000, 4.000], mean observation: 0.495 [0.400, 0.560], loss: 8.033206, mean_absolute_error: 33.922779, mean_q: 43.561615
[F[K  47246/500000: episode: 753, duration: 1.192s, episode steps: 53, steps per second: 44, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.717 [0.000, 4.000], mean observation: 0.497 [0.390, 0.570], loss: 7.678075, mean_absolute_error: 34.564686, mean_q: 44.489391
[F[K  47284/500000: episode: 754, duration: 0.810s, episode steps: 38, steps per second: 47, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.632 [0.000, 3.000], mean observation: 0.513 [0.470, 0.640], loss: 8.544219, mean_absolute_error: 34.919048, mean_q: 44.703003
[F[K  47347/500000: episode: 755, duration: 1.318s, episode steps: 63, steps per second: 48, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.683 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 7.979121, mean_absolute_error: 35.069908, mean_q: 45.004776
[F[K  47383/500000: episode: 756, duration: 0.770s, episode steps: 36, steps per second: 47, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 3.000], mean observation: 0.523 [0.470, 0.650], loss: 7.605871, mean_absolute_error: 34.580574, mean_q: 44.140034
[F[K  47461/500000: episode: 757, duration: 1.499s, episode steps: 78, steps per second: 52, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.936 [0.000, 4.000], mean observation: 0.517 [0.480, 0.600], loss: 8.195465, mean_absolute_error: 34.133690, mean_q: 43.856766
[F[K  47536/500000: episode: 758, duration: 1.589s, episode steps: 75, steps per second: 47, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.494 [0.450, 0.530], loss: 8.479720, mean_absolute_error: 34.223984, mean_q: 43.985622
[F[K  47609/500000: episode: 759, duration: 1.472s, episode steps: 73, steps per second: 50, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.068 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 7.434518, mean_absolute_error: 34.715431, mean_q: 44.680733
[F[K  47654/500000: episode: 760, duration: 0.997s, episode steps: 45, steps per second: 45, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.556 [0.000, 4.000], mean observation: 0.492 [0.360, 0.560], loss: 7.158182, mean_absolute_error: 33.753716, mean_q: 43.267330
[F[K  47725/500000: episode: 761, duration: 1.416s, episode steps: 71, steps per second: 50, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.296 [0.000, 4.000], mean observation: 0.515 [0.470, 0.630], loss: 7.098831, mean_absolute_error: 34.234207, mean_q: 44.084625
[F[K  47786/500000: episode: 762, duration: 1.192s, episode steps: 61, steps per second: 51, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.131 [0.000, 4.000], mean observation: 0.495 [0.470, 0.510], loss: 7.904441, mean_absolute_error: 33.543385, mean_q: 43.091217
[F[K  47890/500000: episode: 763, duration: 2.025s, episode steps: 104, steps per second: 51, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.769 [0.000, 4.000], mean observation: 0.485 [0.350, 0.540], loss: 7.077397, mean_absolute_error: 34.681572, mean_q: 44.480305
[F[K  47971/500000: episode: 764, duration: 1.684s, episode steps: 81, steps per second: 48, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.210 [0.000, 4.000], mean observation: 0.506 [0.480, 0.530], loss: 7.651598, mean_absolute_error: 34.528099, mean_q: 44.370152
[F[K  48044/500000: episode: 765, duration: 1.525s, episode steps: 73, steps per second: 48, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.808 [0.000, 4.000], mean observation: 0.492 [0.430, 0.530], loss: 7.752793, mean_absolute_error: 33.471382, mean_q: 43.072620
[F[K  48093/500000: episode: 766, duration: 1.096s, episode steps: 49, steps per second: 45, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.224 [0.000, 4.000], mean observation: 0.499 [0.430, 0.530], loss: 7.049418, mean_absolute_error: 34.575329, mean_q: 44.609955
[F[K  48174/500000: episode: 767, duration: 1.600s, episode steps: 81, steps per second: 51, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.938 [0.000, 4.000], mean observation: 0.500 [0.420, 0.580], loss: 7.219399, mean_absolute_error: 34.302948, mean_q: 44.181969
[F[K  48243/500000: episode: 768, duration: 1.396s, episode steps: 69, steps per second: 49, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.512 [0.500, 0.590], loss: 7.035538, mean_absolute_error: 34.063740, mean_q: 43.801025
[F[K  48301/500000: episode: 769, duration: 1.061s, episode steps: 58, steps per second: 55, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.207 [0.000, 4.000], mean observation: 0.518 [0.480, 0.650], loss: 8.119130, mean_absolute_error: 34.744530, mean_q: 44.664803
[F[K  48341/500000: episode: 770, duration: 0.811s, episode steps: 40, steps per second: 49, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.450 [0.000, 4.000], mean observation: 0.525 [0.470, 0.640], loss: 7.180073, mean_absolute_error: 33.720379, mean_q: 43.313011
[F[K  48403/500000: episode: 771, duration: 1.287s, episode steps: 62, steps per second: 48, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.726 [0.000, 4.000], mean observation: 0.505 [0.490, 0.530], loss: 7.434239, mean_absolute_error: 35.406620, mean_q: 45.499210
[F[K  48489/500000: episode: 772, duration: 1.840s, episode steps: 86, steps per second: 47, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.070 [0.000, 4.000], mean observation: 0.500 [0.380, 0.620], loss: 7.854024, mean_absolute_error: 34.160305, mean_q: 43.901638
[F[K  48552/500000: episode: 773, duration: 1.209s, episode steps: 63, steps per second: 52, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 7.046174, mean_absolute_error: 34.363865, mean_q: 44.074188
[F[K  48630/500000: episode: 774, duration: 1.421s, episode steps: 78, steps per second: 55, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.244 [0.000, 4.000], mean observation: 0.502 [0.470, 0.540], loss: 8.026861, mean_absolute_error: 34.165272, mean_q: 43.861427
[F[K  48770/500000: episode: 775, duration: 3.037s, episode steps: 140, steps per second: 46, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.497 [0.410, 0.550], loss: 7.464861, mean_absolute_error: 33.893520, mean_q: 43.583008
[F[K  48816/500000: episode: 776, duration: 0.916s, episode steps: 46, steps per second: 50, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.152 [0.000, 4.000], mean observation: 0.500 [0.350, 0.630], loss: 7.391712, mean_absolute_error: 34.630993, mean_q: 44.489136
[F[K  48919/500000: episode: 777, duration: 2.095s, episode steps: 103, steps per second: 49, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.223 [0.000, 4.000], mean observation: 0.519 [0.480, 0.650], loss: 7.342690, mean_absolute_error: 34.128227, mean_q: 43.870659
[F[K  48986/500000: episode: 778, duration: 1.298s, episode steps: 67, steps per second: 52, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.821 [0.000, 4.000], mean observation: 0.485 [0.400, 0.530], loss: 8.496563, mean_absolute_error: 33.753567, mean_q: 43.460335
[F[K  49039/500000: episode: 779, duration: 1.011s, episode steps: 53, steps per second: 52, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.170 [0.000, 4.000], mean observation: 0.516 [0.480, 0.650], loss: 7.938752, mean_absolute_error: 34.342297, mean_q: 44.026363
[F[K  49096/500000: episode: 780, duration: 1.220s, episode steps: 57, steps per second: 47, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.947 [0.000, 4.000], mean observation: 0.501 [0.420, 0.570], loss: 7.986433, mean_absolute_error: 34.487747, mean_q: 44.252888
[F[K  49150/500000: episode: 781, duration: 1.317s, episode steps: 54, steps per second: 41, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.056 [0.000, 4.000], mean observation: 0.504 [0.470, 0.580], loss: 8.388984, mean_absolute_error: 34.503197, mean_q: 44.308750
[F[K  49241/500000: episode: 782, duration: 1.791s, episode steps: 91, steps per second: 51, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.934 [0.000, 4.000], mean observation: 0.499 [0.430, 0.540], loss: 7.287845, mean_absolute_error: 34.161316, mean_q: 43.935669
[F[K  49405/500000: episode: 783, duration: 3.731s, episode steps: 164, steps per second: 44, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.232 [0.000, 4.000], mean observation: 0.505 [0.400, 0.640], loss: 7.221245, mean_absolute_error: 33.950256, mean_q: 43.682056
[F[K  49487/500000: episode: 784, duration: 1.663s, episode steps: 82, steps per second: 49, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.659 [0.000, 4.000], mean observation: 0.500 [0.470, 0.530], loss: 6.667495, mean_absolute_error: 34.484734, mean_q: 44.320713
[F[K  49591/500000: episode: 785, duration: 1.920s, episode steps: 104, steps per second: 54, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.356 [0.000, 4.000], mean observation: 0.485 [0.370, 0.530], loss: 7.137654, mean_absolute_error: 34.189049, mean_q: 44.098087
[F[K  49654/500000: episode: 786, duration: 1.185s, episode steps: 63, steps per second: 53, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.413 [0.000, 4.000], mean observation: 0.488 [0.370, 0.530], loss: 7.329424, mean_absolute_error: 34.781113, mean_q: 44.719410
[F[K  49689/500000: episode: 787, duration: 0.757s, episode steps: 35, steps per second: 46, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.457 [0.000, 3.000], mean observation: 0.514 [0.470, 0.650], loss: 7.507113, mean_absolute_error: 34.214542, mean_q: 44.028873
[F[K  49760/500000: episode: 788, duration: 1.667s, episode steps: 71, steps per second: 43, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.563 [0.000, 4.000], mean observation: 0.511 [0.480, 0.590], loss: 7.108869, mean_absolute_error: 34.109768, mean_q: 44.008106
[F[K  49818/500000: episode: 789, duration: 1.233s, episode steps: 58, steps per second: 47, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.483 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 7.382496, mean_absolute_error: 34.436634, mean_q: 44.366585
[F[K  49892/500000: episode: 790, duration: 1.499s, episode steps: 74, steps per second: 49, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.257 [0.000, 4.000], mean observation: 0.501 [0.410, 0.630], loss: 7.065861, mean_absolute_error: 34.747101, mean_q: 44.787952
[F[K  49958/500000: episode: 791, duration: 1.438s, episode steps: 66, steps per second: 46, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.288 [0.000, 4.000], mean observation: 0.503 [0.430, 0.610], loss: 7.897582, mean_absolute_error: 34.054798, mean_q: 43.833008
[F[K  50022/500000: episode: 792, duration: 1.464s, episode steps: 64, steps per second: 44, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.479 [0.350, 0.530], loss: 6.744348, mean_absolute_error: 34.363136, mean_q: 44.269367
[F[K  50099/500000: episode: 793, duration: 1.666s, episode steps: 77, steps per second: 46, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.468 [0.000, 4.000], mean observation: 0.491 [0.350, 0.570], loss: 7.504512, mean_absolute_error: 34.603306, mean_q: 44.495941
[F[K  50153/500000: episode: 794, duration: 0.996s, episode steps: 54, steps per second: 54, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.037 [0.000, 4.000], mean observation: 0.493 [0.450, 0.530], loss: 8.835320, mean_absolute_error: 34.700829, mean_q: 44.483456
[F[K  50238/500000: episode: 795, duration: 1.672s, episode steps: 85, steps per second: 51, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 4.000], mean observation: 0.501 [0.380, 0.610], loss: 7.978591, mean_absolute_error: 35.133526, mean_q: 45.212490
[F[K  50294/500000: episode: 796, duration: 1.180s, episode steps: 56, steps per second: 47, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.786 [0.000, 4.000], mean observation: 0.482 [0.390, 0.530], loss: 7.037892, mean_absolute_error: 34.582527, mean_q: 44.523518
[F[K  50377/500000: episode: 797, duration: 1.713s, episode steps: 83, steps per second: 48, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.490 [0.380, 0.540], loss: 7.557799, mean_absolute_error: 34.902046, mean_q: 44.851658
[F[K  50425/500000: episode: 798, duration: 1.171s, episode steps: 48, steps per second: 41, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.958 [0.000, 4.000], mean observation: 0.516 [0.470, 0.620], loss: 7.619213, mean_absolute_error: 35.168766, mean_q: 45.159149
[F[K  50483/500000: episode: 799, duration: 1.232s, episode steps: 58, steps per second: 47, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.052 [0.000, 4.000], mean observation: 0.522 [0.470, 0.610], loss: 8.600104, mean_absolute_error: 34.283245, mean_q: 43.997066
[F[K  50520/500000: episode: 800, duration: 0.756s, episode steps: 37, steps per second: 49, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.946 [0.000, 4.000], mean observation: 0.488 [0.350, 0.530], loss: 6.407843, mean_absolute_error: 34.758121, mean_q: 44.533245
[F[K  50592/500000: episode: 801, duration: 1.613s, episode steps: 72, steps per second: 45, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.486 [0.000, 4.000], mean observation: 0.506 [0.480, 0.550], loss: 6.937479, mean_absolute_error: 35.152939, mean_q: 45.228230
[F[K  50663/500000: episode: 802, duration: 1.479s, episode steps: 71, steps per second: 48, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.662 [0.000, 4.000], mean observation: 0.490 [0.400, 0.530], loss: 7.854507, mean_absolute_error: 35.304497, mean_q: 45.376129
[F[K  50742/500000: episode: 803, duration: 1.774s, episode steps: 79, steps per second: 45, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.823 [0.000, 4.000], mean observation: 0.506 [0.400, 0.630], loss: 7.107417, mean_absolute_error: 35.293148, mean_q: 45.301899
[F[K  50808/500000: episode: 804, duration: 1.317s, episode steps: 66, steps per second: 50, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.364 [0.000, 4.000], mean observation: 0.507 [0.480, 0.540], loss: 7.792009, mean_absolute_error: 35.378654, mean_q: 45.381710
[F[K  50872/500000: episode: 805, duration: 1.385s, episode steps: 64, steps per second: 46, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.515 [0.490, 0.560], loss: 7.178245, mean_absolute_error: 34.980011, mean_q: 44.879642
[F[K  50952/500000: episode: 806, duration: 1.481s, episode steps: 80, steps per second: 54, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.517 [0.480, 0.600], loss: 7.857471, mean_absolute_error: 35.038860, mean_q: 44.872650
[F[K  51062/500000: episode: 807, duration: 2.280s, episode steps: 110, steps per second: 48, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.318 [0.000, 4.000], mean observation: 0.491 [0.390, 0.530], loss: 7.390130, mean_absolute_error: 34.730362, mean_q: 44.591824
[F[K  51096/500000: episode: 808, duration: 0.757s, episode steps: 34, steps per second: 45, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.527 [0.470, 0.650], loss: 7.499736, mean_absolute_error: 34.643559, mean_q: 44.435074
[F[K  51183/500000: episode: 809, duration: 1.561s, episode steps: 87, steps per second: 56, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.839 [0.000, 4.000], mean observation: 0.503 [0.450, 0.560], loss: 8.060885, mean_absolute_error: 35.012024, mean_q: 44.871662
[F[K  51251/500000: episode: 810, duration: 1.123s, episode steps: 68, steps per second: 61, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.510 [0.490, 0.560], loss: 7.608834, mean_absolute_error: 35.508320, mean_q: 45.635384
[F[K  51410/500000: episode: 811, duration: 3.392s, episode steps: 159, steps per second: 47, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.969 [0.000, 4.000], mean observation: 0.503 [0.400, 0.650], loss: 7.869883, mean_absolute_error: 34.720974, mean_q: 44.524807
[F[K  51492/500000: episode: 812, duration: 1.739s, episode steps: 82, steps per second: 47, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.490 [0.430, 0.520], loss: 7.706496, mean_absolute_error: 35.395271, mean_q: 45.376766
[F[K  51552/500000: episode: 813, duration: 1.072s, episode steps: 60, steps per second: 56, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.317 [0.000, 4.000], mean observation: 0.504 [0.470, 0.570], loss: 7.351961, mean_absolute_error: 34.778393, mean_q: 44.598984
[F[K  51612/500000: episode: 814, duration: 1.211s, episode steps: 60, steps per second: 50, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.950 [0.000, 4.000], mean observation: 0.483 [0.390, 0.530], loss: 8.170401, mean_absolute_error: 34.617851, mean_q: 44.434376
[F[K  51653/500000: episode: 815, duration: 0.863s, episode steps: 41, steps per second: 48, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.503 [0.370, 0.650], loss: 8.364278, mean_absolute_error: 35.469307, mean_q: 45.496254
[F[K  51721/500000: episode: 816, duration: 1.308s, episode steps: 68, steps per second: 52, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.985 [0.000, 4.000], mean observation: 0.495 [0.360, 0.590], loss: 7.754906, mean_absolute_error: 35.335178, mean_q: 45.335602
[F[K  51794/500000: episode: 817, duration: 1.251s, episode steps: 73, steps per second: 58, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.384 [0.000, 4.000], mean observation: 0.505 [0.440, 0.590], loss: 7.449700, mean_absolute_error: 34.416225, mean_q: 44.120258
[F[K  51872/500000: episode: 818, duration: 1.394s, episode steps: 78, steps per second: 56, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.923 [0.000, 4.000], mean observation: 0.510 [0.460, 0.570], loss: 7.555325, mean_absolute_error: 34.526634, mean_q: 44.356396
[F[K  51914/500000: episode: 819, duration: 0.776s, episode steps: 42, steps per second: 54, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.381 [0.000, 4.000], mean observation: 0.491 [0.350, 0.540], loss: 7.453361, mean_absolute_error: 34.494099, mean_q: 44.364822
[F[K  51973/500000: episode: 820, duration: 1.041s, episode steps: 59, steps per second: 57, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.576 [0.000, 4.000], mean observation: 0.502 [0.460, 0.570], loss: 8.442176, mean_absolute_error: 35.113052, mean_q: 45.053410
[F[K  52053/500000: episode: 821, duration: 1.609s, episode steps: 80, steps per second: 50, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.492 [0.410, 0.530], loss: 7.777185, mean_absolute_error: 34.573109, mean_q: 44.428558
[F[K  52114/500000: episode: 822, duration: 1.264s, episode steps: 61, steps per second: 48, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.492 [0.000, 4.000], mean observation: 0.519 [0.480, 0.640], loss: 7.210432, mean_absolute_error: 34.825817, mean_q: 44.848545
[F[K  52187/500000: episode: 823, duration: 1.457s, episode steps: 73, steps per second: 50, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.822 [0.000, 4.000], mean observation: 0.484 [0.370, 0.510], loss: 7.477589, mean_absolute_error: 34.037766, mean_q: 43.819893
[F[K  52245/500000: episode: 824, duration: 1.060s, episode steps: 58, steps per second: 55, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.515 [0.470, 0.610], loss: 8.157165, mean_absolute_error: 34.772419, mean_q: 44.634293
[F[K  52309/500000: episode: 825, duration: 1.357s, episode steps: 64, steps per second: 47, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.266 [0.000, 4.000], mean observation: 0.480 [0.360, 0.520], loss: 8.076740, mean_absolute_error: 34.243973, mean_q: 43.920753
[F[K  52384/500000: episode: 826, duration: 1.592s, episode steps: 75, steps per second: 47, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.280 [0.000, 4.000], mean observation: 0.487 [0.390, 0.530], loss: 6.931108, mean_absolute_error: 34.525867, mean_q: 44.471737
[F[K  52451/500000: episode: 827, duration: 1.156s, episode steps: 67, steps per second: 58, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.896 [0.000, 4.000], mean observation: 0.511 [0.460, 0.620], loss: 8.142455, mean_absolute_error: 34.688004, mean_q: 44.481556
[F[K  52524/500000: episode: 828, duration: 1.290s, episode steps: 73, steps per second: 57, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.932 [0.000, 4.000], mean observation: 0.501 [0.450, 0.550], loss: 6.779034, mean_absolute_error: 34.967331, mean_q: 44.986668
[F[K  52600/500000: episode: 829, duration: 1.400s, episode steps: 76, steps per second: 54, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.026 [0.000, 4.000], mean observation: 0.512 [0.490, 0.570], loss: 7.848519, mean_absolute_error: 35.024067, mean_q: 44.983318
[F[K  52793/500000: episode: 830, duration: 3.329s, episode steps: 193, steps per second: 58, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.280 [0.000, 4.000], mean observation: 0.513 [0.440, 0.690], loss: 8.111430, mean_absolute_error: 34.685204, mean_q: 44.497765
[F[K  52872/500000: episode: 831, duration: 1.380s, episode steps: 79, steps per second: 57, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.190 [0.000, 4.000], mean observation: 0.506 [0.490, 0.540], loss: 7.906239, mean_absolute_error: 34.698853, mean_q: 44.519836
[F[K  52931/500000: episode: 832, duration: 1.163s, episode steps: 59, steps per second: 51, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.220 [0.000, 4.000], mean observation: 0.515 [0.470, 0.620], loss: 7.269881, mean_absolute_error: 34.848465, mean_q: 44.877106
[F[K  52992/500000: episode: 833, duration: 1.135s, episode steps: 61, steps per second: 54, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.738 [0.000, 4.000], mean observation: 0.483 [0.400, 0.530], loss: 7.716764, mean_absolute_error: 34.262169, mean_q: 44.092278
[F[K  53156/500000: episode: 834, duration: 2.981s, episode steps: 164, steps per second: 55, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.988 [0.000, 4.000], mean observation: 0.494 [0.390, 0.570], loss: 7.311197, mean_absolute_error: 34.863548, mean_q: 44.708664
[F[K  53253/500000: episode: 835, duration: 1.599s, episode steps: 97, steps per second: 61, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.509 [0.470, 0.560], loss: 7.965772, mean_absolute_error: 34.872570, mean_q: 44.748348
[F[K  53342/500000: episode: 836, duration: 1.408s, episode steps: 89, steps per second: 63, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.011 [0.000, 4.000], mean observation: 0.489 [0.410, 0.530], loss: 8.174542, mean_absolute_error: 34.696697, mean_q: 44.463406
[F[K  53398/500000: episode: 837, duration: 0.916s, episode steps: 56, steps per second: 61, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.232 [0.000, 4.000], mean observation: 0.493 [0.400, 0.530], loss: 7.936101, mean_absolute_error: 34.317337, mean_q: 44.017178
[F[K  53507/500000: episode: 838, duration: 2.074s, episode steps: 109, steps per second: 53, episode reward: 109.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.670 [0.000, 4.000], mean observation: 0.522 [0.470, 0.650], loss: 8.115240, mean_absolute_error: 34.971939, mean_q: 44.875332
[F[K  53568/500000: episode: 839, duration: 1.027s, episode steps: 61, steps per second: 59, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.754 [0.000, 4.000], mean observation: 0.525 [0.470, 0.640], loss: 7.159617, mean_absolute_error: 34.853130, mean_q: 44.811535
[F[K  53672/500000: episode: 840, duration: 1.889s, episode steps: 104, steps per second: 55, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.548 [0.000, 4.000], mean observation: 0.506 [0.470, 0.540], loss: 7.361448, mean_absolute_error: 34.816494, mean_q: 44.746140
[F[K  53745/500000: episode: 841, duration: 1.334s, episode steps: 73, steps per second: 55, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.425 [0.000, 4.000], mean observation: 0.484 [0.400, 0.520], loss: 8.205077, mean_absolute_error: 35.076790, mean_q: 44.939110
[F[K  53868/500000: episode: 842, duration: 1.999s, episode steps: 123, steps per second: 62, episode reward: 123.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.163 [0.000, 4.000], mean observation: 0.499 [0.410, 0.590], loss: 7.519848, mean_absolute_error: 34.932083, mean_q: 44.780998
[F[K  53941/500000: episode: 843, duration: 1.242s, episode steps: 73, steps per second: 59, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.959 [0.000, 4.000], mean observation: 0.481 [0.370, 0.530], loss: 7.727954, mean_absolute_error: 35.223907, mean_q: 45.220600
[F[K  54003/500000: episode: 844, duration: 1.201s, episode steps: 62, steps per second: 52, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.968 [0.000, 4.000], mean observation: 0.505 [0.480, 0.540], loss: 6.994012, mean_absolute_error: 34.999180, mean_q: 44.745289
[F[K  54107/500000: episode: 845, duration: 1.979s, episode steps: 104, steps per second: 53, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.240 [0.000, 4.000], mean observation: 0.498 [0.440, 0.550], loss: 7.743270, mean_absolute_error: 34.975548, mean_q: 44.881554
[F[K  54275/500000: episode: 846, duration: 2.851s, episode steps: 168, steps per second: 59, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.345 [0.000, 4.000], mean observation: 0.502 [0.470, 0.570], loss: 7.883965, mean_absolute_error: 35.090988, mean_q: 45.013355
[F[K  54348/500000: episode: 847, duration: 1.425s, episode steps: 73, steps per second: 51, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.822 [0.000, 4.000], mean observation: 0.496 [0.420, 0.530], loss: 7.342245, mean_absolute_error: 34.698174, mean_q: 44.450783
[F[K  54414/500000: episode: 848, duration: 1.245s, episode steps: 66, steps per second: 53, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.303 [0.000, 4.000], mean observation: 0.517 [0.480, 0.620], loss: 7.646614, mean_absolute_error: 34.769169, mean_q: 44.589520
[F[K  54510/500000: episode: 849, duration: 1.490s, episode steps: 96, steps per second: 64, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.208 [0.000, 4.000], mean observation: 0.504 [0.420, 0.620], loss: 7.987226, mean_absolute_error: 35.021832, mean_q: 44.929821
[F[K  54676/500000: episode: 850, duration: 3.524s, episode steps: 166, steps per second: 47, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.367 [0.000, 4.000], mean observation: 0.489 [0.390, 0.530], loss: 7.201501, mean_absolute_error: 34.854279, mean_q: 44.767109
[F[K  54733/500000: episode: 851, duration: 1.184s, episode steps: 57, steps per second: 48, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.737 [0.000, 4.000], mean observation: 0.504 [0.420, 0.630], loss: 7.694706, mean_absolute_error: 34.930180, mean_q: 44.841423
[F[K  54837/500000: episode: 852, duration: 2.041s, episode steps: 104, steps per second: 51, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.212 [0.000, 4.000], mean observation: 0.498 [0.430, 0.550], loss: 6.731908, mean_absolute_error: 35.165798, mean_q: 45.149132
[F[K  54917/500000: episode: 853, duration: 1.377s, episode steps: 80, steps per second: 58, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.837 [0.000, 4.000], mean observation: 0.504 [0.480, 0.530], loss: 8.234860, mean_absolute_error: 35.276451, mean_q: 45.258053
[F[K  54989/500000: episode: 854, duration: 1.173s, episode steps: 72, steps per second: 61, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.389 [0.000, 4.000], mean observation: 0.514 [0.470, 0.580], loss: 8.009673, mean_absolute_error: 35.297909, mean_q: 45.226524
[F[K  55060/500000: episode: 855, duration: 1.315s, episode steps: 71, steps per second: 54, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.690 [0.000, 4.000], mean observation: 0.505 [0.460, 0.590], loss: 7.229211, mean_absolute_error: 35.661488, mean_q: 45.757504
[F[K  55094/500000: episode: 856, duration: 0.678s, episode steps: 34, steps per second: 50, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.706 [0.000, 4.000], mean observation: 0.516 [0.470, 0.640], loss: 7.237543, mean_absolute_error: 35.508984, mean_q: 45.606579
[F[K  55159/500000: episode: 857, duration: 1.116s, episode steps: 65, steps per second: 58, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.046 [0.000, 4.000], mean observation: 0.497 [0.370, 0.600], loss: 7.952047, mean_absolute_error: 35.498207, mean_q: 45.475906
[F[K  55217/500000: episode: 858, duration: 0.953s, episode steps: 58, steps per second: 61, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.845 [0.000, 4.000], mean observation: 0.478 [0.350, 0.520], loss: 6.786199, mean_absolute_error: 35.539608, mean_q: 45.644447
[F[K  55314/500000: episode: 859, duration: 1.663s, episode steps: 97, steps per second: 58, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.320 [0.000, 4.000], mean observation: 0.500 [0.370, 0.620], loss: 7.558444, mean_absolute_error: 35.936844, mean_q: 46.084625
[F[K  55386/500000: episode: 860, duration: 1.262s, episode steps: 72, steps per second: 57, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.972 [0.000, 4.000], mean observation: 0.502 [0.390, 0.660], loss: 7.260788, mean_absolute_error: 35.645069, mean_q: 45.751675
[F[K  55458/500000: episode: 861, duration: 1.413s, episode steps: 72, steps per second: 51, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.486 [0.000, 4.000], mean observation: 0.516 [0.480, 0.580], loss: 8.863292, mean_absolute_error: 35.502789, mean_q: 45.483383
[F[K  55536/500000: episode: 862, duration: 1.402s, episode steps: 78, steps per second: 56, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.192 [0.000, 4.000], mean observation: 0.472 [0.350, 0.530], loss: 7.078201, mean_absolute_error: 35.115570, mean_q: 45.085079
[F[K  55595/500000: episode: 863, duration: 1.121s, episode steps: 59, steps per second: 53, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.153 [0.000, 4.000], mean observation: 0.503 [0.450, 0.560], loss: 7.568272, mean_absolute_error: 34.816437, mean_q: 44.846188
[F[K  55710/500000: episode: 864, duration: 2.031s, episode steps: 115, steps per second: 57, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.617 [0.000, 4.000], mean observation: 0.497 [0.380, 0.570], loss: 7.645303, mean_absolute_error: 36.077328, mean_q: 46.245567
[F[K  55808/500000: episode: 865, duration: 1.473s, episode steps: 98, steps per second: 67, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.439 [0.000, 4.000], mean observation: 0.495 [0.450, 0.530], loss: 7.674554, mean_absolute_error: 35.347763, mean_q: 45.348858
[F[K  55937/500000: episode: 866, duration: 2.703s, episode steps: 129, steps per second: 48, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.496 [0.000, 4.000], mean observation: 0.498 [0.420, 0.550], loss: 8.968201, mean_absolute_error: 35.704018, mean_q: 45.812225
[F[K  56003/500000: episode: 867, duration: 1.201s, episode steps: 66, steps per second: 55, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.515 [0.000, 4.000], mean observation: 0.494 [0.370, 0.570], loss: 8.749043, mean_absolute_error: 35.797138, mean_q: 45.847214
[F[K  56039/500000: episode: 868, duration: 0.609s, episode steps: 36, steps per second: 59, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 3.000], mean observation: 0.522 [0.470, 0.640], loss: 6.901206, mean_absolute_error: 34.915672, mean_q: 44.804264
[F[K  56103/500000: episode: 869, duration: 1.192s, episode steps: 64, steps per second: 54, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.953 [0.000, 4.000], mean observation: 0.489 [0.450, 0.520], loss: 7.588519, mean_absolute_error: 35.425499, mean_q: 45.350883
[F[K  56143/500000: episode: 870, duration: 0.752s, episode steps: 40, steps per second: 53, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.522 [0.470, 0.620], loss: 8.206390, mean_absolute_error: 34.468315, mean_q: 44.293308
[F[K  56220/500000: episode: 871, duration: 1.428s, episode steps: 77, steps per second: 54, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 0.482 [0.390, 0.520], loss: 7.566442, mean_absolute_error: 34.946079, mean_q: 44.938152
[F[K  56291/500000: episode: 872, duration: 1.360s, episode steps: 71, steps per second: 52, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.803 [0.000, 4.000], mean observation: 0.495 [0.430, 0.530], loss: 7.952116, mean_absolute_error: 35.690849, mean_q: 45.755474
[F[K  56336/500000: episode: 873, duration: 0.927s, episode steps: 45, steps per second: 49, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.644 [0.000, 4.000], mean observation: 0.495 [0.390, 0.530], loss: 9.417961, mean_absolute_error: 35.543808, mean_q: 45.499863
[F[K  56390/500000: episode: 874, duration: 1.166s, episode steps: 54, steps per second: 46, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.519 [0.000, 4.000], mean observation: 0.484 [0.410, 0.520], loss: 7.356079, mean_absolute_error: 35.674847, mean_q: 45.763424
[F[K  56484/500000: episode: 875, duration: 1.952s, episode steps: 94, steps per second: 48, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.745 [0.000, 4.000], mean observation: 0.504 [0.480, 0.550], loss: 8.174586, mean_absolute_error: 35.088982, mean_q: 44.978588
[F[K  56529/500000: episode: 876, duration: 0.874s, episode steps: 45, steps per second: 51, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.422 [0.000, 3.000], mean observation: 0.523 [0.470, 0.620], loss: 9.309205, mean_absolute_error: 34.772236, mean_q: 44.601101
[F[K  56586/500000: episode: 877, duration: 1.265s, episode steps: 57, steps per second: 45, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.175 [0.000, 4.000], mean observation: 0.525 [0.470, 0.640], loss: 7.884142, mean_absolute_error: 35.602562, mean_q: 45.645458
[F[K  56673/500000: episode: 878, duration: 1.641s, episode steps: 87, steps per second: 53, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.126 [0.000, 4.000], mean observation: 0.481 [0.380, 0.530], loss: 7.311065, mean_absolute_error: 35.360611, mean_q: 45.463783
[F[K  56709/500000: episode: 879, duration: 0.695s, episode steps: 36, steps per second: 52, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.194 [0.000, 3.000], mean observation: 0.502 [0.380, 0.650], loss: 7.930288, mean_absolute_error: 35.908791, mean_q: 46.102158
[F[K  56777/500000: episode: 880, duration: 1.194s, episode steps: 68, steps per second: 57, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.941 [0.000, 4.000], mean observation: 0.486 [0.360, 0.530], loss: 7.323227, mean_absolute_error: 35.737118, mean_q: 45.898853
[F[K  56829/500000: episode: 881, duration: 0.940s, episode steps: 52, steps per second: 55, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.288 [0.000, 4.000], mean observation: 0.510 [0.470, 0.630], loss: 6.646014, mean_absolute_error: 36.199566, mean_q: 46.505440
[F[K  56933/500000: episode: 882, duration: 1.650s, episode steps: 104, steps per second: 63, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.489 [0.380, 0.540], loss: 8.017292, mean_absolute_error: 35.501320, mean_q: 45.684883
[F[K  56996/500000: episode: 883, duration: 1.227s, episode steps: 63, steps per second: 51, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.857 [0.000, 4.000], mean observation: 0.498 [0.410, 0.540], loss: 7.433886, mean_absolute_error: 35.607819, mean_q: 45.694054
[F[K  57144/500000: episode: 884, duration: 3.082s, episode steps: 148, steps per second: 48, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.081 [0.000, 4.000], mean observation: 0.502 [0.400, 0.620], loss: 7.431193, mean_absolute_error: 35.559559, mean_q: 45.652454
[F[K  57222/500000: episode: 885, duration: 1.603s, episode steps: 78, steps per second: 49, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.910 [0.000, 4.000], mean observation: 0.502 [0.460, 0.550], loss: 8.960170, mean_absolute_error: 35.747955, mean_q: 45.677834
[F[K  57296/500000: episode: 886, duration: 1.232s, episode steps: 74, steps per second: 60, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.216 [0.000, 4.000], mean observation: 0.489 [0.420, 0.530], loss: 8.129012, mean_absolute_error: 35.977432, mean_q: 46.131847
[F[K  57379/500000: episode: 887, duration: 1.287s, episode steps: 83, steps per second: 64, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.169 [0.000, 4.000], mean observation: 0.501 [0.410, 0.580], loss: 7.822322, mean_absolute_error: 35.913471, mean_q: 45.946117
[F[K  57416/500000: episode: 888, duration: 0.722s, episode steps: 37, steps per second: 51, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.622 [0.000, 3.000], mean observation: 0.524 [0.470, 0.650], loss: 11.326131, mean_absolute_error: 35.272560, mean_q: 45.300598
[F[K  57484/500000: episode: 889, duration: 1.479s, episode steps: 68, steps per second: 46, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.471 [0.000, 4.000], mean observation: 0.503 [0.390, 0.620], loss: 8.387528, mean_absolute_error: 35.314392, mean_q: 45.259838
[F[K  57557/500000: episode: 890, duration: 1.646s, episode steps: 73, steps per second: 44, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.219 [0.000, 4.000], mean observation: 0.500 [0.490, 0.530], loss: 7.327677, mean_absolute_error: 35.510338, mean_q: 45.604427
[F[K  57627/500000: episode: 891, duration: 1.390s, episode steps: 70, steps per second: 50, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.014 [0.000, 4.000], mean observation: 0.485 [0.420, 0.530], loss: 8.230000, mean_absolute_error: 36.222130, mean_q: 46.463512
[F[K  57695/500000: episode: 892, duration: 1.303s, episode steps: 68, steps per second: 52, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.265 [0.000, 4.000], mean observation: 0.500 [0.380, 0.590], loss: 7.422351, mean_absolute_error: 36.124687, mean_q: 46.373428
[F[K  57761/500000: episode: 893, duration: 1.567s, episode steps: 66, steps per second: 42, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.818 [0.000, 4.000], mean observation: 0.505 [0.440, 0.580], loss: 8.109621, mean_absolute_error: 35.950588, mean_q: 46.201118
[F[K  57852/500000: episode: 894, duration: 1.959s, episode steps: 91, steps per second: 46, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.593 [0.000, 4.000], mean observation: 0.489 [0.380, 0.540], loss: 7.998176, mean_absolute_error: 35.636806, mean_q: 45.779957
[F[K  57922/500000: episode: 895, duration: 1.430s, episode steps: 70, steps per second: 49, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.900 [0.000, 4.000], mean observation: 0.473 [0.350, 0.530], loss: 7.945731, mean_absolute_error: 35.766502, mean_q: 45.962082
[F[K  57979/500000: episode: 896, duration: 1.191s, episode steps: 57, steps per second: 48, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.489 [0.380, 0.530], loss: 6.747471, mean_absolute_error: 35.513458, mean_q: 45.572433
[F[K  58037/500000: episode: 897, duration: 1.133s, episode steps: 58, steps per second: 51, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.534 [0.000, 4.000], mean observation: 0.512 [0.470, 0.600], loss: 7.993853, mean_absolute_error: 35.354534, mean_q: 45.323917
[F[K  58122/500000: episode: 898, duration: 1.614s, episode steps: 85, steps per second: 53, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.059 [0.000, 4.000], mean observation: 0.486 [0.420, 0.510], loss: 7.672853, mean_absolute_error: 35.962849, mean_q: 46.232475
[F[K  58207/500000: episode: 899, duration: 1.824s, episode steps: 85, steps per second: 47, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.494 [0.000, 4.000], mean observation: 0.496 [0.470, 0.530], loss: 8.495115, mean_absolute_error: 35.965164, mean_q: 46.133049
[F[K  58266/500000: episode: 900, duration: 1.065s, episode steps: 59, steps per second: 55, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.119 [0.000, 4.000], mean observation: 0.513 [0.500, 0.580], loss: 7.988081, mean_absolute_error: 36.107571, mean_q: 46.347637
[F[K  58322/500000: episode: 901, duration: 0.984s, episode steps: 56, steps per second: 57, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.750 [0.000, 4.000], mean observation: 0.507 [0.460, 0.570], loss: 8.276744, mean_absolute_error: 36.396065, mean_q: 46.679783
[F[K  58361/500000: episode: 902, duration: 0.780s, episode steps: 39, steps per second: 50, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.333 [0.000, 3.000], mean observation: 0.513 [0.470, 0.660], loss: 7.215409, mean_absolute_error: 36.209167, mean_q: 46.378654
[F[K  58426/500000: episode: 903, duration: 1.539s, episode steps: 65, steps per second: 42, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.385 [0.000, 4.000], mean observation: 0.509 [0.490, 0.550], loss: 7.605264, mean_absolute_error: 35.716652, mean_q: 45.812927
[F[K  58486/500000: episode: 904, duration: 1.110s, episode steps: 60, steps per second: 54, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.517 [0.000, 4.000], mean observation: 0.479 [0.350, 0.530], loss: 8.127101, mean_absolute_error: 35.403122, mean_q: 45.432110
[F[K  58656/500000: episode: 905, duration: 3.730s, episode steps: 170, steps per second: 46, episode reward: 170.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.188 [0.000, 4.000], mean observation: 0.499 [0.380, 0.620], loss: 8.025623, mean_absolute_error: 36.009029, mean_q: 46.076431
[F[K  58728/500000: episode: 906, duration: 1.525s, episode steps: 72, steps per second: 47, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.653 [0.000, 4.000], mean observation: 0.501 [0.470, 0.530], loss: 7.260181, mean_absolute_error: 35.618488, mean_q: 45.639000
[F[K  58841/500000: episode: 907, duration: 2.259s, episode steps: 113, steps per second: 50, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.805 [0.000, 4.000], mean observation: 0.497 [0.430, 0.540], loss: 7.946879, mean_absolute_error: 36.016533, mean_q: 46.171318
[F[K  58897/500000: episode: 908, duration: 0.996s, episode steps: 56, steps per second: 56, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.492 [0.360, 0.560], loss: 7.230253, mean_absolute_error: 35.970158, mean_q: 46.200466
[F[K  58993/500000: episode: 909, duration: 1.933s, episode steps: 96, steps per second: 50, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.698 [0.000, 4.000], mean observation: 0.494 [0.370, 0.580], loss: 7.519129, mean_absolute_error: 36.246319, mean_q: 46.518951
[F[K  59076/500000: episode: 910, duration: 1.634s, episode steps: 83, steps per second: 51, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.337 [0.000, 4.000], mean observation: 0.497 [0.420, 0.580], loss: 7.602048, mean_absolute_error: 35.571136, mean_q: 45.669327
[F[K  59138/500000: episode: 911, duration: 1.047s, episode steps: 62, steps per second: 59, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.694 [0.000, 4.000], mean observation: 0.484 [0.390, 0.530], loss: 7.947596, mean_absolute_error: 36.234669, mean_q: 46.367271
[F[K  59199/500000: episode: 912, duration: 1.337s, episode steps: 61, steps per second: 46, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.656 [0.000, 4.000], mean observation: 0.520 [0.470, 0.610], loss: 8.081296, mean_absolute_error: 35.174751, mean_q: 45.039070
[F[K  59261/500000: episode: 913, duration: 1.280s, episode steps: 62, steps per second: 48, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.855 [0.000, 4.000], mean observation: 0.481 [0.370, 0.530], loss: 7.710639, mean_absolute_error: 35.807606, mean_q: 45.807892
[F[K  59339/500000: episode: 914, duration: 1.682s, episode steps: 78, steps per second: 46, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.872 [0.000, 4.000], mean observation: 0.510 [0.490, 0.580], loss: 8.361415, mean_absolute_error: 36.267307, mean_q: 46.467720
[F[K  59428/500000: episode: 915, duration: 1.727s, episode steps: 89, steps per second: 52, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.764 [0.000, 4.000], mean observation: 0.506 [0.430, 0.650], loss: 8.379219, mean_absolute_error: 35.812973, mean_q: 45.943588
[F[K  59511/500000: episode: 916, duration: 1.546s, episode steps: 83, steps per second: 54, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.193 [0.000, 4.000], mean observation: 0.492 [0.430, 0.520], loss: 8.045428, mean_absolute_error: 36.152546, mean_q: 46.340195
[F[K  59566/500000: episode: 917, duration: 1.059s, episode steps: 55, steps per second: 52, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.127 [0.000, 4.000], mean observation: 0.505 [0.430, 0.630], loss: 7.858178, mean_absolute_error: 36.082043, mean_q: 46.251865
[F[K  59621/500000: episode: 918, duration: 1.155s, episode steps: 55, steps per second: 48, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.364 [0.000, 4.000], mean observation: 0.502 [0.400, 0.630], loss: 7.600737, mean_absolute_error: 35.604233, mean_q: 45.654953
[F[K  59694/500000: episode: 919, duration: 1.340s, episode steps: 73, steps per second: 54, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.658 [0.000, 4.000], mean observation: 0.509 [0.480, 0.610], loss: 7.555024, mean_absolute_error: 36.659534, mean_q: 46.855698
[F[K  59735/500000: episode: 920, duration: 0.892s, episode steps: 41, steps per second: 46, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.171 [0.000, 4.000], mean observation: 0.483 [0.350, 0.530], loss: 8.434437, mean_absolute_error: 35.431664, mean_q: 45.499466
[F[K  59798/500000: episode: 921, duration: 1.149s, episode steps: 63, steps per second: 55, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.762 [0.000, 4.000], mean observation: 0.505 [0.460, 0.580], loss: 7.139026, mean_absolute_error: 35.764008, mean_q: 45.947704
[F[K  59923/500000: episode: 922, duration: 2.471s, episode steps: 125, steps per second: 51, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.568 [0.000, 4.000], mean observation: 0.512 [0.490, 0.600], loss: 8.049401, mean_absolute_error: 36.336052, mean_q: 46.677563
[F[K  60064/500000: episode: 923, duration: 2.801s, episode steps: 141, steps per second: 50, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.312 [0.000, 4.000], mean observation: 0.490 [0.410, 0.530], loss: 8.853945, mean_absolute_error: 35.758919, mean_q: 45.805046
[F[K  60131/500000: episode: 924, duration: 1.458s, episode steps: 67, steps per second: 46, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.269 [0.000, 4.000], mean observation: 0.500 [0.480, 0.530], loss: 7.801408, mean_absolute_error: 35.728786, mean_q: 45.886715
[F[K  60176/500000: episode: 925, duration: 0.981s, episode steps: 45, steps per second: 46, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.519 [0.470, 0.630], loss: 9.018831, mean_absolute_error: 35.884464, mean_q: 45.986229
[F[K  60252/500000: episode: 926, duration: 1.380s, episode steps: 76, steps per second: 55, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.605 [0.000, 4.000], mean observation: 0.507 [0.430, 0.630], loss: 8.215113, mean_absolute_error: 35.576534, mean_q: 45.621552
[F[K  60300/500000: episode: 927, duration: 0.982s, episode steps: 48, steps per second: 49, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.917 [0.000, 4.000], mean observation: 0.491 [0.420, 0.530], loss: 7.689135, mean_absolute_error: 34.286179, mean_q: 44.018906
[F[K  60416/500000: episode: 928, duration: 2.249s, episode steps: 116, steps per second: 52, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.845 [0.000, 4.000], mean observation: 0.498 [0.400, 0.600], loss: 7.566944, mean_absolute_error: 35.639088, mean_q: 45.750572
[F[K  60469/500000: episode: 929, duration: 1.100s, episode steps: 53, steps per second: 48, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.508 [0.470, 0.610], loss: 7.612275, mean_absolute_error: 35.832012, mean_q: 46.012474
[F[K  60566/500000: episode: 930, duration: 2.171s, episode steps: 97, steps per second: 45, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.433 [0.000, 4.000], mean observation: 0.492 [0.400, 0.530], loss: 7.568338, mean_absolute_error: 35.451641, mean_q: 45.478233
[F[K  60645/500000: episode: 931, duration: 1.593s, episode steps: 79, steps per second: 50, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.810 [0.000, 4.000], mean observation: 0.487 [0.380, 0.530], loss: 7.282636, mean_absolute_error: 35.461361, mean_q: 45.554844
[F[K  60704/500000: episode: 932, duration: 1.161s, episode steps: 59, steps per second: 51, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.814 [0.000, 4.000], mean observation: 0.481 [0.400, 0.530], loss: 7.989978, mean_absolute_error: 35.611355, mean_q: 45.686295
[F[K  60759/500000: episode: 933, duration: 1.152s, episode steps: 55, steps per second: 48, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.927 [0.000, 4.000], mean observation: 0.509 [0.470, 0.600], loss: 8.036233, mean_absolute_error: 35.762840, mean_q: 45.938957
[F[K  60820/500000: episode: 934, duration: 0.928s, episode steps: 61, steps per second: 66, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.967 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 7.077669, mean_absolute_error: 34.532906, mean_q: 44.461708
[F[K  60899/500000: episode: 935, duration: 1.316s, episode steps: 79, steps per second: 60, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.405 [0.000, 4.000], mean observation: 0.502 [0.470, 0.530], loss: 7.914698, mean_absolute_error: 36.133881, mean_q: 46.272541
[F[K  60978/500000: episode: 936, duration: 1.603s, episode steps: 79, steps per second: 49, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.823 [0.000, 4.000], mean observation: 0.493 [0.430, 0.530], loss: 8.532451, mean_absolute_error: 35.925056, mean_q: 46.068466
[F[K  61049/500000: episode: 937, duration: 1.429s, episode steps: 71, steps per second: 50, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.817 [0.000, 4.000], mean observation: 0.514 [0.470, 0.650], loss: 6.961649, mean_absolute_error: 36.089127, mean_q: 46.177193
[F[K  61118/500000: episode: 938, duration: 1.548s, episode steps: 69, steps per second: 45, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.174 [0.000, 4.000], mean observation: 0.496 [0.450, 0.520], loss: 8.123474, mean_absolute_error: 35.217686, mean_q: 45.192425
[F[K  61192/500000: episode: 939, duration: 1.543s, episode steps: 74, steps per second: 48, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.635 [0.000, 4.000], mean observation: 0.489 [0.370, 0.540], loss: 7.491621, mean_absolute_error: 35.718658, mean_q: 45.827370
[F[K  61261/500000: episode: 940, duration: 1.296s, episode steps: 69, steps per second: 53, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.855 [0.000, 4.000], mean observation: 0.496 [0.390, 0.540], loss: 7.109246, mean_absolute_error: 36.273129, mean_q: 46.555962
[F[K  61371/500000: episode: 941, duration: 1.859s, episode steps: 110, steps per second: 59, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.573 [0.000, 4.000], mean observation: 0.500 [0.460, 0.530], loss: 7.690400, mean_absolute_error: 36.020130, mean_q: 46.179245
[F[K  61430/500000: episode: 942, duration: 1.124s, episode steps: 59, steps per second: 52, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.390 [0.000, 4.000], mean observation: 0.509 [0.490, 0.540], loss: 7.106688, mean_absolute_error: 36.195049, mean_q: 46.517883
[F[K  61496/500000: episode: 943, duration: 1.335s, episode steps: 66, steps per second: 49, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.258 [0.000, 4.000], mean observation: 0.507 [0.450, 0.620], loss: 7.463731, mean_absolute_error: 36.994488, mean_q: 47.424492
[F[K  61570/500000: episode: 944, duration: 1.310s, episode steps: 74, steps per second: 57, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.581 [0.000, 4.000], mean observation: 0.515 [0.480, 0.630], loss: 7.480061, mean_absolute_error: 35.961048, mean_q: 46.179150
[F[K  61631/500000: episode: 945, duration: 0.916s, episode steps: 61, steps per second: 67, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.574 [0.000, 4.000], mean observation: 0.498 [0.410, 0.570], loss: 7.782881, mean_absolute_error: 36.437870, mean_q: 46.634308
[F[K  61756/500000: episode: 946, duration: 1.963s, episode steps: 125, steps per second: 64, episode reward: 125.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.984 [0.000, 4.000], mean observation: 0.497 [0.390, 0.590], loss: 7.696715, mean_absolute_error: 35.933968, mean_q: 46.041824
[F[K  61856/500000: episode: 947, duration: 1.780s, episode steps: 100, steps per second: 56, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.480 [0.000, 4.000], mean observation: 0.491 [0.410, 0.530], loss: 7.643228, mean_absolute_error: 35.598442, mean_q: 45.661686
[F[K  61970/500000: episode: 948, duration: 1.886s, episode steps: 114, steps per second: 60, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.035 [0.000, 4.000], mean observation: 0.515 [0.470, 0.640], loss: 7.609598, mean_absolute_error: 35.808079, mean_q: 45.869667
[F[K  62046/500000: episode: 949, duration: 0.942s, episode steps: 76, steps per second: 81, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.276 [0.000, 4.000], mean observation: 0.500 [0.390, 0.620], loss: 8.279766, mean_absolute_error: 36.099125, mean_q: 46.306446
[F[K  62166/500000: episode: 950, duration: 2.086s, episode steps: 120, steps per second: 58, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.867 [0.000, 4.000], mean observation: 0.479 [0.370, 0.520], loss: 7.582388, mean_absolute_error: 36.336109, mean_q: 46.588184
[F[K  62342/500000: episode: 951, duration: 3.118s, episode steps: 176, steps per second: 56, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.386 [0.000, 4.000], mean observation: 0.482 [0.360, 0.520], loss: 8.187650, mean_absolute_error: 36.398258, mean_q: 46.565884
[F[K  62394/500000: episode: 952, duration: 0.954s, episode steps: 52, steps per second: 54, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.654 [0.000, 4.000], mean observation: 0.499 [0.360, 0.620], loss: 7.521459, mean_absolute_error: 36.652309, mean_q: 46.989395
[F[K  62470/500000: episode: 953, duration: 1.317s, episode steps: 76, steps per second: 58, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.013 [0.000, 4.000], mean observation: 0.478 [0.360, 0.520], loss: 6.858184, mean_absolute_error: 35.568443, mean_q: 45.675735
[F[K  62538/500000: episode: 954, duration: 1.260s, episode steps: 68, steps per second: 54, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.824 [0.000, 4.000], mean observation: 0.500 [0.440, 0.560], loss: 7.517963, mean_absolute_error: 35.873394, mean_q: 46.038315
[F[K  62619/500000: episode: 955, duration: 1.298s, episode steps: 81, steps per second: 62, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.926 [0.000, 4.000], mean observation: 0.503 [0.440, 0.590], loss: 7.706513, mean_absolute_error: 36.374599, mean_q: 46.628197
[F[K  62666/500000: episode: 956, duration: 0.805s, episode steps: 47, steps per second: 58, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.957 [0.000, 4.000], mean observation: 0.490 [0.360, 0.540], loss: 6.723012, mean_absolute_error: 36.143867, mean_q: 46.276867
[F[K  62741/500000: episode: 957, duration: 1.341s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.653 [0.000, 4.000], mean observation: 0.516 [0.470, 0.590], loss: 6.670481, mean_absolute_error: 36.334053, mean_q: 46.609146
[F[K  62830/500000: episode: 958, duration: 1.421s, episode steps: 89, steps per second: 63, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.135 [0.000, 4.000], mean observation: 0.493 [0.380, 0.550], loss: 7.867419, mean_absolute_error: 36.581051, mean_q: 46.921295
[F[K  62869/500000: episode: 959, duration: 0.639s, episode steps: 39, steps per second: 61, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.462 [0.000, 4.000], mean observation: 0.501 [0.370, 0.650], loss: 7.579072, mean_absolute_error: 36.989311, mean_q: 47.660603
[F[K  62931/500000: episode: 960, duration: 1.079s, episode steps: 62, steps per second: 57, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.242 [0.000, 4.000], mean observation: 0.511 [0.490, 0.590], loss: 8.005846, mean_absolute_error: 35.949245, mean_q: 46.112427
[F[K  62999/500000: episode: 961, duration: 1.160s, episode steps: 68, steps per second: 59, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.118 [0.000, 4.000], mean observation: 0.521 [0.470, 0.610], loss: 7.994473, mean_absolute_error: 35.857624, mean_q: 45.945179
[F[K  63173/500000: episode: 962, duration: 2.636s, episode steps: 174, steps per second: 66, episode reward: 174.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.052 [0.000, 4.000], mean observation: 0.487 [0.380, 0.540], loss: 7.725415, mean_absolute_error: 36.483410, mean_q: 46.836792
[F[K  63217/500000: episode: 963, duration: 0.742s, episode steps: 44, steps per second: 59, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.114 [0.000, 4.000], mean observation: 0.498 [0.360, 0.590], loss: 7.575351, mean_absolute_error: 36.519463, mean_q: 46.775681
[F[K  63292/500000: episode: 964, duration: 1.326s, episode steps: 75, steps per second: 57, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.120 [0.000, 4.000], mean observation: 0.483 [0.370, 0.530], loss: 7.223053, mean_absolute_error: 36.702637, mean_q: 47.041092
[F[K  63436/500000: episode: 965, duration: 2.688s, episode steps: 144, steps per second: 54, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.181 [0.000, 4.000], mean observation: 0.499 [0.430, 0.580], loss: 7.779141, mean_absolute_error: 37.023300, mean_q: 47.484077
[F[K  63498/500000: episode: 966, duration: 1.288s, episode steps: 62, steps per second: 48, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.048 [0.000, 4.000], mean observation: 0.517 [0.470, 0.590], loss: 8.363169, mean_absolute_error: 36.606674, mean_q: 46.974720
[F[K  63535/500000: episode: 967, duration: 0.672s, episode steps: 37, steps per second: 55, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.757 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 6.846853, mean_absolute_error: 37.353493, mean_q: 47.866047
[F[K  63631/500000: episode: 968, duration: 1.746s, episode steps: 96, steps per second: 55, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.083 [0.000, 4.000], mean observation: 0.505 [0.420, 0.650], loss: 7.272679, mean_absolute_error: 37.408436, mean_q: 47.909885
[F[K  63667/500000: episode: 969, duration: 0.774s, episode steps: 36, steps per second: 46, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.639 [0.000, 4.000], mean observation: 0.520 [0.470, 0.640], loss: 7.906667, mean_absolute_error: 36.411961, mean_q: 46.727966
[F[K  63736/500000: episode: 970, duration: 1.203s, episode steps: 69, steps per second: 57, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.826 [0.000, 4.000], mean observation: 0.482 [0.390, 0.530], loss: 8.341633, mean_absolute_error: 36.903557, mean_q: 47.187813
[F[K  63794/500000: episode: 971, duration: 0.965s, episode steps: 58, steps per second: 60, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.138 [0.000, 4.000], mean observation: 0.515 [0.500, 0.580], loss: 8.560104, mean_absolute_error: 36.451107, mean_q: 46.757294
[F[K  63852/500000: episode: 972, duration: 1.235s, episode steps: 58, steps per second: 47, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.483 [0.000, 4.000], mean observation: 0.492 [0.380, 0.530], loss: 7.589161, mean_absolute_error: 36.743668, mean_q: 47.052132
[F[K  63937/500000: episode: 973, duration: 1.576s, episode steps: 85, steps per second: 54, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.953 [0.000, 4.000], mean observation: 0.501 [0.460, 0.550], loss: 7.759595, mean_absolute_error: 36.961418, mean_q: 47.364960
[F[K  63993/500000: episode: 974, duration: 1.118s, episode steps: 56, steps per second: 50, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.643 [0.000, 4.000], mean observation: 0.494 [0.440, 0.530], loss: 8.026806, mean_absolute_error: 37.153706, mean_q: 47.605389
[F[K  64064/500000: episode: 975, duration: 1.219s, episode steps: 71, steps per second: 58, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.873 [0.000, 4.000], mean observation: 0.501 [0.480, 0.530], loss: 7.468478, mean_absolute_error: 36.755772, mean_q: 46.932629
[F[K  64108/500000: episode: 976, duration: 0.804s, episode steps: 44, steps per second: 55, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.523 [0.000, 4.000], mean observation: 0.521 [0.470, 0.620], loss: 9.067187, mean_absolute_error: 36.738163, mean_q: 47.039516
[F[K  64174/500000: episode: 977, duration: 1.272s, episode steps: 66, steps per second: 52, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.121 [0.000, 4.000], mean observation: 0.518 [0.490, 0.610], loss: 8.602469, mean_absolute_error: 36.642910, mean_q: 46.874413
[F[K  64262/500000: episode: 978, duration: 1.577s, episode steps: 88, steps per second: 56, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.852 [0.000, 4.000], mean observation: 0.486 [0.390, 0.530], loss: 7.043571, mean_absolute_error: 36.850368, mean_q: 47.128464
[F[K  64332/500000: episode: 979, duration: 1.286s, episode steps: 70, steps per second: 54, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.643 [0.000, 4.000], mean observation: 0.494 [0.380, 0.530], loss: 8.036534, mean_absolute_error: 36.124451, mean_q: 46.205341
[F[K  64407/500000: episode: 980, duration: 1.351s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.493 [0.350, 0.550], loss: 8.447545, mean_absolute_error: 37.549088, mean_q: 48.022781
[F[K  64475/500000: episode: 981, duration: 1.345s, episode steps: 68, steps per second: 51, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.721 [0.000, 4.000], mean observation: 0.490 [0.440, 0.530], loss: 8.284348, mean_absolute_error: 37.054153, mean_q: 47.485458
[F[K  64549/500000: episode: 982, duration: 1.245s, episode steps: 74, steps per second: 59, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.517 [0.480, 0.610], loss: 8.167820, mean_absolute_error: 37.066803, mean_q: 47.361290
[F[K  64749/500000: episode: 983, duration: 3.106s, episode steps: 200, steps per second: 64, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.290 [0.000, 4.000], mean observation: 0.500 [0.420, 0.570], loss: 8.217923, mean_absolute_error: 37.062260, mean_q: 47.359932
[F[K  64786/500000: episode: 984, duration: 0.672s, episode steps: 37, steps per second: 55, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.541 [0.000, 3.000], mean observation: 0.514 [0.470, 0.660], loss: 6.805785, mean_absolute_error: 36.605610, mean_q: 46.875038
[F[K  64852/500000: episode: 985, duration: 1.210s, episode steps: 66, steps per second: 55, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.516 [0.470, 0.630], loss: 9.201460, mean_absolute_error: 37.540340, mean_q: 47.924030
[F[K  65003/500000: episode: 986, duration: 2.733s, episode steps: 151, steps per second: 55, episode reward: 151.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.046 [0.000, 4.000], mean observation: 0.486 [0.340, 0.530], loss: 7.806988, mean_absolute_error: 36.686745, mean_q: 46.984619
[F[K  65078/500000: episode: 987, duration: 1.332s, episode steps: 75, steps per second: 56, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.067 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 8.601482, mean_absolute_error: 36.995235, mean_q: 47.345497
[F[K  65179/500000: episode: 988, duration: 1.649s, episode steps: 101, steps per second: 61, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.505 [0.000, 4.000], mean observation: 0.480 [0.360, 0.530], loss: 7.739793, mean_absolute_error: 37.128311, mean_q: 47.559841
[F[K  65213/500000: episode: 989, duration: 0.516s, episode steps: 34, steps per second: 66, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.765 [0.000, 3.000], mean observation: 0.513 [0.470, 0.640], loss: 7.876762, mean_absolute_error: 35.774574, mean_q: 45.811703
[F[K  65394/500000: episode: 990, duration: 2.612s, episode steps: 181, steps per second: 69, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.193 [0.000, 4.000], mean observation: 0.500 [0.410, 0.600], loss: 8.463318, mean_absolute_error: 36.779846, mean_q: 47.016251
[F[K  65441/500000: episode: 991, duration: 0.861s, episode steps: 47, steps per second: 55, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.957 [0.000, 4.000], mean observation: 0.477 [0.350, 0.530], loss: 7.762117, mean_absolute_error: 36.057613, mean_q: 46.174576
[F[K  65543/500000: episode: 992, duration: 1.495s, episode steps: 102, steps per second: 68, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.451 [0.000, 4.000], mean observation: 0.484 [0.380, 0.530], loss: 7.807794, mean_absolute_error: 36.413326, mean_q: 46.587208
[F[K  65617/500000: episode: 993, duration: 1.163s, episode steps: 74, steps per second: 64, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.784 [0.000, 4.000], mean observation: 0.500 [0.410, 0.580], loss: 7.782185, mean_absolute_error: 37.136658, mean_q: 47.549938
[F[K  65688/500000: episode: 994, duration: 1.163s, episode steps: 71, steps per second: 61, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.310 [0.000, 4.000], mean observation: 0.508 [0.470, 0.580], loss: 8.272297, mean_absolute_error: 37.207211, mean_q: 47.595051
[F[K  65756/500000: episode: 995, duration: 1.208s, episode steps: 68, steps per second: 56, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.338 [0.000, 4.000], mean observation: 0.503 [0.470, 0.540], loss: 7.792277, mean_absolute_error: 37.099277, mean_q: 47.409401
[F[K  65804/500000: episode: 996, duration: 0.974s, episode steps: 48, steps per second: 49, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.506 [0.500, 0.530], loss: 8.036181, mean_absolute_error: 36.729649, mean_q: 47.176285
[F[K  65839/500000: episode: 997, duration: 0.652s, episode steps: 35, steps per second: 54, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.486 [0.000, 3.000], mean observation: 0.510 [0.460, 0.650], loss: 8.346547, mean_absolute_error: 36.957092, mean_q: 47.460918
[F[K  65896/500000: episode: 998, duration: 1.050s, episode steps: 57, steps per second: 54, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.702 [0.000, 4.000], mean observation: 0.505 [0.370, 0.650], loss: 8.234649, mean_absolute_error: 37.438938, mean_q: 47.867455
[F[K  65947/500000: episode: 999, duration: 0.854s, episode steps: 51, steps per second: 60, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.980 [0.000, 4.000], mean observation: 0.516 [0.470, 0.610], loss: 8.055692, mean_absolute_error: 37.589912, mean_q: 48.123043
[F[K  66009/500000: episode: 1000, duration: 0.973s, episode steps: 62, steps per second: 64, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.242 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 6.947259, mean_absolute_error: 37.480747, mean_q: 47.884537
[F[K  66127/500000: episode: 1001, duration: 1.401s, episode steps: 118, steps per second: 84, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.195 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 8.040653, mean_absolute_error: 36.905624, mean_q: 47.252815
[F[K  66213/500000: episode: 1002, duration: 1.119s, episode steps: 86, steps per second: 77, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.942 [0.000, 4.000], mean observation: 0.505 [0.470, 0.580], loss: 8.109224, mean_absolute_error: 37.665203, mean_q: 48.272648
[F[K  66304/500000: episode: 1003, duration: 1.157s, episode steps: 91, steps per second: 79, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.330 [0.000, 4.000], mean observation: 0.484 [0.390, 0.530], loss: 8.103089, mean_absolute_error: 37.012619, mean_q: 47.365971
[F[K  66355/500000: episode: 1004, duration: 0.762s, episode steps: 51, steps per second: 67, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.490 [0.000, 3.000], mean observation: 0.511 [0.470, 0.630], loss: 7.413884, mean_absolute_error: 37.084381, mean_q: 47.473610
[F[K  66451/500000: episode: 1005, duration: 1.413s, episode steps: 96, steps per second: 68, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.094 [0.000, 4.000], mean observation: 0.491 [0.420, 0.530], loss: 7.805643, mean_absolute_error: 37.459721, mean_q: 47.974316
[F[K  66485/500000: episode: 1006, duration: 0.498s, episode steps: 34, steps per second: 68, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.676 [0.000, 3.000], mean observation: 0.515 [0.470, 0.650], loss: 7.974094, mean_absolute_error: 37.469700, mean_q: 47.962334
[F[K  66528/500000: episode: 1007, duration: 0.651s, episode steps: 43, steps per second: 66, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.791 [0.000, 4.000], mean observation: 0.509 [0.460, 0.630], loss: 9.076440, mean_absolute_error: 37.602318, mean_q: 48.145813
[F[K  66577/500000: episode: 1008, duration: 0.848s, episode steps: 49, steps per second: 58, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.347 [0.000, 4.000], mean observation: 0.500 [0.370, 0.640], loss: 8.030179, mean_absolute_error: 37.949238, mean_q: 48.551807
[F[K  66631/500000: episode: 1009, duration: 0.860s, episode steps: 54, steps per second: 63, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.167 [0.000, 4.000], mean observation: 0.504 [0.410, 0.600], loss: 7.739352, mean_absolute_error: 37.061001, mean_q: 47.441093
[F[K  66682/500000: episode: 1010, duration: 0.899s, episode steps: 51, steps per second: 57, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.843 [0.000, 4.000], mean observation: 0.507 [0.490, 0.530], loss: 8.439180, mean_absolute_error: 37.177013, mean_q: 47.620525
[F[K  66747/500000: episode: 1011, duration: 1.136s, episode steps: 65, steps per second: 57, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.938 [0.000, 4.000], mean observation: 0.503 [0.450, 0.570], loss: 7.041429, mean_absolute_error: 37.008137, mean_q: 47.446682
[F[K  66845/500000: episode: 1012, duration: 1.798s, episode steps: 98, steps per second: 55, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.255 [0.000, 4.000], mean observation: 0.477 [0.360, 0.540], loss: 7.760535, mean_absolute_error: 37.079327, mean_q: 47.517593
[F[K  66902/500000: episode: 1013, duration: 0.874s, episode steps: 57, steps per second: 65, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.053 [0.000, 4.000], mean observation: 0.503 [0.440, 0.640], loss: 8.026352, mean_absolute_error: 37.238720, mean_q: 47.621826
[F[K  66978/500000: episode: 1014, duration: 1.475s, episode steps: 76, steps per second: 52, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.566 [0.000, 4.000], mean observation: 0.508 [0.470, 0.600], loss: 8.297976, mean_absolute_error: 36.908501, mean_q: 47.322937
[F[K  67028/500000: episode: 1015, duration: 0.877s, episode steps: 50, steps per second: 57, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.460 [0.000, 4.000], mean observation: 0.508 [0.470, 0.550], loss: 8.436048, mean_absolute_error: 37.535824, mean_q: 48.182724
[F[K  67086/500000: episode: 1016, duration: 1.000s, episode steps: 58, steps per second: 58, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.966 [0.000, 4.000], mean observation: 0.519 [0.500, 0.620], loss: 8.708068, mean_absolute_error: 37.447746, mean_q: 47.838295
[F[K  67134/500000: episode: 1017, duration: 0.892s, episode steps: 48, steps per second: 54, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.604 [0.000, 4.000], mean observation: 0.495 [0.360, 0.600], loss: 8.016509, mean_absolute_error: 37.032681, mean_q: 47.411346
[F[K  67218/500000: episode: 1018, duration: 1.448s, episode steps: 84, steps per second: 58, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.893 [0.000, 4.000], mean observation: 0.515 [0.480, 0.570], loss: 7.878855, mean_absolute_error: 37.523594, mean_q: 47.969208
[F[K  67329/500000: episode: 1019, duration: 1.597s, episode steps: 111, steps per second: 70, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.775 [0.000, 4.000], mean observation: 0.492 [0.380, 0.540], loss: 8.219757, mean_absolute_error: 37.854298, mean_q: 48.456554
[F[K  67391/500000: episode: 1020, duration: 0.903s, episode steps: 62, steps per second: 69, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.758 [0.000, 4.000], mean observation: 0.521 [0.480, 0.630], loss: 8.740952, mean_absolute_error: 36.990696, mean_q: 47.385860
[F[K  67457/500000: episode: 1021, duration: 1.166s, episode steps: 66, steps per second: 57, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.227 [0.000, 4.000], mean observation: 0.480 [0.390, 0.520], loss: 8.703103, mean_absolute_error: 37.279774, mean_q: 47.809841
[F[K  67527/500000: episode: 1022, duration: 1.300s, episode steps: 70, steps per second: 54, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.057 [0.000, 4.000], mean observation: 0.483 [0.400, 0.530], loss: 7.075491, mean_absolute_error: 37.711254, mean_q: 48.267982
[F[K  67562/500000: episode: 1023, duration: 0.620s, episode steps: 35, steps per second: 56, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.486 [0.000, 4.000], mean observation: 0.525 [0.470, 0.630], loss: 8.013271, mean_absolute_error: 38.363750, mean_q: 49.129822
[F[K  67635/500000: episode: 1024, duration: 1.201s, episode steps: 73, steps per second: 61, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.753 [0.000, 4.000], mean observation: 0.512 [0.500, 0.570], loss: 7.380051, mean_absolute_error: 37.413643, mean_q: 47.927071
[F[K  67704/500000: episode: 1025, duration: 1.172s, episode steps: 69, steps per second: 59, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.391 [0.000, 4.000], mean observation: 0.496 [0.400, 0.560], loss: 8.477862, mean_absolute_error: 37.726643, mean_q: 48.209248
[F[K  67773/500000: episode: 1026, duration: 1.008s, episode steps: 69, steps per second: 68, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.739 [0.000, 4.000], mean observation: 0.496 [0.370, 0.590], loss: 8.001731, mean_absolute_error: 37.107712, mean_q: 47.559444
[F[K  67935/500000: episode: 1027, duration: 2.722s, episode steps: 162, steps per second: 60, episode reward: 162.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.031 [0.000, 4.000], mean observation: 0.489 [0.360, 0.530], loss: 8.763946, mean_absolute_error: 37.945927, mean_q: 48.612404
[F[K  68033/500000: episode: 1028, duration: 1.719s, episode steps: 98, steps per second: 57, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.143 [0.000, 4.000], mean observation: 0.487 [0.430, 0.530], loss: 8.482140, mean_absolute_error: 37.049248, mean_q: 47.519222
[F[K  68211/500000: episode: 1029, duration: 3.177s, episode steps: 178, steps per second: 56, episode reward: 178.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.202 [0.000, 4.000], mean observation: 0.502 [0.380, 0.640], loss: 8.007108, mean_absolute_error: 38.042389, mean_q: 48.703827
[F[K  68290/500000: episode: 1030, duration: 1.157s, episode steps: 79, steps per second: 68, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.304 [0.000, 4.000], mean observation: 0.497 [0.390, 0.590], loss: 8.552994, mean_absolute_error: 37.683159, mean_q: 48.167740
[F[K  68379/500000: episode: 1031, duration: 1.513s, episode steps: 89, steps per second: 59, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.629 [0.000, 4.000], mean observation: 0.482 [0.380, 0.530], loss: 8.763359, mean_absolute_error: 37.290836, mean_q: 47.594116
[F[K  68452/500000: episode: 1032, duration: 1.200s, episode steps: 73, steps per second: 61, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.959 [0.000, 4.000], mean observation: 0.484 [0.360, 0.520], loss: 7.625041, mean_absolute_error: 37.758442, mean_q: 48.263138
[F[K  68572/500000: episode: 1033, duration: 2.010s, episode steps: 120, steps per second: 60, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.450 [0.000, 4.000], mean observation: 0.495 [0.420, 0.540], loss: 7.449238, mean_absolute_error: 37.758732, mean_q: 48.348030
[F[K  68710/500000: episode: 1034, duration: 2.390s, episode steps: 138, steps per second: 58, episode reward: 138.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.964 [0.000, 4.000], mean observation: 0.507 [0.480, 0.550], loss: 7.824064, mean_absolute_error: 37.622593, mean_q: 48.140068
[F[K  68778/500000: episode: 1035, duration: 1.182s, episode steps: 68, steps per second: 58, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.147 [0.000, 4.000], mean observation: 0.498 [0.420, 0.580], loss: 8.395879, mean_absolute_error: 37.069393, mean_q: 47.420414
[F[K  68836/500000: episode: 1036, duration: 1.071s, episode steps: 58, steps per second: 54, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.328 [0.000, 4.000], mean observation: 0.523 [0.470, 0.650], loss: 8.509295, mean_absolute_error: 37.853062, mean_q: 48.340202
[F[K  68885/500000: episode: 1037, duration: 0.764s, episode steps: 49, steps per second: 64, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.061 [0.000, 4.000], mean observation: 0.524 [0.470, 0.630], loss: 7.806828, mean_absolute_error: 36.914185, mean_q: 47.291233
[F[K  68978/500000: episode: 1038, duration: 1.624s, episode steps: 93, steps per second: 57, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.194 [0.000, 4.000], mean observation: 0.487 [0.400, 0.530], loss: 8.524289, mean_absolute_error: 38.243046, mean_q: 48.934219
[F[K  69083/500000: episode: 1039, duration: 1.507s, episode steps: 105, steps per second: 70, episode reward: 105.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.543 [0.000, 4.000], mean observation: 0.504 [0.450, 0.580], loss: 6.896013, mean_absolute_error: 37.788303, mean_q: 48.347599
[F[K  69148/500000: episode: 1040, duration: 0.938s, episode steps: 65, steps per second: 69, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.123 [0.000, 4.000], mean observation: 0.484 [0.420, 0.510], loss: 8.096920, mean_absolute_error: 37.855911, mean_q: 48.485802
[F[K  69213/500000: episode: 1041, duration: 1.132s, episode steps: 65, steps per second: 57, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.723 [0.000, 3.000], mean observation: 0.497 [0.370, 0.630], loss: 7.629385, mean_absolute_error: 37.242466, mean_q: 47.717133
[F[K  69265/500000: episode: 1042, duration: 0.996s, episode steps: 52, steps per second: 52, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.288 [0.000, 4.000], mean observation: 0.497 [0.350, 0.630], loss: 7.532983, mean_absolute_error: 37.396152, mean_q: 48.000004
[F[K  69373/500000: episode: 1043, duration: 1.954s, episode steps: 108, steps per second: 55, episode reward: 108.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.741 [0.000, 4.000], mean observation: 0.486 [0.410, 0.540], loss: 7.768952, mean_absolute_error: 37.567165, mean_q: 48.187759
[F[K  69463/500000: episode: 1044, duration: 1.547s, episode steps: 90, steps per second: 58, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.911 [0.000, 4.000], mean observation: 0.508 [0.470, 0.560], loss: 7.961321, mean_absolute_error: 37.975918, mean_q: 48.738350
[F[K  69533/500000: episode: 1045, duration: 1.235s, episode steps: 70, steps per second: 57, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.271 [0.000, 4.000], mean observation: 0.477 [0.370, 0.530], loss: 6.930812, mean_absolute_error: 37.779434, mean_q: 48.422150
[F[K  69602/500000: episode: 1046, duration: 1.309s, episode steps: 69, steps per second: 53, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.486 [0.390, 0.510], loss: 8.053619, mean_absolute_error: 37.640392, mean_q: 48.211010
[F[K  69693/500000: episode: 1047, duration: 1.394s, episode steps: 91, steps per second: 65, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.473 [0.000, 4.000], mean observation: 0.515 [0.460, 0.640], loss: 8.220077, mean_absolute_error: 37.974541, mean_q: 48.511154
[F[K  69797/500000: episode: 1048, duration: 1.723s, episode steps: 104, steps per second: 60, episode reward: 104.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.404 [0.000, 4.000], mean observation: 0.486 [0.360, 0.530], loss: 8.805339, mean_absolute_error: 38.041790, mean_q: 48.673164
[F[K  69862/500000: episode: 1049, duration: 1.218s, episode steps: 65, steps per second: 53, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.369 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 8.645482, mean_absolute_error: 37.785961, mean_q: 48.363541
[F[K  69915/500000: episode: 1050, duration: 0.924s, episode steps: 53, steps per second: 57, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.472 [0.000, 4.000], mean observation: 0.506 [0.470, 0.530], loss: 6.631842, mean_absolute_error: 38.189114, mean_q: 48.914497
[F[K  70018/500000: episode: 1051, duration: 1.813s, episode steps: 103, steps per second: 57, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.680 [0.000, 4.000], mean observation: 0.502 [0.420, 0.570], loss: 7.721673, mean_absolute_error: 37.306931, mean_q: 47.672466
[F[K  70087/500000: episode: 1052, duration: 1.202s, episode steps: 69, steps per second: 57, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.087 [0.000, 4.000], mean observation: 0.516 [0.490, 0.570], loss: 8.183246, mean_absolute_error: 38.087097, mean_q: 48.679634
[F[K  70132/500000: episode: 1053, duration: 0.893s, episode steps: 45, steps per second: 50, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.490 [0.350, 0.550], loss: 6.712640, mean_absolute_error: 37.400078, mean_q: 47.932632
[F[K  70192/500000: episode: 1054, duration: 1.011s, episode steps: 60, steps per second: 59, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.583 [0.000, 4.000], mean observation: 0.510 [0.440, 0.620], loss: 6.782004, mean_absolute_error: 38.577393, mean_q: 49.514706
[F[K  70254/500000: episode: 1055, duration: 1.040s, episode steps: 62, steps per second: 60, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.984 [0.000, 4.000], mean observation: 0.517 [0.470, 0.600], loss: 7.339128, mean_absolute_error: 37.469578, mean_q: 48.039009
[F[K  70334/500000: episode: 1056, duration: 1.355s, episode steps: 80, steps per second: 59, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.875 [0.000, 4.000], mean observation: 0.497 [0.360, 0.590], loss: 7.836249, mean_absolute_error: 38.103951, mean_q: 48.723186
[F[K  70393/500000: episode: 1057, duration: 1.110s, episode steps: 59, steps per second: 53, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.458 [0.000, 4.000], mean observation: 0.481 [0.390, 0.530], loss: 8.903154, mean_absolute_error: 38.169025, mean_q: 48.881344
[F[K  70455/500000: episode: 1058, duration: 1.157s, episode steps: 62, steps per second: 54, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.129 [0.000, 4.000], mean observation: 0.492 [0.420, 0.520], loss: 9.589348, mean_absolute_error: 37.806797, mean_q: 48.331337
[F[K  70581/500000: episode: 1059, duration: 2.201s, episode steps: 126, steps per second: 57, episode reward: 126.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.373 [0.000, 4.000], mean observation: 0.500 [0.410, 0.570], loss: 7.550334, mean_absolute_error: 38.325344, mean_q: 49.148575
[F[K  70671/500000: episode: 1060, duration: 1.495s, episode steps: 90, steps per second: 60, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.144 [0.000, 4.000], mean observation: 0.509 [0.470, 0.580], loss: 8.261645, mean_absolute_error: 37.899635, mean_q: 48.551781
[F[K  70742/500000: episode: 1061, duration: 1.249s, episode steps: 71, steps per second: 57, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.732 [0.000, 4.000], mean observation: 0.511 [0.470, 0.610], loss: 10.040406, mean_absolute_error: 38.374641, mean_q: 49.076397
[F[K  70810/500000: episode: 1062, duration: 1.161s, episode steps: 68, steps per second: 59, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.956 [0.000, 4.000], mean observation: 0.509 [0.490, 0.540], loss: 8.622457, mean_absolute_error: 37.929295, mean_q: 48.637909
[F[K  70885/500000: episode: 1063, duration: 1.227s, episode steps: 75, steps per second: 61, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.933 [0.000, 4.000], mean observation: 0.498 [0.390, 0.580], loss: 8.054165, mean_absolute_error: 38.799572, mean_q: 49.776707
[F[K  71019/500000: episode: 1064, duration: 2.101s, episode steps: 134, steps per second: 64, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.104 [0.000, 4.000], mean observation: 0.502 [0.420, 0.590], loss: 8.514862, mean_absolute_error: 37.968437, mean_q: 48.590492
[F[K  71081/500000: episode: 1065, duration: 1.068s, episode steps: 62, steps per second: 58, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.032 [0.000, 4.000], mean observation: 0.506 [0.430, 0.590], loss: 8.455853, mean_absolute_error: 37.340237, mean_q: 47.881329
[F[K  71145/500000: episode: 1066, duration: 1.134s, episode steps: 64, steps per second: 56, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.969 [0.000, 4.000], mean observation: 0.496 [0.380, 0.580], loss: 8.065401, mean_absolute_error: 37.784248, mean_q: 48.422924
[F[K  71219/500000: episode: 1067, duration: 1.414s, episode steps: 74, steps per second: 52, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.243 [0.000, 4.000], mean observation: 0.503 [0.430, 0.580], loss: 8.026587, mean_absolute_error: 38.326351, mean_q: 49.116596
[F[K  71281/500000: episode: 1068, duration: 1.140s, episode steps: 62, steps per second: 54, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.839 [0.000, 4.000], mean observation: 0.498 [0.440, 0.540], loss: 7.425272, mean_absolute_error: 37.884617, mean_q: 48.607910
[F[K  71366/500000: episode: 1069, duration: 1.477s, episode steps: 85, steps per second: 58, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.176 [0.000, 4.000], mean observation: 0.494 [0.410, 0.560], loss: 8.267940, mean_absolute_error: 38.204491, mean_q: 48.884129
[F[K  71442/500000: episode: 1070, duration: 1.276s, episode steps: 76, steps per second: 60, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.671 [0.000, 4.000], mean observation: 0.520 [0.490, 0.600], loss: 7.719083, mean_absolute_error: 38.049614, mean_q: 48.748081
[F[K  71501/500000: episode: 1071, duration: 1.065s, episode steps: 59, steps per second: 55, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.102 [0.000, 4.000], mean observation: 0.485 [0.410, 0.520], loss: 7.946907, mean_absolute_error: 38.798077, mean_q: 49.679058
[F[K  71541/500000: episode: 1072, duration: 0.554s, episode steps: 40, steps per second: 72, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.472 [0.350, 0.530], loss: 7.964816, mean_absolute_error: 38.428837, mean_q: 49.317741
[F[K  71610/500000: episode: 1073, duration: 1.456s, episode steps: 69, steps per second: 47, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.725 [0.000, 4.000], mean observation: 0.487 [0.360, 0.550], loss: 7.082137, mean_absolute_error: 38.743900, mean_q: 49.628231
[F[K  71689/500000: episode: 1074, duration: 1.360s, episode steps: 79, steps per second: 58, episode reward: 79.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.747 [0.000, 4.000], mean observation: 0.503 [0.450, 0.590], loss: 7.017073, mean_absolute_error: 38.587238, mean_q: 49.547127
[F[K  71757/500000: episode: 1075, duration: 1.167s, episode steps: 68, steps per second: 58, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.853 [0.000, 4.000], mean observation: 0.504 [0.430, 0.640], loss: 8.656878, mean_absolute_error: 39.027374, mean_q: 49.928108
[F[K  71939/500000: episode: 1076, duration: 2.976s, episode steps: 182, steps per second: 61, episode reward: 182.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.912 [0.000, 4.000], mean observation: 0.491 [0.370, 0.540], loss: 8.915586, mean_absolute_error: 38.395508, mean_q: 49.118057
[F[K  72029/500000: episode: 1077, duration: 1.622s, episode steps: 90, steps per second: 55, episode reward: 90.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.922 [0.000, 4.000], mean observation: 0.485 [0.390, 0.530], loss: 8.232569, mean_absolute_error: 38.773659, mean_q: 49.654980
[F[K  72098/500000: episode: 1078, duration: 1.209s, episode steps: 69, steps per second: 57, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.174 [0.000, 4.000], mean observation: 0.479 [0.360, 0.520], loss: 7.467395, mean_absolute_error: 38.486061, mean_q: 49.299168
[F[K  72161/500000: episode: 1079, duration: 1.140s, episode steps: 63, steps per second: 55, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.429 [0.000, 4.000], mean observation: 0.492 [0.420, 0.530], loss: 8.774341, mean_absolute_error: 38.990963, mean_q: 49.695698
[F[K  72235/500000: episode: 1080, duration: 1.269s, episode steps: 74, steps per second: 58, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.297 [0.000, 4.000], mean observation: 0.486 [0.400, 0.520], loss: 6.849914, mean_absolute_error: 38.586956, mean_q: 49.401558
[F[K  72302/500000: episode: 1081, duration: 1.224s, episode steps: 67, steps per second: 55, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.896 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 8.236984, mean_absolute_error: 38.645954, mean_q: 49.458126
[F[K  72379/500000: episode: 1082, duration: 1.328s, episode steps: 77, steps per second: 58, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.727 [0.000, 4.000], mean observation: 0.502 [0.440, 0.610], loss: 8.905463, mean_absolute_error: 39.086979, mean_q: 49.890701
[F[K  72476/500000: episode: 1083, duration: 1.664s, episode steps: 97, steps per second: 58, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.433 [0.000, 4.000], mean observation: 0.506 [0.470, 0.590], loss: 8.790019, mean_absolute_error: 38.744877, mean_q: 49.524960
[F[K  72612/500000: episode: 1084, duration: 2.231s, episode steps: 136, steps per second: 61, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.125 [0.000, 4.000], mean observation: 0.492 [0.350, 0.560], loss: 8.324336, mean_absolute_error: 39.144192, mean_q: 49.998772
[F[K  72712/500000: episode: 1085, duration: 1.705s, episode steps: 100, steps per second: 59, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.710 [0.000, 4.000], mean observation: 0.513 [0.490, 0.580], loss: 8.097229, mean_absolute_error: 38.570282, mean_q: 49.302120
[F[K  72768/500000: episode: 1086, duration: 0.938s, episode steps: 56, steps per second: 60, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.036 [0.000, 4.000], mean observation: 0.520 [0.470, 0.600], loss: 8.331294, mean_absolute_error: 38.683941, mean_q: 49.408100
[F[K  72809/500000: episode: 1087, duration: 0.713s, episode steps: 41, steps per second: 57, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.561 [0.000, 4.000], mean observation: 0.512 [0.470, 0.650], loss: 8.013219, mean_absolute_error: 39.219681, mean_q: 50.257454
[F[K  72916/500000: episode: 1088, duration: 1.817s, episode steps: 107, steps per second: 59, episode reward: 107.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.150 [0.000, 4.000], mean observation: 0.497 [0.360, 0.600], loss: 9.349026, mean_absolute_error: 38.122421, mean_q: 48.765781
[F[K  72953/500000: episode: 1089, duration: 0.696s, episode steps: 37, steps per second: 53, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.432 [0.000, 3.000], mean observation: 0.499 [0.360, 0.640], loss: 8.507438, mean_absolute_error: 38.045025, mean_q: 48.640644
[F[K  73009/500000: episode: 1090, duration: 1.018s, episode steps: 56, steps per second: 55, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.488 [0.360, 0.530], loss: 8.871112, mean_absolute_error: 38.342541, mean_q: 49.031597
[F[K  73082/500000: episode: 1091, duration: 1.414s, episode steps: 73, steps per second: 52, episode reward: 73.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.521 [0.000, 4.000], mean observation: 0.511 [0.490, 0.570], loss: 8.273741, mean_absolute_error: 39.159824, mean_q: 50.069424
[F[K  73126/500000: episode: 1092, duration: 0.886s, episode steps: 44, steps per second: 50, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.864 [0.000, 4.000], mean observation: 0.502 [0.360, 0.650], loss: 8.220406, mean_absolute_error: 39.573769, mean_q: 50.745762
[F[K  73209/500000: episode: 1093, duration: 1.366s, episode steps: 83, steps per second: 61, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.157 [0.000, 4.000], mean observation: 0.489 [0.420, 0.520], loss: 8.449357, mean_absolute_error: 38.723816, mean_q: 49.470787
[F[K  73315/500000: episode: 1094, duration: 1.919s, episode steps: 106, steps per second: 55, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.877 [0.000, 4.000], mean observation: 0.494 [0.400, 0.530], loss: 8.934677, mean_absolute_error: 38.601868, mean_q: 49.323795
[F[K  73391/500000: episode: 1095, duration: 1.419s, episode steps: 76, steps per second: 54, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.882 [0.000, 4.000], mean observation: 0.518 [0.480, 0.650], loss: 8.406828, mean_absolute_error: 39.010727, mean_q: 49.936844
[F[K  73444/500000: episode: 1096, duration: 0.803s, episode steps: 53, steps per second: 66, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.189 [0.000, 4.000], mean observation: 0.523 [0.470, 0.650], loss: 8.095146, mean_absolute_error: 38.850971, mean_q: 49.658833
[F[K  73624/500000: episode: 1097, duration: 3.016s, episode steps: 180, steps per second: 60, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.956 [0.000, 4.000], mean observation: 0.513 [0.490, 0.620], loss: 8.962168, mean_absolute_error: 38.584919, mean_q: 49.380024
[F[K  73686/500000: episode: 1098, duration: 1.232s, episode steps: 62, steps per second: 50, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.968 [0.000, 4.000], mean observation: 0.518 [0.470, 0.640], loss: 9.400935, mean_absolute_error: 38.615356, mean_q: 49.436764
[F[K  73855/500000: episode: 1099, duration: 2.811s, episode steps: 169, steps per second: 60, episode reward: 169.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.154 [0.000, 4.000], mean observation: 0.504 [0.470, 0.560], loss: 8.243433, mean_absolute_error: 39.047909, mean_q: 49.937187
[F[K  73931/500000: episode: 1100, duration: 1.282s, episode steps: 76, steps per second: 59, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.487 [0.000, 4.000], mean observation: 0.478 [0.350, 0.540], loss: 7.633979, mean_absolute_error: 39.161205, mean_q: 50.074738
[F[K  73980/500000: episode: 1101, duration: 0.887s, episode steps: 49, steps per second: 55, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.518 [0.470, 0.610], loss: 7.102519, mean_absolute_error: 39.318939, mean_q: 50.329155
[F[K  74078/500000: episode: 1102, duration: 1.778s, episode steps: 98, steps per second: 55, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.112 [0.000, 4.000], mean observation: 0.489 [0.370, 0.550], loss: 8.415304, mean_absolute_error: 38.663658, mean_q: 49.405106
[F[K  74172/500000: episode: 1103, duration: 1.544s, episode steps: 94, steps per second: 61, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.830 [0.000, 4.000], mean observation: 0.484 [0.350, 0.530], loss: 7.406907, mean_absolute_error: 39.631508, mean_q: 50.744495
[F[K  74211/500000: episode: 1104, duration: 0.715s, episode steps: 39, steps per second: 55, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.436 [0.000, 4.000], mean observation: 0.518 [0.470, 0.630], loss: 8.713573, mean_absolute_error: 38.379124, mean_q: 49.199821
[F[K  74275/500000: episode: 1105, duration: 1.159s, episode steps: 64, steps per second: 55, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.844 [0.000, 4.000], mean observation: 0.497 [0.460, 0.530], loss: 7.593164, mean_absolute_error: 38.907478, mean_q: 49.836899
[F[K  74342/500000: episode: 1106, duration: 1.093s, episode steps: 67, steps per second: 61, episode reward: 67.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.496 [0.430, 0.530], loss: 8.282420, mean_absolute_error: 39.507294, mean_q: 50.481987
[F[K  74417/500000: episode: 1107, duration: 1.196s, episode steps: 75, steps per second: 63, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.067 [0.000, 4.000], mean observation: 0.497 [0.440, 0.520], loss: 8.811440, mean_absolute_error: 38.423496, mean_q: 49.188644
[F[K  74479/500000: episode: 1108, duration: 1.130s, episode steps: 62, steps per second: 55, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.887 [0.000, 4.000], mean observation: 0.500 [0.410, 0.570], loss: 8.541931, mean_absolute_error: 39.303185, mean_q: 50.312191
[F[K  74556/500000: episode: 1109, duration: 1.430s, episode steps: 77, steps per second: 54, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.478 [0.340, 0.520], loss: 7.985294, mean_absolute_error: 39.222633, mean_q: 50.164261
[F[K  74689/500000: episode: 1110, duration: 2.357s, episode steps: 133, steps per second: 56, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.053 [0.000, 4.000], mean observation: 0.495 [0.420, 0.540], loss: 8.451158, mean_absolute_error: 38.650490, mean_q: 49.448730
[F[K  74761/500000: episode: 1111, duration: 1.451s, episode steps: 72, steps per second: 50, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.847 [0.000, 4.000], mean observation: 0.489 [0.370, 0.530], loss: 9.255208, mean_absolute_error: 39.453213, mean_q: 50.568035
[F[K  74818/500000: episode: 1112, duration: 1.227s, episode steps: 57, steps per second: 46, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.825 [0.000, 4.000], mean observation: 0.502 [0.410, 0.620], loss: 9.108809, mean_absolute_error: 38.915813, mean_q: 49.782444
[F[K  74882/500000: episode: 1113, duration: 1.117s, episode steps: 64, steps per second: 57, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.094 [0.000, 4.000], mean observation: 0.491 [0.420, 0.520], loss: 7.442029, mean_absolute_error: 39.164291, mean_q: 50.160816
[F[K  74951/500000: episode: 1114, duration: 1.418s, episode steps: 69, steps per second: 49, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.101 [0.000, 4.000], mean observation: 0.492 [0.380, 0.580], loss: 9.158974, mean_absolute_error: 38.814285, mean_q: 49.698242
[F[K  75039/500000: episode: 1115, duration: 1.400s, episode steps: 88, steps per second: 63, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.443 [0.000, 4.000], mean observation: 0.507 [0.470, 0.570], loss: 8.858876, mean_absolute_error: 38.480667, mean_q: 49.232742
[F[K  75117/500000: episode: 1116, duration: 1.501s, episode steps: 78, steps per second: 52, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 2.090 [0.000, 4.000], mean observation: 0.506 [0.430, 0.620], loss: 9.735548, mean_absolute_error: 39.242268, mean_q: 50.180931